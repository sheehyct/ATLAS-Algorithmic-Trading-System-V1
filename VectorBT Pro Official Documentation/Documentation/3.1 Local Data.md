---
title: Local
description: Documentation on handling local data in VectorBT PRO
icon: material/folder-open
---

# :material-folder-open: Local

Making repeated requests to remote API endpoints can be costly, so caching data locally is important.
Fortunately, VBT provides several methods for managing local data.

## Pickling

Like any class that subclasses [Pickleable](https://vectorbt.pro/pvt_6d1b3986/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable),
you can save any [Data](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data) instance to disk using
[Pickleable.save](https://vectorbt.pro/pvt_6d1b3986/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.save), and load it again with
[Pickleable.load](https://vectorbt.pro/pvt_6d1b3986/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.load). This process pickles the
entire Python object, including stored Pandas objects, symbol dictionaries, and settings:

```pycon
>>> from vectorbtpro import *

>>> yf_data = vbt.YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> yf_data.save("yf_data")  # (1)!

>>> yf_data = vbt.YFData.load("yf_data")  # (2)!
>>> yf_data = yf_data.update(end="2020-01-06")
>>> yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
2020-01-05 00:00:00+00:00  7411.317383  136.276779
```

1. The `.pickle` extension is added to the file name automatically.
2. You can load the object back in a new runtime or even on another machine.
Just make sure you are using a compatible VBT version.

!!! important
    The class definition is not saved. If a new version of VBT introduces a breaking change
    to the [Data](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data) constructor, the object might not load.
    In this case, you can manually create a new instance:

    ```pycon
    >>> yf_data = vbt.YFData(**vbt.Configured.load("yf_data").config)
    ```

## Saving

While pickling is a fast and convenient way to store Python objects of any size, the pickled file is
essentially a black box that requires a Python interpreter to access its contents. This means it cannot
be imported by most other data-driven tools, which makes it unusable for many tasks. To overcome this
limitation, the [Data](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data) class allows you to save only the
stored Pandas objects into one or more tabular format files.

### CSV

The first supported file format is [CSV](https://en.wikipedia.org/wiki/Comma-separated_values),
which is implemented by the instance method [Data.to_csv](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_csv).
This method takes the path to the directory where the data should be stored (`path_or_buf`) and saves each
symbol in a separate file using [DataFrame.to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html).

By default, it appends the `.csv` extension to each symbol and saves the files in the current directory:

```pycon
>>> yf_data.to_csv()
```

!!! info
    Multiple symbols cannot be stored in a single CSV file.

You can list all CSV files in the current directory using [list_files](https://vectorbt.pro/pvt_6d1b3986/api/utils/path_/#vectorbtpro.utils.path_.list_files):

```pycon
>>> vbt.list_files("*.csv")
[PosixPath('ETH-USD.csv'), PosixPath('BTC-USD.csv')]
```

A cleaner approach is to save all the data in a separate directory:

```pycon
>>> vbt.remove_file("BTC-USD.csv")  # (1)!
>>> vbt.remove_file("ETH-USD.csv")

>>> yf_data.to_csv("data", mkdir_kwargs=dict(mkdir=True))  # (2)!
```

1. Delete the previously created CSV files from the current directory.
2. Save the files to the directory named `data`. If the directory does not exist, create a new one by
passing the keyword argument `mkdir_kwargs` to [check_mkdir](https://vectorbt.pro/pvt_6d1b3986/api/utils/path_/#vectorbtpro.utils.path_.check_mkdir).

To save the data as tab-separated values (TSV):

```pycon
>>> yf_data.to_csv("data", ext="tsv")

>>> vbt.list_files("data/*.tsv")
[PosixPath('data/BTC-USD.tsv'), PosixPath('data/ETH-USD.tsv')]
```

!!! hint
    You do not need to pass `sep`: VBT will recognize the extension and use the correct delimiter.
    You can still override this argument if you want to split the data by a custom character.

Similar to [Data.pull](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.pull), you can provide
any argument as a feature/symbol dictionary of type [key_dict](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.key_dict)
to define different rules for different symbols. For example, you can store the symbols from our example
in separate directories:

```pycon
>>> yf_data.to_csv(
...     vbt.key_dict({
...         "BTC-USD": "btc_data",
...         "ETH-USD": "eth_data"
...     }),
...     mkdir_kwargs=dict(mkdir=True)
... )
```

You can also specify the path to each file by using `path_or_buf` (the first argument):

```pycon
>>> yf_data.to_csv(
...     vbt.key_dict({
...         "BTC-USD": "data/btc_usd.csv",
...         "ETH-USD": "data/eth_usd.csv"
...     }),
...     mkdir_kwargs=dict(mkdir=True)
... )
```

To delete an entire directory, for example as part of a cleanup process:

```pycon
>>> vbt.remove_dir("btc_data", with_contents=True)
>>> vbt.remove_dir("eth_data", with_contents=True)
>>> vbt.remove_dir("data", with_contents=True)
```

### HDF

The second supported file format is [HDF](https://en.wikipedia.org/wiki/Hierarchical_Data_Format),
which is implemented by the instance method [Data.to_hdf](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_hdf).
Unlike [Data.to_csv](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_csv), this method can store
multiple symbols in a single file, where symbols are distributed as HDF keys.

By default, it creates a new file with the same name as the data class and the `.h5` extension,
saving each symbol under a separate key using [DataFrame.to_hdf](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_hdf.html):

```pycon
>>> yf_data.to_hdf()

>>> vbt.list_files("*.h5")
[PosixPath('YFData.h5')]
```

To see the list of all groups and keys in an HDF file:

```pycon
>>> with pd.HDFStore("YFData.h5") as store:
...     print(store.keys())
['/BTC-USD', '/ETH-USD']
```

Use the `key` argument to manually specify the key for a particular symbol:

```pycon
>>> yf_data.to_hdf(
...     key=vbt.key_dict({
...         "BTC-USD": "btc_usd",
...         "ETH-USD": "eth_usd"
...     })
... )
```

!!! hint
    If there is only one symbol, you do not need to use [key_dict](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.key_dict).
    You can simply pass `key="btc_usd"`.

You can also specify the path to each file by using `path_or_buf` (the first argument):

```pycon
>>> yf_data.to_hdf(
...     vbt.key_dict({
...         "BTC-USD": "btc_usd.h5",
...         "ETH-USD": "eth_usd.h5"
...     })
... )
```

You can combine the arguments `path_or_buf` and `key`.

All other arguments behave the same as for [Data.to_csv](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_csv).

### Feather & Parquet

The third supported option is saving to a Feather or Parquet file, which are handled by
the instance methods [Data.to_feather](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_feather) and
[Data.to_parquet](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_parquet). Feather is
an unmodified raw columnar Arrow memory format designed for short-term storage. Parquet
is often more expensive to write but offers more layers of encoding and compression, making Parquet
files usually much smaller than Feather files. Another key difference is that you cannot
partition data with Feather, nor can you natively store the index. The index must be stored as a separate
column, which is handled automatically by VBT. Here we will show how to save to Parquet.

By default, the `.parquet` extension is appended to each symbol, and files are saved in the current directory:

```pycon
>>> yf_data.to_parquet()
```

!!! info
    Multiple symbols cannot be stored in a single Parquet file.

Other saving options are very similar to the CSV saving options.

In addition to saving each DataFrame to a separate Parquet file, you can partition the DataFrame
either by columns or by rows. Partitioning by columns is controlled by the `partition_cols` argument,
which takes a list of column names. The columns must be present in your data and are partitioned in the
order you provide. Partitioning by rows is controlled by the `partition_by` argument, which
accepts a grouping instruction such as a frequency, an index (including multi-index), a Pandas grouper or resampler,
or a VBT [Grouper](https://vectorbt.pro/pvt_6d1b3986/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper) instance.
Together with `groupby_kwargs`, this creates a new `Grouper` instance for partitioning.
The groups are attached as columns to each DataFrame, and the names of these columns are provided as
`partition_cols`. By default, if there is only one column, it will be named "group";
if there are multiple columns, they will be named "group_{index}". To use your own column names,
enable `keep_groupby_names`.

!!! info
    When `partition_cols` or `partition_by` is provided, each symbol will be stored in a separate directory.

Let's partition our data into two-day groups and save each DataFrame to its own Parquet directory:

```pycon
>>> yf_data.to_parquet(partition_by="2 days")
```

To visualize the directory tree for the `BTC-USD` symbol:

```pycon
>>> vbt.print_dir_tree("BTC-USD")
BTC-USD
├── group=2020-01-01%2000%3A00%3A00.000000000
│   └── 190335d948d04504b42a8d35ffb08f47-0.parquet
└── group=2020-01-03%2000%3A00%3A00.000000000
    └── 190335d948d04504b42a8d35ffb08f47-0.parquet

2 directories, 2 files
```

### SQL

The fourth supported option is saving to a SQL database, which is implemented by the instance method
[Data.to_sql](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_sql). This requires
[SQLAlchemy](https://www.sqlalchemy.org/) to be installed. The method takes the engine (object, name,
or URL) and saves each symbol as a separate table in the database managed by the engine using
[`pd.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).
You can create the engine manually using SQLAlchemy's `create_engine` function, or pass a URL and
it will be created for you; in this case, the engine will be disposed at the end of the call.

Let's create a SQL database file in the working directory and store the data there:

```pycon
>>> SQLITE_URL = "sqlite:///yf_data.db"

>>> yf_data.to_sql(SQLITE_URL)
```

If you want to continue working with the same engine and avoid creating another engine object,
pass `return_engine=True` to return the engine object:

```pycon
>>> engine = yf_data.to_sql(SQLITE_URL, if_exists="replace", return_engine=True)
```

You can also specify the schema using the `schema` argument. Note that some databases,
such as SQLite, do not support the concept of a schema. If the schema does not exist, it will be
created automatically.

```pycon
>>> POSTGRESQL_URL = "postgresql://postgres:postgres@localhost:5432"

>>> yf_data.to_sql(POSTGRESQL_URL, schema="yf_data")
```

Use the `table` argument to manually specify the table name for a particular symbol:

```pycon
>>> yf_data.to_sql(
...     POSTGRESQL_URL,
...     table=vbt.key_dict({
...         "BTC-USD": "BTC_USD",
...         "ETH-USD": "ETH_USD"
...     })
... )
```

!!! info
    If the index is datetime-like and/or there are datetime-like columns, the method will localize or convert
    them to UTC first. To adjust this behavior, set the `to_utc` argument to False to deactivate,
    to "index" to apply it to the index only, or to "columns" to apply it to columns only.
    Additionally, the UTC timezone will be removed if `remove_utc_tz` is True (default) since some
    databases do not support timezone-aware timestamps; other timezones are not affected.

### DuckDB

!!! hint
    The previous method (using SQLAlchemy) can also be used to write to a DuckDB database.
    For this, you need to install the [duckdb-engine](https://pypi.org/project/duckdb-engine/) extension.

The fifth supported option is very similar to the previous one and allows you to save data to a DuckDB
database or to CSV, Parquet, or JSON files. It is implemented by the instance method
[Data.to_duckdb](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_duckdb). This requires
[DuckDB](https://duckdb.org/) to be installed. The method takes the connection (object or URL)
and saves each symbol as a separate table in the database managed by the connection, or to file(s)
using a SQL query. The connection can be created manually using DuckDB's `connect` function,
or you can pass a URL and it will be created for you. If neither a connection nor connection-related
keyword arguments are provided, the default in-memory connection is used.

Let's create a DuckDB database file in the working directory and store the data there:

```pycon
>>> DUCKDB_URL = "database.duckdb"

>>> yf_data.to_duckdb(DUCKDB_URL)
```

You can also specify the catalog and schema using the `catalog` and `schema` arguments, respectively.
If the schema does not exist, it will be created automatically.

```pycon
>>> yf_data.to_duckdb(DUCKDB_URL, schema="yf_data")
```

Use the `table` argument to manually specify the table name for a particular symbol:

```pycon
>>> yf_data.to_duckdb(
...     DUCKDB_URL,
...     table=vbt.key_dict({
...         "BTC-USD": "BTC_USD",
...         "ETH-USD": "ETH_USD"
...     })
... )
```

There is also an option to save each DataFrame to a CSV, Parquet, or JSON file rather than to
the database itself. To do this, use `write_format`, `write_path`, and `write_options`.
This operation is performed using [`COPY TO`](https://duckdb.org/docs/sql/statements/copy.html).
If `write_path` is a directory (which is the working directory by default), each DataFrame
will be saved to a file based on the specified format. The format is not needed if `write_path` points
to a file with a recognizable extension. The options argument can be used to specify
writing options; it can be either a string as in DuckDB's documentation, such as
`HEADER 1, DELIMITER ','`, or a dictionary that will be translated into such a string by
VBT, such as `dict(header=1, delimiter=',')`.

Let's save all data to Parquet files:

```pycon
>>> yf_data.to_duckdb(
...     DUCKDB_URL,
...     write_path="data",
...     write_format="parquet",
...     mkdir_kwargs=dict(mkdir=True)
... )
```

!!! info
    See the notes in the SQL section.

## Loading

To import any previously stored data in a tabular format, you can use Pandas or VBT's preset
data classes, which are specifically designed for this purpose.

### CSV

You can import each CSV dataset manually using [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):

```pycon
>>> yf_data.to_csv()

>>> pd.read_csv("BTC-USD.csv", index_col=0, parse_dates=True)
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0 
```

To combine the imported datasets and wrap them with [Data](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data),
you can use [Data.from_data](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.from_data):

```pycon
>>> btc_usd = pd.read_csv("BTC-USD.csv", index_col=0, parse_dates=True)
>>> eth_usd = pd.read_csv("ETH-USD.csv", index_col=0, parse_dates=True)

>>> data = vbt.Data.from_data({"BTC-USD": btc_usd, "ETH-USD": eth_usd})
>>> data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

To make it easier for users and eliminate the need to manually search for, fetch, and merge CSV data,
VBT provides the [CSVData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) class,
which can recursively search directories for CSV files, resolve path expressions using
[glob](https://docs.python.org/3/library/glob.html), translate matched paths into symbols,
and import and join tabular data—all automatically with a single command.
It is a subclass of [FileData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData),
which enables all of these features.

At the core of the path matching functionality is the class method
[FileData.pull](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull), which iterates
over the specified paths. For each one, it finds the matching absolute paths using the class method
[FileData.match_path](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.match_path), then calls
the abstract class method [FileData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.fetch_key)
to pull the data from the file at that path.

To see how [FileData.match_path](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.match_path)
works, create a directory named `data` and store several empty files in it:

```pycon
>>> vbt.make_dir("data", exist_ok=True)
>>> vbt.make_file("data/file1.csv")
>>> vbt.make_file("data/file2.tsv")
>>> vbt.make_file("data/file3")

>>> vbt.make_dir("data/sub-data", exist_ok=True)
>>> vbt.make_file("data/sub-data/file1.csv")
>>> vbt.make_file("data/sub-data/file2.tsv")
>>> vbt.make_file("data/sub-data/file3")
```

To view the directory structure you just created:

```pycon
>>> vbt.print_dir_tree("data")
data
├── file1.csv
├── file2.tsv
├── file3
└── sub-data
    ├── file1.csv
    ├── file2.tsv
    └── file3

1 directories, 6 files
```

Match all files in a directory:

```pycon
>>> vbt.FileData.match_path("data")
[PosixPath("data/file1.csv"),
 PosixPath("data/file2.tsv"),
 PosixPath("data/file3")]
```

Match all CSV files in a directory:

```pycon
>>> vbt.FileData.match_path("data/*.csv")
[PosixPath("data/file1.csv")]
```

Match all CSV files in a directory recursively:

```pycon
>>> vbt.FileData.match_path("data/**/*.csv")
[PosixPath("data/file1.csv"), PosixPath("data/sub-data/file1.csv")]
```

For more details, see the [glob](https://docs.python.org/3/library/glob.html) documentation.

Returning to [FileData.pull](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull): this method
can match one or multiple path expressions as shown above, provided either as `symbols` (if `paths` is None)
or as `paths`. When you pass paths as symbols, the method calls
[FileData.path_to_symbol](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.path_to_symbol)
on each matched path to extract the symbol name (by default, this is the stem of the path):

```pycon
>>> vbt.CSVData.pull("BTC-USD.csv").symbols
["BTC-USD"]

>>> vbt.CSVData.pull(["BTC-USD.csv", "ETH-USD.csv"]).symbols
["BTC-USD", "ETH-USD"]

>>> vbt.CSVData.pull("*.csv").symbols
["BTC-USD", "ETH-USD"]

>>> vbt.CSVData.pull(["BTC/USD", "ETH/USD"], paths="*.csv").symbols  # (1)!
["BTC/USD", "ETH/USD"]

>>> vbt.CSVData.pull(  # (2)!
...     ["BTC/USD", "ETH/USD"], 
...     paths=["BTC-USD.csv", "ETH-USD.csv"]
... ).symbols
["BTC/USD", "ETH/USD"]
```

1. Explicitly specify the symbols.
2. Explicitly specify both the symbols and the paths.

!!! note
    Remember to filter by the `.csv`, `.tsv`, or any other extension in your path expression.

When you use a wildcard like `*.csv`, VBT will sort the matched paths (for each path expression).
To disable sorting, set `sort_paths` to False. If you want to turn off the path matching mechanism entirely,
you can set `match_paths` to False, which will send all arguments directly to
[CSVData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.fetch_key):

```pycon
>>> vbt.CSVData.pull(
...     ["BTC/USD", "ETH/USD"], 
...     paths=vbt.key_dict({
...         "BTC/USD": "BTC-USD.csv",
...         "ETH/USD": "ETH-USD.csv"
...     }),
...     match_paths=False
... ).symbols
["BTC/USD", "ETH/USD"]
```

!!! hint
    Instead of paths, you can use any object type supported by the `filepath_or_buffer` argument in
    [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

To summarize the techniques described above, let's create an empty directory called `data` again,
write the `BTC-USD` symbol to a CSV file and the `ETH-USD` symbol to a TSV file, then load both
datasets with a single `fetch` call:

```pycon
>>> vbt.remove_dir("data", with_contents=True)

>>> yf_data.to_csv(
...     "data",
...     ext=vbt.key_dict({
...         "BTC-USD": "csv",
...         "ETH-USD": "tsv"
...     }), 
...     mkdir_kwargs=dict(mkdir=True)
... )

>>> csv_data = vbt.CSVData.pull(["data/*.csv", "data/*.tsv"])  # (1)!
```

1. The delimiter is automatically detected based on the file's extension.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> csv_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

!!! note
    Providing two paths with wildcards (`*`) does not guarantee you will get exactly two symbols:
    each wildcard may match more than one path. Think of the two expressions above as being OR'ed
    into a single expression `data/*.{csv,tsv}` (which, unfortunately, is not supported by
    [glob](https://docs.python.org/3/library/glob.html)).

Last but not least is regex matching with `match_regex`. This tells VBT to iterate over all
matched paths and further validate them using a regular expression:

```pycon
>>> vbt.CSVData.pull(
...     "data/**/*",  # (1)!
...     match_regex=r"^.*\.(csv|tsv)$"  # (2)!
... ).close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

1. Recursively collect the paths of all files in any subdirectory of `data`.
2. Filter out any paths that do not end with `.csv` or `.tsv`.

Any other argument is passed directly to
[CSVData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.fetch_key)
and then to [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

#### Chunking

Instead of reading everything into memory at once, Pandas allows you to read data in chunks.
For CSV files, this means you can load only a subset of lines into memory at any given time.
While this is a great feature for working with large datasets, chunking does not have many benefits
when your goal is simply to load all the data into memory anyway.

However, chunking becomes very useful for data filtering! The class
[CSVData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData), as well as the function
[pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) that it uses,
do not have arguments for skipping rows based on their content—only by row index.
For example, to skip any data before `2020-01-03`, you would need to load all the data into memory first.
But if the dataset is too large, this could exhaust your system's RAM. To work around this,
you can split the data into chunks and check conditions on each chunk separately.

You have two options:

1. Use `chunksize` to divide data into chunks of fixed length.
2. Use `iterator` to return an iterator so you can read chunks of variable length.

Both options make [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
to return an iterator of type `TextFileReader`. To take advantage of this,
[CSVData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.fetch_key) accepts a
user-defined function `chunk_func` that should 1) accept the iterator, 2) select, process, and
concatenate chunks, and 3) return a Series or DataFrame.

Let's fetch only the rows whose date ends with an even day:

```pycon
>>> csv_data = vbt.CSVData.pull(
...     ["data/*.csv", "data/*.tsv"],
...     chunksize=1,  # (1)!
...     chunk_func=lambda iterator: pd.concat([
...         df 
...         for df in iterator
...         if (df.index.day % 2 == 0).all()
...     ], axis=0)
... )
```

1. Each chunk will be a DataFrame with one row.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> csv_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

!!! note
    Chunking is most useful when memory usage is a greater concern than speed.

### HDF

Each HDF dataset can be imported manually using [pandas.read_hdf](https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html):

```pycon
>>> yf_data.to_hdf()

>>> pd.read_hdf("YFData.h5", key="BTC-USD")
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

Just like [CSVData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) for CSV data,
VBT provides a preset class [HDFData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData)
specifically for reading HDF files. It shares the same parent class
[FileData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData)
and uses its fetcher [FileData.pull](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull).
Unlike CSV datasets, where each file stores one dataset, HDF datasets are stored by key within a single HDF file.
Since groups and keys in HDF files follow a [POSIX](https://en.wikipedia.org/wiki/POSIX)-style
hierarchy with `/` separators, you can query them just like you would query directories and files
in a typical file system.

Let's demonstrate this by using [HDFData.match_path](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData.match_path),
which extends [FileData.match_path](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.match_path)
with proper discovery and handling of HDF groups and keys:

```pycon
>>> vbt.HDFData.match_path("YFData.h5")
[PosixPath("YFData.h5/BTC-USD"), PosixPath("YFData.h5/ETH-USD")]
```

As you can see, the HDF file is now treated as a directory, while groups and keys are treated
as subdirectories and files, respectively. This makes importing HDF datasets just as easy as importing CSV datasets:

```pycon
>>> vbt.HDFData.pull("YFData.h5/BTC-USD").symbols  # (1)!
["BTC-USD"]

>>> vbt.HDFData.pull("YFData.h5").symbols  # (2)!
["BTC-USD", "ETH-USD"]

>>> vbt.HDFData.pull("YFData.h5/BTC*").symbols  # (3)!
["BTC-USD"]

>>> vbt.HDFData.pull("*.h5/BTC-*").symbols  # (4)!
["BTC-USD"]
```

1. Matches the key `BTC-USD` in `YFData.h5`.
2. Matches all keys in `YFData.h5`.
3. Matches all keys starting with `BTC` in `YFData.h5`.
4. Matches all keys starting with `BTC` in all HDF files.

Any other argument works the same as for [CSVData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData),
but now is passed directly to [HDFData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData.fetch_key)
and then to [pandas.read_hdf](https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html).

#### Chunking

Chunking for HDF files works just like it does for CSV files, with two exceptions: the data must be
saved as a [PyTables](https://www.pytables.org/) Table structure by using `format="table"`, and
the iterator will be of type `TableIterator` instead of `TextFileReader`.

```pycon
>>> yf_data.to_hdf(format="table")

>>> hdf_data = vbt.HDFData.pull(
...     "YFData.h5",
...     chunksize=1,
...     chunk_func=lambda iterator: pd.concat([
...         df 
...         for df in iterator
...         if (df.index.day % 2 == 0).all()
...     ], axis=0)
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> hdf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

### Feather & Parquet

Each Parquet dataset can be manually imported using
[pandas.read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html):

```pycon
>>> yf_data.to_parquet()

>>> pd.read_parquet("BTC-USD.parquet")
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

The same applies for partitioned datasets:

```pycon
>>> yf_data2 = vbt.YFData.pull(
...     ["BNB-USD", "XRP-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )
>>> yf_data2.to_parquet(partition_by="2D")

>>> pd.read_parquet("BNB-USD")  # (1)!
                                Open       High        Low      Close  \
Date                                                                    
2020-01-01 00:00:00+00:00  13.730962  13.873946  13.654942  13.689083   
2020-01-02 00:00:00+00:00  13.698126  13.715548  12.989974  13.027011   
2020-01-03 00:00:00+00:00  13.035329  13.763709  13.012638  13.660452   
2020-01-04 00:00:00+00:00  13.667442  13.921914  13.560008  13.891512   

                              Volume  Dividends  Stock Splits  \
Date                                                            
2020-01-01 00:00:00+00:00  172980718        0.0           0.0   
2020-01-02 00:00:00+00:00  156376427        0.0           0.0   
2020-01-03 00:00:00+00:00  173683857        0.0           0.0   
2020-01-04 00:00:00+00:00  182230374        0.0           0.0   

                                                   group  
Date                                                      
2020-01-01 00:00:00+00:00  2020-01-01 00:00:00.000000000  
2020-01-02 00:00:00+00:00  2020-01-01 00:00:00.000000000  
2020-01-03 00:00:00+00:00  2020-01-03 00:00:00.000000000  
2020-01-04 00:00:00+00:00  2020-01-03 00:00:00.000000000  
```

1. The path points to a directory.

Just like with other classes for loading data from local files, VBT provides preset classes
[FeatherData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/feather/#vectorbtpro.data.custom.feather.FeatherData)
and [ParquetData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/parquet/#vectorbtpro.data.custom.parquet.ParquetData) for reading
Feather and Parquet files, respectively. They share the same parent class
[FileData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData) and use its fetcher
[FileData.pull](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull).

First, let's discover any Parquet files or directories stored in the current working directory using
[ParquetData.list_paths](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/parquet/#vectorbtpro.data.custom.parquet.ParquetData.list_paths).
This function searches for files with the ".parquet" extension as well as directories containing
partitioned datasets that follow the [Hive partitioning scheme](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.HivePartitioning.html).

```pycon
>>> vbt.ParquetData.list_paths()
[PosixPath('BNB-USD'),
 PosixPath('BTC-USD.parquet'),
 PosixPath('ETH-USD.parquet'),
 PosixPath('XRP-USD')]
 
>>> vbt.ParquetData.is_parquet_file("BTC-USD.parquet")
True

>>> vbt.ParquetData.is_parquet_dir("BNB-USD")
True
```

This makes importing Parquet datasets just as easy as importing any other file-based dataset:

```pycon
>>> vbt.ParquetData.pull("BTC-USD.parquet").symbols  # (1)!
['BTC-USD']

>>> vbt.ParquetData.pull(["BTC-USD.parquet", "ETH-USD.parquet"]).symbols
['BTC-USD', 'ETH-USD']

>>> vbt.ParquetData.pull("*.parquet").symbols  # (2)!
['BTC-USD', 'ETH-USD']

>>> vbt.ParquetData.pull("BNB-USD").symbols  # (3)!
['BNB-USD']

>>> vbt.ParquetData.pull(["BNB-USD", "XRP-USD"]).symbols
['BNB-USD', 'XRP-USD']

>>> vbt.ParquetData.pull().symbols  # (4)!
['BNB-USD', 'BTC-USD', 'ETH-USD', 'XRP-USD']
```

1. Pull a single-file dataset.
2. Pull all single-file datasets.
3. Pull a partitioned dataset.
4. Pull all single-file and partitioned datasets.

You might be wondering what happens to the "group" column in partitioned datasets.
Whenever a partitioned dataset is pulled and VBT detects one or more partition groups named
"group" or "group_{index}", these columns are automatically ignored because they were
most likely generated by user-defined row partitioning with `partition_by`. Such groups are
referred to as default groups.

```pycon
>>> vbt.ParquetData.list_partition_cols("BNB-USD")
['group']

>>> vbt.ParquetData.is_default_partition_col("group")
True

>>> vbt.ParquetData.pull("BNB-USD").features
['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']
```

You can disable this behavior by setting `keep_partition_cols` to True:

```pycon
>>> vbt.ParquetData.pull("BNB-USD", keep_partition_cols=True).features
['Open',
 'High',
 'Low',
 'Close',
 'Volume',
 'Dividends',
 'Stock Splits',
 'group']
```

### SQL

Each SQL table can be manually imported using [pandas.read_sql_table](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html):

```pycon
>>> pd.read_sql_table(
...     "BTC-USD", 
...     POSTGRESQL_URL, 
...     schema="yf_data",
...     index_col="Date", 
...     parse_dates={"Date": dict(utc=True)}
... )
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0 
```

You can also execute any query using [pandas.read_sql_query](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html):

```pycon
>>> pd.read_sql_query(
...     'SELECT * FROM yf_data."BTC-USD"', 
...     POSTGRESQL_URL, 
...     index_col="Date", 
...     parse_dates={"Date": dict(utc=True)}
... )
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0 
```

But you might be asking: how do we know what schemas and tables are stored in our database?
We can call [SQLData.list_schemas](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.list_schemas)
and [SQLData.list_tables](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.list_tables), respectively.
Most methods in [SQLData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData), including these two,
require that the engine be provided.

```pycon
>>> vbt.SQLData.list_schemas(engine=POSTGRESQL_URL)
['information_schema', 'public', 'yf_data']

>>> vbt.SQLData.list_tables(engine=POSTGRESQL_URL)
['information_schema:_pg_foreign_data_wrappers',
 'information_schema:_pg_foreign_servers',
 ...
 'yf_data:BTC-USD',
 'yf_data:ETH-USD']

>>> vbt.SQLData.list_tables(engine=POSTGRESQL_URL, schema="yf_data")
['BTC-USD', 'ETH-USD']
```

To fetch the actual data, you can use [SQLData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.fetch_key),
which calls `pd.read_sql_query` and performs many pre-processing and post-processing steps
under the hood. For example, it can inspect the database and map column indices to names,
which is not natively supported by the Pandas method. This is useful when specifying information
per column based on position, such as when providing `index_col` (which, by the way,
defaults to 0 - the first column). It also properly handles datetime indexes and columns,
and can automatically retrieve all tables under a schema or even from the entire database.

Let's pull all the tables we stored previously, both implicitly and explicitly:

```pycon
>>> vbt.SQLData.pull(engine=SQLITE_URL).symbols   # (1)!
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(["BTC-USD", "ETH-USD"], engine=SQLITE_URL).symbols  # (2)!
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(engine=POSTGRESQL_URL, schema="yf_data").symbols  # (3)!
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(["yf_data:BTC-USD", "yf_data:ETH-USD"], engine=POSTGRESQL_URL).symbols  # (4)!
['yf_data:BTC-USD', 'yf_data:ETH-USD']
```

1. Pull all tables from the database.
2. Pull specific tables from the database.
3. Pull all tables under a schema from the database.
4. Pull tables from the database, specifying the schema for each.

You can also specify information per feature or symbol by providing it as an instance of
[key_dict](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.key_dict):

```pycon
>>> vbt.SQLData.pull(
...     ["BTCUSD", "ETHUSD"], 
...     schema="yf_data",
...     table=vbt.key_dict({
...         "BTCUSD": "BTC-USD",
...         "ETHUSD": "ETH-USD",
...     }),
...     engine=POSTGRESQL_URL
... ).symbols
['BTCUSD', 'ETHUSD']
```

This is especially useful for executing custom queries:

```pycon
>>> vbt.SQLData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": 'SELECT * FROM yf_data."BTC-USD"',
...         "ETH-USD": 'SELECT * FROM yf_data."ETH-USD"',
...     }), 
...     index_col="Date",
...     engine=POSTGRESQL_URL
... ).symbols
['BTC-USD', 'ETH-USD']
```

!!! note
    When executing a custom query, most preprocessings are not available because the 
    query cannot be easily introspected. For example, you must provide a column name under `index_col`
    (or False to avoid using any column as an index).

Because different engines have different configurations, and you may not want to repeatedly
pass them when pulling, you can save the respective configuration to the global settings.
First, create an engine name, and then save all the keyword arguments you would normally
pass to [SQLData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.fetch_key)
under this engine name in `vbt.settings.data.engines`. This is easily accomplished using
[SQLData.set_engine_settings](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.set_engine_settings),
which takes an engine name and the keyword arguments to save. If the engine name is new,
make sure to set `populate_` to True.

```pycon
>>> vbt.SQLData.set_engine_settings(
...     engine_name="sqlite",
...     populate_=True,
...     engine=SQLITE_URL
... )

>>> vbt.SQLData.set_engine_settings(
...     engine_name="postgresql",
...     populate_=True,
...     engine=POSTGRESQL_URL,
...     schema="yf_data"
... )
```

If any argument is None during fetching, it will be taken from these settings first.
You only need to provide an engine name as `engine` or `engine_name`:

```pycon
>>> vbt.SQLData.pull(engine_name="sqlite").symbols
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(engine_name="postgresql").symbols
['BTC-USD', 'ETH-USD']
```

You can also save specific arguments you want to use under each engine name.
For example, let's define the default engine name:

```pycon
>>> vbt.SQLData.set_custom_settings(engine_name="postgresql")

>>> vbt.SQLData.pull().symbols
['BTC-USD', 'ETH-USD']
```

To fetch specific columns, use the `columns` argument:

```pycon
>>> vbt.SQLData.pull(columns=["High", "Low"]).features
['High', 'Low']
```

!!! hint
    If you want to pull a single column and keep it as a DataFrame, set `squeeze` to False.

Unlike the Pandas method, you can also filter by any start and end condition.
When `align_dates` is True (default), and `start` and/or `end` is provided,
[SQLData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.fetch_key)
will fetch just one row of data first. It will check whether the index of this row
is datetime-like, and if so, will treat the provided `start` and/or `end` as datetime-like
objects. This means converting them to `datetime` objects and then localizing or converting them
to the timezone of the index.

!!! note
    If you used [Data.to_sql](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_sql) to save the data,
    be sure to use the same `to_utc` option when pulling as you did when saving.

```pycon
>>> vbt.SQLData.pull(start="2020-01-03").close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Most databases either do not support timezones or store data in UTC, so the default
behavior is to localize any timezone-naive datetime to UTC. The user is then responsible
for providing the correct timezone. If a timezone is provided via `tz` and the provided
datetime is timezone-naive, it will be localized to `tz` and then converted to the timezone
of the index. If `to_utc` is True, it will also be converted to UTC. If the index has no timezone,
the provided datetime will be converted either to `tz` or to UTC (if `to_utc` is True),
and then the timezone will be removed.

Let's demonstrate using a custom timezone by saving and then pulling the price of AAPL:

```pycon
>>> aapl_data = vbt.YFData.fetch("AAPL", start="2022", end="2023")
>>> aapl_data.close
Date
2022-01-03 00:00:00-05:00    180.190979
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171844
                                    ...
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
2022-12-30 00:00:00-05:00    129.378006
Name: Close, Length: 251, dtype: float64

>>> aapl_data.to_sql("sqlite")  # (1)!
>>> aapl_data.to_sql("postgresql")  # (2)!

>>> vbt.SQLData.pull(
...     "AAPL", 
...     engine_name="sqlite", 
...     start="2022-12-23",
...     end="2022-12-30",
...     tz="America/New_York"
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64

>>> vbt.SQLData.pull(
...     "AAPL", 
...     engine_name="postgresql", 
...     start="2022-12-23",
...     end="2022-12-30",
...     tz="America/New_York"
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64
```

1. The first argument can also be an engine name from the global settings.
2. The schema defined in the global settings will be used by default.

All datetime pre-processing can be disabled by turning off `align_dates`:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL", 
...     start="2022-12-23",
...     end="2022-12-30",
...     tz="America/New_York",
...     align_dates=False
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64
```

When you need to execute a custom query, filtering must be performed inside the query itself:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL", 
...     query="""
...         SELECT *
...         FROM yf_data."AAPL" 
...         WHERE yf_data."AAPL"."Date" >= :start AND yf_data."AAPL"."Date" < :end
...     """,
...     tz="America/New_York",
...     index_col="Date",
...     params={
...         "start": vbt.datetime("2022-12-23", tz="America/New_York"),
...         "end": vbt.datetime("2022-12-30", tz="America/New_York")
...     }
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64
```

To filter by row number, a column with row numbers must be included first. This can be done
automatically by using [Data.to_sql](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_sql) with
`attach_row_number` set to True.

```pycon
>>> aapl_data.to_sql("sqlite", attach_row_number=True, if_exists="replace")
```

Then, when pulling, you can use `start_row` and `end_row` directly:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL", 
...     start_row=5,
...     end_row=10,
...     tz="America/New_York"
... ).close
Date
2022-01-10 00:00:00-05:00    170.469116
2022-01-11 00:00:00-05:00    173.330231
2022-01-12 00:00:00-05:00    173.775711
2022-01-13 00:00:00-05:00    170.469116
2022-01-14 00:00:00-05:00    171.340347
Freq: D, Name: Close, dtype: float64
```

#### Chunking

Chunking for SQL databases works the same way as it does for CSV files:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL",
...     chunksize=1,
...     chunk_func=lambda iterator: pd.concat([
...         df 
...         for df in iterator
...         if df.index.is_month_start.all()
...     ], axis=0),
...     tz="America/New_York"
... ).close
Date
2022-02-01 00:00:00-05:00    172.864914
2022-03-01 00:00:00-05:00    161.774826
2022-04-01 00:00:00-04:00    172.787796
2022-06-01 00:00:00-04:00    147.627930
2022-07-01 00:00:00-04:00    137.919113
2022-08-01 00:00:00-04:00    160.334793
2022-09-01 00:00:00-04:00    157.028458
2022-11-01 00:00:00-04:00    149.761551
2022-12-01 00:00:00-05:00    147.679932
Name: Close, dtype: float64
```

### DuckDB

!!! hint
    The previous method (using SQLAlchemy) can also be used to read from a DuckDB database.
    To do this, you will need to install the [duckdb-engine](https://pypi.org/project/duckdb-engine/) extension.

To fetch data using DuckDB, you can use the convenient class 
[DuckDBData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData). This class provides
methods for discovering catalogs, schemas, and tables, as well as methods for fetching the data itself,
such as [DuckDBData.fetch_key](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData.fetch_key).
Let's delete the existing DuckDB database, create a new one by setting its URL globally,
add some data, and review the stored objects:

```pycon
>>> vbt.remove_file(DUCKDB_URL)

>>> vbt.DuckDBData.set_custom_settings(
...     connection=DUCKDB_URL
... )

>>> yf_data.to_duckdb(schema="yf_data")

>>> vbt.DuckDBData.list_catalogs()
['database']

>>> vbt.DuckDBData.list_schemas()
['main', 'yf_data']

>>> vbt.DuckDBData.list_tables()
['yf_data:BTC-USD', 'yf_data:ETH-USD']
```

To fetch the data, use [DuckDBData.pull](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData.pull).
If you provide one or more keys, they will be used as table names. Without any arguments, the method
will first determine which tables are stored in the database and then pull them:

```pycon
>>> vbt.DuckDBData.pull("yf_data:BTC-USD").symbols
['yf_data:BTC-USD']

>>> vbt.DuckDBData.pull("BTC-USD", schema="yf_data").symbols
['BTC-USD']

>>> vbt.DuckDBData.pull(["BTC-USD", "ETH-USD"], schema="yf_data").symbols
['BTC-USD', 'ETH-USD']

>>> vbt.DuckDBData.pull().symbols
['yf_data:BTC-USD', 'yf_data:ETH-USD']
```

If you want to keep symbol names and table names separate, you can provide each table 
explicitly using a dictionary with [key_dict](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.key_dict):

```pycon
>>> vbt.DuckDBData.pull(
...     ["BTC", "ETH"], 
...     table=vbt.key_dict(BTC="BTC-USD", ETH="ETH-USD"),
...     schema="yf_data"
... ).symbols
['BTC', 'ETH']
```

You can follow the same approach to execute custom queries:

```pycon
>>> vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": 'SELECT * FROM yf_data."BTC-USD"', 
...         "ETH-USD": 'SELECT * FROM yf_data."ETH-USD"'
...     })
... ).symbols
['BTC-USD', 'ETH-USD']
```

In addition to querying tables, you can also read CSV, Parquet, and JSON files.
The reading behavior is similar to the writing process shown in
[Data.to_duckdb](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.to_duckdb),
where there are three arguments: `read_format`, `read_path`, and `read_options`.
The first argument specifies the format of the file(s), the second specifies the file path(s),
and the third controls the options for reading the files. If `read_format` is not provided,
it is parsed from the file extension and used to call the corresponding `read_{format}` function in
the SQL query, for example, [`read_parquet`](https://duckdb.org/docs/data/parquet/overview)
for Parquet files.

```pycon
>>> vbt.DuckDBData.pull(read_path="data").symbols
['BTC-USD', 'ETH-USD']

>>> vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"],
...     read_path=vbt.key_dict({
...          "BTC-USD": "data/BTC-USD.parquet",
...          "ETH-USD": "data/ETH-USD.parquet"
...     })
... ).symbols
['BTC-USD', 'ETH-USD']

>>> vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"],
...     query=vbt.key_dict({
...          "BTC-USD": "SELECT * FROM read_parquet('data/BTC-USD.parquet')",
...          "ETH-USD": "SELECT * FROM read_parquet('data/ETH-USD.parquet')"
...     })
... ).symbols
['BTC-USD', 'ETH-USD']
```

## Updating

Updating local data works similarly to updating remote data, but it only affects the contents of the 
local file or database. When updating, the system will download new data, append it to the local dataset, 
and, if necessary, remove any duplicates. As a result, the file or database will always reflect the most 
up-to-date data available according to the update logic. 

You can trigger an update manually, or you can configure automatic updates by setting schedules or making 
calls from your code. The specifics depend on how you have configured your data pipeline and storage. 

Be sure to use the appropriate tools and methods provided by your data source and storage format to ensure
reliable and consistent updates.

### CSV & HDF

Tabular data, such as CSV and HDF files, can be read line by line, allowing you to monitor data updates.
The classes [CSVData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) and
[HDFData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) can be updated like any other preset data
class by tracking the last row index in [Data.returned_kwargs](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.returned_kwargs).
Whenever an update is triggered, this index is used as the starting row for reading the dataset. After the
update, the end row becomes the new last row index.

Let's separately download data for `BTC-USD` and `ETH-USD`, save them to a single HDF file,
and read the entire file using [HDFData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData):

```pycon
>>> yf_data_btc = vbt.YFData.pull(
...     "BTC-USD", 
...     start="2020-01-01", 
...     end="2020-01-03"
... )
>>> yf_data_eth = vbt.YFData.pull(
...     "ETH-USD", 
...     start="2020-01-03", 
...     end="2020-01-05"
... )

>>> yf_data_btc.to_hdf("data.h5", key="yf_data_btc")
>>> yf_data_eth.to_hdf("data.h5", key="yf_data_eth")

>>> hdf_data = vbt.HDFData.pull(["BTC-USD", "ETH-USD"], paths="data.h5")
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> hdf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215         NaN
2020-01-03 00:00:00+00:00          NaN  134.171707
2020-01-04 00:00:00+00:00          NaN  135.069366
```

Now, let's look at the last row index in each dataset:

```pycon
>>> hdf_data.returned_kwargs
key_dict({'BTC-USD': {'last_row': 1}, 'ETH-USD': {'last_row': 1}})
```

We see that the third row in each dataset is the new start row (1 row for the header and
1 row for the data). Let's append new data to the `BTC-USD` dataset and then update our
[HDFData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) instance:

```pycon
>>> yf_data_btc = yf_data_btc.update(end="2020-01-06")
>>> yf_data_btc.to_hdf("data.h5", key="yf_data_btc")

>>> hdf_data = hdf_data.update()
>>> hdf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215         NaN
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
2020-01-05 00:00:00+00:00  7411.317383         NaN
```

The `BTC-USD` dataset has been updated with 3 new data points, while the `ETH-USD` dataset remains
unchanged. This is reflected in the last row index:

```pycon
>>> hdf_data.returned_kwargs
key_dict({
    'BTC-USD': {'last_row': 4, 'tz_convert': datetime.timezone.utc}, 
    'ETH-USD': {'last_row': 1, 'tz_convert': datetime.timezone.utc}
})
```

You can repeat this workflow as often as needed.

### Feather & Parquet

Feather and Parquet classes do not have any `start` or `end` arguments to select a date range to pull,
but you can still load all the data and append any differences to what you already have. This operation
is efficient because reading these formats is highly optimized. In newer versions of Pandas, you can also
filter partitions using the `filters` argument (supported only for Parquet):

```pycon
>>> parquet_data = vbt.ParquetData.pull(
...     "BNB-USD", 
...     filters=[("group", "<", "2020-01-03")]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
Name: Close, dtype: float64

>>> parquet_data = parquet_data.update(
...     filters=[("group", ">=", "2020-01-03")]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
2020-01-03 00:00:00+00:00    13.660452
2020-01-04 00:00:00+00:00    13.891512
Freq: D, Name: Close, dtype: float64
```

In newer versions of PyArrow, you can use the same argument to select rows:

```pycon
>>> parquet_data = vbt.ParquetData.pull(
...     "BNB-USD", 
...     filters=[("Date", "<", vbt.timestamp("2020-01-03", tz="UTC"))]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
Name: Close, dtype: float64

>>> parquet_data = parquet_data.update(
...     filters=[("Date", ">=", vbt.timestamp("2020-01-03", tz="UTC"))]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
2020-01-03 00:00:00+00:00    13.660452
2020-01-04 00:00:00+00:00    13.891512
Freq: D, Name: Close, dtype: float64
```

!!! important
    There is an issue with PyArrow starting from version 12.0.0 that makes it impossible
    to filter by timezone-aware timestamps: https://github.com/apache/arrow/issues/37355.
    This issue is said to be resolved in version 14.0.0.

### SQL

The class [SQLData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData) can be updated
in two ways: by using the last row number or the last index. The first method works only if you have
attached a column with row numbers, the name of this column is known and stored in
[Data.returned_kwargs](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.returned_kwargs) (this happens
automatically), and index-based filtering (`start` and/or `end`) is not used. If these conditions are met,
this method will be used by default; it will retrieve the last row number from the DataFrame and
pass it as `start_row`.

```pycon
>>> aapl_data.to_sql("postgresql", attach_row_number=True, if_exists="replace")

>>> sql_data = vbt.SQLData.pull(
...     "AAPL", 
...     end_row=5,
...     tz="America/New_York"
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
Freq: D, Name: Close, dtype: float64

>>> sql_data = sql_data.update(end_row=10)
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
2022-01-10 00:00:00-05:00    170.469131
2022-01-11 00:00:00-05:00    173.330261
2022-01-12 00:00:00-05:00    173.775726
2022-01-13 00:00:00-05:00    170.469131
2022-01-14 00:00:00-05:00    171.340317
Freq: B, Name: Close, dtype: float64
```

The second approach is enabled if the first approach is disabled and row-based filtering 
(`start_row` and/or `end_row`) is not used. It will extract the last index from 
[Data.last_index](https://vectorbt.pro/pvt_6d1b3986/api/data/base/#vectorbtpro.data.base.Data.last_index) and pass it as `start`, 
regardless of the index data type.

```pycon
>>> aapl_data.to_sql("postgresql", attach_row_number=False, if_exists="replace")

>>> sql_data = vbt.SQLData.pull(
...     "AAPL", 
...     end="2022-01-08",
...     tz="America/New_York"
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
Freq: D, Name: Close, dtype: float64

>>> sql_data = sql_data.update(end="2022-01-15")
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
2022-01-10 00:00:00-05:00    170.469131
2022-01-11 00:00:00-05:00    173.330261
2022-01-12 00:00:00-05:00    173.775726
2022-01-13 00:00:00-05:00    170.469131
2022-01-14 00:00:00-05:00    171.340317
Freq: B, Name: Close, dtype: float64
```

If the data was originally pulled using a custom query, both approaches will be disabled,
and you will need to implement either method manually.

```pycon
>>> sql_data = vbt.SQLData.pull(
...     "AAPL", 
...     query="""
...         SELECT *
...         FROM yf_data."AAPL" 
...         WHERE yf_data."AAPL"."Date" >= :start AND yf_data."AAPL"."Date" < :end
...     """,
...     tz="America/New_York",
...     index_col="Date",
...     params={
...         "start": vbt.datetime("2022-01-01", tz="America/New_York"),
...         "end": vbt.datetime("2022-01-08", tz="America/New_York")
...     }
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
Freq: D, Name: Close, dtype: float64

>>> sql_data = sql_data.update(
...     params={
...         "start": vbt.datetime("2022-01-08", tz="America/New_York"),
...         "end": vbt.datetime("2022-01-18", tz="America/New_York")
...     }
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190948
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171844
2022-01-06 00:00:00-05:00    170.281006
2022-01-07 00:00:00-05:00    170.449326
2022-01-10 00:00:00-05:00    170.469116
2022-01-11 00:00:00-05:00    173.330231
2022-01-12 00:00:00-05:00    173.775742
2022-01-13 00:00:00-05:00    170.469116
2022-01-14 00:00:00-05:00    171.340332
Freq: B, Name: Close, dtype: float64
```

### DuckDB

!!! hint
    The previous method (using SQLAlchemy) can also be used to update from a DuckDB database.
    To do this, you must install the [duckdb-engine](https://pypi.org/project/duckdb-engine/) extension.

To update an existing [DuckDBData](https://vectorbt.pro/pvt_6d1b3986/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData)
with new data, you can use the `start` and `end` arguments, or construct a SQL query that returns
the desired data range.

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     end="2020-01-03",
...     schema="yf_data"
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(end=None)
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Alternatively, you can do this manually using a custom SQL query:

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * FROM yf_data."BTC-USD" 
...             WHERE "Date" < TIMESTAMP '2020-01-03 00:00:00.000000'
...         """, 
...         "ETH-USD": """
...             SELECT * FROM yf_data."ETH-USD" 
...             WHERE "Date" < TIMESTAMP '2020-01-03 00:00:00.000000'
...         """
...     })
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * FROM yf_data."BTC-USD" 
...             WHERE "Date" >= TIMESTAMP '2020-01-03 00:00:00.000000'
...         """, 
...         "ETH-USD": """
...             SELECT * FROM yf_data."ETH-USD" 
...             WHERE "Date" >= TIMESTAMP '2020-01-03 00:00:00.000000'
...         """
...     })
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

You can also use prepared statements as shown below:

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * FROM yf_data."BTC-USD" 
...             WHERE "Date" >= $start AND "Date" < $end
...         """, 
...         "ETH-USD": """
...             SELECT * FROM yf_data."ETH-USD" 
...             WHERE "Date" >= $start AND "Date" < $end
...         """
...     }),
...     params=dict(
...         start=vbt.datetime("2020-01-01"),
...         end=vbt.datetime("2020-01-03")
...     )
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(
...     params=dict(
...         start=vbt.datetime("2020-01-03"),
...         end=vbt.datetime("2020-01-05")
...     )
... )
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Similarly, you can filter based on the dynamically generated row number:

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * EXCLUDE (Row) FROM (
...                 SELECT row_number() OVER () AS "Row", * FROM yf_data."BTC-USD"
...             )
...             WHERE Row >= $start_row AND Row < $end_row
...         """, 
...         "ETH-USD": """
...             SELECT * EXCLUDE (Row) FROM (
...                 SELECT row_number() OVER () AS "Row", * FROM yf_data."ETH-USD"
...             )
...             WHERE Row >= $start_row AND Row < $end_row
...         """
...     }),
...     params=dict(start_row=1, end_row=3)
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(params=dict(start_row=3, end_row=5))
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

[:material-language-python: Python code](https://vectorbt.pro/pvt_6d1b3986/assets/jupytext/documentation/data/local.py.txt){ .md-button target="blank_" }