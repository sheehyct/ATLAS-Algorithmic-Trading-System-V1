---
link: https://vectorbt.pro/become-a-member/
parent: https://vectorbt.pro/
name: Become a member
type: page
---

# Become a member

Once you become an eligible member, you will be added as a collaborator to the private GitHub
repository, invited to our quant community on Discord, and granted access to exclusive website
content :star2:

<div class="grid cards" markdown>

- :material-calendar-clock-outline:{ .lg .middle } __Monthly__

    ---

    Pay monthly and cancel anytime :ok_hand:

    Starts at __$25 a month__ [^1]

    * GitHub Sponsors (credit and debit cards):

    [:simple-github:&nbsp;&nbsp;Join on GitHub](https://github.com/sponsors/polakowo/sponsorships?pay_prorated=false&tier_id=124816){ .md-button .md-button--gh-sponsor }

    * Ko-fi (PayPal, Stripe, and more):

    [:simple-kofi:&nbsp;&nbsp;Join on Ko-fi](https://ko-fi.com/vectorbtpro/tiers){ .md-button .md-button--kofi-sponsor }

    * Patreon (PayPal):

    [:simple-patreon:&nbsp;&nbsp;Join on Patreon](https://github.com/sponsors/patreon/become-sponsor/polakowo?campaign_id=12220591&cents=2500){ .md-button .md-button--patreon-sponsor }

    <small>For Patreon sponsorships, please subscribe via [GitHub Sponsors](https://github.com/sponsors/polakowo/) by clicking "Become a patron".</small>

    <hr>
    For personal use only. By becoming a member, you agree to the [Terms of Use](https://vectorbt.pro/pvt_41531c19/terms/terms-of-use).

- :material-calendar-text-outline:{ .lg .middle } __1-Year__

    ---

    Pay once and enjoy access for one year! :thumbsup:

    Starts at ~~$300~~ __$240__ or ~~$25~~ __$20 a month__ [^1]

    * Ko-fi (PayPal, Stripe, and more):

    [:simple-kofi:&nbsp;&nbsp;Join on Ko-fi](https://ko-fi.com/s/88d8ca176c){ .md-button .md-button--kofi-sponsor }

    <hr>
    For personal use only. By becoming a member, you agree to the [Terms of Use](https://vectorbt.pro/pvt_41531c19/terms/terms-of-use).

- :material-calendar-check-outline:{ .lg .middle } __Lifetime__

    ---

    Pay once and enjoy access forever! :metal:

    Starts at __$500 one time__ [^1]

    * Ko-fi (PayPal, Stripe and more):

    [:simple-kofi:&nbsp;&nbsp;Join on Ko-fi](https://ko-fi.com/s/0a60e96ab8){ .md-button .md-button--kofi-sponsor }

    * Cryptocurrency:

    __:material-sale-outline: 10% discount__

    Please contact me at olegpolakow@vectorbt.pro.

    <hr>
    For personal use only. By becoming a member, you agree to the [Terms of Use](https://vectorbt.pro/pvt_41531c19/terms/terms-of-use).

- :material-office-building-outline:{ .lg .middle } __Organization__

    ---

    Grant access for an organization. Please contact me at olegpolakow@vectorbt.pro.

</div>

<small>
  :octicons-light-bulb-16:
  **Tip:** This is a limited-time offer for early contributors. With each new milestone, the price
  will gradually increase to match that of competitors. By sponsoring now, you become an early
  contributor and lock in this price to access all features that will be released.
</small>

[^1]: You can always switch to lifetime access by applying half of the amount you have paid monthly
as a credit and paying only the remainder (with a minimum of $150). For example, if you have been a
sponsor for 8 months, you can use 4 months, or $200 / 2 = $100 as a credit and pay $400 instead of
$500. If you are still in your first month, you get the full month as a credit. To proceed, contact
me at olegpolakow@vectorbt.pro :purple_heart:{: .heart }

---
link: https://vectorbt.pro/pvt_41531c19/
parent: https://vectorbt.pro/
children:
  - https://vectorbt.pro/pvt_41531c19/getting-started/
  - https://vectorbt.pro/pvt_41531c19/features/
  - https://vectorbt.pro/pvt_41531c19/tutorials/
  - https://vectorbt.pro/pvt_41531c19/documentation/
  - https://vectorbt.pro/pvt_41531c19/cookbook/
  - https://vectorbt.pro/pvt_41531c19/api/
  - https://vectorbt.pro/pvt_41531c19/terms/
name: Getting started
type: page
---

# Getting started

![](https://vectorbt.pro/pvt_41531c19/assets/logo/header.svg)

![](https://vectorbt.pro/pvt_41531c19/assets/badges/version.svg)
![](https://vectorbt.pro/pvt_41531c19/assets/badges/python-version.svg)
![](https://vectorbt.pro/pvt_41531c19/assets/badges/license.svg)
![](https://vectorbt.pro/pvt_41531c19/assets/badges/build-passing.svg)

We are excited to have you here!

We envision a future where everyone, whether you are a seasoned trader, a data scientist with a Ph.D., 
or a hobbyist, has access to a full range of cutting-edge tools to guide your decisions and compete 
with major players :shark:

This tool is the first part of an expanding ecosystem designed to support that vision. It already 
breaks barriers and opens a new realm of possibilities for analyzing financial data of any 
complexity. All you need is your time and motivation to embrace our approach and extract insights 
from data that once looked like ordinary numbers!

??? youtube "Installation Guide + Subscriber Perks on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/GkJEQ4fm2_E?si=qUBfW-OoIImGm4bw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Discord

<iframe src="https://discord.com/widget?id=918629562441695344&theme=dark" width="350" height="500" allowtransparency="true" frameborder="0" sandbox="allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts"></iframe>

Before you get started with VBT, be sure to join our [Discord server](https://discord.gg/eQ9sVr5vb9).
On our server, we support each other, report issues, request new features, and chat!

:octicons-light-bulb-16: After joining, please post your GitHub account name (for example, [polakowo](https://github.com/polakowo))
in the verification channel so we can manually link your Discord and GitHub accounts. We will then assign
you a member role that unlocks all other channels. If you end your sponsorship, you will lose access
to the server. If you restart your sponsorship, please repeat the steps above.

## Website

To ensure only sponsors can access exclusive content, this project is split into two websites:
the public site at https://vectorbt.pro and the private site you are currently using. The link to
the private website is provided in the private repository (bookmark this page:
https://github.com/polakowo/vectorbt.pro/blob/pvt-links/README.md :bookmark:).
The private website is a directory on the same host as the public site, but it is preceded by a random
hash that changes periodically to help prevent information theft. You can tell which site you are on
by checking the lock icon in the top left corner: a closed lock means the public site, and an open
lock means the private site.

The website is organized into the following main sections:

<br>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-clock-fast:{ .icon }

__Getting started__

Contains the installation guide for `vectorbtpro` and the change log.

[:octicons-arrow-right-24: Installation](https://vectorbt.pro/pvt_41531c19/getting-started/installation/)

[:octicons-arrow-right-24: Release notes](https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/)
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-lightning-bolt:{ .icon } 

__Features__

Contains examples of major enhancements added with each release.

[:octicons-arrow-right-24: Features](https://vectorbt.pro/pvt_41531c19/features/overview/)
</div>
</div>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-clipboard-check-outline:{ .icon } 

__Tutorials__

Contains all the exclusive tutorials.

[:octicons-arrow-right-24: Tutorials](https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/)
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-format-font:{ .icon } 

__Documentation__

Contains user-friendly documentation for the latest version of `vectorbtpro`.

[:octicons-arrow-right-24: Documentation](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/)
</div>
</div>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-connection:{ .icon } 

__API__

Contains API documentation for the latest version of `vectorbtpro`.

[:octicons-arrow-right-24: API](https://vectorbt.pro/pvt_41531c19/api/)
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-food-outline:{ .icon } 

__Cookbook__

Contains short and practical examples for using `vectorbtpro`.

[:octicons-arrow-right-24: Cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/)
</div>
</div>

## General design

If you have used other backtesting frameworks before, you may expect to implement your trading
strategy by creating a Python class, overriding some methods, and working with a limited set of
commands to communicate with the backtester, such as placing orders. Using VBT is quite
different: since it is a quantitative analysis package that operates mainly on arrays, it is more
similar to libraries like Pandas than to frameworks such as backtrader.

The main technical difference between a framework and a library comes down to something called
inversion of control. When you use a framework, the _framework_ controls the flow. It provides
places for you to plug in your code, and then it calls your code as needed. When using a library,
_you_ are in control of the application's flow; you decide when and where to call the library.
Because of this, VBT's functionality is distributed across many modules, each optional
and typically with its own documentation.

For example, while VBT provides [this extensive module](https://vectorbt.pro/pvt_41531c19/api/data/base/) and
[related documentation](https://vectorbt.pro/pvt_41531c19/documentation/data/) for storing and manipulating data, you can
skip it entirely and use just Pandas and NumPy arrays. This flexibility makes it difficult
to create a perfect getting-started guide, since each use case is different and requires a unique
set of modules. As experience shows, you will end up using only a fraction of the functions that
VBT offers!

## First steps

After you have been added as a collaborator and accepted the repository invitation,
the first step is to [install the package](https://vectorbt.pro/pvt_41531c19/getting-started/installation/).

What should you do next? Here are some recommended steps:

<br>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-alphabetical-variant:{ .icon } 

__1 - Fundamentals__

Learn the fundamental concepts of VBT. Why do we use Numba?
What is represented by rows and columns? Why is broadcasting so important?
How do most VBT classes work?

[:octicons-arrow-right-24: Fundamentals](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/)

[:octicons-arrow-right-24: Building blocks](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/)
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-chart-line-variant:{ .icon } 

__2 - Basic RSI strategy__

Apply the fundamental concepts to backtest a basic RSI strategy. Try running the example on
your own data, test a different set of parameters, add trading commissions, and experiment!

[:octicons-arrow-right-24: Basic RSI strategy](https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/)
</div>
</div>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-lightning-bolt-outline:{ .icon } 

__3 - SuperFast SuperTrend__

Learn to develop, compile, and backtest a Supertrend indicator. Adapt the example
to an indicator that interests you. See [this documentation](https://vectorbt.pro/pvt_41531c19/documentation/indicators/) for help.

[:octicons-arrow-right-24: SuperFast SuperTrend](https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/)
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-broadcast:{ .icon } 

__4 - Signal development__

After building an indicator, learn to detect events in the data and turn them into
signals that can be backtested. This is one of the most important tutorials, so do not miss it!

[:octicons-arrow-right-24: Signal development](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/)
</div>
</div>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-chart-areaspline:{ .icon } 

__5 - Portfolio__

After creating signal arrays, learn to simulate them and analyze their performance.
To better understand how the engine works, see [this](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/)
and then [this](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/) documentation.

[:octicons-arrow-right-24: Portfolio](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-signals/)
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-check-all:{ .icon } 

__5 - Cross-validation__

Do you have a promising trading strategy that performs well on historical data? It is time
to cross-validate! Cross-validation not only helps detect overfitting, but it can also
reveal the market conditions where your strategy performs best and worst.

[:octicons-arrow-right-24: Cross-validation](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/)
</div>
</div>
<div class="showcase-row" markdown>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-bird:{ .icon } 

__Freedom__

By now, you should have enough experience to backtest signal-based strategies.
Visit the remaining tutorials and documentation, and get familiar with the [API documentation](https://vectorbt.pro/pvt_41531c19/api/)
to build a backtesting pipeline from scratch. Good luck!
</div>
<div class="showcase-column" data-aos="fade-up" markdown>
:material-robot:{ .icon } 

__Ask QuantGPT__

<a href="https://www.quantgpt.chat/" class="link-with-border" markdown>
![](https://vectorbt.pro/pvt_41531c19/assets/quantgpt.light.webp#only-light){: loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/quantgpt.dark.webp#only-dark){: loading=lazy }
</a>
</div>
</div>

## LLM-friendly content

Documentation is also available in [llms.txt format](https://llmstxt.org/), which is a simple markdown
standard that LLMs can consume easily.

There are two ways to access the LLM-friendly documentation:

* [llms.txt](https://vectorbt.pro/pvt_41531c19/llms.txt) is essentially a sitemap, listing all the pages in the documentation.

* [llms-docs.txt](https://vectorbt.pro/pvt_41531c19/llms-docs.txt) contains the general documentation (may exceed the context window of your LLM).

* [llms-api.txt](https://vectorbt.pro/pvt_41531c19/llms-api.txt) contains the API documentation (may exceed the context window of your LLM).

* [llms-full.txt](https://vectorbt.pro/pvt_41531c19/llms-full.txt) contains the entire documentation (may exceed the context window of your LLM).

!!! info
    In addition, you can access the Markdown source files of the documentation by appending `.md` to the URL
    of any page. For example, the Markdown file for the first page of the cross-validation tutorial—[`/tutorials/cross-validation/`](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/)—can be found at [`/tutorials/cross-validation.md`](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation.md).

### MCP server

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/) is a new way to interact
with software using LLMs. It enables your agents to access information they need without prior
knowledge of the software. For example, you can ask an agent to _"list all arguments related to stop
orders in `vbt.Portfolio.from_signals`"_ and it will call the VBT's MCP server to retrieve the relevant
information. Under the hood, the MCP server calls native VBT functions for retrieval, so it is always
up-to-date with the latest version of VBT, but this also means that VBT must be
[installed](https://vectorbt.pro/pvt_41531c19/getting-started/installation/) and running on your machine.

#### ChatVBT

[ChatVBT](https://vectorbt.pro/pvt_41531c19/features/intelligence/#chatvbt) is a chat interface for VBT that you can run in a Jupyter notebook
or any Python environment. It supports a wide range of LLMs, output formats, and can be configured to use
different embedding providers and knowledge assets. It also can access the MCP server tools for
real-time information on your VBT installation. To use the MCP server with ChatVBT, simply
use `vbt.interact()` instead of `vbt.chat()`. For LLM configurations (such as how to generate
embeddings and completions locally for free), see [this section](https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#provider-configs).

#### Claude Desktop

Follow [this guide](https://modelcontextprotocol.io/docs/develop/connect-local-servers)
to set up the MCP server for Claude Desktop.

Add the following "VectorBT PRO" entry to the `mcpServers` section in `claude_desktop_config.json`:

```json
{
  "mcpServers": {
    "VectorBT PRO": {
      "command": "/absolute/path/to/python",  // (1)!
      "args": [
        "-m",
        "vectorbtpro.mcp_server"
      ]
    }
  }
}
```

1. See below.

To find the absolute path to your Python interpreter, use the following command:

=== "Linux/macOS"

    ```bash
    which python
    ```

=== "Windows"

    ```bash
    where python
    ```

!!! note
    This must be the same Python interpreter that you used to install `vectorbtpro`.

By default, VBT uses the OpenAI API for embeddings. Thus, you need to set the `OPENAI_API_KEY` environment
variable to use embeddings. You also need to set the `GITHUB_TOKEN` environment variable to download
the knowledge assets if you haven't done so already. This happens automatically when you use
[SearchVBT](https://vectorbt.pro/pvt_41531c19/features/intelligence/#searchvbt) or [ChatVBT](https://vectorbt.pro/pvt_41531c19/features/intelligence/#chatvbt) for the first time.

!!! tip
    You can use [this guide](https://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety)
    to create and manage your keys globally.

If you want to change default settings of VBT (such as the default embedding provider), you can
change the settings, [save them](https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#settings), and then set the `VBT_SETTINGS_PATH`
environment variable to point to the settings file in the `env` section of the MCP server configuration.

For example, you can define both the GitHub token and OpenAI key directly in the settings file:

=== "INI"

    ```ini title="File vbt.cfg"
    [knowledge.assets.vbt]
    token = "<YOUR_GITHUB_TOKEN>"

    [knowledge.chat.embeddings_configs.openai.client_kwargs]
    api_key = "<YOUR_OPENAI_API_KEY>"
    ```

=== "YAML"

    ```yaml title="File vbt.yml"
    knowledge:
      assets:
        vbt:
          token: <YOUR_GITHUB_TOKEN>
      chat:
        embeddings_configs:
          openai:
            client_kwargs:
              api_key: <YOUR_OPENAI_API_KEY>
    ```

=== "TOML"

    ```toml title="File vbt.toml"
    [knowledge.assets.vbt]
    token = "<YOUR_GITHUB_TOKEN>"

    [knowledge.chat.embeddings_configs.openai.client_kwargs]
    api_key = "<YOUR_OPENAI_API_KEY>"
    ```

Alternatively, if you don't have an OpenAI API key or want to generate embeddings locally at zero cost
(given you have enough RAM and compute), you can use the following configuration:

=== "INI"

    ```ini title="File vbt.cfg"
    [knowledge.assets.vbt]
    token = "<YOUR_GITHUB_TOKEN>"

    [knowledge.chat]
    embeddings = "ollama"
    ```

=== "YAML"

    ```yaml title="File vbt.yml"
    knowledge:
      assets:
        vbt:
          token: <YOUR_GITHUB_TOKEN>
    knowledge:
      chat:
        embeddings: ollama
    ```

=== "TOML"

    ```toml title="File vbt.toml"
    [knowledge.assets.vbt]
    token = "<YOUR_GITHUB_TOKEN>"

    [knowledge.chat]
    embeddings = "ollama"
    ```

!!! example "Cookbook"
    Explore [more provider configurations](https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#provider-configs).

And then set the `VBT_SETTINGS_PATH` environment variable in the MCP server configuration:

=== "INI"

    ```json
    {
      "mcpServers": {
        "VectorBT PRO": {
          ...
          "env": {
            "VBT_SETTINGS_PATH": "/absolute/path/to/vbt.cfg"
          }
        }
      }
    }
    ```

=== "YAML"

    ```json
    {
      "mcpServers": {
        "VectorBT PRO": {
          ...
          "env": {
            "VBT_SETTINGS_PATH": "/absolute/path/to/vbt.yml"
          }
        }
      }
    }
    ```

=== "TOML"

    ```json
    {
      "mcpServers": {
        "VectorBT PRO": {
          ...
          "env": {
            "VBT_SETTINGS_PATH": "/absolute/path/to/vbt.toml"
          }
        }
      }
    }
    ```

!!! tip
    Generally, it's recommended to test the [search and chat features](https://vectorbt.pro/pvt_41531c19/features/intelligence/) of VBT
    before setting up the MCP server. This way, you can ensure that the knowledge assets are downloaded
    and the embeddings are set up correctly. For example, you can use:

    ```python
    vbt.search("hello!")
    ```

Restart the Claude Desktop app to apply the changes.

You're now ready to chat with VBT! :sparkling_heart:{ .icon .heart }

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/installation/
parent: https://vectorbt.pro/pvt_41531c19/getting-started/
name: Installation
type: page
---

# Installation

!!! info
    VectorBT® PRO is entirely different from the [open-source version](https://github.com/polakowo/vectorbt).

    In fact, the PRO version overhauls the core to support groundbreaking features.

    To avoid importing outdated code, make sure to import **vectorbtpro** only!

??? youtube "MacOS Installation Guide on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/lFmeqhFwH3M?si=2vNVrZu4Q1_hdoBd" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

??? youtube "Windows Installation Guide on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/bN5BOOb4Yd4?si=YeIo5p-ADMJhwzKm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

??? youtube "Linux Installation Guide on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/TjsFsxuWY4I?si=lXtGhlEq1czCOeWT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

## Recommendations

!!! tip
    Creating a local environment is a great option for development because it gives you full control,
    better performance, and access to native tooling.

    However, if you have trouble setting it up or want to try vectorbtpro quickly and safely, you can
    either build the supplied [Docker container](#with-docker) or run vectorbtpro directly in
    [Google Colab](#google-colab).

The following recommendations apply only to local installations.

### Windows

If you are using Windows, it is recommended to use [WSL](https://learn.microsoft.com/en-us/windows/wsl/setup/environment)
for development.

### New environment

If you plan to use vectorbtpro locally, it is best to create a new environment dedicated to vectorbtpro.

#### Option 1: uv

The easiest way to get started is to install [uv](https://github.com/astral-sh/uv), a lightning-fast
package manager that handles virtual environments and dependency resolution in one tool.

First, install `uv`:

=== "Linux/macOS"

    ```shell
    curl -LsSf https://astral.sh/uv/install.sh | sh
    ```

=== "macOS (Homebrew)"

    ```shell
    brew install uv
    ```

=== "Windows"

    ```shell
    powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"
    ```

=== "pipx (cross-platform)"

    ```shell
    pipx install uv
    ```

After installing `uv`, create a new isolated environment (similar to a conda environment) for your project:

```shell
uv venv vectorbtpro --python 3.12
```

This creates a virtual environment named `vectorbtpro` located in the current directory. Now, activate it:

=== "Linux/macOS"

    ```shell
    source vectorbtpro/bin/activate
    ```

=== "Windows"

    ```shell
    vectorbtpro\Scripts\activate
    ```

!!! note
    You need to activate the environment each time you start a new terminal session.

To check you're in the correct environment, your terminal prompt should now be prefixed with `(vectorbtpro)`.

That's it! You're using `uv` to manage environments and install packages with speed and simplicity.

#### Option 2: Conda

Another easy option is to [download Anaconda](https://www.anaconda.com/download), which provides a
graphical installer and includes many popular data science packages required by vectorbtpro, such as
NumPy, Pandas, Plotly, and more.

After installing Anaconda, [create a new environment](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands):

```shell
conda create --name vectorbtpro python=3.12
```

[Activate the new environment](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#activating-an-environment):

```shell
conda activate vectorbtpro
```

!!! note
    You need to activate the environment each time you start a new terminal session.

You should now see `vectorbtpro` in the list of all environments, and it should be active (notice the `*`):

```shell
conda info --envs
```

You are now ready to install the actual package.

#### Option 3: IDE

If you primarily use an IDE, you can create a separate environment for each project:

* [PyCharm](https://www.jetbrains.com/help/pycharm/configuring-python-interpreter.html).

* [Visual Studio Code](https://code.visualstudio.com/docs/python/environments).

## With pip

The PRO version can be installed using `pip`.

!!! tip
    It is highly recommended to create a new virtual environment dedicated to vectorbtpro, such as one
    made with [Anaconda](https://www.anaconda.com/).

Uninstall the open-source version if it is already installed:

```shell
pip uninstall VBT
```

### HTTPS

Install the base PRO version (with recommended dependencies) using `git+https`:

=== "uv"

    ```shell
    uv pip install -U "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git"
    ```

=== "pip"

    ```shell
    pip install -U "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git"
    ```

!!! info
    This process may require at least 1GB of disk space and take several minutes to finish.
    
!!! tip
    Whenever you are prompted for a password, paste the token that you generated earlier.

    To avoid entering the token repeatedly, you can
    [add it to your system](https://stackoverflow.com/a/68781050)
    or set an environment variable `GH_TOKEN`, then install the package as follows:

    === "uv"

        ```shell
        uv pip install -U "vectorbtpro[base] @ git+https://${GH_TOKEN}@github.com/polakowo/vectorbt.pro.git"
        ```

    === "pip"

        ```shell
        pip install -U "vectorbtpro[base] @ git+https://${GH_TOKEN}@github.com/polakowo/vectorbt.pro.git"
        ```

    On some systems, such as macOS, the token is often remembered automatically.

    Learn more about managing tokens [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens).

#### Without TA-Lib

To install the base version without TA-Lib:

=== "uv"

    ```shell
    uv pip install -U "vectorbtpro[base-no-talib] @ git+https://github.com/polakowo/vectorbt.pro.git"
    ```

=== "pip"

    ```shell
    pip install -U "vectorbtpro[base-no-talib] @ git+https://github.com/polakowo/vectorbt.pro.git"
    ```

#### Lightweight

To install the lightweight version (with only required dependencies):

=== "uv"

    ```shell
    uv pip install -U git+https://github.com/polakowo/vectorbt.pro.git
    ```

=== "pip"

    ```shell
    pip install -U git+https://github.com/polakowo/vectorbt.pro.git
    ```

For more optional dependencies, see [pyproject.toml](https://github.com/polakowo/vectorbt.pro/blob/main/pyproject.toml).

### SSH

To install the base version with `git+ssh`:

=== "uv"

    ```shell
    uv pip install -U "vectorbtpro[base] @ git+ssh://git@github.com/polakowo/vectorbt.pro.git"
    ```

=== "pip"

    ```shell
    pip install -U "vectorbtpro[base] @ git+ssh://git@github.com/polakowo/vectorbt.pro.git"
    ```

See [Connecting to GitHub with SSH](https://docs.github.com/en/authentication/connecting-to-github-with-ssh).

### Updating

When a new version of vectorbtpro is released, the package __will not__ update automatically. You
will need to install the new version manually. Fortunately, you can use the exact same command you
used to install the package to update it.

### Specific branch

Append `@` followed by [a branch name](https://github.com/polakowo/vectorbt.pro/branches/all) to the command.

For example, to install the `develop` branch:

=== "uv"

    ```shell
    uv pip install -U "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git@develop"
    ```

=== "pip"

    ```shell
    pip install -U "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git@develop"
    ```

!!! note
    If you have the latest regular version installed, you must uninstall it first:

    === "uv"

        ```shell
        uv pip uninstall vectorbtpro
        ```

    === "pip"

        ```shell
        pip uninstall vectorbtpro
        ```

### Specific release

Append `@` followed by [a release name](https://github.com/polakowo/vectorbt.pro/releases) to the command.

For example, to install the release `v2024.1.30`:

=== "uv"

    ```shell
    uv pip install "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git@v2024.1.30"
    ```

=== "pip"

    ```shell
    pip install "vectorbtpro[base] @ git+https://github.com/polakowo/vectorbt.pro.git@v2024.1.30"
    ```

### As Python dependency

With [setuptools](https://setuptools.readthedocs.io/en/latest/), you can add vectorbtpro as a dependency
to your Python package by listing it in `setup.py` or in your
[requirements files](https://pip.pypa.io/en/latest/user_guide/#requirements-files):

```python
# setup.py
setup(
    # ...
    install_requires=[
        "vectorbtpro @ git+https://github.com/polakowo/vectorbt.pro.git"
    ]
    # ...
)
```

## With git

You can also clone vectorbtpro directly from Git:

=== "HTTPS"

    ```shell
    git clone https://github.com/polakowo/vectorbt.pro.git vectorbtpro
    ```

=== "SSH"

    ```shell
    git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro
    ```

Install the package:

=== "uv"

    ```shell
    uv pip install -e vectorbtpro
    ```

=== "pip"

    ```shell
    pip install -e vectorbtpro
    ```

### Shallow clone

The command above takes about 1GB of disk space. To create a shallow clone:

=== "HTTPS"

    ```shell
    git clone https://github.com/polakowo/vectorbt.pro.git vectorbtpro --depth=1
    ```

=== "SSH"

    ```shell
    git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1
    ```

To convert the clone back into a complete one:

```shell
git pull --unshallow
```

## With Docker

Using [Docker](https://www.docker.com/) is an excellent way to get started in just a few minutes, as it
includes all dependencies pre-installed.

### JupyterLab

[This Docker image](https://github.com/polakowo/vectorbt.pro/blob/main/Dockerfile.jupyter) is based on
[Jupyter Docker Stacks](https://jupyter-docker-stacks.readthedocs.io/en/latest/), a collection of
ready-to-run Docker images that include Jupyter applications and interactive computing tools.
Specifically, the image builds on [jupyter/scipy-notebook](https://jupyter-docker-stacks.readthedocs.io/en/latest/using/selecting.html#jupyter-scipy-notebook),
which includes a minimally-functional JupyterLab server and preinstalled popular packages from the
scientific Python ecosystem. It also extends the image with Plotly and Dash for interactive visualizations
and plots, as well as vectorbtpro and all of its optional dependencies. The image requires the vectorbtpro
source to be present in the current directory.

Before proceeding, make sure to [install Docker](https://docs.docker.com/install/).

Start Docker using Docker Desktop.

#### Building

Clone the vectorbtpro repository if you have not already. Run this command from the directory where you
want vectorbtpro to be located, such as in Documents/GitHub:

```shell
git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1
```

Enter the directory:

```shell
cd vectorbtpro
```

Build the image (this may take some time):

```shell
docker build . -t vectorbtpro
```

Create a working directory inside the current directory:

```shell
mkdir work
```

#### Running

Start a container running a JupyterLab server on port 8888:

```shell
docker run -it --rm -p 8888:8888 -v "$PWD/work":/home/jovyan/work vectorbtpro
```

!!! info
    The `-v` flag in this command mounts the working directory on the host (`{PWD/work}` in the example)
    as `/home/jovyan/work` in the container. The server logs will appear in the terminal. The
    [--rm flag](https://docs.docker.com/engine/reference/run/#clean-up---rm) tells Docker to
    automatically clean up the container and remove the file system when the container exits. However,
    any changes made to the `~/work` directory and its files inside the container will remain on the host.
    The [-it flag](https://docs.docker.com/engine/reference/commandline/run/#assign-name-and-allocate-pseudo-tty---name--it)
    allocates a pseudo-TTY.

If port 8888 is already in use, you can specify a different port (for example, 10000):

```shell
docker run -it --rm -p 10000:8888 -v "$PWD/work":/home/jovyan/work vectorbtpro
```

Once the server has started, go to its address in a browser. The address will be printed in the console,
for example: `http://127.0.0.1:8888/lab?token=9e85949d9901633d1de9dad7a963b43257e29fb232883908`

!!! note
    Change the port if needed.

This will open JupyterLab, where you can create a new notebook and start working with vectorbtpro :tada:

To use files from your host, place them into the `work` directory on your host, and they will appear in the
JupyterLab file browser. Alternatively, you can drag and drop files directly into the JupyterLab file browser.

#### Stopping

To stop the container, first press ++ctrl+c++, and then when prompted, type `y` and press ++enter++.

#### Updating

To upgrade the Docker image to a new version of vectorbtpro, first update your local repository from the remote:

```shell
git pull
```

Then rebuild the image:

```shell
docker build . -t vectorbtpro
```

!!! info
    This will not rebuild the entire image, only the vectorbtpro installation step.

### Development

[This Docker image](https://github.com/polakowo/vectorbt.pro/blob/main/Dockerfile.dev) is recommended for
development with Visual Studio Code, PyCharm (Professional), and remote connections. It includes
vectorbtpro and all optional dependencies. The image requires the vectorbtpro source to be present
in the current directory.

#### Visual Studio Code

This image also allows you to start developing right away in Visual Studio Code, without manually setting up
Python, Jupyter, or any dependencies.

Make sure you have the following installed:

* [Docker](https://docs.docker.com/install/)

* [Visual Studio Code](https://code.visualstudio.com/)

* [Dev Containers extension](https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers)

Clone the repository if you have not already:

```shell
git clone git@github.com:polakowo/vectorbt.pro.git vectorbtpro --depth=1
```

Open the repository in Visual Studio Code.

When prompted, click "Reopen in Container".

!!! tip
    If you do not see the prompt, open the Command Palette (++ctrl+shift+p++) and
    run "Dev Containers: Reopen in Container".

Visual Studio Code will:

* Build the Docker image using `Dockerfile.dev`.

* Mount your project at `/workspace`.

* Install the required Python and Jupyter extensions.

* Set up a ready-to-use environment for development and notebooks.

## Google Colab

[:material-notebook-outline: Notebook](https://colab.research.google.com/drive/1A9RxtYgGkUT_NbRxp3Z8h-fTRnVR3WRa?usp=sharing){ .md-button target="blank_" }

## Manually

If you experience connectivity issues, you can also install the package manually:

1. Go to https://github.com/polakowo/vectorbt.pro.

2. Click the `Code` dropdown button and then click "Download ZIP".

3. Unzip the downloaded archive.

4. Open the unzipped folder using your terminal.

5. Install the package using pip:

=== "uv"

    ```shell
    uv pip install ".[base]"
    ```

=== "pip"

    ```shell
    pip install ".[base]"
    ```

### Custom release

To install a custom release:

1. Go to https://github.com/polakowo/vectorbt.pro/releases.

2. Select a release.

3. Download the file with the `.whl` suffix.

4. Open the folder containing the wheel file using your terminal.

5. Install the wheel using pip:

=== "uv"

    ```shell
    uv pip install wheel
    uv pip install "filename.whl[base]"
    ```

=== "pip"

    ```shell
    pip install wheel
    pip install "filename.whl[base]"
    ```

Replace `filename` with the actual file name.

!!! note
    If the file name ends with (1) because there is already a file with the same name,
    make sure to remove the previous file and remove the (1) suffix from the newer one.

## Troubleshooting

* [TA-Lib](https://github.com/mrjbq7/ta-lib#dependencies)

* [Jupyter Notebook and JupyterLab](https://plotly.com/python/getting-started/#jupyter-notebook-support)

* [Apple M1](https://github.com/polakowo/vectorbt/issues/320)

* ["fatal error: 'H5public.h' file not found"](https://stackoverflow.com/a/71340786)

* ["RuntimeError: CMake must be installed to build qdldl"](https://github.com/robertmartin8/PyPortfolioOpt/issues/274#issuecomment-1551221810)

* ["AttributeError: 'ZMQInteractiveShell' object has no attribute 'magic'"](https://github.com/polakowo/vectorbt/issues/779#issuecomment-2724967944)

* pybind11:

If you receive the error "ModuleNotFoundError: No module named 'pybind11'",
install `pybind11` before installing vectorbtpro:

=== "uv"

    ```shell
    uv pip install pybind11
    ```

=== "pip"

    ```shell
    pip install pybind11
    ```

* llvmlite:

If you receive the error "Cannot uninstall 'llvmlite'", install `llvmlite`
before installing vectorbtpro:

=== "uv"

    ```shell
    uv pip install --ignore-installed 'llvmlite'
    ```

=== "pip"

    ```shell
    pip install --ignore-installed 'llvmlite'
    ```

* Plotly:

If image generation hangs (for example, when calling `show_svg()`), downgrade the Kaleido package:

=== "uv"

    ```shell
    uv pip install kaleido==0.1.0post1
    ```

=== "pip"

    ```shell
    pip install kaleido==0.1.0post1
    ```

* osqp:

If you are on a Mac and encounter an error during the installation of the osqp package, install `cmake`:

```shell
brew install cmake
```

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#option-1-token
parent: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#authentication
name: 'Option 1: Token'
type: heading 4
---

#### Option 1: Token

After being added as a collaborator and accepting the repository invitation, your next step is to
create a [Personal Access Token] for your GitHub account. This allows you to access the PRO repository
programmatically, such as from the command line or in GitHub Actions workflows:

1. Go to https://github.com/settings/tokens.

2. Click on [Generate a new token (classic)].

3. Enter a name (for example, "terminal").

4. Set the expiration to a fixed number of days.

5. Select the [`repo`][scopes] scope.

6. Generate the token and save it in a secure location.

    [Personal Access Token]: https://docs.github.com/en/github/authenticating-to-github/creating-a-personal-access-token
    [Generate a new token]: https://github.com/settings/tokens/new
    [scopes]: https://docs.github.com/en/developers/apps/scopes-for-oauth-apps#available-scopes

!!! important
    After a few months, GitHub may send you an email notifying you that your personal access
    token has expired. If this happens, simply follow the steps above to generate a new token.
    This is unrelated to your membership status!

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#option-2-credential-manager
parent: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#authentication
name: 'Option 2: Credential Manager'
type: heading 4
---

#### Option 2: Credential Manager

Alternatively, you can use [Git Credential Manager](https://github.com/git-ecosystem/git-credential-manager)
instead of creating a personal access token.

!!! note
    Git Credential Manager only works with HTTPS.

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#git
parent: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#requirements
name: Git
type: heading 3
---

### Git

If you do not have Git, [install it](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#ta-lib
parent: https://vectorbt.pro/pvt_41531c19/getting-started/installation/#requirements
name: TA-Lib
type: heading 3
---

### TA-Lib

To use TA-Lib *locally*, you need to install the actual library.
Follow [these instructions](https://github.com/mrjbq7/ta-lib#dependencies).

!!! tip
    If you encounter issues installing TA-Lib, or if you do not need it for your work, you can also
    install vectorbtpro [without TA-Lib](#without-ta-lib).

---
link: https://vectorbt.pro/pvt_41531c19/features/overview/
parent: https://vectorbt.pro/pvt_41531c19/features/
name: Overview
type: page
---

# Overview

In addition to the [features](https://vectorbt.dev/getting-started/features/) available in the open-source
version, VBT introduces many significant enhancements and optimizations in the following areas:

<div class="grid cards" markdown>

- [:material-chart-bubble:{ .overview }__Analysis__](https://vectorbt.pro/pvt_41531c19/features/analysis/)

- [:material-database:{ .overview }__Data__](https://vectorbt.pro/pvt_41531c19/features/data/)

- [:material-chart-timeline-variant:{ .overview }__Indicators__](https://vectorbt.pro/pvt_41531c19/features/indicators/)

- [:material-brain:{ .overview }__Intelligence__ :material-alert-decagram:{ .mdx-pulse title="Recently added" }](https://vectorbt.pro/pvt_41531c19/features/intelligence/)

- [:material-magnify-expand:{ .overview }__Optimization__](https://vectorbt.pro/pvt_41531c19/features/optimization/)

- [:material-speedometer:{ .overview }__Performance__](https://vectorbt.pro/pvt_41531c19/features/performance/)

- [:material-chart-areaspline:{ .overview }__Portfolio__](https://vectorbt.pro/pvt_41531c19/features/portfolio/)

- [:material-nut:{ .overview }__Productivity__](https://vectorbt.pro/pvt_41531c19/features/productivity/)

</div>

!!! info
    To keep pages concise, only a selection of the most interesting features from each release is highlighted.
    Full release notes are available exclusively to subscribers. If you are on the private website, 
    navigate to _Getting started_ → _Release notes_.

    Tags indicate releases where features were introduced for the first time. Please note that most 
    features are continuously updated, so the following examples are intended to be run with the 
    __latest__ version of VBT installed :writing_hand:

    ```python title="Import required by code examples"
    from vectorbtpro import *  # (1)!
    ```

    1. To view what is imported, call `whats_imported()`.

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Analysis
type: page
icon: material-chart-bubble
---

Analysis features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#simulation-ranges
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Simulation ranges
type: heading 2
---

## Simulation ranges

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_5_15.svg){ loading=lazy }

- [x] Each simulation starts and ends at specific points, usually matching the first and last rows of your
data. You can set a different simulation range beforehand, and now, you can also adjust this range during
the simulation. This flexibility lets you stop the simulation when further processing is unnecessary.
Additionally, the date range is saved in the portfolio object, so all metrics and subplots recognize it.
Processing only the relevant dates speeds up execution and adds a new dimension to your analysis:
isolated time windows :microscope:

**Example 1: Simulate a quick liquidation scenario**

```pycon
>>> @njit
... def post_segment_func_nb(c):
...     value = vbt.pf_nb.get_group_value_nb(c, c.group)
...     if value <= 0:
...         vbt.pf_nb.stop_group_sim_nb(c, c.group)  # (1)!

>>> pf = vbt.PF.from_random_signals(
...     "BTC-USD", 
...     n=10, 
...     seed=42,
...     sim_start="auto",  # (2)!
...     post_segment_func_nb=post_segment_func_nb,
...     leverage=10,
... )
>>> pf.plot_value()  # (3)!
```

1. Stop the simulation of the current group if its value turns negative.

2. Start the simulation at the first signal.

3. Make sure all metrics and subplots use only data up to the liquidation point, even if the portfolio
contains the full original data set (2014 → today).

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/liquidation.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/liquidation.dark.svg#only-dark){: .iimg loading=lazy }

**Example 2: Analyze a date range of an already simulated portfolio**

```pycon
>>> pf = vbt.PF.from_random_signals("BTC-USD", n=10, seed=42)

>>> pf.get_sharpe_ratio(sim_start="2023", sim_end="2024")  # (1)!
1.7846214408154346

>>> pf.get_sharpe_ratio(sim_start="2023", sim_end="2024", rec_sim_range=True)  # (2)!
1.8377982089422782

>>> pf.returns_stats(settings=dict(sim_start="2023", sim_end="2024"))  # (3)!
Start Index                  2023-01-01 00:00:00+00:00
End Index                    2023-12-31 00:00:00+00:00
Total Duration                       365 days 00:00:00
Total Return [%]                             84.715081
Benchmark Return [%]                        155.417419
Annualized Return [%]                        84.715081
Annualized Volatility [%]                     38.49976
Max Drawdown [%]                             20.057773
Max Drawdown Duration                102 days 00:00:00
Sharpe Ratio                                  1.784621
Calmar Ratio                                  4.223554
Omega Ratio                                   1.378076
Sortino Ratio                                 3.059933
Skew                                          -0.39136
Kurtosis                                     13.607937
Tail Ratio                                    1.323376
Common Sense Ratio                            1.823713
Value at Risk                                -0.028314
Alpha                                        -0.103145
Beta                                          0.770428
dtype: object
```

1. Consider only the returns within the specified date range when calculating the Sharpe ratio.
Note that returns may still be affected by data outside the date range (such as open positions).

2. Recursively apply the date range to all metrics that the Sharpe ratio depends on, such as equity,
cash, and orders, treating data outside the range as if it does not exist.

3. Make sure the date range is used consistently for all statistics.

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#expanding-trade-metrics
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Expanding trade metrics
type: heading 2
---

## Expanding trade metrics

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_11_0.svg){ loading=lazy }

- [x] Regular metrics like MAE and MFE only represent the final point of each trade. But what if you want
to see how these metrics develop during the trade? You can now analyze expanding trade metrics as
DataFrames!

**Visualize the expanding MFE using projections**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_random_signals(data, n=50, tp_stop=0.5)
>>> pf.trades.plot_expanding_mfe_returns().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/expanding_mfe_returns.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/expanding_mfe_returns.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#trade-signals
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Trade signals
type: heading 2
---

## Trade signals

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_2.svg){ loading=lazy }

- [x] New trade plotting method that separates entry and exit trades into long entries, long exits,
short entries, and short exits. It supports different styles for positions.

**Plot trade signals of a Bollinger Bands strategy**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> bb = data.run("bbands")
>>> long_entries = data.hlc3.vbt.crossed_above(bb.upper) & (bb.bandwidth < 0.1)
>>> long_exits = data.hlc3.vbt.crossed_below(bb.upper) & (bb.bandwidth > 0.5)
>>> short_entries = data.hlc3.vbt.crossed_below(bb.lower) & (bb.bandwidth < 0.1)
>>> short_exits = data.hlc3.vbt.crossed_above(bb.lower) & (bb.bandwidth > 0.5)
>>> pf = vbt.PF.from_signals(
...     data, 
...     long_entries=long_entries, 
...     long_exits=long_exits, 
...     short_entries=short_entries, 
...     short_exits=short_exits
... )
>>> pf.plot_trade_signals().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/trade_signals.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/trade_signals.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#edge-ratio
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Edge ratio
type: heading 2
---

## Edge ratio

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_1.svg){ loading=lazy }

- [x] [Edge ratio](https://www.buildalpha.com/eratio/) is a unique metric for quantifying entry
profitability. Unlike most performance metrics, the edge ratio accounts for both open profits and
losses. This can help you find better trade exits.

**Compare the edge ratio of an EMA crossover to a random strategy**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> fast_ema = data.run("ema", 10, hide_params=True)
>>> slow_ema = data.run("ema", 20, hide_params=True)
>>> entries = fast_ema.real_crossed_above(slow_ema)
>>> exits = fast_ema.real_crossed_below(slow_ema)
>>> pf = vbt.PF.from_signals(data, entries, exits, direction="both")
>>> rand_pf = vbt.PF.from_random_signals(data, n=pf.orders.count() // 2)  # (1)!
>>> fig = pf.trades.plot_running_edge_ratio(
...     trace_kwargs=dict(line_color="limegreen", name="Edge Ratio (S)")
... )
>>> fig = rand_pf.trades.plot_running_edge_ratio(
...     trace_kwargs=dict(line_color="mediumslateblue", name="Edge Ratio (R)"),
...     fig=fig
... )
>>> fig.show()
```

1. The random strategy should have a similar number of orders to allow comparison.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/running_edge_ratio.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/running_edge_ratio.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#trade-history
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Trade history
type: heading 2
---

## Trade history

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_1.svg){ loading=lazy }

- [x] Trade history is a human-readable DataFrame listing orders, extended with useful details about
entry trades, exit trades, and positions.

**Get the trade history of a random portfolio with one signal**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], missing_index="drop")
>>> pf = vbt.PF.from_random_signals(
...     data, 
...     n=1,
...     run_kwargs=dict(hide_params=True),
...     tp_stop=0.5, 
...     sl_stop=0.1
... )
>>> pf.trade_history
    Order Id   Column              Signal Index            Creation Index  \
0         0  BTC-USD 2016-02-20 00:00:00+00:00 2016-02-20 00:00:00+00:00   
1         1  BTC-USD 2016-02-20 00:00:00+00:00 2016-06-12 00:00:00+00:00   
2         0  ETH-USD 2019-05-25 00:00:00+00:00 2019-05-25 00:00:00+00:00   
3         1  ETH-USD 2019-05-25 00:00:00+00:00 2019-07-15 00:00:00+00:00   

                  Fill Index  Side    Type Stop Type      Size       Price  \
0 2016-02-20 00:00:00+00:00   Buy  Market      None  0.228747  437.164001   
1 2016-06-12 00:00:00+00:00  Sell  Market        TP  0.228747  655.746002   
2 2019-05-25 00:00:00+00:00   Buy  Market      None  0.397204  251.759872   
3 2019-07-15 00:00:00+00:00  Sell  Market        SL  0.397204  226.583885   

    Fees   PnL  Return Direction  Status  Entry Trade Id  Exit Trade Id  \
0   0.0  50.0     0.5      Long  Closed               0             -1   
1   0.0  50.0     0.5      Long  Closed              -1              0   
2   0.0 -10.0    -0.1      Long  Closed               0             -1   
3   0.0 -10.0    -0.1      Long  Closed              -1              0   

    Position Id  
0            0  
1            0  
2            0  
3            0  
```

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#patterns
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Patterns
type: heading 2
---

## Patterns

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_5_0.svg){ loading=lazy }

- [x] Patterns are distinctive formations created by price movements on a chart and are central to
technical analysis. There are now new dedicated functions and classes for detecting patterns of any
complexity in any type of time series data. The idea is simple: fit a pattern to align with the
scale and period of your selected data window, then compute the element-wise distance between
them to get a single similarity score. You can adjust the threshold for this score to decide above
which value a data window should be marked as "matched." Thanks to Numba, this operation can be
performed hundreds of thousands of times per second! :mag_right:

!!! example "Tutorial"
    Learn more in the [Patterns and projections](https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections) tutorial.

**Find and plot a descending triangle pattern**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> data.hlc3.vbt.find_pattern(
...     pattern=[5, 1, 3, 1, 2, 1],
...     window=100,
...     max_window=700,
... ).loc["2017":"2019"].plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/patterns.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/patterns.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#projections
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Projections
type: heading 2
---

## Projections

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_5_0.svg){ loading=lazy }

- [x] There are cleaner ways to analyze events and their impact on price than conventional backtesting.
Meet projections! :wave: Not only can they help you assess event performance visually and
quantitatively, but they can also project events into the future to support trading. This is done
by extracting the price range after each event, collecting all these price ranges into a multidimensional
array, and then deriving confidence intervals and other useful statistics from that array. When
combined with patterns, these tools are a quantitative analyst's dream! :stars:

!!! example "Tutorial"
    Learn more in the [Patterns and projections](https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections) tutorial.

**Find occurrences of the price moving similarly to the last week and project them**

```pycon
>>> data = vbt.YFData.pull("ETH-USD")
>>> pattern_ranges = data.hlc3.vbt.find_pattern(
...     pattern=data.close.iloc[-7:],
...     rescale_mode="rebase"
... )
>>> delta_ranges = pattern_ranges.with_delta(7)
>>> fig = data.iloc[-7:].plot(plot_volume=False)
>>> delta_ranges.plot_projections(fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/projections.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#mae-and-mfe
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: MAE and MFE
type: heading 2
---

## MAE and MFE

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] [Maximum Adverse Excursion (MAE)](https://analyzingalpha.com/maximum-adverse-excursion)
helps you see the maximum loss taken during a trade, also known as the maximum drawdown of the
position. [Maximum Favorable Excursion (MFE)](https://analyzingalpha.com/maximum-favorable-excursion)
shows the highest profit reached during a trade. Analyzing MAE and MFE statistics can help you
improve your exit strategies.

**Analyze the MAE of a random portfolio without SL**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_random_signals(data, n=50)
>>> pf.trades.plot_mae_returns().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/mae_without_sl.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/mae_without_sl.dark.svg#only-dark){: .iimg loading=lazy }

**Analyze the MAE of a random portfolio with SL**

```pycon
>>> pf = vbt.PF.from_random_signals(data, n=50, sl_stop=0.1)
>>> pf.trades.plot_mae_returns().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/mae_with_sl.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/mae_with_sl.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#ohlc-native-classes
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: OHLC-native classes
type: heading 2
---

## OHLC-native classes

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] Previously, OHLC data was used for simulation, but only the close price was analyzed.
Now, most classes let you track all OHLC data for more accurate quantitative and qualitative analysis.

**Plot trades of a random portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020-01", end="2020-03")
>>> pf = vbt.PF.from_random_signals(
...     open=data.open,
...     high=data.high,
...     low=data.low,
...     close=data.close,
...     n=10
... )
>>> pf.trades.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/ohlc_native_classes.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/ohlc_native_classes.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#benchmark
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: Benchmark
type: heading 2
---

## Benchmark

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_4.svg){ loading=lazy }

- [x] The benchmark can now be easily set for your entire portfolio.

**Compare Microsoft to S&P 500**

```pycon
>>> data = vbt.YFData.pull(["SPY", "MSFT"], start="2010", missing_columns="drop")

>>> pf = vbt.PF.from_holding(
...     close=data.data["MSFT"]["Close"],
...     bm_close=data.data["SPY"]["Close"]
... )
>>> pf.plot_cumulative_returns().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/benchmark.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/benchmark.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/analysis/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/analysis/#analysis
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/analysis.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/data/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Data
type: page
icon: material-database
---

Data features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#findatapy
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: FinDataPy
type: heading 2
---

## FinDataPy

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_8_20.svg){ loading=lazy }

- [x] Introducing a new class for [findatapy](https://github.com/cuemacro/findatapy), designed to 
pull data from Bloomberg, Eikon, Quandl, Dukascopy, and other data sources.

**Discover tickers on Dukascopy and pull one day of tick data**

```pycon
>>> vbt.FinPyData.list_symbols(data_source="dukascopy")
['fx.dukascopy.tick.NYC.AUDCAD',
  'fx.dukascopy.tick.NYC.AUDCHF',
  'fx.dukascopy.tick.NYC.AUDJPY',
  ...
  'fx.dukascopy.tick.NYC.USDTRY',
  'fx.dukascopy.tick.NYC.USDZAR',
  'fx.dukascopy.tick.NYC.ZARJPY']
  
>>> data = vbt.FinPyData.pull(  # (1)!
...     "fx.dukascopy.tick.NYC.EURUSD",
...     start="14 Jun 2016",
...     end="15 Jun 2016"
... )
>>> data.get()
                                      close
Date                                      
2016-06-14 00:00:00.844000+00:00  1.128795
2016-06-14 00:00:01.591000+00:00  1.128790
2016-06-14 00:00:01.743000+00:00  1.128775
2016-06-14 00:00:02.464000+00:00  1.128770
2016-06-14 00:00:02.971000+00:00  1.128760
...                                    ...
2016-06-14 23:59:57.733000+00:00  1.121020
2016-06-14 23:59:58.239000+00:00  1.121030
2016-06-14 23:59:58.953000+00:00  1.121035
2016-06-14 23:59:59.004000+00:00  1.121050
2016-06-14 23:59:59.934000+00:00  1.121055

[82484 rows x 1 columns]

>>> data = vbt.FinPyData.pull(  # (2)!
...     "EURUSD",
...     start="14 Jun 2016",
...     end="15 Jun 2016",
...     timeframe="tick",
...     category="fx",
...     data_source="dukascopy",
...     fields=["bid", "ask", "bidv", "askv"]
... )
>>> data.get()
                                      bid      ask  bidv   askv
Date                                                           
2016-06-14 00:00:00.844000+00:00  1.12877  1.12882  1.00  10.12
2016-06-14 00:00:01.591000+00:00  1.12877  1.12881  1.00   1.00
2016-06-14 00:00:01.743000+00:00  1.12875  1.12880  3.11   3.00
2016-06-14 00:00:02.464000+00:00  1.12875  1.12879  2.21   1.00
2016-06-14 00:00:02.971000+00:00  1.12875  1.12877  2.21   1.00
...                                   ...      ...   ...    ...
2016-06-14 23:59:57.733000+00:00  1.12100  1.12104  1.24   1.50
2016-06-14 23:59:58.239000+00:00  1.12101  1.12105  9.82   1.12
2016-06-14 23:59:58.953000+00:00  1.12102  1.12105  1.50   1.12
2016-06-14 23:59:59.004000+00:00  1.12103  1.12107  1.50   1.12
2016-06-14 23:59:59.934000+00:00  1.12103  1.12108  1.87   2.25

[82484 rows x 4 columns]
```

1. String format.

2. Keyword format.

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#databento
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Databento
type: heading 2
---

## Databento

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_12_23.svg){ loading=lazy }

- [x] Introducing a new class specialized in pulling data from [Databento](https://databento.com/).

**Get best bid and offer (BBO) data from Databento**

```pycon
>>> vbt.BentoData.set_custom_settings(
...     client_config=dict(
...         key="YOUR_KEY"
...     )
... )
>>> params = dict(
...     symbols="ESH3",
...     dataset="GLBX.MDP3",
...     start=vbt.timestamp("2022-10-28 20:30:00"),
...     end=vbt.timestamp("2022-10-28 21:00:00"),
...     schema="tbbo"
... )
>>> vbt.BentoData.get_cost(**params)
1.2002885341644287e-05

>>> data = vbt.BentoData.pull(**params)
>>> data.get()
                                                                ts_event  \
ts_recv                                                                   
2022-10-28 20:30:59.047138053+00:00 2022-10-28 20:30:59.046914657+00:00   
2022-10-28 20:37:53.112494436+00:00 2022-10-28 20:37:53.112246421+00:00   
...
2022-10-28 20:59:15.075191111+00:00 2022-10-28 20:59:15.074953895+00:00   
2022-10-28 20:59:34.607239899+00:00 2022-10-28 20:59:34.606984277+00:00   

                                      rtype  publisher_id  instrument_id  \
ts_recv                                                                   
2022-10-28 20:30:59.047138053+00:00      1             1         206299   
2022-10-28 20:37:53.112494436+00:00      1             1         206299   
...
2022-10-28 20:59:15.075191111+00:00      1             1         206299   
2022-10-28 20:59:34.607239899+00:00      1             1         206299   

                                    action side  depth    price  size  flags  \
ts_recv                                                                        
2022-10-28 20:30:59.047138053+00:00      T    B      0  3955.25     1      0   
2022-10-28 20:37:53.112494436+00:00      T    A      0  3955.00     1      0   
...
2022-10-28 20:59:15.075191111+00:00      T    A      0  3953.75     1      0   
2022-10-28 20:59:34.607239899+00:00      T    A      0  3954.50     2      0   

                                      ts_in_delta  sequence  bid_px_00  \
ts_recv                                                                 
2022-10-28 20:30:59.047138053+00:00        18553  73918214    3954.75   
2022-10-28 20:37:53.112494436+00:00        18334  73926240    3955.00   
...
2022-10-28 20:59:15.075191111+00:00        19294  73945515    3953.75   
2022-10-28 20:59:34.607239899+00:00        18701  73945932    3954.50   

                                      ask_px_00  bid_sz_00  ask_sz_00  \
ts_recv                                                                
2022-10-28 20:30:59.047138053+00:00    3955.25          1          1   
2022-10-28 20:37:53.112494436+00:00    3955.75          1          1   
...
2022-10-28 20:59:15.075191111+00:00    3956.00          1          1   
2022-10-28 20:59:34.607239899+00:00    3956.00          4          1   

                                      bid_ct_00  ask_ct_00 symbol  
ts_recv                                                           
2022-10-28 20:30:59.047138053+00:00          1          1   ESH3  
2022-10-28 20:37:53.112494436+00:00          1          1   ESH3  
...
2022-10-28 20:59:15.075191111+00:00          1          1   ESH3  
2022-10-28 20:59:34.607239899+00:00          3          1   ESH3  
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#sql-queries
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: SQL queries
type: heading 2
---

## SQL queries

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_10_22.svg){ loading=lazy }

- [x] Thanks to [DuckDB](https://duckdb.org/), you can now run SQL queries directly on data instances.

**Run a rolling average of 14 days on minute data using SQL**

```pycon
>>> data = vbt.TVData.pull(
...     "AAPL",
...     exchange="NASDAQ",
...     timeframe="1 minute",
...     tz="America/New_York"
... )

>>> data.sql("""
...     SELECT datetime, AVG(Close) OVER(
...         ORDER BY "datetime" ASC
...         RANGE BETWEEN INTERVAL 14 DAYS PRECEDING AND CURRENT ROW
...     ) AS "Moving Average"
...     FROM "AAPL";
... """)
datetime
2023-09-11 09:30:00-04:00    180.080000
2023-09-11 09:31:00-04:00    179.965000
2023-09-11 09:32:00-04:00    180.000000
2023-09-11 09:33:00-04:00    180.022500
2023-09-11 09:34:00-04:00    179.984000
                                    ...
2023-10-20 15:55:00-04:00    177.786669
2023-10-20 15:56:00-04:00    177.785492
2023-10-20 15:57:00-04:00    177.784322
2023-10-20 15:58:00-04:00    177.783166
2023-10-20 15:59:00-04:00    177.781986
Name: Moving Average, Length: 11700, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#duckdb
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: DuckDB
type: heading 2
---

## DuckDB

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_10_22.svg){ loading=lazy }

- [x] [DuckDB](https://duckdb.org/) is a high-performance analytical database system that offers
a robust SQL dialect for interacting with various data stores. Not only can it run analytical queries on 
local data, even if the data does not fit into memory and without needing a distributed framework, 
but it can also query CSV, Parquet, and JSON files directly.

**Save minute data to a DuckDB database, and read one day**

```pycon
>>> data = vbt.TVData.pull(
...     "AAPL",
...     exchange="NASDAQ",
...     timeframe="1 minute",
...     tz="America/New_York"
... )

>>> URL = "database.duckdb"
>>> data.to_duckdb(connection=URL)

>>> day_data = vbt.DuckDBData.pull(
...     "AAPL",
...     start="2023-10-02 09:30:00",
...     end="2023-10-02 16:00:00",
...     tz="America/New_York",
...     connection=URL
... )
>>> day_data.get()
                              Open    High     Low   Close    Volume
datetime
2023-10-02 09:30:00-04:00  171.260  171.34  170.93  171.10   61654.0
2023-10-02 09:31:00-04:00  171.130  172.37  171.13  172.30   53481.0
2023-10-02 09:32:00-04:00  172.310  172.64  172.16  172.64   44750.0
2023-10-02 09:33:00-04:00  172.640  172.97  172.54  172.78   53195.0
2023-10-02 09:34:00-04:00  172.780  173.07  172.75  173.00   47416.0
...                            ...     ...     ...     ...       ...
2023-10-02 15:55:00-04:00  173.300  173.51  173.26  173.51   61619.0
2023-10-02 15:56:00-04:00  173.525  173.59  173.42  173.43   45066.0
2023-10-02 15:57:00-04:00  173.430  173.55  173.42  173.50   45220.0
2023-10-02 15:58:00-04:00  173.510  173.60  173.46  173.56   47371.0
2023-10-02 15:59:00-04:00  173.560  173.78  173.56  173.75  161253.0

[390 rows x 5 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#sqlalchemy
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: SQLAlchemy
type: heading 2
---

## SQLAlchemy

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_10_22.svg){ loading=lazy }

- [x] [SQLAlchemy](https://www.sqlalchemy.org/) provides a standard interface that allows developers 
to create database-agnostic code for communicating with a wide range of SQL database engines.
With its help, you can now easily store data in SQL databases and read from them as well.

**Save minute data to a PostgreSQL database, and read one day**

```pycon
>>> data = vbt.TVData.pull(
...     "AAPL",
...     exchange="NASDAQ",
...     timeframe="1 minute",
...     tz="America/New_York"
... )

>>> URL = "postgresql://postgres:postgres@localhost:5432"
>>> data.to_sql(engine=URL)

>>> day_data = vbt.SQLData.pull(
...     "AAPL",
...     start="2023-10-02 09:30:00",
...     end="2023-10-02 16:00:00",
...     tz="America/New_York",
...     engine=URL
... )
>>> day_data.get()
                              Open    High     Low   Close    Volume
datetime
2023-10-02 09:30:00-04:00  171.260  171.34  170.93  171.10   61654.0
2023-10-02 09:31:00-04:00  171.130  172.37  171.13  172.30   53481.0
2023-10-02 09:32:00-04:00  172.310  172.64  172.16  172.64   44750.0
2023-10-02 09:33:00-04:00  172.640  172.97  172.54  172.78   53195.0
2023-10-02 09:34:00-04:00  172.780  173.07  172.75  173.00   47416.0
...                            ...     ...     ...     ...       ...
2023-10-02 15:55:00-04:00  173.300  173.51  173.26  173.51   61619.0
2023-10-02 15:56:00-04:00  173.525  173.59  173.42  173.43   45066.0
2023-10-02 15:57:00-04:00  173.430  173.55  173.42  173.50   45220.0
2023-10-02 15:58:00-04:00  173.510  173.60  173.46  173.56   47371.0
2023-10-02 15:59:00-04:00  173.560  173.78  173.56  173.75  161253.0

[390 rows x 5 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#pyarrow-fastparquet
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: PyArrow & FastParquet
type: heading 2
---

## PyArrow & FastParquet

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_10_22.svg){ loading=lazy }

- [x] Data can be written to Feather with PyArrow and to Parquet using PyArrow or FastParquet.
Parquet performs especially well in write-once, read-many scenarios, providing highly efficient data
compression and decompression, making it a great choice for storing time series data.

**Save minute data to a Parquet dataset partitioned by day, and read one day**

```pycon
>>> data = vbt.TVData.pull(
...     "AAPL",
...     exchange="NASDAQ",
...     timeframe="1 minute",
...     tz="America/New_York"
... )

>>> data.to_parquet(partition_by="day")  # (1)!

>>> day_data = vbt.ParquetData.pull("AAPL", filters=[("group", "==", "2023-10-02")])
>>> day_data.get()
                              Open    High     Low   Close    Volume
datetime
2023-10-02 09:30:00-04:00  171.260  171.34  170.93  171.10   61654.0
2023-10-02 09:31:00-04:00  171.130  172.37  171.13  172.30   53481.0
2023-10-02 09:32:00-04:00  172.310  172.64  172.16  172.64   44750.0
2023-10-02 09:33:00-04:00  172.640  172.97  172.54  172.78   53195.0
2023-10-02 09:34:00-04:00  172.780  173.07  172.75  173.00   47416.0
...                            ...     ...     ...     ...       ...
2023-10-02 15:55:00-04:00  173.300  173.51  173.26  173.51   61619.0
2023-10-02 15:56:00-04:00  173.525  173.59  173.42  173.43   45066.0
2023-10-02 15:57:00-04:00  173.430  173.55  173.42  173.50   45220.0
2023-10-02 15:58:00-04:00  173.510  173.60  173.46  173.56   47371.0
2023-10-02 15:59:00-04:00  173.560  173.78  173.56  173.75  161253.0

[390 rows x 5 columns]
```

1. Without `partition_by`, the data will be saved to a single Parquet file. You will still be able to
filter rows by any column in newer versions of PyArrow and Pandas.

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#feature-oriented-data
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Feature-oriented data
type: heading 2
---

## Feature-oriented data

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_14_0.svg){ loading=lazy }

- [x] The main limitation of VBT's data class was that it could only store data in a
symbol-oriented format. This meant features such as OHLC had to be combined into a single DataFrame
beforehand. This approach can be somewhat counterproductive, as in VBT, we typically work with these
features separately. For example, when calling `data.close`, VBT scans for "close" columns across 
all symbols, extracts them, and concatenates them into another DataFrame. To address this, the data
class has been redesigned to natively support storing feature-oriented data as well.

**Create a feature-oriented data instance from various portfolio time series**

```pycon
>>> data = vbt.YFData.pull(["AAPL", "MSFT", "GOOG"])
>>> pf = data.run("from_random_signals", n=[10, 20, 30])

>>> pf_data = vbt.Data.from_data(
...     vbt.feature_dict({
...         "cash": pf.cash,
...         "assets": pf.assets,
...         "asset_value": pf.asset_value,
...         "value": pf.value
...     })
... )
>>> pf_data.get(feature="cash", symbol=(10, "AAPL"))
Date
1980-12-12 05:00:00+00:00      100.000000
1980-12-15 05:00:00+00:00      100.000000
1980-12-16 05:00:00+00:00      100.000000
1980-12-17 05:00:00+00:00      100.000000
1980-12-18 05:00:00+00:00      100.000000
                                      ...
2023-08-25 04:00:00+00:00    81193.079771
2023-08-28 04:00:00+00:00    81193.079771
2023-08-29 04:00:00+00:00    81193.079771
2023-08-30 04:00:00+00:00    81193.079771
2023-08-31 04:00:00+00:00    81193.079771
Name: (10, AAPL), Length: 10770, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#parallel-data
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Parallel data
type: heading 2
---

## Parallel data

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_10_0.svg){ loading=lazy }

- [x] Data fetching and updating can be easily parallelized.

**Benchmark fetching multiple symbols serially and concurrently**

```pycon
>>> symbols = ["SPY", "TLT", "XLF", "XLE", "XLU", "XLK", "XLB", "XLP", "XLY", "XLI", "XLV"]

>>> with vbt.Timer() as timer:
...     data = vbt.YFData.pull(symbols)
>>> print(timer.elapsed())
4.52 seconds

>>> with vbt.Timer() as timer:
...     data = vbt.YFData.pull(symbols, execute_kwargs=dict(engine="threadpool"))
>>> print(timer.elapsed())
918.54 milliseconds
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#trading-view
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Trading View
type: heading 2
---

## Trading View

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_9_0.svg){ loading=lazy }

- [x] A new class specialized for pulling data from [TradingView](https://www.tradingview.com/) is
now available.

**Pull 1-minute AAPL data**

```pycon
>>> data = vbt.TVData.pull(
...     "NASDAQ:AAPL",
...     timeframe="1 minute",
...     tz="US/Eastern"
... )
>>> data.get()
                              Open    High     Low   Close   Volume
datetime
2022-12-05 09:30:00-05:00  147.75  148.31  147.50  148.28  37769.0
2022-12-05 09:31:00-05:00  148.28  148.67  148.28  148.49  10525.0
2022-12-05 09:32:00-05:00  148.50  148.73  148.30  148.30   4860.0
2022-12-05 09:33:00-05:00  148.25  148.73  148.25  148.64   5306.0
2022-12-05 09:34:00-05:00  148.62  148.97  148.52  148.97   5808.0
...                           ...     ...     ...     ...      ...
2023-01-17 15:55:00-05:00  135.80  135.91  135.80  135.86  37573.0
2023-01-17 15:56:00-05:00  135.85  135.88  135.80  135.88  18796.0
2023-01-17 15:57:00-05:00  135.88  135.93  135.85  135.91  21019.0
2023-01-17 15:58:00-05:00  135.90  135.97  135.89  135.95  20934.0
2023-01-17 15:59:00-05:00  135.94  136.00  135.84  135.94  86696.0

[11310 rows x 5 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#symbol-search
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Symbol search
type: heading 2
---

## Symbol search

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] Most data classes can retrieve the complete list of symbols available for an exchange, and
optionally filter the list using a [globbing](https://en.wikipedia.org/wiki/Glob_(programming)) or
[regular expression](https://en.wikipedia.org/wiki/Regular_expression) pattern. This also works for
local data classes.

**Get all XRP pairs listed on Binance**

```pycon
>>> vbt.BinanceData.list_symbols("XRP*")
{'XRPAUD',
  'XRPBEARBUSD',
  'XRPBEARUSDT',
  'XRPBIDR',
  'XRPBKRW',
  'XRPBNB',
  'XRPBRL',
  'XRPBTC',
  'XRPBULLBUSD',
  'XRPBULLUSDT',
  'XRPBUSD',
  'XRPDOWNUSDT',
  'XRPETH',
  'XRPEUR',
  'XRPGBP',
  'XRPNGN',
  'XRPPAX',
  'XRPRUB',
  'XRPTRY',
  'XRPTUSD',
  'XRPUPUSDT',
  'XRPUSDC',
  'XRPUSDT'}
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#symbol-classes
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Symbol classes
type: heading 2
---

## Symbol classes

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] Since VBT leverages multi-indexes in Pandas, you can associate each symbol with one or more
classes, such as sectors. This allows you to analyze the performance of a trading strategy relative
to each class.

**Compare equal-weighted portfolios for three sectors**

```pycon
>>> classes = vbt.symbol_dict({
...     "MSFT": dict(sector="Technology"),
...     "GOOGL": dict(sector="Technology"),
...     "META": dict(sector="Technology"),
...     "JPM": dict(sector="Finance"),
...     "BAC": dict(sector="Finance"),
...     "WFC": dict(sector="Finance"),
...     "AMZN": dict(sector="Retail"),
...     "WMT": dict(sector="Retail"),
...     "BABA": dict(sector="Retail"),
... })
>>> data = vbt.YFData.pull(
...     list(classes.keys()),
...     classes=classes,
...     missing_index="drop"
... )
>>> pf = vbt.PF.from_orders(
...     data,
...     size=vbt.index_dict({0: 1 / 3}),  # (1)!
...     size_type="targetpercent",
...     group_by="sector",
...     cash_sharing=True
... )
>>> pf.value.vbt.plot().show()
```

1. There are three assets in each group. Allocate 33.3% to each asset at the first bar.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/classes.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/classes.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#runnable-data
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Runnable data
type: heading 2
---

## Runnable data

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_5_0.svg){ loading=lazy }

- [x] Tired of figuring out which arguments are required by an indicator? Data instances can now
recognize the arguments of any indicator or function, map them to column names, and run the
function by passing in the required columns. You can also change the mapping, override indicator
parameters, and query indicators by name. The data instance will search all integrated indicator
packages and return the first (and best) match it finds.

**Run Stochastic RSI by data**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> stochrsi = data.run("stochrsi")
>>> stochrsi.fastd
Date
2014-09-17 00:00:00+00:00          NaN
2014-09-18 00:00:00+00:00          NaN
2014-09-19 00:00:00+00:00          NaN
2014-09-20 00:00:00+00:00          NaN
2014-09-21 00:00:00+00:00          NaN
                                    ...
2023-01-15 00:00:00+00:00    96.168788
2023-01-16 00:00:00+00:00    91.733393
2023-01-17 00:00:00+00:00    78.295255
2023-01-18 00:00:00+00:00    48.793133
2023-01-20 00:00:00+00:00    26.242474
Name: Close, Length: 3047, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#data-transformation
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Data transformation
type: heading 2
---

## Data transformation

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_3.svg){ loading=lazy }

- [x] After fetching data, how do you change it? There is a new method that puts all symbols into a
single DataFrame and passes it to a UDF for transformation.

**Remove weekends**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], start="2020-01-01", end="2020-01-14")
>>> new_data = data.transform(lambda df: df[~df.index.weekday.isin([5, 6])])
>>> new_data.close
symbol                         BTC-USD     ETH-USD
Date
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-06 00:00:00+00:00  7769.219238  144.304153
2020-01-07 00:00:00+00:00  8163.692383  143.543991
2020-01-08 00:00:00+00:00  8079.862793  141.258133
2020-01-09 00:00:00+00:00  7879.071289  138.979202
2020-01-10 00:00:00+00:00  8166.554199  143.963776
2020-01-13 00:00:00+00:00  8144.194336  144.226593
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#synthetic-ohlc
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Synthetic OHLC
type: heading 2
---

## Synthetic OHLC

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_1.svg){ loading=lazy }

- [x] New basic models are available for generating synthetic OHLC data.
These are especially useful for leakage detection.

**Generate 3 months of synthetic data using Geometric Brownian Motion**

```pycon
>>> data = vbt.GBMOHLCData.pull("R", start="2022-01", end="2022-04")
>>> data.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/synthetic_ohlc.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/synthetic_ohlc.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#data-saver
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Data saver
type: heading 2
---

## Data saver

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_1.svg){ loading=lazy }

- [x] Imagine a script that can periodically pull the latest data from an exchange and save it to
disk, all without your intervention. VBT implements two classes that can do exactly this: one that
saves to CSV and another that saves to HDF.

**BTCUSDT_1m_saver.py**

```python
from vectorbtpro import *

import logging
logging.basicConfig(level=logging.INFO)

if __name__ == "__main__":
    if vbt.CSVDataSaver.file_exists():
        csv_saver = vbt.CSVDataSaver.load()
        csv_saver.update()
        init_save = False
    else:
        data = vbt.BinanceData.pull(
            "BTCUSDT",
            start="10 minutes ago UTC",
            timeframe="1 minute"
        )
        csv_saver = vbt.CSVDataSaver(data)
        init_save = True
    csv_saver.update_every(1, "minute", init_save=init_save)
    csv_saver.save()  # (1)!
```

1. The CSV data saver only stores the latest data update, which serves as the starting point for the
next update. Be sure to save it and re-use it in the next runtime.

**Run in console and then interrupt**

```console
$ python BTCUSDT_1m_saver.py
2023-02-01 12:26:36.744000+00:00 - 2023-02-01 12:36:00+00:00: : 1it [00:01,  1.22s/it]
INFO:vectorbtpro.data.saver:Saved initial 10 rows from 2023-02-01 12:27:00+00:00 to 2023-02-01 12:36:00+00:00
INFO:vectorbtpro.utils.schedule_:Starting schedule manager with jobs [Every 1 minute do update(save_kwargs=None) (last run: [never], next run: 2023-02-01 13:37:38)]
INFO:vectorbtpro.data.saver:Saved 2 rows from 2023-02-01 12:36:00+00:00 to 2023-02-01 12:37:00+00:00
INFO:vectorbtpro.data.saver:Saved 2 rows from 2023-02-01 12:37:00+00:00 to 2023-02-01 12:38:00+00:00
INFO:vectorbtpro.utils.schedule_:Stopping schedule manager
```

**Run in console again to continue**

```console
$ python BTCUSDT_1m_saver.py
INFO:vectorbtpro.utils.schedule_:Starting schedule manager with jobs [Every 1 minute do update(save_kwargs=None) (last run: [never], next run: 2023-02-01 13:42:08)]
INFO:vectorbtpro.data.saver:Saved 5 rows from 2023-02-01 12:38:00+00:00 to 2023-02-01 12:42:00+00:00
INFO:vectorbtpro.utils.schedule_:Stopping schedule manager
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#polygonio
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Polygon.io
type: heading 2
---

## Polygon.io

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_1.svg){ loading=lazy }

- [x] Welcome a new class specialized in pulling data from [Polygon.io](https://polygon.io/)!

**Get one month of 30-minute AAPL data from Polygon.io**

```pycon
>>> vbt.PolygonData.set_custom_settings(
...     client_config=dict(
...         api_key="YOUR_API_KEY"
...     )
... )
>>> data = vbt.PolygonData.pull(
...     "AAPL",
...     start="2022-12-01",  # (1)!
...     end="2023-01-01",
...     timeframe="30 minutes",
...     tz="US/Eastern"
... )
>>> data.get()
                              Open    High     Low     Close   Volume  \
Open time                                                              
2022-12-01 04:00:00-05:00  148.08  148.08  147.04  147.3700  50886.0   
2022-12-01 04:30:00-05:00  147.37  147.37  147.12  147.2600  16575.0   
2022-12-01 05:00:00-05:00  147.31  147.51  147.20  147.3800  20753.0   
2022-12-01 05:30:00-05:00  147.43  147.56  147.38  147.3800   7388.0   
2022-12-01 06:00:00-05:00  147.30  147.38  147.24  147.2400   7416.0   
...                           ...     ...     ...       ...      ...   
2022-12-30 17:30:00-05:00  129.94  130.05  129.91  129.9487  35694.0   
2022-12-30 18:00:00-05:00  129.95  130.00  129.94  129.9500  15595.0   
2022-12-30 18:30:00-05:00  129.94  130.05  129.94  130.0100  20287.0   
2022-12-30 19:00:00-05:00  129.99  130.04  129.99  130.0000  12490.0   
2022-12-30 19:30:00-05:00  130.00  130.04  129.97  129.9700  28271.0   

                            Trade count      VWAP  
Open time                                         
2022-12-01 04:00:00-05:00         1024  147.2632  
2022-12-01 04:30:00-05:00          412  147.2304  
2022-12-01 05:00:00-05:00          306  147.3466  
2022-12-01 05:30:00-05:00          201  147.4818  
2022-12-01 06:00:00-05:00          221  147.2938  
...                                ...       ...  
2022-12-30 17:30:00-05:00          350  129.9672  
2022-12-30 18:00:00-05:00          277  129.9572  
2022-12-30 18:30:00-05:00          312  130.0034  
2022-12-30 19:00:00-05:00          176  130.0140  
2022-12-30 19:30:00-05:00          366  129.9941  

[672 rows x 7 columns]
```

1. In the timezone provided via `tz`.

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#alpha-vantage
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Alpha Vantage
type: heading 2
---

## Alpha Vantage

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_1.svg){ loading=lazy }

- [x] Welcome a new class specialized in pulling data from [Alpha Vantage](https://www.alphavantage.co/)!

**Get Stochastic RSI of IBM from Alpha Vantage**

```pycon
>>> data = vbt.AVData.pull(
...     "IBM",
...     category="technical-indicators",
...     function="STOCHRSI",
...     params=dict(fastkperiod=14)
... )
>>> data.get()
                              FastD     FastK
1999-12-07 00:00:00+00:00  100.0000  100.0000
1999-12-08 00:00:00+00:00  100.0000  100.0000
1999-12-09 00:00:00+00:00   77.0255   31.0765
1999-12-10 00:00:00+00:00   43.6922    0.0000
1999-12-13 00:00:00+00:00   12.0197    4.9826
...                             ...       ...
2023-01-26 00:00:00+00:00   11.7960    0.0000
2023-01-27 00:00:00+00:00    3.7773    0.0000
2023-01-30 00:00:00+00:00    4.4824   13.4471
2023-01-31 00:00:00+00:00    7.8258   10.0302
2023-02-01 16:00:01+00:00   13.0966   15.8126

[5826 rows x 2 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#nasdaq-data-link
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Nasdaq Data Link
type: heading 2
---

## Nasdaq Data Link

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_1.svg){ loading=lazy }

- [x] Welcome a new class specialized in pulling data from [Nasdaq Data Link](https://data.nasdaq.com/)!

**Get Index of Consumer Sentiment from Nasdaq Data Link**

```pycon
>>> data = vbt.NDLData.pull("UMICH/SOC1")
>>> data.get()
                            Index
Date                            
1952-11-30 00:00:00+00:00   86.2
1953-02-28 00:00:00+00:00   90.7
1953-08-31 00:00:00+00:00   80.8
1953-11-30 00:00:00+00:00   80.7
1954-02-28 00:00:00+00:00   82.0
...                          ...
2022-08-31 00:00:00+00:00   58.2
2022-09-30 00:00:00+00:00   58.6
2022-10-31 00:00:00+00:00   59.9
2022-11-30 00:00:00+00:00   56.8
2022-12-31 00:00:00+00:00   59.7

[632 rows x 1 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#data-merging
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Data merging
type: heading 2
---

## Data merging

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_0.svg){ loading=lazy }

- [x] Often, there is a need to backtest symbols from different exchanges by placing them in the 
same basket. For this purpose, VBT offers a class method that can merge multiple data instances 
into a single one. You can not only combine multiple symbols, but also merge datasets for a single 
symbol—all done automatically!

**Pull BTC datasets from various exchanges and plot them relative to their mean**

```pycon
>>> binance_data = vbt.CCXTData.pull("BTCUSDT", exchange="binance")
>>> bybit_data = vbt.CCXTData.pull("BTCUSDT", exchange="bybit")
>>> bitfinex_data = vbt.CCXTData.pull("BTC/USDT", exchange="bitfinex")
>>> kucoin_data = vbt.CCXTData.pull("BTC-USDT", exchange="kucoin")

>>> data = vbt.Data.merge([
...     binance_data.rename({"BTCUSDT": "Binance"}),
...     bybit_data.rename({"BTCUSDT": "Bybit"}),
...     bitfinex_data.rename({"BTC/USDT": "Bitfinex"}),
...     kucoin_data.rename({"BTC-USDT": "KuCoin"}),
... ], missing_index="drop", silence_warnings=True)

>>> @njit
... def rescale_nb(x):
...     return (x - x.mean()) / x.mean()

>>> rescaled_close = data.close.vbt.row_apply(rescale_nb)
>>> rescaled_close = rescaled_close.vbt.rolling_mean(30)
>>> rescaled_close.loc["2023":"2023"].vbt.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/data_merging.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/data_merging.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#alpaca
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Alpaca
type: heading 2
---

## Alpaca

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_2.svg){ loading=lazy }

- [x] Welcome a new class specialized in pulling data from [Alpaca](https://alpaca.markets/)!

**Get one week of adjusted 1-minute AAPL data from Alpaca**

```pycon
>>> vbt.AlpacaData.set_custom_settings(
...     client_config=dict(
...         api_key="YOUR_API_KEY",
...         secret_key="YOUR_API_SECRET"
...     )
... )
>>> data = vbt.AlpacaData.pull(
...     "AAPL",
...     start="one week ago 00:00",  # (1)!
...     end="15 minutes ago",  # (2)!
...     timeframe="1 minute",
...     adjustment="all",
...     tz="US/Eastern"
... )
>>> data.get()
                                Open      High       Low     Close  Volume  \
Open time                                                                   
2023-01-30 04:00:00-05:00  145.5400  145.5400  144.0100  144.0200  5452.0   
2023-01-30 04:01:00-05:00  144.0800  144.0800  144.0000  144.0500  3616.0   
2023-01-30 04:02:00-05:00  144.0300  144.0400  144.0100  144.0100  1671.0   
2023-01-30 04:03:00-05:00  144.0100  144.0300  144.0000  144.0300  4721.0   
2023-01-30 04:04:00-05:00  144.0200  144.0200  144.0200  144.0200  1343.0   
...                             ...       ...       ...       ...     ...   
2023-02-03 19:54:00-05:00  154.3301  154.3301  154.3301  154.3301   347.0   
2023-02-03 19:55:00-05:00  154.3300  154.3400  154.3200  154.3400  1438.0   
2023-02-03 19:56:00-05:00  154.3400  154.3400  154.3300  154.3300   588.0   
2023-02-03 19:58:00-05:00  154.3500  154.3500  154.3500  154.3500   555.0   
2023-02-03 19:59:00-05:00  154.3400  154.3900  154.3300  154.3900  3835.0   

                            Trade count        VWAP  
Open time                                           
2023-01-30 04:00:00-05:00          165  144.376126  
2023-01-30 04:01:00-05:00           81  144.036336  
2023-01-30 04:02:00-05:00           52  144.035314  
2023-01-30 04:03:00-05:00           56  144.012680  
2023-01-30 04:04:00-05:00           40  144.021854  
...                                ...         ...  
2023-02-03 19:54:00-05:00           21  154.331340  
2023-02-03 19:55:00-05:00           38  154.331756  
2023-02-03 19:56:00-05:00           17  154.338971  
2023-02-03 19:58:00-05:00           27  154.343090  
2023-02-03 19:59:00-05:00           58  154.357219  

[4224 rows x 7 columns]
```

1. In the timezone provided via `tz`.

2. Remove if you have a paid plan.

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#local-data
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: Local data
type: heading 2
---

## Local data

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Once you have fetched remote data, you will most likely want to save it to disk. There are two 
new options for this: you can either serialize the entire data class, or save the actual data to 
CSV or HDF5. Each dataset can be stored in a single flat file, which makes handling the data 
easier than using a database. After saving, you can easily load the data back either by 
deserializing or by using data classes that specialize in loading from CSV and HDF5 files. 
These classes support a range of features, including filtering by row and datetime ranges, 
updating, chunking, and even a smart dataset search that can recursively walk through 
sub-directories and return datasets that match a specific glob pattern or regular expression :magnet:

**Fetch and save symbols separately, then load them back jointly**

```pycon
>>> btc_data = vbt.BinanceData.pull("BTCUSDT")
>>> eth_data = vbt.BinanceData.pull("ETHUSDT")

>>> btc_data.to_hdf()
>>> eth_data.to_hdf()

>>> data = vbt.BinanceData.from_hdf(start="2020", end="2021")
```

[=100% "Key 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> data.close
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2020-01-01 00:00:00+00:00   7200.85   130.77
2020-01-02 00:00:00+00:00   6965.71   127.19
2020-01-03 00:00:00+00:00   7344.96   134.35
2020-01-04 00:00:00+00:00   7354.11   134.20
2020-01-05 00:00:00+00:00   7358.75   135.37
...                             ...      ...
2020-12-27 00:00:00+00:00  26281.66   685.11
2020-12-28 00:00:00+00:00  27079.41   730.41
2020-12-29 00:00:00+00:00  27385.00   732.00
2020-12-30 00:00:00+00:00  28875.54   752.17
2020-12-31 00:00:00+00:00  28923.63   736.42

[366 rows x 2 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/data/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/data/#data
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/data.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Indicators
type: page
icon: material-chart-timeline-variant
---

Indicator features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#hurst-exponent
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Hurst exponent
type: heading 2
---

## Hurst exponent

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_6_19.svg){ loading=lazy }

- [x] Estimating the Hurst exponent provides insight into whether your data is a pure white-noise random
process or exhibits underlying trends. VBT offers five (!) different implementations.

**Plot rolling Hurst exponent by method**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="12 months ago")
>>> hurst = vbt.HURST.run(data.close, method=["standard", "logrs", "rs", "dma", "dsod"])
>>> fig = vbt.make_subplots(specs=[[dict(secondary_y=True)]])
>>> data.plot(plot_volume=False, ohlc_trace_kwargs=dict(opacity=0.3), fig=fig)
>>> fig = hurst.hurst.vbt.plot(fig=fig, add_trace_kwargs=dict(secondary_y=True))
>>> fig = fig.select_range(start=hurst.param_defaults["window"])
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/hurst.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/hurst.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#smart-money-concepts
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Smart Money Concepts
type: heading 2
---

## Smart Money Concepts

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_6_19.svg){ loading=lazy }

- [x] VBT integrates most of the indicators from the
[Smart Money Concepts (SMC)](https://github.com/joshyattridge/smart-money-concepts) library.

**Plot previous high and low**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="6 months ago")
>>> phl = vbt.smc("previous_high_low").run(  # (1)!
...     data.open,
...     data.high,
...     data.low,
...     data.close,
...     data.volume,
...     time_frame=vbt.Default("7D")
... )
>>> fig = data.plot()
>>> phl.previous_high.rename("previous_high").vbt.plot(fig=fig)
>>> phl.previous_low.rename("previous_low").vbt.plot(fig=fig)
>>> (phl.broken_high == 1).rename("broken_high").vbt.signals.plot_as_markers(
...     y=phl.previous_high, 
...     trace_kwargs=dict(marker=dict(color="limegreen")),
...     fig=fig
... )
>>> (phl.broken_low == 1).rename("broken_low").vbt.signals.plot_as_markers(
...     y=phl.previous_low, 
...     trace_kwargs=dict(marker=dict(color="orangered")),
...     fig=fig
... )
>>> fig.show()
```

1. Each SMC indicator requires OHLCV.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/smc.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/smc.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#signal-unraveling
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Signal unraveling
type: heading 2
---

## Signal unraveling

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_2_22.svg){ loading=lazy }

- [x] To backtest each signal individually, you can now "unravel" each signal, or each pair of entry and exit
signals, into its own column. This creates a wide, two-dimensional mask that, when backtested, returns
performance metrics for each signal rather than for the entire column.

**For each signal, create a separate position with own stop orders**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> fast_sma = data.run("talib_func:sma", timeperiod=20)  # (1)!
>>> slow_sma = data.run("talib_func:sma", timeperiod=50)
>>> entries = fast_sma.vbt.crossed_above(slow_sma)
>>> exits = fast_sma.vbt.crossed_below(slow_sma)
>>> entries, exits = entries.vbt.signals.unravel_between(exits, relation="anychain")  # (2)!
>>> pf = vbt.PF.from_signals(
...     data, 
...     long_entries=entries, 
...     short_entries=exits, 
...     size=100,  # (3)!
...     size_type="value",
...     init_cash="auto",  # (4)!
...     tp_stop=0.2, 
...     sl_stop=0.1, 
...     group_by=vbt.ExceptLevel("signal"),  # (5)!
...     cash_sharing=True
... )
>>> pf.positions.returns.to_pd(ignore_index=True).vbt.barplot(
...     trace_kwargs=dict(marker=dict(colorscale="Spectral"))
... ).show()  # (6)!
```

1. Run a TA-Lib function faster without building an indicator: `"talib_func:sma"` vs `"talib:sma"`.

2. Place each pair of entry->exit and exit->entry signals in a separate column.

3. Order $100 worth of the asset.

4. Simulate with infinite cash.

5. Combine all columns (positions) under the same asset into a single portfolio.

6. Show position returns by signal index.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/signal_unraveling.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/signal_unraveling.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#lightweight-ta-lib
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Lightweight TA-Lib
type: heading 2
---

## Lightweight TA-Lib

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_14_0.svg){ loading=lazy }

- [x] TA-Lib functions wrapped with the indicator factory are very powerful because, unlike the official
TA-Lib implementation, they can broadcast, handle DataFrames, skip missing values, and even resample to a
different timeframe. Although TA-Lib functions are very fast, wrapping them with the indicator factory
adds some overhead. To keep both the speed of TA-Lib and the power of VBT, the added features have
been separated into a lightweight function that you can call just like a regular TA-Lib function.

**Run and plot an RSI resampled to the monthly timeframe**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> run_rsi = vbt.talib_func("rsi")
>>> rsi = run_rsi(data.close, timeperiod=12, timeframe="M")  # (1)!
>>> rsi
Date
2014-09-17 00:00:00+00:00          NaN
2014-09-18 00:00:00+00:00          NaN
2014-09-19 00:00:00+00:00          NaN
2014-09-20 00:00:00+00:00          NaN
2014-09-21 00:00:00+00:00          NaN
                                    ...
2024-01-18 00:00:00+00:00    64.210811
2024-01-19 00:00:00+00:00    64.210811
2024-01-20 00:00:00+00:00    64.210811
2024-01-21 00:00:00+00:00    64.210811
2024-01-22 00:00:00+00:00    64.210811
Freq: D, Name: Close, Length: 3415, dtype: float64

>>> plot_rsi = vbt.talib_plot_func("rsi")
>>> plot_rsi(rsi).show()
```

1. Parameters are applied to the target timeframe.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/native_talib.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/native_talib.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicator-search
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Indicator search
type: heading 2
---

## Indicator search

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_11_1.svg){ loading=lazy }

- [x] VBT implements or integrates more than 500 indicators, making it hard to keep track
of them all. To make indicators easier to find, several new methods are available for globally searching
for indicators.

**List all moving average indicators**

```pycon
>>> vbt.IF.list_indicators("*ma")
[
    'vbt:MA',
    'talib:DEMA',
    'talib:EMA',
    'talib:KAMA',
    'talib:MA',
    ...
    'technical:ZEMA',
    'technical:ZLEMA',
    'technical:ZLHMA',
    'technical:ZLMA'
]

>>> vbt.indicator("technical:ZLMA")  # (1)!
vectorbtpro.indicators.factory.technical.ZLMA
```

1. Same as `vbt.IF.get_indicator`.

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators-for-ml
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Indicators for ML
type: heading 2
---

## Indicators for ML

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_2.svg){ loading=lazy }

- [x] Want to feed indicators as features to a machine-learning model? There is no need to run 
them individually: you can tell VBT to run all indicators from an indicator package on 
the given data instance. The data instance will recognize the input names of each indicator and supply 
the required data. You can also easily change the defaults for each indicator.

**Run all talib indicators on entire BTC-USD history**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> features = data.run("talib", mavp=vbt.run_arg_dict(periods=14))
>>> features.shape
(3046, 175)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#signal-detection
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Signal detection
type: heading 2
---

## Signal detection

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_0.svg){ loading=lazy }

- [x] VBT includes an indicator that uses a robust peak detection algorithm based on z-scores.
This indicator can be used to identify outbreaks and outliers in any time series data.

**Detect sudden changes in the bandwidth of a Bollinger Bands indicator**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> fig = vbt.make_subplots(rows=2, cols=1, shared_xaxes=True)
>>> bbands = data.run("bbands")
>>> bbands.loc["2022"].plot(add_trace_kwargs=dict(row=1, col=1), fig=fig)
>>> sigdet = vbt.SIGDET.run(bbands.bandwidth, factor=5)
>>> sigdet.loc["2022"].plot(add_trace_kwargs=dict(row=2, col=1), fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/signal_detection.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/signal_detection.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#pivot-detection
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Pivot detection
type: heading 2
---

## Pivot detection

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_1.svg){ loading=lazy }

- [x] The pivot detection indicator is a tool for finding when the price trend is reversing. By identifying
support and resistance areas, it helps spot significant price changes while filtering out short-term
fluctuations and reducing noise. It works simply: a peak is registered when the price jumps above
one threshold, and a valley is recorded when the price falls below another. Another advantage is
that, unlike the [regular Zig Zag indicator](https://www.investopedia.com/ask/answers/030415/what-zig-zag-indicator-formula-and-how-it-calculated.asp),
which tends to look ahead, our indicator only returns confirmed pivot points and is safe to use in backtesting.

**Plot the last pivot value**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2023")
>>> fig = data.plot(plot_volume=False)
>>> pivot_info = data.run("pivotinfo", up_th=1.0, down_th=0.5)
>>> pivot_info.plot(fig=fig, conf_value_trace_kwargs=dict(visible=False))
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/pivot_detection.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/pivot_detection.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#technical-indicators
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Technical indicators
type: heading 2
---

## Technical indicators

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] VBT integrates most of the indicators and consensus classes from freqtrade's
[technical](https://github.com/freqtrade/technical) library.

**Compute and plot the summary consensus with a one-liner**

```pycon
>>> vbt.YFData.pull("BTC-USD").run("sumcon", smooth=100).plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/sumcon.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/sumcon.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#renko-chart
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Renko chart
type: heading 2
---

## Renko chart

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] Unlike regular charts, a [Renko chart](https://www.investopedia.com/terms/r/renkochart.asp)
is built using price movements. Each "brick" appears when the price changes by a specified amount.
Because the output has irregular time intervals, only one column can be processed at once. As with
everything, VBT's implementation can translate a huge number of data points very fast thanks to Numba.

**Resample closing price into a Renko format**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2021", end="2022")
>>> renko_ohlc = data.close.vbt.to_renko_ohlc(1000, reset_index=True)  # (1)!
>>> renko_ohlc.vbt.ohlcv.plot().show()
```

1. Bitcoin is very volatile, so the brick size is set to 1000.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/renko_chart.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/renko_chart.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#rolling-ols
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Rolling OLS
type: heading 2
---

## Rolling OLS

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] Rolling regressions are models for analyzing changing relationships among variables over time.
In VBT, this is implemented as an indicator that takes two time series and returns the slope, intercept,
prediction, error, and the z-score of the error at each time step. This indicator can be used for
cointegration tests, such as determining optimal rebalancing timings in pairs trading, and is also
(literally) 1000x faster than the statsmodels equivalent
[RollingOLS](https://www.statsmodels.org/dev/generated/statsmodels.regression.rolling.RollingOLS.html) :fire:

**Determine the spread between BTC and ETH**

```pycon
>>> data = vbt.YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2022", 
...     end="2023",
...     missing_index="drop"
... )
>>> ols = vbt.OLS.run(
...     data.get("Close", "BTC-USD"), 
...     data.get("Close", "ETH-USD")
... )
>>> ols.plot_zscore().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/rolling_ols.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/rolling_ols.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#ta-lib-time-frames
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: TA-Lib time frames
type: heading 2
---

## TA-Lib time frames

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_0.svg){ loading=lazy }

- [x] Comparing indicators on different time frames involves many nuances. Now, all TA-Lib indicators
support a parameter that resamples the input arrays to a target time frame, calculates the indicator, and
then resamples the output arrays back to the original time frame. This makes parameterized MTF analysis
easier than ever!

!!! example "Tutorial"
    Learn more in the [MTF analysis](https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis) tutorial.

**Run SMA on multiple time frames and display the whole thing as a heatmap**

```pycon
>>> h1_data = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="3 months ago UTC", 
...     timeframe="1h"
... )
>>> mtf_sma = vbt.talib("SMA").run(
...     h1_data.close, 
...     timeperiod=14, 
...     timeframe=["1d", "4h", "1h"], 
...     skipna=True
... )
>>> mtf_sma.real.vbt.ts_heatmap().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/talib_time_frames.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/talib_time_frames.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#1d-native-indicators
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: 1D-native indicators
type: heading 2
---

## 1D-native indicators

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_10.svg){ loading=lazy }

- [x] Previously, custom indicators could only be created by accepting two-dimensional input arrays, which
forced users to adapt all functions accordingly. Now, the indicator factory can split each input array
along columns and pass one column at a time, making it much easier to design indicators that are meant
to be run natively on one-dimensional data (such as TA-Lib!).

**Create a TA-Lib powered STOCHRSI indicator**

```pycon
>>> import talib

>>> params = dict(
...     rsi_period=14, 
...     fastk_period=5, 
...     slowk_period=3, 
...     slowk_matype=0, 
...     slowd_period=3, 
...     slowd_matype=0
... )

>>> def stochrsi_1d(close, *args):
...     rsi = talib.RSI(close, args[0])
...     k, d = talib.STOCH(rsi, rsi, rsi, *args[1:])
...     return rsi, k, d
    
>>> STOCHRSI = vbt.IF(
...     input_names=["close"], 
...     param_names=list(params.keys()),
...     output_names=["rsi", "k", "d"]
... ).with_apply_func(stochrsi_1d, takes_1d=True, **params)

>>> data = vbt.YFData.pull("BTC-USD", start="2022-01", end="2022-06")
>>> stochrsi = STOCHRSI.run(data.close)
>>> fig = stochrsi.k.rename("%K").vbt.plot()
>>> stochrsi.d.rename("%D").vbt.plot(fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/stochrsi.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/stochrsi.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#parallelizable-indicators
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Parallelizable indicators
type: heading 2
---

## Parallelizable indicators

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_10.svg){ loading=lazy }

- [x] Processing parameter combinations with the indicator factory can be distributed across multiple threads,
processes, or even in the cloud. This is a huge help when working with slow indicators :snail:

**Benchmark a serial and multithreaded rolling min-max indicator**

```pycon
>>> @njit
... def minmax_nb(close, window):
...     return (
...         vbt.nb.rolling_min_nb(close, window),
...         vbt.nb.rolling_max_nb(close, window)
...     )

>>> MINMAX = vbt.IF(
...     class_name="MINMAX",
...     input_names=["close"], 
...     param_names=["window"], 
...     output_names=["min", "max"]
... ).with_apply_func(minmax_nb, window=14)

>>> data = vbt.YFData.pull("BTC-USD")
```

```pycon
>>> %%timeit
>>> minmax = MINMAX.run(
...     data.close, 
...     np.arange(2, 200),
...     jitted_loop=True
... )
420 ms ± 2.05 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

```pycon
>>> %%timeit
>>> minmax = MINMAX.run(
...     data.close, 
...     np.arange(2, 200),
...     jitted_loop=True,
...     jitted_warmup=True,  # (1)!
...     execute_kwargs=dict(engine="threadpool", n_chunks="auto")  # (2)!
... )
120 ms ± 355 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

1. Run one parameter combination to compile the indicator before running others
in a multithreaded fashion.

2. One Numba loop per thread, with the same number of threads as there are cores.

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#ta-lib-plotting
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: TA-Lib plotting
type: heading 2
---

## TA-Lib plotting

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_9.svg){ loading=lazy }

- [x] Every TA-Lib indicator is fully capable of plotting itself, based entirely on output flags!

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2021")

>>> vbt.talib("MACD").run(data.close).plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/talib.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/talib.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicator-expressions
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Indicator expressions
type: heading 2
---

## Indicator expressions

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_8.svg){ loading=lazy }

- [x] Indicators can now be created from expressions. An indicator expression is a regular string representing
Python code enhanced with various extensions. The indicator factory automatically derives all required
information, such as inputs, parameters, outputs, NumPy, VBT, and TA-Lib functions, and even
complex indicators, thanks to a unique format and built-in matching mechanism. Designing indicators
has never been easier!

**Build a MACD indicator from an expression**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2021")

>>> expr = """
... MACD:
... fast_ema = @talib_ema(close, @p_fast_w)
... slow_ema = @talib_ema(close, @p_slow_w)
... macd = fast_ema - slow_ema
... signal = @talib_ema(macd, @p_signal_w)
... macd, signal
... """
>>> MACD = vbt.IF.from_expr(expr, fast_w=12, slow_w=26, signal_w=9)  # (1)!
>>> macd = MACD.run(data.close)
>>> fig = macd.macd.rename("MACD").vbt.plot()
>>> macd.signal.rename("Signal").vbt.plot(fig=fig)
>>> fig.show()
```

1. No need to manually set `input_names`, `param_names`, or other information.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/indicator_expressions.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/indicator_expressions.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#worldquant-alphas
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: WorldQuant Alphas
type: heading 2
---

## WorldQuant Alphas

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_8.svg){ loading=lazy }

- [x] Each of [WorldQuant's 101 Formulaic Alphas](https://arxiv.org/pdf/1601.00991.pdf) is now available
as an indicator :eyes:

**Run the first alpha**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD", "XRP-USD"], missing_index="drop")

>>> vbt.wqa101(1).run(data.close).out
symbol                      BTC-USD   ETH-USD   XRP-USD
Date                                                   
2017-11-09 00:00:00+00:00  0.166667  0.166667  0.166667
2017-11-10 00:00:00+00:00  0.166667  0.166667  0.166667
2017-11-11 00:00:00+00:00  0.166667  0.166667  0.166667
2017-11-12 00:00:00+00:00  0.166667  0.166667  0.166667
2017-11-13 00:00:00+00:00  0.166667  0.166667  0.166667
...                             ...       ...       ...
2023-01-31 00:00:00+00:00  0.166667  0.166667  0.166667
2023-02-01 00:00:00+00:00  0.000000  0.000000  0.500000
2023-02-02 00:00:00+00:00  0.000000  0.000000  0.500000
2023-02-03 00:00:00+00:00  0.000000  0.500000  0.000000
2023-02-04 00:00:00+00:00 -0.166667  0.333333  0.333333

[1914 rows x 3 columns]
```

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#robust-crossovers
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: Robust crossovers
type: heading 2
---

## Robust crossovers

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Crossovers are now robust to NaNs.

**Remove a bunch of data points and plot the crossovers**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2022-01", end="2022-03")
>>> fast_sma = vbt.talib("SMA").run(data.close, vbt.Default(5)).real
>>> slow_sma = vbt.talib("SMA").run(data.close, vbt.Default(10)).real
>>> fast_sma.iloc[np.random.choice(np.arange(len(fast_sma)), 5)] = np.nan
>>> slow_sma.iloc[np.random.choice(np.arange(len(slow_sma)), 5)] = np.nan
>>> crossed_above = fast_sma.vbt.crossed_above(slow_sma, dropna=True)
>>> crossed_below = fast_sma.vbt.crossed_below(slow_sma, dropna=True)

>>> fig = fast_sma.rename("Fast SMA").vbt.lineplot()
>>> slow_sma.rename("Slow SMA").vbt.lineplot(fig=fig)
>>> crossed_above.vbt.signals.plot_as_entries(fast_sma, fig=fig)
>>> crossed_below.vbt.signals.plot_as_exits(fast_sma, fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/resilient_crossovers.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/resilient_crossovers.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/indicators/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/indicators/#indicators
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/indicators.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Intelligence
type: page
icon: material-brain
---

Intelligence features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#function-calling
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Function calling
type: heading 2
tags:
  - recently_added
---

## Function calling

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2025_10_15.svg){ loading=lazy }

- [x] VBT now supports function calling across all LLMs, allowing you to use the MCP server
and define custom functions that the model can invoke, and all of this without any additional setup!
This feature is particularly useful for complex queries, agentic workflows, and interactive applications
where you want the model to perform specific tasks or calculations on your behalf.

!!! info
    The first time you run this command, it may take up to 15 minutes to prepare and embed documents.
    However, most of the preparation steps are cached and stored, so future searches will be much
    faster and will not require repeating the process.

!!! example "Cookbook"
    Explore more configurations in the knowledge section of the [Cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/).

**Ask VBT a question using ChatGPT and let it use the MCP server**

```pycon
>>> env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"

>>> vbt.interact(  # (1)!
    "How to rebalance weekly?", 
    tool_display_format="compact",  # (2)!
    formatter="html",
)
```

1. Use `interact()` instead of `chat()` to enable function calling.

2. Optional. Use a compact display format for tools. By default, tools are displayed in a collapsed format.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/function_calling.light.gif#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/function_calling.dark.gif#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#reasoning-steps
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Reasoning steps
type: heading 2
tags:
  - recently_added
---

## Reasoning steps

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2025_10_15.svg){ loading=lazy }

- [x] VBT now supports reasoning steps across all LLMs, allowing you to see how the model arrived
at its conclusion. This is particularly useful for complex queries that require multiple steps to answer.

!!! info
    The first time you run this command, it may take up to 15 minutes to prepare and embed documents.
    However, most of the preparation steps are cached and stored, so future searches will be much
    faster and will not require repeating the process.

!!! example "Cookbook"
    Explore more configurations in the knowledge section of the [Cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/).

**Ask VBT a question using ChatGPT and see its reasoning steps**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"
>>> env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"

>>> vbt.chat(
...     "How to rebalance weekly?",
...     reasoning=dict(effort="low", summary="auto"),  # (1)!
...     formatter="html"
... )
```

1. Enable reasoning with low effort and automatic summary for OpenAI models.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/reasoning_steps.light.gif#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/reasoning_steps.dark.gif#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#source-refactorer
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Source refactorer
type: heading 2
---

## Source refactorer

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2025_5_11.svg){ loading=lazy }

- [x] Use the source refactorer to automatically enhance any Python code, whether it is a full package,
Python object, or raw string. If the source is larger than expected, it intelligently splits it
into clean AST-based chunks, refactors each using an LLM, and merges them back into polished code.
You can choose to return the result, update the source in place, or copy it to your clipboard.
For full transparency, you can also preview the diff directly in your browser :pencil:

**Complete a signal function**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"
>>> env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"

>>> source = """
... @njit
... def signal_func_nb(c, entries, exits):
...     is_entry = vbt.pf_nb.select_nb(c, entries)
...     is_exit = vbt.pf_nb.select_nb(c, exits)
...     
...     # TODO: Enter only if no other asset is active
...
...     return is_entry, is_exit, False, False
... """  # (1)!

>>> new_source = vbt.refactor_source(
...     source, 
...     attach_knowledge=True,  # (2)!
...     model="gpt-5-mini",
...     reasoning_effort="high",
...     show_diff=True,  # (3)!
... )
>>> print(new_source)

@njit
def signal_func_nb(c, entries, exits):
    is_entry = vbt.pf_nb.select_nb(c, entries)
    is_exit = vbt.pf_nb.select_nb(c, exits)
    for col in range(c.from_col, c.to_col):
        if col != c.col and c.last_position[col] != 0:
            is_entry = False
            break
    return is_entry, is_exit, False, False
```

1. Use **TODO** or **FIXME** comments to provide custom instructions.

2. Attach relevant knowledge from the website and Discord.

3. Show the produced changes in a browser.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/source_refiner.light.png#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/source_refiner.dark.png#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#quick-search-chat
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Quick search & chat
type: heading 2
---

## Quick search & chat

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2025_5_11.svg){ loading=lazy }

- [x] In addition to embeddings, which require prior generation, VBT now supports
[BM25](https://en.wikipedia.org/wiki/Okapi_BM25) for fast, fully offline lexical search.
This is perfect for quickly finding something specific.

**Search VBT knowledge for a warning**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"

>>> vbt.quick_search("UserWarning: Symbols have mismatching index")
```

=== "Page 1"

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/quick_search1.light.png#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/quick_search1.dark.png#only-dark){: .iimg loading=lazy }

=== "Page 2"

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/quick_search2.light.png#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/quick_search2.dark.png#only-dark){: .iimg loading=lazy }

=== "Page 3"

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/quick_search3.light.png#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/quick_search3.dark.png#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#chatvbt
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: ChatVBT
type: heading 2
---

## ChatVBT

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2025_3_1.svg){ loading=lazy }

- [x] Similar to SearchVBT, ChatVBT takes search results and forwards
them to an LLM for completion. This allows you to interact seamlessly with the entire VBT knowledge base,
receiving detailed and context-aware responses.

!!! info
    The first time you run this command, it may take up to 15 minutes to prepare and embed documents.
    However, most of the preparation steps are cached and stored, so future searches will be much
    faster and will not require repeating the process.

!!! example "Cookbook"
    Explore more configurations in the knowledge section of the [Cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/).

**Ask VBT a question using ChatGPT**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"
>>> env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"

>>> vbt.chat("How to rebalance weekly?", formatter="html")
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/chatvbt.light.gif#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/chatvbt.dark.gif#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#searchvbt
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: SearchVBT
type: heading 2
---

## SearchVBT

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2025_3_1.svg){ loading=lazy }

- [x] Want to find specific information using natural language on the website or Discord?
VBT provides a powerful smart search feature called SearchVBT. Enter your query and it will
generate an HTML page with well-structured search results. Behind the scenes, SearchVBT uses a
[RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation) pipeline to embed, rank, and
retrieve only the most relevant documents from VBT, ensuring precise and efficient search results.

!!! info
    The first time you run this command, it may take up to 15 minutes to prepare and embed documents.
    However, most preparation steps are cached and stored, so future searches will be significantly
    faster without needing to repeat the process.

!!! example "Cookbook"
    Explore more configurations in the knowledge section of the [Cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/).

**Search VBT knowledge for a warning**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"
>>> env["OPENAI_API_KEY"] = "<YOUR_API_KEY>"

>>> vbt.search("How to run indicator expressions?")
```

=== "Page 1"

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/searchvbt1.light.png#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/searchvbt1.dark.png#only-dark){: .iimg loading=lazy }

=== "Page 2"

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/searchvbt2.light.png#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/searchvbt2.dark.png#only-dark){: .iimg loading=lazy }

=== "Page 3"

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/searchvbt3.light.png#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/features/searchvbt3.dark.png#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#self-aware-classes
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Self-aware classes
type: heading 2
---

## Self-aware classes

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_12_15.svg){ loading=lazy }

- [x] Each VBT class offers methods to explore its features, including its API, associated
documentation, Discord messages, and code examples. You can even interact with it directly via an LLM!

**Ask portfolio optimizer a question**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"
>>> env["OPENAI_API_KEY"] = "<YOUR_API_KEY>"

>>> vbt.PortfolioOptimizer.find_assets().get("link")
['https://vectorbt.pro/pvt_xxxxxxxx/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base',
  'https://vectorbt.pro/pvt_xxxxxxxx/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable',
  'https://vectorbt.pro/pvt_xxxxxxxx/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping',
  ...
  'https://vectorbt.pro/pvt_xxxxxxxx/features/optimization/#riskfolio-lib',
  'https://vectorbt.pro/pvt_xxxxxxxx/features/optimization/#portfolio-optimization',
  'https://vectorbt.pro/pvt_xxxxxxxx/features/optimization/#pyportfolioopt',
  ...
  'https://discord.com/channels/x/918629995415502888/1064943203071045753',
  'https://discord.com/channels/x/918629995415502888/1067718833646874634',
  'https://discord.com/channels/x/918629995415502888/1067718855734075403',
  ...]

>>> vbt.PortfolioOptimizer.chat("How to rebalance weekly?", formatter="html")
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/knowledge_assets.light.gif#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/knowledge_assets.dark.gif#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#knowledge-assets
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: Knowledge assets
type: heading 2
---

## Knowledge assets

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_11_12.svg){ loading=lazy }

- [x] Each [release](https://github.com/polakowo/vectorbt.pro/releases) now includes valuable
knowledge assets—JSON files containing private website content and the complete "vectorbt.pro" Discord
history. These assets can be used with LLMs and services like Cursor. In addition, VBT offers
a palette of classes for working with these assets, providing functions such as
converting to Markdown and HTML files, browsing the website offline, performing targeted searches,
interacting with LLMs, and much more!

**Gather all pages and messages that define signal_func_nb**

```pycon
>>> env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"

>>> pages_asset = vbt.PagesAsset.pull()  # (1)!
>>> messages_asset = vbt.MessagesAsset.pull()
>>> vbt_asset = pages_asset + messages_asset
>>> code = vbt_asset.find_code("def signal_func_nb", return_type="item")
>>> code.print_sample()  # (2)!
```

1. The first pull will download the assets, while subsequent pulls will use the cached versions.

2. Print a random code snippet.

```yaml
link: https://discord.com/channels/x/918630948248125512/1251081573147742298
block: https://discord.com/channels/x/918630948248125512/1251081573147742298
thread: https://discord.com/channels/x/918630948248125512/1250844139952541837
reference: https://discord.com/channels/x/918630948248125512/1250844139952541837
replies:

- https://discord.com/channels/x/918630948248125512/1251083513336299610
channel: support
timestamp: '2024-06-14 07:51:31'
author: '@polakowo'
content: Something like this
mentions:

- '@fei'
attachments:

- file_name: Screenshot_2024-06-13_at_20.29.45-B4517.png
  content: |-
    Here's the text extracted from the image:

    ```python
    @njit
    def signal_func_nb(c, entries, exits, wait):
        is_entry = vbt.pf_nb.select_nb(c, entries)
        if is_entry:
            return True, False, False, False
        is_exit = vbt.pf_nb.select_nb(c, exits)
        if is_exit:
            if vbt.pf_nb.in_position_nb(c):
                last_order = vbt.pf_nb.get_last_order_nb(c)
                if c.index[c.i] - c.index[last_order["idx"]] >= wait:
                    return False, True, False, False
        return False, False, False, False

    pf = vbt.PF.from_random_signals(
        "BTC-USD",
        n=100,
        signal_func_nb=signal_func_nb,
        signal_args=(
            vbt.Rep("entries"),
            vbt.Rep("exits"),
            vbt.dt.to_ns(vbt.timedelta("1000 days"))
        )
    )
    ```
reactions: 0
```

---
link: https://vectorbt.pro/pvt_41531c19/features/intelligence/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/intelligence/#intelligence
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/intelligence.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Optimization
type: page
icon: material-magnify-expand
---

Optimization features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#purged-cv
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Purged CV
type: heading 2
---

## Purged CV

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_4_1.svg){ loading=lazy }

- [x] Added support for walk-forward cross-validation (CV) with purging, as well as combinatorial
CV with purging and embargoing, based on Marcos Lopez de Prado's
[Advances in Financial Machine Learning](https://www.wiley.com/en-us/Advances+in+Financial+Machine+Learning-p-9781119482086).

**Create and plot a combinatorial splitter with purging and embargoing**

```pycon
>>> splitter = vbt.Splitter.from_purged_kfold(
...     vbt.date_range("2024", "2025"), 
...     n_folds=10,
...     n_test_folds=2, 
...     purge_td="3 days",
...     embargo_td="3 days"
... )
>>> splitter.plots().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/purged_cv.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/purged_cv.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#paramables
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Paramables
type: heading 2
---

## Paramables

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_4_1.svg){ loading=lazy }

- [x] Each analyzable VBT object (such as data, indicator, or portfolio) can now be split into
items, which are multiple objects of the same type, each containing only one column or group. This
makes it possible to use VBT objects as standalone parameters and process only a subset of
information at a time, such as a symbol in a data instance or a parameter combination in an
indicator.

**Combine outputs of a SMA indicator combinatorially**

```pycon
>>> @vbt.parameterized(merge_func="column_stack")
... def get_signals(fast_sma, slow_sma):  # (1)!
...     entries = fast_sma.crossed_above(slow_sma)
...     exits = fast_sma.crossed_below(slow_sma)
...     return entries, exits

>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"])
>>> sma = data.run("talib:sma", timeperiod=range(20, 50, 2))  # (2)!
>>> fast_sma = sma.rename_levels({"sma_timeperiod": "fast"})  # (3)!
>>> slow_sma = sma.rename_levels({"sma_timeperiod": "slow"})
>>> entries, exits = get_signals(
...     vbt.Param(fast_sma, condition="__fast__ < __slow__"),  # (4)!
...     vbt.Param(slow_sma)
... )
>>> entries.columns
MultiIndex([(20, 22, 'BTC-USD'),
            (20, 22, 'ETH-USD'),
            (20, 24, 'BTC-USD'),
            (20, 24, 'ETH-USD'),
            (20, 26, 'BTC-USD'),
            (20, 26, 'ETH-USD'),
            ...
            (44, 46, 'BTC-USD'),
            (44, 46, 'ETH-USD'),
            (44, 48, 'BTC-USD'),
            (44, 48, 'ETH-USD'),
            (46, 48, 'BTC-USD'),
            (46, 48, 'ETH-USD')],
            names=['fast', 'slow', 'symbol'], length=210)
```

1. Regular function that takes two indicators and returns signals.

2. Run an SMA indicator once on all time periods.

3. Copy the indicator and rename the parameter level to get a distinct indicator instance.

4. Pass both indicator instances as parameters. This splits each instance into smaller
instances with only one column. Also, remove all columns where the fast window is greater than or
equal to the slow window.

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#lazy-parameter-grids
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Lazy parameter grids
type: heading 2
---

## Lazy parameter grids

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_12_23.svg){ loading=lazy }

- [x] The [parameterized decorator](#parameterized-decorator) no longer needs to materialize
parameter grids if you are only interested in a subset of all parameter combinations. This change
enables the generation of random parameter combinations almost instantly, no matter how large the
total number of possible combinations is.

**Test a random subset of a huge number of parameter combinations**

```pycon
>>> @vbt.parameterized(merge_func="concat")
... def test_combination(data, n, sl_stop, tsl_stop, tp_stop):
...     return data.run(
...         "from_random_signals", 
...         n=n, 
...         sl_stop=sl_stop,
...         tsl_stop=tsl_stop,
...         tp_stop=tp_stop,
...     ).total_return

>>> n = np.arange(10, 100)
>>> sl_stop = np.arange(1, 1000) / 1000
>>> tsl_stop = np.arange(1, 1000) / 1000
>>> tp_stop = np.arange(1, 1000) / 1000
>>> len(n) * len(sl_stop) * len(tsl_stop) * len(tp_stop)
89730269910

>>> test_combination(
...     vbt.YFData.pull("BTC-USD"),
...     n=vbt.Param(n),
...     sl_stop=vbt.Param(sl_stop),
...     tsl_stop=vbt.Param(tsl_stop),
...     tp_stop=vbt.Param(tp_stop),
...     _random_subset=10
... )
n   sl_stop  tsl_stop  tp_stop
34  0.188    0.916     0.749       6.869901
44  0.176    0.734     0.550       6.186478
50  0.421    0.245     0.253       0.540188
51  0.033    0.951     0.344       6.514647
    0.915    0.461     0.322       2.915987
73  0.057    0.690     0.008      -0.204080
74  0.368    0.360     0.935      14.207262
76  0.771    0.342     0.187      -0.278499
83  0.796    0.788     0.730       6.450076
96  0.873    0.429     0.815      18.670965
dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#mono-chunks
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Mono-chunks
type: heading 2
---

## Mono-chunks

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_13_0.svg){ loading=lazy }

- [x] The [parameterized decorator](#parameterized-decorator) now supports splitting parameter
combinations into "mono-chunks," merging the parameter values within each chunk into a single value,
and running the entire chunk with a single function call. This means you are no longer limited to
processing only one parameter combination at a time :cloud_tornado: Keep in mind that your function
must be adapted to handle multiple parameter values, and you should modify the merging function as
needed.

**Test 100 combinations of SL and TP values per thread**

```pycon
>>> @vbt.parameterized(
...     merge_func="concat", 
...     mono_chunk_len=100,  # (1)!
...     chunk_len="auto",  # (2)!
...     engine="threadpool",  # (3)!
...     warmup=True  # (4)!
... )  
... @njit(nogil=True)
... def test_stops_nb(close, entries, exits, sl_stop, tp_stop):
...     sim_out = vbt.pf_nb.from_signals_nb(
...         target_shape=(close.shape[0], sl_stop.shape[1]),
...         group_lens=np.full(sl_stop.shape[1], 1),
...         close=close,
...         long_entries=entries,
...         short_entries=exits,
...         sl_stop=sl_stop,
...         tp_stop=tp_stop,
...         save_returns=True
...     )
...     return vbt.ret_nb.total_return_nb(sim_out.in_outputs.returns)

>>> data = vbt.YFData.pull("BTC-USD", start="2020")  # (5)!
>>> entries, exits = data.run("randnx", n=10, hide_params=True, unpack=True)  # (6)!
>>> sharpe_ratios = test_stops_nb(
...     vbt.to_2d_array(data.close),
...     vbt.to_2d_array(entries),
...     vbt.to_2d_array(exits),
...     sl_stop=vbt.Param(np.arange(0.01, 1.0, 0.01), mono_merge_func=np.column_stack),  # (7)!
...     tp_stop=vbt.Param(np.arange(0.01, 1.0, 0.01), mono_merge_func=np.column_stack)
... )
>>> sharpe_ratios.vbt.heatmap().show()
```

1. 100 values are combined into one array, forming a single mono-chunk.

2. Execute N mono-chunks in parallel, where N is the number of cores.

3. Use multithreading.

4. Execute one mono-chunk to compile the function before distributing other chunks.

5. The function above operates with only one symbol.

6. Pick 10 entries and exits randomly.

7. For each mono-chunk, stack all values into a two-dimensional array.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/chunked_params.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/chunked_params.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#cv-decorator
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: CV decorator
type: heading 2
---

## CV decorator

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_1.svg){ loading=lazy }

- [x] Most cross-validation tasks involve testing a grid of parameter combinations on the training data,
selecting the best parameter combination, and validating it on the test data. This process
must be repeated for each split. The cross-validation decorator combines the parameterized and
split decorators to automate this task.

!!! example "Tutorial"
    Learn more in the [Cross-validation](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation) tutorial.

**Cross-validate a SMA crossover using random search**

```pycon
>>> @vbt.cv_split(
...     splitter="from_rolling", 
...     splitter_kwargs=dict(length=365, split=0.5, set_labels=["train", "test"]),
...     takeable_args=["data"],
...     parameterized_kwargs=dict(random_subset=100),
...     merge_func="concat"
... )
... def sma_crossover_cv(data, fast_period, slow_period, metric):
...     fast_sma = data.run("sma", fast_period, hide_params=True)
...     slow_sma = data.run("sma", slow_period, hide_params=True)
...     entries = fast_sma.real_crossed_above(slow_sma)
...     exits = fast_sma.real_crossed_below(slow_sma)
...     pf = vbt.PF.from_signals(data, entries, exits, direction="both")
...     return pf.deep_getattr(metric)

>>> sma_crossover_cv(
...     vbt.YFData.pull("BTC-USD", start="4 years ago"),
...     vbt.Param(np.arange(20, 50), condition="x < slow_period"),
...     vbt.Param(np.arange(20, 50)),
...     "trades.expectancy"
... )
```

[=100% "Split 7/7"]{: .candystripe .candystripe-animate }

```pycon
split  set    fast_period  slow_period
0      train  20           25               8.015725
        test   20           23               0.573465
1      train  40           48              -4.356317
        test   39           40               5.666271
2      train  24           45              18.253340
        test   22           36             111.202831
3      train  20           31              54.626024
        test   20           25              -1.596945
4      train  25           48              41.328588
        test   25           30               6.620254
5      train  26           32               7.178085
        test   24           29               4.087456
6      train  22           23              -0.581255
        test   22           31              -2.494519
dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#split-decorator
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Split decorator
type: heading 2
---

## Split decorator

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_1.svg){ loading=lazy }

- [x] Normally, to run a function on each split, you need to build a splitter specifically targeted
at the input data provided to the function. This means that each time the input data changes, you must
recreate the splitter. The split decorator automates this process by wrapping the function,
giving it access to all arguments so it can make splitting decisions as needed.
Essentially, it can "infect" any Python function with splitting functionality :microbe:

!!! example "Tutorial"
    Learn more in the [Cross-validation](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation) tutorial.

**Get total return from holding in each quarter**

```pycon
>>> @vbt.split(
...     splitter="from_grouper", 
...     splitter_kwargs=dict(by="Q"),
...     takeable_args=["data"],
...     merge_func="concat"
... )
... def get_quarter_return(data):
...     return data.returns.vbt.returns.total()

>>> data = vbt.YFData.pull("BTC-USD")
>>> get_quarter_return(data.loc["2021"])
Date
2021Q1    1.005805
2021Q2   -0.407050
2021Q3    0.304383
2021Q4   -0.037627
Freq: Q-DEC, dtype: float64

>>> get_quarter_return(data.loc["2022"])
Date
2022Q1   -0.045047
2022Q2   -0.572515
2022Q3    0.008429
2022Q4   -0.143154
Freq: Q-DEC, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#conditional-parameters
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Conditional parameters
type: heading 2
---

## Conditional parameters

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_1.svg){ loading=lazy }

- [x] Parameters can depend on each other. For example, when testing a crossover of moving averages,
it makes no sense to test a fast window that is longer than the slow window. By filtering
out such cases, you only need to evaluate about half as many parameter combinations.

**Test slow windows being longer than fast windows by at least 5**

```pycon
>>> @vbt.parameterized(merge_func="column_stack")
... def ma_crossover_signals(data, fast_window, slow_window):
...     fast_sma = data.run("sma", fast_window, short_name="fast_sma")
...     slow_sma = data.run("sma", slow_window, short_name="slow_sma")
...     entries = fast_sma.real_crossed_above(slow_sma.real)
...     exits = fast_sma.real_crossed_below(slow_sma.real)
...     return entries, exits

>>> entries, exits = ma_crossover_signals(
...     vbt.YFData.pull("BTC-USD", start="one year ago UTC"),
...     vbt.Param(np.arange(5, 50), condition="slow_window - fast_window >= 5"),
...     vbt.Param(np.arange(5, 50))
... )
>>> entries.columns
MultiIndex([( 5, 10),
            ( 5, 11),
            ( 5, 12),
            ( 5, 13),
            ( 5, 14),
            ...
            (42, 48),
            (42, 49),
            (43, 48),
            (43, 49),
            (44, 49)],
            names=['fast_window', 'slow_window'], length=820)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#splitter
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Splitter
type: heading 2
---

## Splitter

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_8_0.svg){ loading=lazy }

- [x] Splitters in [scikit-learn](https://scikit-learn.org/stable/) are not ideal for validating ML-based
and rule-based trading strategies. VBT provides a juggernaut class that supports many splitting
schemes that are safe for backtesting, including rolling windows, expanding windows, time-anchored windows,
random windows for block bootstraps, and even Pandas-native `groupby` and `resample` instructions such as
"M" for monthly frequency. As a bonus, the produced splits can be easily analyzed and
visualized! For example, you can detect any split or set overlaps, convert all splits into a single
boolean mask for custom analysis, group splits and sets, and analyze their distribution relative to each other.
This class contains more lines of code than the entire [backtesting.py](https://github.com/kernc/backtesting.py)
package, so do not underestimate the new king in town! :rhinoceros:

!!! example "Tutorial"
    Learn more in the [Cross-validation](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation) tutorial.

**Roll a 360-day window and split it equally into train and test sets**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="4 years ago")
>>> splitter = vbt.Splitter.from_rolling(
...     data.index, 
...     length="360 days",
...     split=0.5,
...     set_labels=["train", "test"],
...     freq="daily"
... )
>>> splitter.plots().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/splitter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/splitter.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#random-search
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Random search
type: heading 2
---

## Random search

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] While grid search tests every possible combination of hyperparameters, random search selects
and tests random combinations of hyperparameters. This is especially useful when there is a huge number of
parameter combinations. Random search has also been shown to find equal or better values than
grid search with fewer function evaluations. The indicator factory, parameterized decorator,
and any method that performs broadcasting now support random search out of the box.

**Test a random subset of SL, TSL, and TP combinations**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020")
>>> stop_values = np.arange(1, 100) / 100  # (1)!
>>> pf = vbt.PF.from_random_signals(
...     data, 
...     n=100, 
...     sl_stop=vbt.Param(stop_values),
...     tsl_stop=vbt.Param(stop_values),
...     tp_stop=vbt.Param(stop_values),
...     broadcast_kwargs=dict(random_subset=1000)  # (2)!
... )
>>> pf.total_return.sort_values(ascending=False)
sl_stop  tsl_stop  tp_stop
0.06     0.85      0.43       2.291260
          0.74      0.40       2.222212
          0.97      0.22       2.149849
0.40     0.10      0.23       2.082935
0.47     0.09      0.25       2.030105
                                    ...
0.51     0.36      0.01      -0.618805
0.53     0.37      0.01      -0.624761
0.35     0.60      0.02      -0.662992
0.29     0.13      0.02      -0.671376
0.46     0.72      0.02      -0.720024
Name: total_return, Length: 1000, dtype: float64
```

1. 100 combinations of each parameter = 100 ^ 3 = 1,000,000 combinations.

2. The indicator factory and parameterized decorator accept this argument directly.

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#parameterized-decorator
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Parameterized decorator
type: heading 2
---

## Parameterized decorator

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] There is a special decorator that allows any Python function to accept multiple
parameter combinations, even if the function itself supports only one. The decorator wraps the function,
gains access to its arguments, identifies all arguments acting as parameters,
builds a grid from them, and calls the underlying function on each parameter combination from that grid.
The execution can be easily parallelized. Once all outputs are ready, it merges them into
a single object. Use cases are endless: from running indicators that cannot be wrapped with the indicator
factory, to parameterizing entire pipelines! :magic_wand:

**Example 1: Parameterize a simple SMA indicator without Indicator Factory**

```pycon
>>> @vbt.parameterized(merge_func="column_stack")  # (1)!
... def sma(close, window):
...     return close.rolling(window).mean()

>>> data = vbt.YFData.pull("BTC-USD")
>>> sma(data.close, vbt.Param(range(20, 50)))
```

1. Use `column_stack` to merge time series in the form of DataFrames and
complex VBT objects such as portfolios.

[=100% "Combination 30/30"]{: .candystripe .candystripe-animate }

```pycon
window                               20            21            22  \
Date                                                                  
2014-09-17 00:00:00+00:00           NaN           NaN           NaN   
2014-09-18 00:00:00+00:00           NaN           NaN           NaN   
2014-09-19 00:00:00+00:00           NaN           NaN           NaN   
...                                 ...           ...           ...   
2024-03-07 00:00:00+00:00  57657.135156  57395.376488  57147.339134   
2024-03-08 00:00:00+00:00  58488.990039  58163.942708  57891.045455   
2024-03-09 00:00:00+00:00  59297.836523  58956.156064  58624.648793   

...

window                               48            49  
Date                                                   
2014-09-17 00:00:00+00:00           NaN           NaN  
2014-09-18 00:00:00+00:00           NaN           NaN  
2014-09-19 00:00:00+00:00           NaN           NaN  
...                                 ...           ...  
2024-03-07 00:00:00+00:00  49928.186686  49758.599330  
2024-03-08 00:00:00+00:00  50483.072266  50303.123565  
2024-03-09 00:00:00+00:00  51040.440837  50846.672353  

[3462 rows x 30 columns]
```

**Example 2: Parameterize an entire Bollinger Bands pipeline**

```pycon
>>> @vbt.parameterized(merge_func="concat")  # (1)!
... def bbands_sharpe(data, timeperiod=14, nbdevup=2, nbdevdn=2, thup=0.3, thdn=0.1):
...     bb = data.run(
...         "talib_bbands", 
...         timeperiod=timeperiod, 
...         nbdevup=nbdevup, 
...         nbdevdn=nbdevdn
...     )
...     bandwidth = (bb.upperband - bb.lowerband) / bb.middleband
...     cond1 = data.low < bb.lowerband
...     cond2 = bandwidth > thup
...     cond3 = data.high > bb.upperband
...     cond4 = bandwidth < thdn
...     entries = (cond1 & cond2) | (cond3 & cond4)
...     exits = (cond1 & cond4) | (cond3 & cond2)
...     pf = vbt.PF.from_signals(data, entries, exits)
...     return pf.sharpe_ratio

>>> bbands_sharpe(
...     vbt.YFData.pull("BTC-USD"),
...     nbdevup=vbt.Param([1, 2]),  # (2)!
...     nbdevdn=vbt.Param([1, 2]),
...     thup=vbt.Param([0.4, 0.5]),
...     thdn=vbt.Param([0.1, 0.2])
... )
```

1. Use `concat` to merge metrics in the form of scalars and Series.

2. Builds the Cartesian product of 4 parameters.

[=100% "Combination 16/16"]{: .candystripe .candystripe-animate }

```pycon
nbdevup  nbdevdn  thup  thdn
1        1        0.4   0.1     1.681532
                        0.2     1.617400
                  0.5   0.1     1.424175
                        0.2     1.563520
          2        0.4   0.1     1.218554
                        0.2     1.520852
                  0.5   0.1     1.242523
                        0.2     1.317883
2        1        0.4   0.1     1.174562
                        0.2     1.469828
                  0.5   0.1     1.427940
                        0.2     1.460635
          2        0.4   0.1     1.000210
                        0.2     1.378108
                  0.5   0.1     1.196087
                        0.2     1.782502
dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#riskfolio-lib
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Riskfolio-Lib
type: heading 2
---

## Riskfolio-Lib

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_7_0.svg){ loading=lazy }

- [x] [Riskfolio-Lib](https://github.com/dcajasn/Riskfolio-Lib) is another increasingly popular library
for portfolio optimization that has been integrated into VBT. Integration was done
by automating typical workflows inside Riskfolio-Lib and putting them into a single function,
so many portfolio optimization problems can be expressed using a single set of keyword arguments
and easily parameterized.

!!! example "Tutorial"
    Learn more in the [Portfolio optimization](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization) tutorial.

**Run Nested Clustered Optimization (NCO) on a monthly basis**

```pycon
>>> data = vbt.YFData.pull(
...     ["SPY", "TLT", "XLF", "XLE", "XLU", "XLK", "XLB", "XLP", "XLY", "XLI", "XLV"],
...     start="2020",
...     end="2023",
...     missing_index="drop"
... )
>>> pfo = vbt.PFO.from_riskfolio(
...     returns=data.close.vbt.to_returns(),
...     port_cls="hc",
...     every="M"
... )
>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/riskfolio_lib.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/riskfolio_lib.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#array-like-parameters
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Array-like parameters
type: heading 2
---

## Array-like parameters

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_5_0.svg){ loading=lazy }

- [x] The broadcasting mechanism has been completely refactored and now supports parameters. Many parameters
in VBT, such as SL and TP, are array-like and can be provided per row, per column, or even per element.
Internally, even a scalar is treated as a regular time series and is broadcast along with other proper
time series. Previously, to test multiple parameter combinations, you had to tile other time series so that
all shapes matched perfectly. With this feature, the tiling procedure is performed automatically!

**Write a steep slope indicator without indicator factory**

```pycon
>>> def steep_slope(close, up_th):
...     r = vbt.broadcast(dict(close=close, up_th=up_th))
...     return r["close"].pct_change() >= r["up_th"]

>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2022")
>>> fig = data.plot(plot_volume=False)
>>> sma = vbt.talib("SMA").run(data.close, timeperiod=50).real
>>> sma.rename("SMA").vbt.plot(fig=fig)
>>> mask = steep_slope(sma, vbt.Param([0.005, 0.01, 0.015]))  # (1)!

>>> def plot_mask_ranges(column, color):
...     mask.vbt.ranges.plot_shapes(
...         column=column,
...         plot_close=False,
...         shape_kwargs=dict(fillcolor=color),
...         fig=fig
...     )
>>> plot_mask_ranges(0.005, "orangered")
>>> plot_mask_ranges(0.010, "orange")
>>> plot_mask_ranges(0.015, "yellow")
>>> fig.update_xaxes(showgrid=False)
>>> fig.update_yaxes(showgrid=False)
>>> fig.show()
```

1. Tests three parameters and generates a mask with three columns, one for each parameter.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/steep_slope.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/steep_slope.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#parameters
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Parameters
type: heading 2
---

## Parameters

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_5_0.svg){ loading=lazy }

- [x] There is a new module for working with parameters.

**Generate 10,000 random parameter combinations for MACD**

```pycon
>>> from itertools import combinations

>>> window_space = np.arange(100)
>>> fastk_windows, slowk_windows = list(zip(*combinations(window_space, 2)))  # (1)!
>>> window_type_space = list(vbt.enums.WType)
>>> param_product = vbt.combine_params(
...     dict(
...         fast_window=vbt.Param(fastk_windows, level=0),  # (2)!
...         slow_window=vbt.Param(slowk_windows, level=0),
...         signal_window=vbt.Param(window_space, level=1),
...         macd_wtype=vbt.Param(window_type_space, level=2),  # (3)!
...         signal_wtype=vbt.Param(window_type_space, level=2),
...     ),
...     random_subset=10_000,
...     build_index=False
... )
>>> pd.DataFrame(param_product)
      fast_window  slow_window  signal_window  macd_wtype  signal_wtype
0               0            1             47           3             3
1               0            2             21           2             2
2               0            2             33           1             1
3               0            2             42           1             1
4               0            3             52           1             1
...           ...          ...            ...         ...           ...
9995           97           99             19           1             1
9996           97           99             92           4             4
9997           98           99              2           2             2
9998           98           99             12           1             1
9999           98           99             81           2             2

[10000 rows x 5 columns]
```

1. Fast windows should be shorter than slow windows.

2. Fast and slow windows were already combined, so they share the same product level.

3. Window types do not need to be combined, so they share the same product level.

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#portfolio-optimization
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Portfolio optimization
type: heading 2
---

## Portfolio optimization

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_0.svg){ loading=lazy }

- [x] Portfolio optimization is the process of creating a portfolio of assets that aims to maximize return
and minimize risk. Usually, this process is performed periodically and involves
generating new weights to rebalance an existing portfolio. As with most things in VBT,
the weight generation step is implemented as a callback by the user, while the optimizer
calls that callback periodically. The final result is a collection of returned weight allocations
that can be analyzed, visualized, and used in actual simulations :pie:

!!! example "Tutorial"
    Learn more in the [Portfolio optimization](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization) tutorial.

**Allocate assets inversely to their total return in the last month**

```pycon
>>> def regime_change_optimize_func(data):
...     returns = data.returns
...     total_return = returns.vbt.returns.total()
...     weights = data.symbol_wrapper.fill_reduced(0)
...     pos_mask = total_return > 0
...     if pos_mask.any():
...         weights[pos_mask] = total_return[pos_mask] / total_return.abs().sum()
...     neg_mask = total_return < 0
...     if neg_mask.any():
...         weights[neg_mask] = total_return[neg_mask] / total_return.abs().sum()
...     return -1 * weights

>>> data = vbt.YFData.pull(
...     ["SPY", "TLT", "XLF", "XLE", "XLU", "XLK", "XLB", "XLP", "XLY", "XLI", "XLV"],
...     start="2020",
...     end="2023",
...     missing_index="drop"
... )
>>> pfo = vbt.PFO.from_optimize_func(
...     data.symbol_wrapper,
...     regime_change_optimize_func,
...     vbt.RepEval("data[index_slice]", context=dict(data=data)),
...     every="M"
... )
>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/portfolio_optimization.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/portfolio_optimization.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#pyportfolioopt
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: PyPortfolioOpt
type: heading 2
---

## PyPortfolioOpt

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_0.svg){ loading=lazy }

- [x] [PyPortfolioOpt](https://github.com/robertmartin8/PyPortfolioOpt) is a popular financial portfolio
optimization package that includes both classical methods (Markowitz 1952 and Black-Litterman),
suggested best practices (such as covariance shrinkage), and many recent developments and novel
features, like L2 regularization, shrunk covariance, and hierarchical risk parity.

!!! example "Tutorial"
    Learn more in the [Portfolio optimization](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization) tutorial.

**Run Nested Clustered Optimization (NCO) on a monthly basis**

```pycon
>>> data = vbt.YFData.pull(
...     ["SPY", "TLT", "XLF", "XLE", "XLU", "XLK", "XLB", "XLP", "XLY", "XLI", "XLV"],
...     start="2020",
...     end="2023",
...     missing_index="drop"
... )
>>> pfo = vbt.PFO.from_pypfopt(
...     returns=data.returns,
...     optimizer="hrp",
...     target="optimize",
...     every="M"
... )
>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/pyportfolioopt.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/pyportfolioopt.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#universal-portfolios
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: Universal Portfolios
type: heading 2
---

## Universal Portfolios

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_0.svg){ loading=lazy }

- [x] [Universal Portfolios](https://github.com/Marigold/universal-portfolios) is a package that
brings together various Online Portfolio Selection (OLPS) algorithms.

!!! example "Tutorial"
    Learn more in the [Portfolio optimization](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization) tutorial.

**Simulate an online minimum-variance portfolio on a weekly time frame**

```pycon
>>> data = vbt.YFData.pull(
...     ["SPY", "TLT", "XLF", "XLE", "XLU", "XLK", "XLB", "XLP", "XLY", "XLI", "XLV"],
...     start="2020",
...     end="2023",
...     missing_index="drop"
... )
>>> pfo = vbt.PFO.from_universal_algo(
...     "MPT",
...     data.resample("W").close,
...     window=52,
...     min_history=4,
...     mu_estimator='historical',
...     cov_estimator='empirical',
...     method='mpt',
...     q=0
... )
>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/universal_portfolios.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/universal_portfolios.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/optimization/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/optimization/#optimization
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/optimization.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Performance
type: page
icon: material-speedometer
---

Performance features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#chunk-caching
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Chunk caching
type: heading 2
---

## Chunk caching

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_5_15.svg){ loading=lazy }

- [x] Most workflows that use the VBT's execution framework—such as data pulling, chunking,
parameterization, splitting, and optimization—can now offload intermediate results to disk and
reload them if the workflow crashes and restarts. You can confidently test billions of parameter
combinations on cloud instances without worrying about losing your data.

**Execute a basic range with random fallouts while caching successful attempts**

```pycon
>>> @vbt.parameterized(cache_chunks=True, chunk_len=1)  # (1)!
... def basic_iterator(i):
...     print("i:", i)
...     rand_number = np.random.uniform()
...     if rand_number < 0.2:
...         print("failed ⛔")
...         raise ValueError
...     return i

>>> attempt = 0
>>> while True:
...     attempt += 1
...     print("attempt", attempt)
...     try:
...         basic_iterator(vbt.Param(np.arange(10)))
...         print("completed 🎉")
...         break
...     except ValueError:
...         pass
attempt 1
i: 0
i: 1
failed ⛔
attempt 2
i: 1
i: 2
i: 3
i: 4
failed ⛔
attempt 3
i: 4
i: 5
i: 6
i: 7
i: 8
i: 9
completed 🎉
```

1. Cache each function call.

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#accumulators
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Accumulators
type: heading 2
---

## Accumulators

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_10.svg){ loading=lazy }

- [x] Most rolling indicators implemented with Pandas and NumPy require multiple passes over the
data. For example, calculating the sum of three arrays requires at least two passes. If you want to
calculate such an indicator iteratively (bar by bar), you either need to pre-calculate everything and
store it in memory or re-calculate each window, which can significantly impact performance.
[Accumulators](https://theboostcpplibraries.com/boost.accumulators), however, maintain an internal
state that allows you to compute an indicator value as soon as a new data point arrives, resulting in
the best possible performance.

**Design a one-pass rolling z-score**

```pycon
>>> @njit
... def fastest_rolling_zscore_1d_nb(arr, window, minp=None, ddof=1):
...     if minp is None:
...         minp = window
...     out = np.full(arr.shape, np.nan)
...     cumsum = 0.0
...     cumsum_sq = 0.0
...     nancnt = 0
...     
...     for i in range(len(arr)):
...         pre_window_value = arr[i - window] if i - window >= 0 else np.nan
...         mean_in_state = vbt.nb.RollMeanAIS(
...             i, arr[i], pre_window_value, cumsum, nancnt, window, minp
...         )
...         mean_out_state = vbt.nb.rolling_mean_acc_nb(mean_in_state)
...         _, _, _, mean = mean_out_state
...         std_in_state = vbt.nb.RollStdAIS(
...             i, arr[i], pre_window_value, cumsum, cumsum_sq, nancnt, window, minp, ddof
...         )
...         std_out_state = vbt.nb.rolling_std_acc_nb(std_in_state)
...         cumsum, cumsum_sq, nancnt, _, std = std_out_state
...         out[i] = (arr[i] - mean) / std
...     return out
    
>>> data = vbt.YFData.pull("BTC-USD")
>>> rolling_zscore = fastest_rolling_zscore_1d_nb(data.returns.values, 14)
>>> data.symbol_wrapper.wrap(rolling_zscore)
Date
2014-09-17 00:00:00+00:00         NaN
2014-09-18 00:00:00+00:00         NaN
2014-09-19 00:00:00+00:00         NaN
                                  ...   
2023-02-01 00:00:00+00:00    0.582381
2023-02-02 00:00:00+00:00   -0.705441
2023-02-03 00:00:00+00:00   -0.217880
Freq: D, Name: BTC-USD, Length: 3062, dtype: float64

>>> (data.returns - data.returns.rolling(14).mean()) / data.returns.rolling(14).std()
Date
2014-09-17 00:00:00+00:00         NaN
2014-09-18 00:00:00+00:00         NaN
2014-09-19 00:00:00+00:00         NaN
                                  ...   
2023-02-01 00:00:00+00:00    0.582381
2023-02-02 00:00:00+00:00   -0.705441
2023-02-03 00:00:00+00:00   -0.217880
Freq: D, Name: Close, Length: 3062, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#chunking
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Chunking
type: heading 2
---

## Chunking

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] A new, innovative chunking mechanism lets you specify how arguments should be chunked. It
automatically splits array-like arguments, passes each chunk to the function for execution, and then
merges the results. This enables you to split large arrays and run any function in a distributed
manner. VBT also features a central registry and provides chunking specifications for all arguments
of most Numba-compiled functions, including simulation functions. Chunking can be enabled with a
single command. You no longer have to worry about out-of-memory errors! :tada:

**Backtest at most 100 parameter combinations at once**

```pycon
>>> @vbt.chunked(
...     chunk_len=100,
...     merge_func="concat",  # (1)!
...     execute_kwargs=dict(  # (2)!
...         clear_cache=True,
...         collect_garbage=True
...     )
... )
... def backtest(data, fast_windows, slow_windows):  # (3)!
...     fast_ma = vbt.MA.run(data.close, fast_windows, short_name="fast")
...     slow_ma = vbt.MA.run(data.close, slow_windows, short_name="slow")
...     entries = fast_ma.ma_crossed_above(slow_ma)
...     exits = fast_ma.ma_crossed_below(slow_ma)
...     pf = vbt.PF.from_signals(data.close, entries, exits)
...     return pf.total_return

>>> param_product = vbt.combine_params(  # (4)!
...     dict(
...         fast_window=vbt.Param(range(2, 100), condition="fast_window < slow_window"),
...         slow_window=vbt.Param(range(2, 100)),
...     ),
...     build_index=False
... )
>>> backtest(
...     vbt.YFData.pull(["BTC-USD", "ETH-USD"]),  # (5)!
...     vbt.Chunked(param_product["fast_window"]),  # (6)!
...     vbt.Chunked(param_product["slow_window"])
... )
```

1. Concatenate the Series returned by each chunk into one Series.

2. Show a progress bar, and also clear cache and collect garbage after processing each chunk.

3. This function takes a data instance, as well as two parameter arrays: fast and slow window lengths.
Both arrays will have the same number of values; for example, the first combination matches the
first value in `fast_windows` with the first value in `slow_windows`.

4. Generate conditional parameter combinations.

5. Do not split the data into chunks.

6. Split both parameter arrays into chunks.

[=100% "Chunk 48/48"]{: .candystripe .candystripe-animate }

```pycon
fast_window  slow_window  symbol 
2            3            BTC-USD    193.124482
                          ETH-USD     12.247315
              4            BTC-USD    159.600953
                          ETH-USD     15.825041
              5            BTC-USD    124.703676
                                        ...    
97           98           ETH-USD      3.947346
              99           BTC-USD     25.551881
                          ETH-USD      3.442949
98           99           BTC-USD     27.943574
                          ETH-USD      3.540720
Name: total_return, Length: 9506, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#parallel-numba
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Parallel Numba
type: heading 2
---

## Parallel Numba

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Most Numba-compiled functions have been rewritten to process columns in parallel using
[automatic parallelization with `@jit`](https://numba.readthedocs.io/en/stable/user/parallel.html).
You can enable this with a single command. This approach is best for lightweight functions that are
applied to wide arrays.

**Benchmark the rolling mean without and with parallelization**

```pycon
>>> df = pd.DataFrame(np.random.uniform(size=(1000, 1000)))

>>> %timeit df.rolling(10).mean()  # (1)!
45.6 ms ± 138 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %timeit df.vbt.rolling_mean(10)  # (2)!
5.33 ms ± 302 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit df.vbt.rolling_mean(10, jitted=dict(parallel=True))  # (3)!
1.82 ms ± 5.21 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

1. Using Pandas.

2. Using Numba without parallelization.

3. Using Numba with parallelization.

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#multithreading
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Multithreading
type: heading 2
---

## Multithreading

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Integration of [ThreadPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#threadpoolexecutor)
from `concurrent.futures`, [ThreadPool](https://pathos.readthedocs.io/en/latest/pathos.html#pathos.pools.ThreadPool)
from `pathos`, and [Dask](https://dask.org/) backend for running multiple chunks across multiple
threads. This is best for speeding up heavyweight functions that release the GIL, such as Numba and C
functions. Multithreading + Chunking + Numba = :muscle:

**Benchmark 1000 random portfolios without and with multithreading**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"])

>>> %timeit vbt.PF.from_random_signals(data.close, n=[100] * 1000)
613 ms ± 37.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit vbt.PF.from_random_signals(data.close, n=[100] * 1000, chunked="threadpool")
294 ms ± 8.91 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#multiprocessing
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Multiprocessing
type: heading 2
---

## Multiprocessing

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Integration of [ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#processpoolexecutor)
from `concurrent.futures`, [ProcessPool](https://pathos.readthedocs.io/en/latest/pathos.html#pathos.pools.ProcessPool)
and [ParallelPool](https://pathos.readthedocs.io/en/latest/pathos.html#pathos.pools.ParallelPool) from
`pathos`, [WorkerPool](https://sybrenjansen.github.io/mpire/usage/workerpool/index.html) from
`mpire`, and [Ray](https://www.ray.io/) backend for running multiple chunks across multiple processes.
This is best for speeding up heavyweight functions that do not release the GIL, such as regular Python
functions, as well as lightweight arguments that are easy to serialize. Ever wanted to test billions of
hyperparameter combinations in just a few minutes? Now you can by scaling functions and entire
applications in the cloud using [Ray clusters](https://docs.ray.io/en/latest/cluster/getting-started.html)
:eyes:

**Benchmark running a slow function on each column without and with multiprocessing**

```pycon
>>> @vbt.chunked(
...     size=vbt.ArraySizer(arg_query="items", axis=1),
...     arg_take_spec=dict(
...         items=vbt.ArraySelector(axis=1)
...     ),
...     merge_func=np.column_stack
... )
... def bubble_sort(items):
...     items = items.copy()
...     for i in range(len(items)):
...         for j in range(len(items) - 1 - i):
...             if items[j] > items[j + 1]:
...                 items[j], items[j + 1] = items[j + 1], items[j]
...     return items
                
>>> items = np.random.uniform(size=(1000, 3))

>>> %timeit bubble_sort(items)
456 ms ± 1.36 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit bubble_sort(items, _execute_kwargs=dict(engine="pathos"))
165 ms ± 1.51 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#jitting
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Jitting
type: heading 2
---

## Jitting

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Jitting stands for just-in-time compiling. In the VBT universe, however, jitting simply
means accelerating. While Numba remains the primary jitter, VBT now allows you to implement
custom jitter classes, such as those for vectorized NumPy or even [JAX](https://github.com/google/jax)
with GPU support. Every jitted function is registered globally, so you can switch between different
implementations or even disable jitting entirely with a single command.

**Run different implementations of the cumulative sum**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="7 days ago")
>>> log_returns = np.log1p(data.close.pct_change())
>>> log_returns.vbt.cumsum()  # (1)!
Date
2023-01-31 00:00:00+00:00    0.000000
2023-02-01 00:00:00+00:00    0.024946
2023-02-02 00:00:00+00:00    0.014271
2023-02-03 00:00:00+00:00    0.013310
2023-02-04 00:00:00+00:00    0.008288
2023-02-05 00:00:00+00:00   -0.007967
2023-02-06 00:00:00+00:00   -0.010087
Freq: D, Name: Close, dtype: float64

>>> log_returns.vbt.cumsum(jitted=False)  # (2)!
Date
2023-01-31 00:00:00+00:00    0.000000
2023-02-01 00:00:00+00:00    0.024946
2023-02-02 00:00:00+00:00    0.014271
2023-02-03 00:00:00+00:00    0.013310
2023-02-04 00:00:00+00:00    0.008288
2023-02-05 00:00:00+00:00   -0.007967
2023-02-06 00:00:00+00:00   -0.010087
Freq: D, Name: Close, dtype: float64

>>> @vbt.register_jitted(task_id_or_func=vbt.nb.nancumsum_nb)  # (3)!
... def nancumsum_np(arr):
...     return np.nancumsum(arr, axis=0)

>>> log_returns.vbt.cumsum(jitted="np")  # (4)!
Date
2023-01-31 00:00:00+00:00    0.000000
2023-02-01 00:00:00+00:00    0.024946
2023-02-02 00:00:00+00:00    0.014271
2023-02-03 00:00:00+00:00    0.013310
2023-02-04 00:00:00+00:00    0.008288
2023-02-05 00:00:00+00:00   -0.007967
2023-02-06 00:00:00+00:00   -0.010087
Freq: D, Name: Close, dtype: float64
```

1. Using the built-in Numba-compiled function.

2. Using the built-in function but with Numba disabled → regular Python → slow!

3. Register a NumPy version for the built-in Numba function.

4. Using the NumPy version.

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#caching
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Caching
type: heading 2
---

## Caching

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Caching has been completely reimplemented and is now managed by a central registry.
This enables tracking useful statistics for all cacheable parts of VBT, such as showing the total
cached size in MB. You get full control and transparency :window:

**Get the cache statistics after computing the statistics of a random portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_random_signals(data.close, n=5)
>>> _ = pf.stats()

>>> pf.get_ca_setup().get_status_overview(
...     filter_func=lambda setup: setup.caching_enabled,
...     include=["hits", "misses", "total_size"]
... )
                                  hits  misses total_size
object                                                  
portfolio:0.drawdowns               0       1    70.9 kB
portfolio:0.exit_trades             0       1    70.5 kB
portfolio:0.filled_close            6       1    24.3 kB
portfolio:0.init_cash               3       1   32 Bytes
portfolio:0.init_position           0       1   32 Bytes
portfolio:0.init_position_value     0       1   32 Bytes
portfolio:0.init_value              5       1   32 Bytes
portfolio:0.input_value             1       1   32 Bytes
portfolio:0.orders                  9       1    69.7 kB
portfolio:0.total_profit            1       1   32 Bytes
portfolio:0.trades                  0       1    70.5 kB
```

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#hyperfast-rolling-metrics
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: Hyperfast rolling metrics
type: heading 2
---

## Hyperfast rolling metrics

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Rolling metrics based on returns have been optimized for maximum performance—up to 1000x speedup!

**Benchmark the rolling Sortino ratio**

```pycon
>>> import quantstats as qs

>>> index = vbt.date_range("2020", periods=100000, freq="1min")
>>> returns = pd.Series(np.random.normal(0, 0.001, size=len(index)), index=index)

>>> %timeit qs.stats.rolling_sortino(returns, rolling_period=10)  # (1)!
2.79 s ± 24.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit returns.vbt.returns.rolling_sortino_ratio(window=10)  # (2)!
8.12 ms ± 199 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. Using QuantStats.

2. Using VBT.

---
link: https://vectorbt.pro/pvt_41531c19/features/performance/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/performance/#performance
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/performance.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Portfolio
type: page
icon: material-chart-areaspline
---

Portfolio features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#asset-weighting
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Asset weighting
type: heading 2
---

## Asset weighting

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_8_20.svg){ loading=lazy }

- [x] Asset weighting lets you fine-tune the influence of individual assets or strategies within your
portfolio, giving you enhanced control over your portfolio's overall performance. The key benefit
is that these weights are not limited to returns—they are consistently applied to all time
series and metrics, including orders, cash flows, and more. This comprehensive approach ensures
that every aspect of your portfolio stays precisely aligned.

**Maximize Sharpe of a random portfolio**

```pycon
>>> data = vbt.YFData.pull(["AAPL", "MSFT", "GOOG"], start="2020")
>>> pf = data.run("from_random_signals", n=vbt.Default(50), seed=42, group_by=True)
>>> pf.get_sharpe_ratio(group_by=False)  # (1)!
symbol
AAPL    1.401012
MSFT    0.456162
GOOG    0.852490
Name: sharpe_ratio, dtype: float64

>>> pf.sharpe_ratio  # (2)!
1.2132857343006869

>>> prices = pf.get_value(group_by=False)
>>> weights = vbt.pypfopt_optimize(prices=prices)  # (3)!
>>> weights
{'AAPL': 0.85232, 'MSFT': 0.0, 'GOOG': 0.14768}

>>> weighted_pf = pf.apply_weights(weights, rescale=True)  # (4)!
>>> weighted_pf.weights
symbol
AAPL    2.55696
MSFT    0.00000
GOOG    0.44304
dtype: float64

>>> weighted_pf.get_sharpe_ratio(group_by=True)  # (5)!
1.426112580298898
```

1. Sharpe ratio for each individual asset.

2. Sharpe ratio for the combined portfolio. The goal is to maximize this value.

3. Maximize Sharpe based on equity (that is, portfolio value) development.

4. Rescale the weights for multiplication and create a new portfolio with the weights applied.

5. Sharpe ratio for the optimized portfolio.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#position-views
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Position views
type: heading 2
---

## Position views

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_8_20.svg){ loading=lazy }

- [x] Position views let you analyze your portfolio by focusing on either long or short positions,
providing a clear and distinct perspective for each investment strategy.

**Separate long and short positions of a basic SMA crossover portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> fast_sma = data.run("talib_func:sma", timeperiod=20)
>>> slow_sma = data.run("talib_func:sma", timeperiod=50)
>>> long_entries = fast_sma.vbt.crossed_above(slow_sma)
>>> short_entries = fast_sma.vbt.crossed_below(slow_sma)
>>> pf = vbt.PF.from_signals(
...     data,
...     long_entries=long_entries,
...     short_entries=short_entries,
...     fees=0.01,
...     fixed_fees=1.0
... )

>>> long_pf = pf.long_view
>>> short_pf = pf.short_view

>>> fig = vbt.make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.01)
>>> fig = long_pf.assets.vbt.plot_against(
...     0, 
...     trace_kwargs=dict(name="Long position", line_shape="hv", line_color="mediumseagreen"),
...     other_trace_kwargs=dict(visible=False),
...     add_trace_kwargs=dict(row=1, col=1),
...     fig=fig
... )
>>> fig = short_pf.assets.vbt.plot_against(
...     0, 
...     trace_kwargs=dict(name="Short position", line_shape="hv", line_color="coral"),
...     other_trace_kwargs=dict(visible=False), 
...     add_trace_kwargs=dict(row=2, col=1),
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/position_views.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/position_views.dark.svg#only-dark){: .iimg loading=lazy }

```pycon
>>> long_pf.sharpe_ratio
0.9185961894435091

>>> short_pf.sharpe_ratio
0.2760864152147919
```

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#index-records
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Index records
type: heading 2
---

## Index records

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_12_0.svg){ loading=lazy }

- [x] How can you backtest time- and asset-anchored queries such as "Order X units of asset Y on date Z"?
Typically, you would need to build a full array and set each detail manually. Now, there is a
simpler way: with preparers and redesigned smart indexing, you can provide all information in a
compressed record format! Behind the scenes, the record array is translated into a set of
[index dictionaries](https://vectorbt.pro/pvt_41531c19/features/productivity/#index-dictionaries)—one for each argument.

**Define a basic signal strategy using records**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], missing_index="drop")
>>> records = [
...     dict(date="2022", symbol="BTC-USD", long_entry=True),  # (1)!
...     dict(date="2022", symbol="ETH-USD", short_entry=True),
...     dict(row=-1, exit=True),
... ]
>>> pf = vbt.PF.from_signals(data, records=records)  # (2)!
>>> pf.orders.readable
    Order Id   Column              Signal Index            Creation Index   
0         0  BTC-USD 2022-01-01 00:00:00+00:00 2022-01-01 00:00:00+00:00  \
1         1  BTC-USD 2023-04-25 00:00:00+00:00 2023-04-25 00:00:00+00:00   
2         0  ETH-USD 2022-01-01 00:00:00+00:00 2022-01-01 00:00:00+00:00   
3         1  ETH-USD 2023-04-25 00:00:00+00:00 2023-04-25 00:00:00+00:00   

                  Fill Index      Size         Price  Fees  Side    Type   
0 2022-01-01 00:00:00+00:00  0.002097  47686.812500   0.0   Buy  Market  \
1 2023-04-25 00:00:00+00:00  0.002097  27534.675781   0.0  Sell  Market   
2 2022-01-01 00:00:00+00:00  0.026527   3769.697021   0.0  Sell  Market   
3 2023-04-25 00:00:00+00:00  0.026527   1834.759644   0.0   Buy  Market   

  Stop Type  
0      None  
1      None  
2      None  
3      None  
```

1. Every broadcastable argument is supported. Rows can be specified using "row", "index", "date", or
"datetime". Columns can be specified with "col", "column", or "symbol". If you do not provide a row
or column, the entire row or column will be set, respectively. If neither is provided, the whole
array will be set. Rows and columns can be given as integer positions, labels, datetimes, or even
complex indexers!

2. Arguments not used in records can still be provided as usual. Arguments used in records can
also be provided to serve as defaults.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio-preparers
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Portfolio preparers
type: heading 2
---

## Portfolio preparers

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_12_0.svg){ loading=lazy }

- [x] When you pass an argument to a simulation method such as `Portfolio.from_signals`,
it goes through a complex preparation pipeline to convert it into a format suitable for Numba.
This pipeline usually involves enum mapping, broadcasting, data type checks, template
substitution, and many other steps. To make VBT more transparent, this pipeline has been
moved to a separate class, giving you full control over the arguments that reach the Numba
functions! You can even extend the preparers to automatically prepare arguments for your
own simulators.

**Get a prepared argument, modify it, and use in a new simulation**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", end="2017-01")
>>> prep_result = vbt.PF.from_holding(
...     data, 
...     stop_ladder="uniform",
...     tp_stop=vbt.Param([
...         [0.1, 0.2, 0.3, 0.4, 0.5],
...         [0.4, 0.5, 0.6],
...     ], keys=["tp_ladder_1", "tp_ladder_2"]),
...     return_prep_result=True  # (1)!
... )
>>> prep_result.target_args["tp_stop"]  # (2)!
array([[0.1, 0.4],
        [0.2, 0.5],
        [0.3, 0.6],
        [0.4, nan],
        [0.5, nan]])
        
>>> new_tp_stop = prep_result.target_args["tp_stop"] + 0.1
>>> new_prep_result = prep_result.replace(target_args=dict(tp_stop=new_tp_stop), nested_=True)  # (3)!
>>> new_prep_result.target_args["tp_stop"]
array([[0.2, 0.5],
        [0.3, 0.6],
        [0.4, 0.7],
        [0.5, nan],
        [0.6, nan]])
        
>>> pf = vbt.PF.from_signals(new_prep_result)  # (4)!
>>> pf.total_return
tp_stop
tp_ladder_1    0.4
tp_ladder_2    0.6
Name: total_return, dtype: float64

>>> sim_out = new_prep_result.target_func(**new_prep_result.target_args)  # (5)!
>>> pf = vbt.PF(order_records=sim_out, **new_prep_result.pf_args)
>>> pf.total_return
tp_stop
tp_ladder_1    0.4
tp_ladder_2    0.6
Name: total_return, dtype: float64
```

1. Returns the result of the preparation.

2. Contains two attributes: target arguments as `target_args` and portfolio arguments as `pf_args`.

3. Replace the argument. Since it is inside another dictionary (`target_args`), you need to enable
`nested_`. The result is a new instance of `PFPrepResult`.

4. Pass the new preparation result as the first argument to the base simulation method.

5. Or simulate manually!

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#stop-laddering
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Stop laddering
type: heading 2
---

## Stop laddering

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_12_0.svg){ loading=lazy }

- [x] Stop laddering is a technique for incrementally moving out of a position. Instead of providing
a single stop value to close a position, you can provide an array of stop values, with each one
removing a certain amount of the position when triggered. You can control this amount by choosing
a different ladder mode. Thanks to a new broadcasting feature that allows arrays to broadcast
along just one axis, the stop values do not need to have the same shape as the data. You can even
provide stop arrays of different shapes as parameters!

**Test two TP ladders**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", end="2017-01")
>>> pf = vbt.PF.from_holding(
...     data, 
...     stop_ladder="uniform",
...     tp_stop=vbt.Param([
...         [0.1, 0.2, 0.3, 0.4, 0.5],
...         [0.4, 0.5, 0.6],
...     ], keys=["tp_ladder_1", "tp_ladder_2"])
... )
>>> pf.trades.plot(column="tp_ladder_1").show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/stop_laddering.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/stop_laddering.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#staticization
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Staticization
type: heading 2
---

## Staticization

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_11_0.svg){ loading=lazy }

- [x] One major limitation of Numba is that functions passed as arguments (that is, callbacks)
make the main function uncacheable, forcing it to be recompiled in every new runtime,
again and again. This especially affects the performance of simulator functions, as they can
take up to a minute to compile. Thankfully, there is a new trick available: "staticization".
Here is how it works. First, the source code of a function is annotated with a special syntax.
The annotated code is then extracted (also called "cutting" :scissors:), modified into a
cacheable version by removing any callbacks from the arguments, and saved to a Python file.
Once the function is called again, the cacheable version is executed. Sound complicated?
Take a look below!

**Define the signal function in a Python file, here signal_func_nb.py**

```python
from vectorbtpro import *

@njit
def signal_func_nb(c, fast_sma, slow_sma):  # (1)!
    long = vbt.pf_nb.iter_crossed_above_nb(c, fast_sma, slow_sma)
    short = vbt.pf_nb.iter_crossed_below_nb(c, fast_sma, slow_sma)
    return long, False, short, False
```

1. Make sure the function is named exactly the same as the callback argument.

**Run a staticized simulation. Running the script again won't re-compile.**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_signals(
...     data,
...     signal_func_nb="signal_func_nb.py",  # (1)!
...     signal_args=(vbt.Rep("fast_sma"), vbt.Rep("slow_sma")),
...     broadcast_named_args=dict(
...         fast_sma=data.run("sma", 20, hide_params=True, unpack=True), 
...         slow_sma=data.run("sma", 50, hide_params=True, unpack=True)
...     ),
...     staticized=True  # (2)!
... )
```

1. Path to the module where the function is located.

2. Handles all the magic :sparkles:

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#position-info
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Position info
type: heading 2
---

## Position info

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_11_0.svg){ loading=lazy }

- [x] Dynamic signal functions now have access to the current position information, such as (open) P&L.

**Enter randomly, exit randomly but only if in profit**

```pycon
>>> @njit
... def signal_func_nb(c, entries, exits):
...     is_entry = vbt.pf_nb.select_nb(c, entries)
...     is_exit = vbt.pf_nb.select_nb(c, exits)
...     if is_entry:
...         return True, False, False, False
...     if is_exit:
...         pos_info = c.last_pos_info[c.col]
...         if pos_info["status"] == vbt.pf_enums.TradeStatus.Open:
...             if pos_info["pnl"] >= 0:
...                 return False, True, False, False
...     return False, False, False, False
    
>>> data = vbt.YFData.pull("BTC-USD")
>>> entries, exits = data.run("RANDNX", n=10, unpack=True)
>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(vbt.Rep("entries"), vbt.Rep("exits")),
...     broadcast_named_args=dict(entries=entries, exits=exits),
...     jitted=False  # (1)!
... )
>>> pf.trades.readable[["Entry Index", "Exit Index", "PnL"]]
                Entry Index                Exit Index           PnL
0 2014-11-01 00:00:00+00:00 2016-01-08 00:00:00+00:00     39.134739
1 2016-03-27 00:00:00+00:00 2016-09-07 00:00:00+00:00     61.220063
2 2016-12-24 00:00:00+00:00 2016-12-31 00:00:00+00:00     14.471414
3 2017-03-16 00:00:00+00:00 2017-08-05 00:00:00+00:00    373.492028
4 2017-09-12 00:00:00+00:00 2018-05-05 00:00:00+00:00    815.699284
5 2019-02-15 00:00:00+00:00 2019-11-10 00:00:00+00:00   2107.383227
6 2019-12-04 00:00:00+00:00 2019-12-10 00:00:00+00:00     12.630214
7 2020-07-12 00:00:00+00:00 2021-11-14 00:00:00+00:00  21346.035444
8 2022-01-15 00:00:00+00:00 2023-03-06 00:00:00+00:00 -11925.133817
```

1. Disable Numba during testing to avoid compilation.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#time-stops
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Time stops
type: heading 2
---

## Time stops

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_11_0.svg){ loading=lazy }

- [x] Joining other stop orders, time stop orders can close a position either after a certain
period of time or on a specific date.

**Enter randomly, exit before the end of the month**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2022-01", end="2022-04")
>>> entries = vbt.pd_acc.signals.generate_random(data.symbol_wrapper, n=10)
>>> pf = vbt.PF.from_signals(data, entries, dt_stop="M")  # (1)!
>>> pf.orders.readable[["Fill Index", "Side", "Stop Type"]]
                  Fill Index  Side Stop Type
0 2022-01-19 00:00:00+00:00   Buy      None
1 2022-01-31 00:00:00+00:00  Sell        DT
2 2022-02-25 00:00:00+00:00   Buy      None
3 2022-02-28 00:00:00+00:00  Sell        DT
4 2022-03-11 00:00:00+00:00   Buy      None
5 2022-03-31 00:00:00+00:00  Sell        DT
```

1. Use `dt_stop` for datetime-based stops and `td_stop` for timedelta-based stops.
Datetime-based stops can be periods ("D"), timestamps ("2023-01-01"), and even specific times
("18:00").

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#target-size-to-signals
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Target size to signals
type: heading 2
---

## Target size to signals

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_10_0.svg){ loading=lazy }

- [x] Target size can be converted to signals using a special signal function, giving access to
stop and limit order functionality. This is especially useful, for example, in portfolio
optimization.

**Perform the Mean-Variance Optimization with SL and TP**

```pycon
>>> data = vbt.YFData.pull(
...     ["SPY", "TLT", "XLF", "XLE", "XLU", "XLK", "XLB", "XLP", "XLY", "XLI", "XLV"],
...     start="2022",
...     end="2023",
...     missing_index="drop"
... )
>>> pfo = vbt.PFO.from_riskfolio(data.returns, every="M")
>>> pf = pfo.simulate(
...     data, 
...     pf_method="from_signals", 
...     sl_stop=0.05,
...     tp_stop=0.1, 
...     stop_exit_price="close"  # (1)!
... )
>>> pf.plot_allocations().show()
```

1. Otherwise, user and stop signals will occur at different times within the same bar,
making it impossible to establish the correct order of execution.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/target_size_to_signals.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/target_size_to_signals.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#target-price
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Target price
type: heading 2
---

## Target price

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_10_0.svg){ loading=lazy }

- [x] Limit and stop orders can now be defined using a target price instead of a delta.

**Set the SL to the previous low in a random portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_random_signals(
...     data, 
...     n=100, 
...     sl_stop=data.low.vbt.ago(1),
...     delta_format="target"
... )
>>> sl_orders = pf.orders.stop_type_sl
>>> signal_index = pf.wrapper.index[sl_orders.signal_idx.values]
>>> hit_index = pf.wrapper.index[sl_orders.idx.values]
>>> hit_after = hit_index - signal_index
>>> hit_after
TimedeltaIndex([ '7 days',  '3 days',  '1 days',  '5 days',  '4 days',
                  '1 days', '28 days',  '1 days',  '1 days',  '1 days',
                  '1 days',  '1 days', '13 days', '10 days',  '5 days',
                  '1 days',  '3 days',  '4 days',  '1 days',  '9 days',
                  '5 days',  '1 days',  '1 days',  '2 days',  '1 days',
                  '1 days',  '1 days',  '3 days',  '1 days',  '1 days',
                  '1 days',  '1 days',  '1 days',  '2 days',  '2 days',
                  '1 days', '12 days',  '3 days',  '1 days',  '1 days',
                  '1 days',  '1 days',  '1 days',  '3 days',  '1 days',
                  '1 days',  '1 days',  '4 days',  '1 days',  '1 days',
                  '2 days',  '6 days', '11 days',  '1 days',  '2 days',
                  '1 days',  '1 days',  '1 days',  '1 days',  '1 days',
                  '4 days', '10 days',  '1 days',  '1 days',  '1 days',
                  '2 days',  '3 days',  '1 days'],
                dtype='timedelta64[ns]', name='Date', freq=None)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#leverage
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Leverage
type: heading 2
---

## Leverage

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_9_0.svg){ loading=lazy }

- [x] Leverage is now an integral part of portfolio simulation. Supports two leverage
modes: `lazy` (enables leverage only if there is not enough cash) and `eager` (enables
leverage while using only part of the available cash). Allows setting leverage per order,
and can also determine the optimal leverage value automatically to fulfill any order
requirement! :person_lifting_weights:

**Explore how leverage affects the equity curve in a random portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2022")
>>> pf = vbt.PF.from_random_signals(
...     data, 
...     n=100,  
...     leverage=vbt.Param([0.5, 1, 2, 3]),
... )
>>> pf.value.vbt.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/leverage.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/leverage.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#order-delays
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Order delays
type: heading 2
---

## Order delays

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_4_1.svg){ loading=lazy }

- [x] By default, VBT executes every order at the end of the current bar. Previously, if you wanted to delay
execution to the next bar, you had to manually shift all order-related arrays by one bar, which made the
process error-prone. Now, you can simply specify how many bars in the past should be used to take order
information from. In addition, the price argument now supports "nextopen" and "nextclose" as options,
providing a one-line solution.

**Compare orders without and with a delay in a random portfolio**

```pycon
>>> pf = vbt.PF.from_random_signals(
...     vbt.YFData.pull("BTC-USD", start="2021-01", end="2021-02"), 
...     n=3, 
...     price=vbt.Param(["close", "nextopen"])
... )
>>> fig = pf.orders["close"].plot(
...     buy_trace_kwargs=dict(name="Buy (close)", marker=dict(symbol="triangle-up-open")),
...     sell_trace_kwargs=dict(name="Buy (close)", marker=dict(symbol="triangle-down-open"))
... )
>>> pf.orders["nextopen"].plot(
...     plot_ohlc=False,
...     plot_close=False,
...     buy_trace_kwargs=dict(name="Buy (nextopen)"),
...     sell_trace_kwargs=dict(name="Sell (nextopen)"),
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/order_delays.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/order_delays.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#signal-callbacks
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Signal callbacks
type: heading 2
---

## Signal callbacks

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_4_0.svg){ loading=lazy }

- [x] Want to customize your simulation based on signals, or even generate signals dynamically according to
the current backtesting environment? Two new callbacks now bring simulator flexibility to the next
level: one lets you generate or override signals for each asset at every bar, and another allows you to
compute user-defined metrics for the entire group at the end of each bar. Both accept a "context"
that contains information about the current simulation state, enabling trading decisions to be made in
a way similar to event-driven backtesters.

**Backtest SMA crossover iteratively**

```pycon
>>> InOutputs = namedtuple("InOutputs", ["fast_sma", "slow_sma"])

>>> def initialize_in_outputs(target_shape):
...     return InOutputs(
...         fast_sma=np.full(target_shape, np.nan),
...         slow_sma=np.full(target_shape, np.nan)
...     )

>>> @njit
... def signal_func_nb(c, fast_window, slow_window):
...     fast_sma = c.in_outputs.fast_sma
...     slow_sma = c.in_outputs.slow_sma
...     fast_start_i = c.i - fast_window + 1
...     slow_start_i = c.i - slow_window + 1
...     if fast_start_i >= 0 and slow_start_i >= 0:
...         fast_sma[c.i, c.col] = np.nanmean(c.close[fast_start_i : c.i + 1])
...         slow_sma[c.i, c.col] = np.nanmean(c.close[slow_start_i : c.i + 1])
...         is_entry = vbt.pf_nb.iter_crossed_above_nb(c, fast_sma, slow_sma)
...         is_exit = vbt.pf_nb.iter_crossed_below_nb(c, fast_sma, slow_sma)
...         return is_entry, is_exit, False, False
...     return False, False, False, False
    
>>> pf = vbt.PF.from_signals(
...     vbt.YFData.pull("BTC-USD"),
...     signal_func_nb=signal_func_nb,
...     signal_args=(50, 200),
...     in_outputs=vbt.RepFunc(initialize_in_outputs),
... )
>>> fig = pf.get_in_output("fast_sma").vbt.plot()
>>> pf.get_in_output("slow_sma").vbt.plot(fig=fig)
>>> pf.orders.plot(plot_ohlc=False, plot_close=False, fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/signal_callbacks.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/signal_callbacks.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#limit-orders
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Limit orders
type: heading 2
---

## Limit orders

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_4_0.svg){ loading=lazy }

- [x] Long-awaited support for limit orders is now available for signal-based simulation! Includes time-in-force 
(TIF) orders such as DAY, GTC, GTD, LOO, and FOK :alarm_clock: You can also reverse a limit order or create it 
using a delta for easier testing.

**Explore how limit delta affects number of orders in a random portfolio**

```pycon
>>> pf = vbt.PF.from_random_signals(
...     vbt.YFData.pull("BTC-USD"), 
...     n=100, 
...     order_type="limit", 
...     limit_delta=vbt.Param(np.arange(0.001, 0.1, 0.001)),  # (1)!
... )
>>> pf.orders.count().vbt.plot(
...     xaxis_title="Limit delta", 
...     yaxis_title="Order count"
... ).show()
```

1. Limit delta is the distance between the close (or any specified price) and the target limit price,
expressed as a percentage. The higher the delta, the lower the chance it will eventually be hit.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/limit_delta.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/limit_delta.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#delta-formats
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Delta formats
type: heading 2
---

## Delta formats

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_4_0.svg){ loading=lazy }

- [x] Previously, stop orders could only be provided as percentages. While this worked for single values,
it often required extra transformations for arrays. For example, setting SL to ATR meant you needed to
know the entry price. More generally, to lock in a specific dollar amount of a trade, you might want to
use a fixed price trailing stop. To address this, VBT now offers multiple stop value formats
("delta formats") to choose from.

**Use ATR as SL**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> atr = vbt.talib("ATR").run(data.high, data.low, data.close).real
>>> pf = vbt.PF.from_holding(
...     data.loc["2022-01-01":"2022-01-07"], 
...     sl_stop=atr.loc["2022-01-01":"2022-01-07"], 
...     delta_format="absolute"
... )
>>> pf.orders.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/delta_formats.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/delta_formats.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#bar-skipping
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Bar skipping
type: heading 2
---

## Bar skipping

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_4_0.svg){ loading=lazy }

- [x] Simulation based on orders and signals can now (partially) skip bars that do not define any orders,
often resulting in significant speedups for strategies with sparsely distributed orders.

**Benchmark bar skipping in a buy-and-hold portfolio**

```pycon
>>> data = vbt.BinanceData.pull("BTCUSDT", start="one month ago UTC", timeframe="minute")
>>> size = data.symbol_wrapper.fill(np.nan)
>>> size[0] = np.inf
```

```pycon
>>> %%timeit
>>> vbt.PF.from_orders(data, size, ffill_val_price=True)  # (1)!
5.92 ms ± 300 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. When enabled (default), this argument forces the simulation to process every bar.

```pycon
>>> %%timeit
>>> vbt.PF.from_orders(data, size, ffill_val_price=False)
2.75 ms ± 16 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#signal-contexts
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Signal contexts
type: heading 2
---

## Signal contexts

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_3.svg){ loading=lazy }

- [x] Signal generation functions have been redesigned to operate on contexts.
This allows you to design more complex signal strategies with less namespace pollution.

!!! example "Tutorial"
    Learn more in the [Signal development](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development) tutorial.

**Entry at the first bar of the week, exit at the last bar of the week**

```pycon
>>> @njit
... def entry_place_func_nb(c, index):  
...     for i in range(c.from_i, c.to_i):  # (1)!
...         if i == 0:
...             return i - c.from_i  # (2)!
...         else:
...             index_before = index[i - 1]
...             index_now = index[i]
...             index_next_week = vbt.dt_nb.future_weekday_nb(index_before, 0)
...             if index_now >= index_next_week:  # (3)!
...                 return i - c.from_i
...     return -1

>>> @njit
... def exit_place_func_nb(c, index):  
...     for i in range(c.from_i, c.to_i):
...         if i == len(index) - 1:
...             return i - c.from_i
...         else:
...             index_now = index[i]
...             index_after = index[i + 1]
...             index_next_week = vbt.dt_nb.future_weekday_nb(index_now, 0)
...             if index_after >= index_next_week:  # (4)!
...                 return i - c.from_i
...     return -1

>>> data = vbt.YFData.pull("BTC-USD", start="2020-01-01", end="2020-01-14")
>>> entries, exits = vbt.pd_acc.signals.generate_both(
...     data.symbol_wrapper.shape,
...     entry_place_func_nb=entry_place_func_nb,
...     entry_place_args=(data.index.vbt.to_ns(),),
...     exit_place_func_nb=exit_place_func_nb,
...     exit_place_args=(data.index.vbt.to_ns(),),
...     wrapper=data.symbol_wrapper
... )
>>> pd.concat((
...     entries.rename("Entries"), 
...     exits.rename("Exits")
... ), axis=1).to_period("W")
                        Entries  Exits
Date                                 
2020-01-06/2020-01-12     True  False
2020-01-06/2020-01-12    False  False
2020-01-06/2020-01-12    False  False
2020-01-06/2020-01-12    False  False
2020-01-06/2020-01-12    False  False
2020-01-06/2020-01-12    False  False
2020-01-06/2020-01-12    False   True
2020-01-13/2020-01-19     True  False
```

1. Iterate over the bars in the current period segment.

2. If a signal should be placed, return an index relative to the segment.

3. Place a signal if the current bar crosses Monday.

4. Place a signal if the next bar crosses Monday.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#pre-computation
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Pre-computation
type: heading 2
---

## Pre-computation

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_10.svg){ loading=lazy }

- [x] There is a tradeoff between memory usage and execution speed: a dataset with 1000 columns is
usually processed much faster than processing a 1-column dataset 1000 times. However, the first
dataset also requires 1000 times more memory than the second. That's why, during the simulation phase,
VBT primarily generates orders, while other portfolio attributes such as balances, equity, and returns
are reconstructed later during the analysis phase if the user needs them. For cases where performance
is the main concern, arguments are now available that let you pre-compute these attributes during
simulation! :fast_forward:

**Benchmark a random portfolio with 1000 columns without and with pre-computation**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
```

```pycon
>>> %%timeit  # (1)!
>>> for n in range(1000):
...     pf = vbt.PF.from_random_signals(data, n=n, save_returns=False)
...     pf.sharpe_ratio
15 s ± 829 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. Good for RAM, bad for performance.

```pycon
>>> %%timeit
>>> pf = vbt.PF.from_random_signals(data, n=np.arange(1000).tolist(), save_returns=False)
>>> pf.sharpe_ratio
855 ms ± 6.26 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

```pycon
>>> %%timeit
>>> pf = vbt.PF.from_random_signals(data, n=np.arange(1000).tolist(), save_returns=True)
>>> pf.sharpe_ratio
593 ms ± 7.07 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#cash-deposits
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Cash deposits
type: heading 2
---

## Cash deposits

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Cash can be deposited or withdrawn at any time.

**DCA $10 into Bitcoin each month**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> cash_deposits = data.symbol_wrapper.fill(0.0)
>>> month_start_mask = ~data.index.tz_convert(None).to_period("M").duplicated()
>>> cash_deposits[month_start_mask] = 10
>>> pf = vbt.PF.from_orders(
...     data.close, 
...     init_cash=0, 
...     cash_deposits=cash_deposits
... )

>>> pf.input_value  # (1)!
1020.0

>>> pf.final_value
20674.328828315127
```

1. Total invested.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#cash-earnings
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Cash earnings
type: heading 2
---

## Cash earnings

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Cash can be continuously earned or spent depending on the current position.

**Backtest Apple without and with dividend reinvestment**

```pycon
>>> data = vbt.YFData.pull("AAPL", start="2010")

>>> pf_kept = vbt.PF.from_holding(  # (1)!
...     data.close,
...     cash_dividends=data.get("Dividends")
... )
>>> pf_kept.cash.iloc[-1]  # (2)!
93.9182408043298

>>> pf_kept.assets.iloc[-1]  # (3)!
15.37212731743495

>>> pf_reinvested = vbt.PF.from_orders(  # (4)!
...     data.close,
...     cash_dividends=data.get("Dividends")
... )
>>> pf_reinvested.cash.iloc[-1]
0.0

>>> pf_reinvested.assets.iloc[-1]
18.203284859405468

>>> fig = pf_kept.value.rename("Value (kept)").vbt.plot()
>>> pf_reinvested.value.rename("Value (reinvested)").vbt.plot(fig=fig)
>>> fig.show()
```

1. Keep dividends as cash.

2. Final cash balance.

3. Final number of shares in the portfolio.

4. Reinvest dividends at the next bar.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/dividends.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/dividends.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#in-place-outputs
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: In-place outputs
type: heading 2
---

## In-place outputs

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] The portfolio can now accept and return any user-defined arrays filled during simulation,
such as signals. In-place output arrays can broadcast together with regular arrays using templates and
broadcastable named arguments. Additionally, VBT will (semi-)automatically determine how to correctly
wrap and index each array, for example, whenever you select a column from the entire portfolio.

**Track the debt of a random portfolio during the simulation**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], missing_index="drop")
>>> size = data.symbol_wrapper.fill(np.nan)
>>> rand_indices = np.random.choice(np.arange(len(size)), 10)
>>> size.iloc[rand_indices[0::2]] = -np.inf
>>> size.iloc[rand_indices[1::2]] = np.inf

>>> @njit
... def post_segment_func_nb(c):
...     for col in range(c.from_col, c.to_col):
...         col_debt = c.last_debt[col]
...         c.in_outputs.debt[c.i, col] = col_debt
...         if col_debt > c.in_outputs.max_debt[col]:
...             c.in_outputs.max_debt[col] = col_debt

>>> pf = vbt.PF.from_def_order_func(
...     data.close, 
...     size=size,
...     post_segment_func_nb=post_segment_func_nb,
...     in_outputs=dict(
...         debt=vbt.RepEval("np.empty_like(close)"),
...         max_debt=vbt.RepEval("np.full(close.shape[1], 0.)")
...     )  # (1)!
... )
>>> pf.get_in_output("debt")  # (2)!
symbol                       BTC-USD    ETH-USD
Date                                           
2017-11-09 00:00:00+00:00   0.000000   0.000000
2017-11-10 00:00:00+00:00   0.000000   0.000000
2017-11-11 00:00:00+00:00   0.000000   0.000000
2017-11-12 00:00:00+00:00   0.000000   0.000000
2017-11-13 00:00:00+00:00   0.000000   0.000000
...                              ...        ...
2023-02-08 00:00:00+00:00  43.746892  25.054571
2023-02-09 00:00:00+00:00  43.746892  25.054571
2023-02-10 00:00:00+00:00  43.746892  25.054571
2023-02-11 00:00:00+00:00  43.746892  25.054571
2023-02-12 00:00:00+00:00  43.746892  25.054571

[1922 rows x 2 columns]

>>> pf.get_in_output("max_debt")  # (3)!
symbol
BTC-USD    75.890464
ETH-USD    25.926328
Name: max_debt, dtype: float64
```

1. Instruct the portfolio class to wait until all arrays are broadcast and create a new floating array of the final shape.

2. The portfolio instance knows how to properly wrap a custom NumPy array as a pandas object.

3. The same applies to reduced NumPy arrays.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#flexible-attributes
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Flexible attributes
type: heading 2
---

## Flexible attributes

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Portfolio attributes can now be partially or even fully computed from user-defined arrays.
This gives you greater control over post-simulation analysis, such as overriding some simulation data,
testing hyperparameters without re-simulating the entire portfolio, or avoiding repeated reconstruction
when caching is disabled.

**Compute the net exposure by caching its components**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_random_signals(data.close, n=100)
>>> value = pf.get_value()  # (1)!
>>> long_exposure = vbt.PF.get_gross_exposure(  # (2)!
...     asset_value=pf.get_asset_value(direction="longonly"),
...     value=value,
...     wrapper=pf.wrapper
... )
>>> short_exposure = vbt.PF.get_gross_exposure(
...     asset_value=pf.get_asset_value(direction="shortonly"),
...     value=value,
...     wrapper=pf.wrapper
... )
>>> del value  # (3)!
>>> net_exposure = vbt.PF.get_net_exposure(
...     long_exposure=long_exposure,
...     short_exposure=short_exposure,
...     wrapper=pf.wrapper
... )
>>> del long_exposure
>>> del short_exposure
>>> net_exposure
Date
2014-09-17 00:00:00+00:00    1.0
2014-09-18 00:00:00+00:00    1.0
2014-09-19 00:00:00+00:00    1.0
2014-09-20 00:00:00+00:00    1.0
2014-09-21 00:00:00+00:00    1.0
                              ... 
2023-02-08 00:00:00+00:00    0.0
2023-02-09 00:00:00+00:00    0.0
2023-02-10 00:00:00+00:00    0.0
2023-02-11 00:00:00+00:00    0.0
2023-02-12 00:00:00+00:00    0.0
Freq: D, Length: 3071, dtype: float64
```

1. Call the instance method to use the data stored in the portfolio.

2. Call the class method to provide all the data explicitly.

3. Delete the object as soon as it is no longer needed to free memory.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#shortcut-properties
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: Shortcut properties
type: heading 2
---

## Shortcut properties

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] [In-place output arrays](#in-place-outputs) can be used to override regular portfolio attributes. The portfolio
will automatically pick the pre-computed array and perform all future calculations using this array,
avoiding redundant reconstruction.

**Modify the returns from within the simulation**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> size = data.symbol_wrapper.fill(np.nan)
>>> rand_indices = np.random.choice(np.arange(len(size)), 10)
>>> size.iloc[rand_indices[0::2]] = -np.inf
>>> size.iloc[rand_indices[1::2]] = np.inf

>>> @njit
... def post_segment_func_nb(c):
...     for col in range(c.from_col, c.to_col):
...         return_now = c.last_return[col]
...         return_now = 0.5 * return_now if return_now > 0 else return_now
...         c.in_outputs.returns[c.i, col] = return_now

>>> pf = vbt.PF.from_def_order_func(
...     data.close,
...     size=size,
...     size_type="targetpercent",
...     post_segment_func_nb=post_segment_func_nb,
...     in_outputs=dict(
...         returns=vbt.RepEval("np.empty_like(close)")
...     )
... )

>>> pf.returns  # (1)!
Date
2014-09-17 00:00:00+00:00    0.000000
2014-09-18 00:00:00+00:00    0.000000
2014-09-19 00:00:00+00:00    0.000000
2014-09-20 00:00:00+00:00    0.000000
2014-09-21 00:00:00+00:00    0.000000
                                  ...   
2023-02-08 00:00:00+00:00   -0.015227
2023-02-09 00:00:00+00:00   -0.053320
2023-02-10 00:00:00+00:00   -0.008439
2023-02-11 00:00:00+00:00    0.005569  << modified
2023-02-12 00:00:00+00:00    0.001849  << modified
Freq: D, Length: 3071, dtype: float64

>>> pf.get_returns()  # (2)!
Date
2014-09-17 00:00:00+00:00    0.000000
2014-09-18 00:00:00+00:00    0.000000
2014-09-19 00:00:00+00:00    0.000000
2014-09-20 00:00:00+00:00    0.000000
2014-09-21 00:00:00+00:00    0.000000
                                  ...   
2023-02-08 00:00:00+00:00   -0.015227
2023-02-09 00:00:00+00:00   -0.053320
2023-02-10 00:00:00+00:00   -0.008439
2023-02-11 00:00:00+00:00    0.011138
2023-02-12 00:00:00+00:00    0.003697
Freq: D, Length: 3071, dtype: float64
```

2. Pre-computed returns.

3. Actual returns.

---
link: https://vectorbt.pro/pvt_41531c19/features/portfolio/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/portfolio/#portfolio
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/portfolio.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/
parent: https://vectorbt.pro/pvt_41531c19/features/
children:
  - https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Productivity
type: page
icon: material-nut
---

Productivity features of VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#iterated-decorator
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Iterated decorator
type: heading 2
---

## Iterated decorator

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_6_19.svg){ loading=lazy }

- [x] Thinking about parallelizing a for-loop? No need to hesitate—VBT has a decorator for that.

**Emulate a parallelized nested loop to get Sharpe by year and month**

```pycon
>>> import calendar

>>> @vbt.iterated(over_arg="year", merge_func="column_stack", engine="pathos")  # (1)!
... @vbt.iterated(over_arg="month", merge_func="concat")  # (2)!
... def get_year_month_sharpe(data, year, month):  # (3)!
...     mask = (data.index.year == year) & (data.index.month == month)
...     if not mask.any():
...         return np.nan
...     year_returns = data.loc[mask].returns
...     return year_returns.vbt.returns.sharpe_ratio()

>>> years = data.index.year.unique().sort_values().rename("year")
>>> months = data.index.month.unique().sort_values().rename("month")
>>> sharpe_matrix = get_year_month_sharpe(
...     data,
...     years,
...     {calendar.month_abbr[month]: month for month in months},  # (4)!
... )
>>> sharpe_matrix.transpose().vbt.heatmap(
...     trace_kwargs=dict(colorscale="RdBu", zmid=0), 
...     yaxis=dict(autorange="reversed")
... ).show()
```

1. Iterate over years (in parallel).

2. Iterate over months (sequentially).

3. The function is called for each combination of year and month.

4. Map month numbers to names and pass them as a dict. VBT will extract the keys and use them as labels.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/iterated_decorator.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/iterated_decorator.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#tasks
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Tasks
type: heading 2
---

## Tasks

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_6_19.svg){ loading=lazy }

- [x] Testing multiple parameter combinations usually involves using the `@vbt.parameterized` decorator.
But what if you want to test entirely uncorrelated configurations or even different functions?
The latest addition to VBT lets you execute any sequence of unrelated tests in parallel
by assigning each test to a task.

**Simulate SL, TSL, and TP parameters in three separate processes and compare their expectancy**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")

>>> task1 = vbt.Task(  # (1)!
...     vbt.PF.from_random_signals, 
...     data, 
...     n=100, seed=42,
...     sl_stop=vbt.Param(np.arange(1, 51) / 100)
... )
>>> task2 = vbt.Task(
...     vbt.PF.from_random_signals, 
...     data, 
...     n=100, seed=42,
...     tsl_stop=vbt.Param(np.arange(1, 51) / 100)
... )
>>> task3 = vbt.Task(
...     vbt.PF.from_random_signals, 
...     data, 
...     n=100, seed=42,
...     tp_stop=vbt.Param(np.arange(1, 51) / 100)
... )
>>> pf1, pf2, pf3 = vbt.execute([task1, task2, task3], engine="pathos")  # (2)!

>>> fig = pf1.trades.expectancy.rename("SL").vbt.plot()
>>> pf2.trades.expectancy.rename("TSL").vbt.plot(fig=fig)
>>> pf3.trades.expectancy.rename("TP").vbt.plot(fig=fig)
>>> fig.show()
```

1. A task consists of a function and the arguments you want to pass to that function.
Just creating a task does not execute the function!

2. Execute all three tasks using multiprocessing.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/tasks.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/tasks.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#nested-progress-bars
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Nested progress bars
type: heading 2
---

## Nested progress bars

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2024_5_15.svg){ loading=lazy }

- [x] Progress bars are now aware of each other. When a new progress bar starts, it checks whether another
progress bar with the same identifier has already finished its task. If so, the new progress bar
will close itself and delegate its progress to the existing one.

**Display progress of three parameters using nested progress bars**

```pycon
>>> symbols = ["BTC-USD", "ETH-USD"]
>>> fast_windows = range(5, 105, 5)
>>> slow_windows = range(5, 105, 5)
>>> sharpe_ratios = dict()

>>> with vbt.ProgressBar(total=len(symbols), bar_id="pbar1") as pbar1:  # (1)!
...     for symbol in symbols:
...         pbar1.set_description(dict(symbol=symbol), refresh=True)
...         data = vbt.YFData.pull(symbol)
...         
...         with vbt.ProgressBar(total=len(fast_windows), bar_id="pbar2") as pbar2:  # (2)!
...             for fast_window in fast_windows:
...                 pbar2.set_description(dict(fast_window=fast_window), refresh=True)
...                 
...                 with vbt.ProgressBar(total=len(slow_windows), bar_id="pbar3") as pbar3:  # (3)!
...                     for slow_window in slow_windows:
...                         if fast_window < slow_window:
...                             pbar3.set_description(dict(slow_window=slow_window), refresh=True)
...                             fast_sma = data.run("talib_func:sma", fast_window)
...                             slow_sma = data.run("talib_func:sma", slow_window)
...                             entries = fast_sma.vbt.crossed_above(slow_sma)
...                             exits = fast_sma.vbt.crossed_below(slow_sma)
...                             pf = vbt.PF.from_signals(data, entries, exits)
...                             sharpe_ratios[(symbol, fast_window, slow_window)] = pf.sharpe_ratio
...                         pbar3.update()
...                         
...                 pbar2.update()
...                 
...         pbar1.update()
```

1. Track iteration over symbols.

2. Track iteration over fast windows.

3. Track iteration over slow windows.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

[=100% "Fast window 20/20"]{: .candystripe .candystripe-animate }

[=100% "Slow window 20/20"]{: .candystripe .candystripe-animate }

```pycon
>>> sharpe_ratios = pd.Series(sharpe_ratios)
>>> sharpe_ratios.index.names = ["symbol", "fast_window", "slow_window"]
>>> sharpe_ratios
symbol   fast_window  slow_window
BTC-USD  5            10             1.063616
                      15             1.218345
                      20             1.273154
                      25             1.365664
                      30             1.394469
                                          ...
ETH-USD  80           90             0.582995
                      95             0.617568
          85           90             0.701215
                      95             0.616037
          90           95             0.566650
Length: 342, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#annotations
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Annotations
type: heading 2
---

## Annotations

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/v2023_12_23.svg){ loading=lazy }

- [x] When writing a function, you can specify the meaning of each argument using an annotation
immediately next to the argument. VBT now provides a rich set of in-house annotations tailored to specific
tasks. For example, whether an argument is a parameter can be specified directly in the function instead of
in the [parameterized decorator](https://vectorbt.pro/pvt_41531c19/features/optimization/#parameterized-decorator).

**Test a cross-validation function with annotations**

```pycon
>>> @vbt.cv_split(
...     splitter="from_rolling", 
...     splitter_kwargs=dict(length=365, split=0.5, set_labels=["train", "test"]),
...     parameterized_kwargs=dict(random_subset=100),
... )
... def sma_crossover_cv(
...     data: vbt.Takeable,  # (1)!
...     fast_period: vbt.Param(condition="x < slow_period"),  # (2)!
...     slow_period: vbt.Param,  # (3)!
...     metric
... ) -> vbt.MergeFunc("concat"):
...     fast_sma = data.run("sma", fast_period, hide_params=True)
...     slow_sma = data.run("sma", slow_period, hide_params=True)
...     entries = fast_sma.real_crossed_above(slow_sma)
...     exits = fast_sma.real_crossed_below(slow_sma)
...     pf = vbt.PF.from_signals(data, entries, exits, direction="both")
...     return pf.deep_getattr(metric)

>>> sma_crossover_cv(
...     vbt.YFData.pull("BTC-USD", start="4 years ago"),
...     np.arange(20, 50),
...     np.arange(20, 50),
...     "trades.expectancy"
... )
split  set    fast_period  slow_period
0      train  22           33             26.351841
        test   21           34             35.788733
1      train  21           46             24.114027
        test   21           39              2.261432
2      train  30           44             29.635233
        test   30           38              1.909916
3      train  20           49             -7.038924
        test   20           44             -1.366734
4      train  28           44              2.144805
        test   29           38             -4.945776
5      train  35           47             -8.877875
        test   34           37              2.792217
6      train  29           41              8.816846
        test   28           43             36.008302
dtype: float64
```

1. The passed argument must be takeable (for selecting subsets by split).

2. Parameter with a condition requiring it to be less than the slow period.

3. Parameter for the slow period.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#dataframe-product
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: DataFrame product
type: heading 2
---

## DataFrame product

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_13_0.svg){ loading=lazy }

- [x] Several parameterized indicators can produce DataFrames with different shapes and columns,
which makes creating a Cartesian product tricky because they often share common column levels 
(such as "symbol") that should not be combined. There is now a method to cross-join 
multiple DataFrames block-wise.

**Enter when SMA goes above WMA, exit when EMA goes below WMA**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], missing_index="drop")
>>> sma = data.run("sma", timeperiod=[10, 20], unpack=True)
>>> ema = data.run("ema", timeperiod=[30, 40], unpack=True)
>>> wma = data.run("wma", timeperiod=[50, 60], unpack=True)
>>> sma, ema, wma = sma.vbt.x(ema, wma)  # (1)!
>>> entries = sma.vbt.crossed_above(wma)
>>> exits = ema.vbt.crossed_below(wma)

>>> entries.columns
MultiIndex([(10, 30, 50, 'BTC-USD'),
            (10, 30, 50, 'ETH-USD'),
            (10, 30, 60, 'BTC-USD'),
            (10, 30, 60, 'ETH-USD'),
            (10, 40, 50, 'BTC-USD'),
            (10, 40, 50, 'ETH-USD'),
            (10, 40, 60, 'BTC-USD'),
            (10, 40, 60, 'ETH-USD'),
            (20, 30, 50, 'BTC-USD'),
            (20, 30, 50, 'ETH-USD'),
            (20, 30, 60, 'BTC-USD'),
            (20, 30, 60, 'ETH-USD'),
            (20, 40, 50, 'BTC-USD'),
            (20, 40, 50, 'ETH-USD'),
            (20, 40, 60, 'BTC-USD'),
            (20, 40, 60, 'ETH-USD')],
            names=['sma_timeperiod', 'ema_timeperiod', 'wma_timeperiod', 'symbol'])
```

1. Build a Cartesian product of three DataFrames while keeping the column level "symbol" untouched.
This can also be done with `vbt.pd_acc.cross(sma, ema, wma)`.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#compression
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Compression
type: heading 2
---

## Compression

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_10_0.svg){ loading=lazy }

- [x] Serialized VBT objects can sometimes use a lot of disk space. With this update, 
VBT now supports a variety of compression algorithms to make files as light as possible! :feather:

**Save data without and with compression**

```pycon
>>> data = vbt.RandomOHLCData.pull("RAND", start="2022", end="2023", timeframe="1 minute")

>>> file_path = data.save()
>>> print(vbt.file_size(file_path))
21.0 MB

>>> file_path = data.save(compression="blosc")
>>> print(vbt.file_size(file_path))
13.3 MB
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#faster-loading
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Faster loading
type: heading 2
---

## Faster loading

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_10_0.svg){ loading=lazy }

- [x] If your pipeline does not need accessors, Plotly graphs, or most other optional 
features, you can disable the auto-import feature entirely to reduce VBT's loading 
time to under a second :hourglass_flowing_sand:

=== "INI"

    **Define importing settings in vbt.cfg**

```ini
    [importing]
    auto_import = False
    ```

=== "YAML"

    ```yaml title="Define importing settings in vbt.yml"
    importing:
      auto_import: false
    ```

=== "TOML"

    ```toml title="Define importing settings in vbt.toml"
    [importing]
    auto_import = false
    ```

```pycon title="Measure the loading time"
>>> start = utc_time()
>>> from vectorbtpro import *
>>> end = utc_time()
>>> end - start
0.580937910079956
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#configuration-files
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Configuration files
type: heading 2
---

## Configuration files

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_9_0.svg){ loading=lazy }

- [x] VBT extends [configparser](https://docs.python.org/3/library/configparser.html) to define its 
own configuration format that allows users to save, introspect, modify, and load any complex 
in-house object. The main advantages of this format are readability and round-tripping: any object can 
be encoded and then decoded back without loss of information. The main features include nested structures, 
references, literal parsing, and evaluation of arbitrary Python expressions. Additionally, 
you can now create a configuration file for VBT and place it in the working directory—
it will be used to update the default settings whenever the package is imported.

=== "INI"

    **Define global settings in vbt.cfg**

```ini
    [plotting]
    default_theme = "dark"

    [portfolio]
    init_cash = 5000

    [data.custom.binance.client_config]
    api_key = "<YOUR_API_KEY>"
    api_secret = "<YOUR_API_SECRET>"

    [data.custom.ccxt.exchanges.binance.exchange_config]
    apiKey = &data.custom.binance.client_config.api_key
    secret = &data.custom.binance.client_config.api_secret
    ```

=== "YAML"

    ```yaml title="Define global settings in vbt.yml"
    plotting:
      default_theme: dark

    portfolio:
      init_cash: 5000

    data.custom.binance.client_config:
      api_key: &api_key <YOUR_API_KEY>
      api_secret: &api_secret <YOUR_API_SECRET>

    data.custom.ccxt.exchanges.binance.exchange_config:
      apiKey: *api_key
      secret: *api_secret
    ```

=== "TOML"

    ```toml title="Define global settings in vbt.toml"
    [plotting]
    default_theme = "dark"

    [portfolio]
    init_cash = 5000

    [data.custom.binance.client_config]
    api_key = "<YOUR_API_KEY>"
    api_secret = "<YOUR_API_SECRET>"

    [data.custom.ccxt.exchanges.binance.exchange_config]
    apiKey = { __vbt_ref__ = "data.custom.binance.client_config.api_key" }
    secret = { __vbt_ref__ = "data.custom.binance.client_config.api_secret" }
    ```

```pycon title="Verify that the settings have been loaded correctly"
>>> from vectorbtpro import *

>>> vbt.settings.portfolio["init_cash"]
5000
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#serialization
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Serialization
type: heading 2
---

## Serialization

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_9_0.svg){ loading=lazy }

- [x] Just like machine learning models, every native VBT object can be serialized and saved to 
a binary file. It has never been easier to share data and insights! Another benefit is that only the 
actual content of each object is serialized, not its class definition, so the loaded object 
always uses the most up-to-date class definition. There is also special logic implemented 
to help you "reconstruct" objects if VBT introduces any breaking API changes :construction_site:

**Backtest each month of data and save the results for later**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2022-01-01", end="2022-06-01")

>>> def backtest_month(close):
...     return vbt.PF.from_random_signals(close, n=10)

>>> month_pfs = data.close.resample(vbt.offset("M")).apply(backtest_month)
>>> month_pfs
Date
2022-01-01 00:00:00+00:00    Portfolio(\n    wrapper=ArrayWrapper(\n       ...
2022-02-01 00:00:00+00:00    Portfolio(\n    wrapper=ArrayWrapper(\n       ...
2022-03-01 00:00:00+00:00    Portfolio(\n    wrapper=ArrayWrapper(\n       ...
2022-04-01 00:00:00+00:00    Portfolio(\n    wrapper=ArrayWrapper(\n       ...
2022-05-01 00:00:00+00:00    Portfolio(\n    wrapper=ArrayWrapper(\n       ...
Freq: MS, Name: Close, dtype: object

>>> vbt.save(month_pfs, "month_pfs")  # (1)!

>>> month_pfs = vbt.load("month_pfs")  # (2)!
>>> month_pfs.apply(lambda pf: pf.total_return)
Date
2022-01-01 00:00:00+00:00   -0.048924
2022-02-01 00:00:00+00:00    0.168370
2022-03-01 00:00:00+00:00    0.016087
2022-04-01 00:00:00+00:00   -0.120525
2022-05-01 00:00:00+00:00    0.110751
Freq: MS, Name: Close, dtype: float64
```

1. Save to disk.

2. Load from disk later.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#data-parsing
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Data parsing
type: heading 2
---

## Data parsing

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] Tired of passing open, high, low, and close as separate time series? Portfolio class methods 
now accept a data instance instead of just close and automatically extract the contained OHLC data. 
This small but handy feature saves you time!

**Run the example above using the new approach**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020-01", end="2020-03")
>>> pf = vbt.PF.from_random_signals(data, n=10)
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#index-dictionaries
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Index dictionaries
type: heading 2
---

## Index dictionaries

Manually creating arrays and setting their data with Pandas can often be challenging. Luckily, there 
is now a feature that offers much-needed assistance! Any broadcastable argument can become an index 
dictionary, which contains instructions on where to set values in the array and fills them in for you. 
It knows exactly which axis needs to be updated and does not create a full array unless necessary, 
saving RAM :heart:

**1) Accumulate daily and exit on Sunday vs 2) accumulate weekly and exit on month end**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"])
>>> tile = pd.Index(["daily", "weekly"], name="strategy")  # (1)!
>>> pf = vbt.PF.from_orders(
...     data.close,
...     size=vbt.index_dict({  # (2)!
...         vbt.idx(
...             vbt.pointidx(every="day"), 
...             vbt.colidx("daily", level="strategy")): 100,  # (3)!
...         vbt.idx(
...             vbt.pointidx(every="sunday"), 
...             vbt.colidx("daily", level="strategy")): -np.inf,  # (4)!
...         vbt.idx(
...             vbt.pointidx(every="monday"), 
...             vbt.colidx("weekly", level="strategy")): 100,
...         vbt.idx(
...             vbt.pointidx(every="monthend"), 
...             vbt.colidx("weekly", level="strategy")): -np.inf,
...     }),
...     size_type="value",
...     direction="longonly",
...     init_cash="auto",
...     broadcast_kwargs=dict(tile=tile)
... )
>>> pf.sharpe_ratio
strategy  symbol 
daily     BTC-USD    0.702259
          ETH-USD    0.782296
weekly    BTC-USD    0.838895
          ETH-USD    0.524215
Name: sharpe_ratio, dtype: float64
```

1. To represent two strategies, you need to tile the same data twice. Create a parameter with
strategy names and pass it as `tile` to the broadcaster so it tiles the columns of each array
(such as price) twice.

2. The index dictionary includes index instructions as keys and data as values to set.
Keys can be row indices, labels, or custom indexer classes such as 
[PointIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PointIdxr).

3. Find the indices of the rows for the start of each day and the column index of "daily", then
set each element at those indices to 100 (= accumulate).

4. Find the indices of the rows that correspond to Sunday. If any value at those indices has already 
been set by a previous instruction, it will be overridden.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#slicing
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Slicing
type: heading 2
---

## Slicing

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] Similar to selecting columns, each VBT object can now slice rows using the
same mechanism as in Pandas :knife: This makes it easy to analyze and plot any subset of 
simulated data, without needing to re-simulate!

**Analyze multiple date ranges of the same portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")
>>> pf = vbt.PF.from_holding(data, freq="d")

>>> pf.sharpe_ratio
1.116727709477293

>>> pf.loc[:"2020"].sharpe_ratio  # (1)!
1.2699801554196481

>>> pf.loc["2021": "2021"].sharpe_ratio  # (2)!
0.9825161170278687

>>> pf.loc["2022":].sharpe_ratio  # (3)!
-1.0423271337174647
```

1. Get the Sharpe ratio during the year 2020 and before.

2. Get the Sharpe ratio during the year 2021.

3. Get the Sharpe ratio during the year 2022 and after.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#column-stacking
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Column stacking
type: heading 2
---

## Column stacking

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] Complex VBT objects of the same type can be easily stacked along columns. For example, 
you can combine multiple unrelated trading strategies into one portfolio for analysis. 
Under the hood, the final object is still represented as a monolithic multi-dimensional structure 
that can be processed even faster than separate merged objects :lungs:

**Analyze two trading strategies separately and then jointly**

```pycon
>>> def strategy1(data):
...     fast_ma = vbt.MA.run(data.close, 50, short_name="fast_ma")
...     slow_ma = vbt.MA.run(data.close, 200, short_name="slow_ma")
...     entries = fast_ma.ma_crossed_above(slow_ma)
...     exits = fast_ma.ma_crossed_below(slow_ma)
...     return vbt.PF.from_signals(
...         data.close, 
...         entries, 
...         exits, 
...         size=100,
...         size_type="value",
...         init_cash="auto"
...     )

>>> def strategy2(data):
...     bbands = vbt.BBANDS.run(data.close, window=14)
...     entries = bbands.close_crossed_below(bbands.lower)
...     exits = bbands.close_crossed_above(bbands.upper)
...     return vbt.PF.from_signals(
...         data.close, 
...         entries, 
...         exits, 
...         init_cash=200
...     )

>>> data1 = vbt.BinanceData.pull("BTCUSDT")
>>> pf1 = strategy1(data1)  # (1)!
>>> pf1.sharpe_ratio
0.9100317671866922

>>> data2 = vbt.BinanceData.pull("ETHUSDT")
>>> pf2 = strategy2(data2)  # (2)!
>>> pf2.sharpe_ratio
-0.11596286232734827

>>> pf_sep = vbt.PF.column_stack((pf1, pf2))  # (3)!
>>> pf_sep.sharpe_ratio
0    0.910032
1   -0.115963
Name: sharpe_ratio, dtype: float64

>>> pf_join = vbt.PF.column_stack((pf1, pf2), group_by=True)  # (4)!
>>> pf_join.sharpe_ratio
0.42820898354646514
```

1. Analyze the first strategy in its own portfolio.

2. Analyze the second strategy in its own portfolio.

3. Analyze both strategies separately in the same portfolio.

4. Analyze both strategies jointly in the same portfolio.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#row-stacking
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Row stacking
type: heading 2
---

## Row stacking

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] Complex VBT objects of the same type can be easily stacked along rows. For example, 
you can append new data to an existing portfolio, or concatenate in-sample portfolios with 
their out-of-sample counterparts :dna:

**Analyze two date ranges separately and then jointly**

```pycon
>>> def strategy(data, start=None, end=None):
...     fast_ma = vbt.MA.run(data.close, 50, short_name="fast_ma")
...     slow_ma = vbt.MA.run(data.close, 200, short_name="slow_ma")
...     entries = fast_ma.ma_crossed_above(slow_ma)
...     exits = fast_ma.ma_crossed_below(slow_ma)
...     return vbt.PF.from_signals(
...         data.close[start:end], 
...         entries[start:end], 
...         exits[start:end], 
...         size=100,
...         size_type="value",
...         init_cash="auto"
...     )

>>> data = vbt.BinanceData.pull("BTCUSDT")

>>> pf_whole = strategy(data)  # (1)!
>>> pf_whole.sharpe_ratio
0.9100317671866922

>>> pf_sub1 = strategy(data, end="2019-12-31")  # (2)!
>>> pf_sub1.sharpe_ratio
0.7810397448678937

>>> pf_sub2 = strategy(data, start="2020-01-01")  # (3)!
>>> pf_sub2.sharpe_ratio
1.070339534746574

>>> pf_join = vbt.PF.row_stack((pf_sub1, pf_sub2))  # (4)!
>>> pf_join.sharpe_ratio
0.9100317671866922
```

1. Analyze the entire range.

2. Analyze the first date range.

3. Analyze the second date range.

4. Combine both date ranges and analyze them together.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#index-alignment
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Index alignment
type: heading 2
---

## Index alignment

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_3_0.svg){ loading=lazy }

- [x] There is no longer a limitation requiring each Pandas array to have the same index.
Indexes of all arrays that should broadcast against each other are automatically aligned, as long as they
have the same data type.

**Predict ETH price with BTC price using linear regression**

```pycon
>>> btc_data = vbt.YFData.pull("BTC-USD")
>>> btc_data.wrapper.shape
(2817, 7)

>>> eth_data = vbt.YFData.pull("ETH-USD")  # (1)!
>>> eth_data.wrapper.shape
(1668, 7)

>>> ols = vbt.OLS.run(  # (2)!
...     btc_data.close,
...     eth_data.close
... )
>>> ols.pred
Date
2014-09-17 00:00:00+00:00            NaN
2014-09-18 00:00:00+00:00            NaN
2014-09-19 00:00:00+00:00            NaN
2014-09-20 00:00:00+00:00            NaN
2014-09-21 00:00:00+00:00            NaN
...                                  ...
2022-05-30 00:00:00+00:00    2109.769242
2022-05-31 00:00:00+00:00    2028.856767
2022-06-01 00:00:00+00:00    1911.555689
2022-06-02 00:00:00+00:00    1930.169725
2022-06-03 00:00:00+00:00    1882.573170
Freq: D, Name: Close, Length: 2817, dtype: float64
```

1. ETH-USD history is shorter than BTC-USD history.

2. This now works! Make sure all arrays share the same timeframe and timezone.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#numba-datetime
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Numba datetime
type: heading 2
---

## Numba datetime

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_3.svg){ loading=lazy }

- [x] Numba does not support datetime indexes (or any other Pandas objects). There are 
also no built-in Numba functions for working with datetime. So, how do you connect data to time? VBT
addresses this gap by implementing a collection of functions to extract various information
from each timestamp, such as the current time and day of the week, to determine whether the bar
is during trading hours.

!!! example "Tutorial"
    Learn more in the [Signal development](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development) tutorial.

**Plot the percentage change from the start of the month to now**

```pycon
>>> @njit
... def month_start_pct_change_nb(arr, index):
...     out = np.full(arr.shape, np.nan)
...     for col in range(arr.shape[1]):
...         for i in range(arr.shape[0]):
...             if i == 0 or vbt.dt_nb.month_nb(index[i - 1]) != vbt.dt_nb.month_nb(index[i]):
...                 month_start_value = arr[i, col]
...             else:
...                 out[i, col] = (arr[i, col] - month_start_value) / month_start_value
...     return out
    
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], start="2022", end="2023")
>>> pct_change = month_start_pct_change_nb(
...     vbt.to_2d_array(data.close), 
...     data.index.vbt.to_ns()  # (1)!
... )
>>> pct_change = data.symbol_wrapper.wrap(pct_change)
>>> pct_change.vbt.plot().show()
```

1. Convert the datetime index to nanosecond format.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/numba_datetime.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/numba_datetime.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#periods-ago
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Periods ago
type: heading 2
---

## Periods ago

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_2_3.svg){ loading=lazy }

- [x] Instead of writing Numba functions, comparing values at different bars can also be done in a vectorized
way with Pandas. The problem is that there are no built-in functions to easily shift values based on 
timedeltas, nor are there rolling functions to check whether an event happened during a past 
period. This gap is filled by various new accessor methods.

!!! example "Tutorial"
    Learn more in the [Signal development](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development) tutorial.

**Check whether the price dropped for 5 consecutive bars**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2022-05", end="2022-08")
>>> mask = (data.close < data.close.vbt.ago(1)).vbt.all_ago(5)
>>> fig = data.plot(plot_volume=False)
>>> mask.vbt.signals.ranges.plot_shapes(
...     plot_close=False, 
...     fig=fig, 
...     shape_kwargs=dict(fillcolor="orangered")
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/periods_ago.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/periods_ago.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#safe-resampling
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Safe resampling
type: heading 2
---

## Safe resampling

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_2.svg){ loading=lazy }

- [x] [Look-ahead bias](https://www.investopedia.com/terms/l/lookaheadbias.asp) is an ongoing 
risk when working with array data, especially on multiple time frames. Using Pandas alone is strongly
discouraged because it does not recognize that financial data mainly involves bars where timestamps are 
the opening times, and events may occur at any time between bars. Pandas thus incorrectly assumes that 
timestamps indicate the exact time of an event. In VBT, there is a complete collection of functions and 
classes for safely resampling and analyzing data!

!!! example "Tutorial"
    Learn more in the [MTF analysis](https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis) tutorial.

**Calculate SMA on multiple time frames and display on the same chart**

```pycon
>>> def mtf_sma(close, close_freq, target_freq, timeperiod=5):
...     target_close = close.vbt.realign_closing(target_freq)  # (1)!
...     target_sma = vbt.talib("SMA").run(target_close, timeperiod=timeperiod).real  # (2)!
...     target_sma = target_sma.rename(f"SMA ({target_freq})")
...     return target_sma.vbt.realign_closing(close.index, freq=close_freq)  # (3)!

>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2023")
>>> fig = mtf_sma(data.close, "D", "daily").vbt.plot()
>>> mtf_sma(data.close, "D", "weekly").vbt.plot(fig=fig)
>>> mtf_sma(data.close, "D", "monthly").vbt.plot(fig=fig)
>>> fig.show()
```

1. Resample the source frequency to the target frequency. Since Close occurs at the end of the bar,
resample it as a "closing event".

2. Calculate the SMA on the target frequency.

3. Resample the target frequency back to the source frequency to show
multiple time frames on the same chart. Because `close` contains gaps, you cannot simply resample to `close_freq`
as this might produce unaligned series. Instead, resample directly to the index of `close`.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/safe_resampling.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/safe_resampling.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#resamplable-objects
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Resamplable objects
type: heading 2
---

## Resamplable objects

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_1_2.svg){ loading=lazy }

- [x] You can resample not only time series, but also complex VBT objects! Under the hood,
each object is made up of a collection of array-like attributes, so resampling means aggregating
all the related information together. This is especially helpful if you want to simulate at a higher
frequency for maximum accuracy and then analyze at a lower frequency for better speed.

!!! example "Tutorial"
    Learn more in the [MTF analysis](https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis) tutorial.

**Plot the monthly return heatmap of a random portfolio**

```pycon
>>> import calendar

>>> data = vbt.YFData.pull("BTC-USD", start="2018", end="2023")
>>> pf = vbt.PF.from_random_signals(data, n=100, direction="both")
>>> mo_returns = pf.resample("M").returns  # (1)!
>>> mo_return_matrix = pd.Series(
...     mo_returns.values, 
...     index=pd.MultiIndex.from_arrays([
...         mo_returns.index.year,
...         mo_returns.index.month
...     ], names=["year", "month"])
... ).unstack("month")
>>> mo_return_matrix.columns = mo_return_matrix.columns.map(lambda x: calendar.month_abbr[x])
>>> mo_return_matrix.vbt.heatmap(
...     is_x_category=True,
...     trace_kwargs=dict(zmid=0, colorscale="Spectral")
... ).show()
```

1. Resample the entire portfolio to monthly frequency and calculate the returns.

![](https://vectorbt.pro/pvt_41531c19/assets/images/features/monthly_return_heatmap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/features/monthly_return_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#formatting-engine
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Formatting engine
type: heading 2
---

## Formatting engine

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_2.svg){ loading=lazy }

- [x] VBT is a comprehensive library that defines thousands of classes, functions, and objects.
When working with these, you may want to "look inside" an object to better understand
its attributes and contents. Fortunately, there is a formatting engine that can accurately format
any in-house object as a human-readable string. Did you know the API documentation is partly
powered by this engine? :wink:

**Introspect a data instance**

```pycon
>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2021")

>>> vbt.pprint(data)  # (1)!
YFData(
    wrapper=ArrayWrapper(...),
    data=symbol_dict({
        'BTC-USD': <pandas.core.frame.DataFrame object at 0x7f7f1fbc6cd0 with shape (366, 7)>
    }),
    single_key=True,
    classes=symbol_dict(),
    fetch_kwargs=symbol_dict({
        'BTC-USD': dict(
            start='2020',
            end='2021'
        )
    }),
    returned_kwargs=symbol_dict({
        'BTC-USD': dict()
    }),
    last_index=symbol_dict({
        'BTC-USD': Timestamp('2020-12-31 00:00:00+0000', tz='UTC')
    }),
    tz_localize=datetime.timezone.utc,
    tz_convert='UTC',
    missing_index='nan',
    missing_columns='raise'
)

>>> vbt.pdir(data)  # (2)!
                                            type                                             path
attr                                                                                                     
align_columns                        classmethod                       vectorbtpro.data.base.Data
align_index                          classmethod                       vectorbtpro.data.base.Data
build_feature_config_doc             classmethod                       vectorbtpro.data.base.Data
...                                          ...                                              ...
vwap                                    property                       vectorbtpro.data.base.Data
wrapper                                 property               vectorbtpro.base.wrapping.Wrapping
xs                                      function          vectorbtpro.base.indexing.PandasIndexer

>>> vbt.phelp(data.get)  # (3)!
YFData.get(
    columns=None,
    symbols=None,
    **kwargs
):
    Get one or more columns of one or more symbols of data.
```

1. Similar to Python's `print` command, pretty-prints the contents of any VBT object.

2. Similar to Python's `dir` command, pretty-prints the attributes of a class, object, or module.

3. Similar to Python's `help` command, pretty-prints the signature and docstring of a function.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#meta-methods
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Meta methods
type: heading 2
---

## Meta methods

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] Many methods, such as rolling apply, now come in two versions: regular (instance methods)
and meta (class methods). Regular methods are bound to a single array and do not need metadata,
while meta methods are not tied to any array and act as micro-pipelines with their own
broadcasting and templating logic. Here, VBT solves one of the main Pandas limitations:
the inability to apply a function to multiple arrays at once.

**Compute the rolling z-score on one array and the rolling correlation coefficient on two arrays**

```pycon
>>> @njit
... def zscore_nb(x):  # (1)!
...     return (x[-1] - np.mean(x)) / np.std(x)

>>> data = vbt.YFData.pull("BTC-USD", start="2020", end="2021")
>>> data.close.rolling(14).apply(zscore_nb, raw=True)  # (2)!
Date
2020-01-01 00:00:00+00:00         NaN
                                  ...
2020-12-27 00:00:00+00:00    1.543527
2020-12-28 00:00:00+00:00    1.734715
2020-12-29 00:00:00+00:00    1.755125
2020-12-30 00:00:00+00:00    2.107147
2020-12-31 00:00:00+00:00    1.781800
Freq: D, Name: Close, Length: 366, dtype: float64

>>> data.close.vbt.rolling_apply(14, zscore_nb)  # (3)!
2020-01-01 00:00:00+00:00         NaN
                                  ...
2020-12-27 00:00:00+00:00    1.543527
2020-12-28 00:00:00+00:00    1.734715
2020-12-29 00:00:00+00:00    1.755125
2020-12-30 00:00:00+00:00    2.107147
2020-12-31 00:00:00+00:00    1.781800
Freq: D, Name: Close, Length: 366, dtype: float64

>>> @njit
... def corr_meta_nb(from_i, to_i, col, a, b):  # (4)!
...     a_window = a[from_i:to_i, col]
...     b_window = b[from_i:to_i, col]
...     return np.corrcoef(a_window, b_window)[1, 0]

>>> data2 = vbt.YFData.pull(["ETH-USD", "XRP-USD"], start="2020", end="2021")
>>> vbt.pd_acc.rolling_apply(  # (5)!
...     14, 
...     corr_meta_nb, 
...     vbt.Rep("a"),
...     vbt.Rep("b"),
...     broadcast_named_args=dict(a=data.close, b=data2.close)
... )
symbol                      ETH-USD   XRP-USD
Date                                         
2020-01-01 00:00:00+00:00       NaN       NaN
...                             ...       ...
2020-12-27 00:00:00+00:00  0.636862 -0.511303
2020-12-28 00:00:00+00:00  0.674514 -0.622894
2020-12-29 00:00:00+00:00  0.712531 -0.773791
2020-12-30 00:00:00+00:00  0.839355 -0.772295
2020-12-31 00:00:00+00:00  0.878897 -0.764446

[366 rows x 2 columns]
```

1. Provides access to the window only.

2. Using Pandas.

3. Using the regular method, which accepts the same function as Pandas.

4. Provides access to one or more entire arrays.

5. Using the meta method, which accepts metadata and variable arguments.

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#array-expressions
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Array expressions
type: heading 2
---

## Array expressions

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] When combining multiple arrays, they often need to be aligned and broadcast before
the operation itself. Pandas alone often falls short because it can be too strict.
Fortunately, VBT includes an accessor class method that can take a regular Python expression,
identify all variable names, extract the arrays from the current context, broadcast them,
and then evaluate the expression (with support for [NumExpr](https://github.com/pydata/numexpr)!) :keyboard:

**Evaluate a multiline array expression based on a Bollinger Bands indicator**

```pycon
>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"])

>>> low = data.low
>>> high = data.high
>>> bb = vbt.talib("BBANDS").run(data.close)
>>> upperband = bb.upperband
>>> lowerband = bb.lowerband
>>> bandwidth = (bb.upperband - bb.lowerband) / bb.middleband
>>> up_th = vbt.Param([0.3, 0.4]) 
>>> low_th = vbt.Param([0.1, 0.2])

>>> expr = """
... narrow_bands = bandwidth < low_th
... above_upperband = high > upperband
... wide_bands = bandwidth > up_th
... below_lowerband = low < lowerband
... (narrow_bands & above_upperband) | (wide_bands & below_lowerband)
... """
>>> mask = vbt.pd_acc.eval(expr)
>>> mask.sum()
low_th  up_th  symbol 
0.1     0.3    BTC-USD    344
                ETH-USD    171
        0.4    BTC-USD    334
                ETH-USD    158
0.2     0.3    BTC-USD    444
                ETH-USD    253
        0.4    BTC-USD    434
                ETH-USD    240
dtype: int64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#resource-management
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Resource management
type: heading 2
---

## Resource management

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] New profiling tools help you measure the execution time and memory usage of any code block :toolbox:

**Profile getting the Sharpe ratio of a random portfolio**

```pycon
>>> data = vbt.YFData.pull("BTC-USD")

>>> with (
...     vbt.Timer() as timer, 
...     vbt.MemTracer() as mem_tracer
... ):
...     print(vbt.PF.from_random_signals(data.close, n=100).sharpe_ratio)
0.33111243921865163

>>> print(timer.elapsed())
74.15 milliseconds

>>> print(mem_tracer.peak_usage())
459.7 kB
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#templates
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: Templates
type: heading 2
---

## Templates

![](https://vectorbt.pro/pvt_41531c19/assets/badges/new-in/1_0_0.svg){ loading=lazy }

- [x] It is easy to extend classes, but since VBT revolves around functions, how do we enhance them or
change their workflow? The easiest way is to introduce a small function (i.e., callback) that the user
can provide and that the main function calls at some point. However, this would require the main function
to know what arguments to pass to the callback and how to handle its outputs. Here is a better idea:
allow most arguments of the main function to become callbacks, then execute those to obtain their actual
values. These arguments are called "templates" and this process is known as "substitution".
Templates are especially useful when some arguments (such as arrays) should be built only
once all required information is available, for example, when other arrays have already been broadcast.
Each substitution opportunity has its own identifier so you can control when a template
should be substituted. In VBT, templates are first-class citizens and are integrated into most
functions for unmatched flexibility! :genie:

**Design a template-enhanced resampling functionality**

```pycon
>>> def resample_apply(index, by, apply_func, *args, template_context={}, **kwargs):
...     grouper = index.vbt.get_grouper(by)  # (1)!
...     results = {}
...     with vbt.ProgressBar() as pbar:
...         for group, group_idxs in grouper:  # (2)!
...             group_index = index[group_idxs]
...             context = {"group": group, "group_index": group_index, **template_context}  # (3)!
...             final_apply_func = vbt.substitute_templates(apply_func, context, eval_id="apply_func")  # (4)!
...             final_args = vbt.substitute_templates(args, context, eval_id="args")
...             final_kwargs = vbt.substitute_templates(kwargs, context, eval_id="kwargs")
...             results[group] = final_apply_func(*final_args, **final_kwargs)
...             pbar.update()
...     return pd.Series(results)

>>> data = vbt.YFData.pull(["BTC-USD", "ETH-USD"], missing_index="drop")
>>> resample_apply(
...     data.index, "Y", 
...     lambda x, y: x.corr(y),  # (5)!
...     vbt.RepEval("btc_close[group_index]"),  # (6)!
...     vbt.RepEval("eth_close[group_index]"),
...     template_context=dict(
...         btc_close=data.get("Close", "BTC-USD"),  # (7)!
...         eth_close=data.get("Close", "ETH-USD")
...     )
... )
```

1. Builds a grouper. Accepts both group-by and resample instructions.

2. Iterates over groups in the grouper. Each group contains a label (such as `2017-01-01 00:00:00+00:00`)
and the row indices corresponding to this label.

3. Creates a new context with information about the current group and any external information provided
by the user.

4. Substitutes the function and arguments using the newly populated context.

5. Simple function to compute the correlation coefficient between two arrays.

6. Defines both arguments as expression templates where data is selected for each group. 
All variables in these expressions will be automatically recognized and replaced by the current context.
After evaluation, the templates will be replaced by their outputs.

7. Specifies any additional information your templates depend on.

[=100% "Group 7/7"]{: .candystripe .candystripe-animate }

```pycon
2017    0.808930
2018    0.897112
2019    0.753659
2020    0.940741
2021    0.553255
2022    0.975911
2023    0.974914
Freq: A-DEC, dtype: float64
```

---
link: https://vectorbt.pro/pvt_41531c19/features/productivity/#and-many-more
parent: https://vectorbt.pro/pvt_41531c19/features/productivity/#productivity
name: And many more...
type: heading 2
---

## And many more...

- [ ] Look forward to more killer features being added every week!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/features/productivity.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/overview/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
name: Overview
type: page
---

# Overview

Here you will find a comprehensive collection of guides and tutorials designed to help you master
the powerful features of VBT. Whether you are a beginner looking to get started or an advanced
user seeking to refine your skills, our tutorials cover everything from basic setup to complex
strategy development.

<div class="grid cards" markdown>

- [:material-chart-line-variant:{ .overview }__Basic RSI strategy__](https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/)

- [:material-lightning-bolt-outline:{ .overview }__SuperFast SuperTrend__](https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/)

- [:material-broadcast:{ .overview }__Signal development__](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/)

- [:material-select-compare:{ .overview }__Stop signals__](https://vectorbt.pro/pvt_41531c19/tutorials/stop-signals/)

- [:material-chart-timeline:{ .overview }__MTF analysis__](https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/)

- [:material-chart-bar-stacked:{ .overview }__Portfolio optimization__](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/)

- [:material-chart-multiline:{ .overview }__Pairs trading__](https://vectorbt.pro/pvt_41531c19/tutorials/pairs-trading/)

- [:material-star-shooting-outline:{ .overview }__Patterns and projections__](https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections/)

- [:material-check-all:{ .overview }__Cross-validation__](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/)

- [:material-link-variant-plus:{ .overview }__More tutorials__](https://vectorbt.pro/pvt_41531c19/tutorials/more-tutorials/)

</div>

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
name: Basic RSI strategy
type: page
icon: material-chart-line-variant
---

# Basic RSI strategy

One of the main strengths of VBT (PRO) is its ability to quickly create and backtest multiple 
strategy configurations. In this introductory example, we will explore the profitability of the 
following RSI strategy, which is commonly used by beginners:

> If the RSI is below 30, it suggests that a stock is reaching oversold conditions and may soon 
> experience a trend reversal or a bounce back to a higher share price. Once the reversal is 
> confirmed, a buy trade is executed. Conversely, if the RSI is above 70, it indicates that a stock 
> is reaching overbought conditions and may experience a trend reversal or a pullback in price. After 
> the reversal is confirmed, a sell trade is executed.

As a bonus, we will gradually expand the analysis to include multiple parameter combinations. 
Sound interesting? Let's get started.

## Single backtest

First, let's handle the data. With a simple one-liner, we can download all available daily data for 
the BTC/USDT pair from Binance:

```pycon
>>> from vectorbtpro import *  # (1)!

>>> data = vbt.BinanceData.pull('BTCUSDT')
>>> data
<vectorbtpro.data.custom.binance.BinanceData at 0x7f9c40c59550>
```

1. To see what is imported, call `whats_imported()`.

[=100% "100%"]{: .candystripe .candystripe-animate }

The returned object is of type [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData),
which extends [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) to interact with the Binance API. 
The [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class is VBT's built-in container 
for retrieving, storing, and managing data. When a DataFrame is received, it is post-processed and 
stored inside the dictionary [Data.data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.data), 
keyed by pair (also known as a "symbol" in VBT). You can access your DataFrame either from this 
dictionary or by using the convenient [Data.get](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get) method, 
which lets you specify one or more columns instead of returning the entire DataFrame.

Let's plot the data using [Data.plot](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.plot):

```pycon
>>> data.plot().show()  # (1)!
```

1. To plot a specific symbol and/or feature, pass it as `symbol` and `feature` respectively.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/ohlcv.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/ohlcv.dark.svg#only-dark){: .iimg loading=lazy }

Another way to describe the data is by using Pandas' 
[info](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html) method. 
The tabular format is very helpful for counting null values (it looks like our data does not have any—great!)

```pycon
>>> data.data['BTCUSDT'].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 1813 entries, 2017-08-17 00:00:00+00:00 to 2022-08-03 00:00:00+00:00
Freq: D
Data columns (total 9 columns):
  #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
  0   Open                1813 non-null   float64
  1   High                1813 non-null   float64
  2   Low                 1813 non-null   float64
  3   Close               1813 non-null   float64
  4   Volume              1813 non-null   float64
  5   Quote volume        1813 non-null   float64
  6   Trade count         1813 non-null   int64  
  7   Taker base volume   1813 non-null   float64
  8   Taker quote volume  1813 non-null   float64
dtypes: float64(8), int64(1)
memory usage: 141.6 KB
```

In our example, we will generate signals based on the opening price and execute them using the 
closing price. We could also place orders as soon as the signal is generated, or at a later time, 
but here we will show how to separate the generation of signals from their execution.

```pycon
>>> open_price = data.get('Open')
>>> close_price = data.get('Close')  # (1)!
```

1. OHLCV can also be accessed directly, like `data.close`.

Now it is time to run the indicator!

VBT supports five different RSI implementations: one written using Numba, 
and four more ported from three different technical analysis libraries. Each indicator 
is wrapped with the versatile [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory).
:mechanical_arm:

To list all available indicators or search for a specific one, use 
[IndicatorFactory.list_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_indicators):

```pycon
>>> vbt.IF.list_indicators("RSI*")
['vbt:RSI', 'talib:RSI', 'pandas_ta:RSI', 'ta:RSIIndicator', 'technical:RSI']
```

You can retrieve the actual indicator class like this:

```pycon
>>> vbt.indicator("talib:RSI")
vectorbtpro.indicators.factory.talib.RSI
```

Or manually:

```pycon
>>> vbt.RSI  # (1)!
vectorbtpro.indicators.custom.RSI

>>> vbt.talib('RSI')  # (2)!
vectorbtpro.indicators.factory.talib.RSI

>>> vbt.ta('RSIIndicator')  # (3)!
vectorbtpro.indicators.factory.ta.RSIIndicator

>>> vbt.pandas_ta('RSI')  # (4)!
vectorbtpro.indicators.factory.pandas_ta.RSI

>>> vbt.technical('RSI')  # (5)!
vectorbtpro.indicators.factory.technical.RSI
```

1. To list all VBT indicators, see the module [custom](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/).

2. To list all supported TA-Lib indicators, call [IndicatorFactory.list_talib_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_talib_indicators).

3. To list all supported TA indicators, call [IndicatorFactory.list_ta_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_ta_indicators).

4. To list all supported Pandas TA indicators, call [IndicatorFactory.list_pandas_ta_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_pandas_ta_indicators).

5. To list all supported technical indicators, call [IndicatorFactory.list_technical_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_technical_indicators).

Here is a general rule for choosing an implementation:

1. Use TA-Lib indicators for the fastest execution (they are written in C).

2. Use VBT indicators for fast execution and plotting (compiled with Numba).

3. Use indicators from other libraries if they offer more features.

To run any indicator, use the `run` method. To see which arguments this method accepts, 
pass it to [phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp):

```pycon
>>> vbt.phelp(vbt.RSI.run)
RSI.run(
    close,
    window=Default(value=14),
    wtype=Default(value='wilder'),
    short_name='rsi',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `RSI` indicator.
    

    * Inputs: `close`

    * Parameters: `window`, `wtype`

    * Outputs: `rsi`
    
    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.
    
    Other keyword arguments are passed to `RSI.run_pipeline`.
```

As shown above, we must at least provide `close`, which can be any numeric time series. 
By default, the rolling window is 14 bars and uses Wilder's smoothed moving average.
Since we want to base our decisions on the opening price, we will pass `open_price` as `close`:

```pycon
>>> rsi = vbt.RSI.run(open_price)
>>> rsi
<vectorbtpro.indicators.custom.RSI at 0x7f9c20921ac8>
```

That's it! By calling the [RSI.run](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI.run)
method, we calculated the RSI values and received an instance with various methods and properties for analysis. 
To retrieve the resulting Pandas object, you can access the `rsi` attribute (see "Outputs" 
in the result of `phelp`).

```pycon
>>> rsi.rsi
Open time
2017-08-17 00:00:00+00:00          NaN
2017-08-18 00:00:00+00:00          NaN
2017-08-19 00:00:00+00:00          NaN
2017-08-20 00:00:00+00:00          NaN
2017-08-21 00:00:00+00:00          NaN
...                                ...
2022-07-30 00:00:00+00:00    60.541637
2022-07-31 00:00:00+00:00    59.503179
2022-08-01 00:00:00+00:00    56.750576
2022-08-02 00:00:00+00:00    56.512434
2022-08-03 00:00:00+00:00    54.177385
Freq: D, Name: Open, Length: 1813, dtype: float64
```

Now that we have the RSI array, we want to generate an entry signal whenever an RSI value crosses below 30,
and an exit signal whenever it crosses above 70:

```pycon
>>> entries = rsi.rsi.vbt.crossed_below(30)  # (1)!
>>> entries
Open time
2017-08-17 00:00:00+00:00    False
2017-08-18 00:00:00+00:00    False
2017-08-19 00:00:00+00:00    False
2017-08-20 00:00:00+00:00    False
2017-08-21 00:00:00+00:00    False
...                            ...
2022-07-30 00:00:00+00:00    False
2022-07-31 00:00:00+00:00    False
2022-08-01 00:00:00+00:00    False
2022-08-02 00:00:00+00:00    False
2022-08-03 00:00:00+00:00    False
Freq: D, Name: Open, Length: 1813, dtype: bool

>>> exits = rsi.rsi.vbt.crossed_above(70)  # (2)!
>>> exits
Open time
2017-08-17 00:00:00+00:00    False
2017-08-18 00:00:00+00:00    False
2017-08-19 00:00:00+00:00    False
2017-08-20 00:00:00+00:00    False
2017-08-21 00:00:00+00:00    False
...                            ...
2022-07-30 00:00:00+00:00    False
2022-07-31 00:00:00+00:00    False
2022-08-01 00:00:00+00:00    False
2022-08-02 00:00:00+00:00    False
2022-08-03 00:00:00+00:00    False
Freq: D, Name: Open, Length: 1813, dtype: bool
```

1. Using [GenericAccessor.crossed_below](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_below).

2. Using [GenericAccessor.crossed_above](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_above).

You can also use the methods [RSI.rsi_crossed_below](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI.rsi_crossed_below) 
and [RSI.rsi_crossed_above](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI.rsi_crossed_above),
which were auto-generated for the `rsi` output by [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory):

```pycon
>>> entries = rsi.rsi_crossed_below(30)
>>> exits = rsi.rsi_crossed_above(70)
```

!!! tip
    If you are curious about what else has been generated, print `dir(rsi)` or explore the 
    [API](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI) generated for the class.

Before proceeding with portfolio modeling, let's plot the RSI and signals to make sure we set 
everything up correctly:

```pycon
>>> def plot_rsi(rsi, entries, exits):
...     fig = rsi.plot()  # (1)!
...     entries.vbt.signals.plot_as_entries(rsi.rsi, fig=fig)  # (2)!
...     exits.vbt.signals.plot_as_exits(rsi.rsi, fig=fig)  # (3)!
...     return fig

>>> plot_rsi(rsi, entries, exits).show()
```

1. Using [RSI.plot](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI.plot).

2. Using [SignalsSRAccessor.plot_as_entries](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_entries).

3. Using [SignalsSRAccessor.plot_as_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_exits).

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/rsi.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/rsi.dark.svg#only-dark){: .iimg loading=lazy }

The chart looks good. But notice how there are several entries between two exits, and vice versa?
How does VBT handle this? When you use [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
VBT will automatically ignore entry signals if a position is already open, and ignore exit signals 
if the position is already closed. To make our analysis clearer, let's keep only the first signal 
in each case:

```pycon
>>> clean_entries, clean_exits = entries.vbt.signals.clean(exits)  # (1)!

>>> plot_rsi(rsi, clean_entries, clean_exits).show()
```

1. Using [SignalsAccessor.clean](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.clean).

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/rsi2.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/rsi2.dark.svg#only-dark){: .iimg loading=lazy }

The difference is clear. But what other methods are available to analyze the distribution of signals? 
How can you *quantify* this analysis? That's what VBT is all about. Let's
compute various statistics for `clean_entries` and `clean_exits` using
[SignalsAccessor](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor):

```pycon
>>> clean_entries.vbt.signals.total()  # (1)!
8

>>> clean_exits.vbt.signals.total()  # (2)!
7

>>> ranges = clean_entries.vbt.signals.between_ranges(target=clean_exits)  # (3)!
>>> ranges.duration.mean(wrap_kwargs=dict(to_timedelta=True))  # (4)!
Timedelta('86 days 10:17:08.571428572')
```

1. Get the total number of entry signals.

2. Get the total number of exit signals.

3. Get range records of type [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges) 
between each entry and exit.

4. Get the average duration between each entry and exit.

We are now ready for modeling! We will use the class method [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
which takes the signal arrays, processes each signal step by step, and generates orders. 
It then creates an instance of [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) 
that you can use to assess the strategy's performance.

Our experiment is simple: buy $100 of Bitcoin when an entry signal is generated and close the position 
when an exit signal is generated. We will start with infinite capital so our buying power is not limited 
at any time.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     close=close_price, 
...     entries=clean_entries, 
...     exits=clean_exits,
...     size=100,
...     size_type='value',
...     init_cash='auto'
... )
>>> pf
<vectorbtpro.portfolio.base.Portfolio at 0x7f9c40eea438>
```

!!! info
    Running the method above for the first time may take some time because it needs to be compiled first.
    Compilation will happen each time a new combination of data types is encountered.
    But do not worry: Numba caches most compiled functions and reuses them in subsequent runs.

!!! tip
    If you check the API for [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
    you will notice that many arguments are set to None. The value `None` has a special meaning: it tells
    VBT to use the default value from the global settings. You can find all the default values for 
    the `Portfolio` class [here](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.portfolio).

Let's print the statistics for our portfolio:

```pycon
>>> pf.stats()
Start                         2017-08-17 00:00:00+00:00
End                           2022-08-03 00:00:00+00:00
Period                               1813 days 00:00:00
Start Value                                       100.0
Min Value                                     97.185676
Max Value                                    203.182943
End Value                                    171.335425
Total Return [%]                              71.335425
Benchmark Return [%]                         446.481746
Total Time Exposure [%]                       38.113624
Max Gross Exposure [%]                            100.0
Max Drawdown [%]                              46.385941
Max Drawdown Duration                1613 days 00:00:00
Total Orders                                         15
Total Fees Paid                                     0.0
Total Trades                                          8
Win Rate [%]                                  71.428571
Best Trade [%]                                54.519055
Worst Trade [%]                              -32.078597
Avg Winning Trade [%]                         26.905709
Avg Losing Trade [%]                         -19.345383
Avg Winning Trade Duration             87 days 09:36:00
Avg Losing Trade Duration              84 days 00:00:00
Profit Factor                                  3.477019
Expectancy                                    13.691111
Sharpe Ratio                                   0.505486
Calmar Ratio                                   0.246836
Omega Ratio                                    1.132505
Sortino Ratio                                  0.796701
dtype: object
```

!!! tip
    That's a lot of statistics, right? If you want to see how they're implemented,
    print `pf.metrics` and check the `calc_func` argument of the metric you're interested in.
    If a function is a lambda, check the source code to see its contents.

Our strategy is not too bad: the portfolio has gained over 71% in profit over the past years,
but simply holding Bitcoin still outperformed it with a massive 450%. Despite Bitcoin's volatility,
the minimum recorded portfolio value was $97 from $100 initially invested. The total time exposure
of 38% means we were in the market 38% of the time. The maximum gross exposure of 100% means we
invested 100% of our available cash on each trade. The maximum drawdown (MDD) of 46% is the largest
drop from a portfolio high to a low (maybe a stop loss could help here?).

The total number of orders matches the total number of (cleaned) signals, but why are there only 
8 trades instead of 15? By default, a trade in VBT is a sell order; as soon as an exit order 
is filled (reducing or closing the current position), the profit and loss (PnL) based 
on the weighted average entry and exit price is calculated. The win rate of 70% means that 70% of 
the trades (sell orders) generated a profit, with the best trade bringing a 54% profit and the 
worst bringing a 32% loss. Since the average winning trade brings more profit than the average 
losing trade brings loss, various metrics such as profit factor and expectancy are positive.

```pycon
>>> pf.plot(settings=dict(bm_returns=False)).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/pf.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/pf.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    One benefit of an interactive plot like this is that you can use tools from the Plotly toolbar
    to draw a vertical line connecting orders, their P&L, and their impact on cumulative returns.
    Give it a try!

So, how can we improve from here?

## Summary

VBT is a powerful tool that allows us to explore new areas more quickly and analyze them in greater detail.
Rather than relying on overused and outdated charts and indicators from books and YouTube videos,
we can create our own tools that align closely with the market. We can backtest thousands of strategy
configurations to see how the market responds to each one—all in just milliseconds.
All it takes is creativity :bulb:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/basic-rsi.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/BasicRSI.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/#using-for-loop
parent: https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/#multiple-backtests
name: Using for-loop
type: heading 3
---

### Using for-loop

Even a simple strategy like ours offers many possible parameters:

1. Lower threshold (`lower_th`).

2. Upper threshold (`upper_th`).

3. Window length (`window`).

4. Smoothing method (`ewm`).

To keep our analysis flexible, we will write a function that lets us specify all of this
information and returns a subset of statistics:

```pycon
>>> def test_rsi(window=14, wtype="wilder", lower_th=30, upper_th=70):
...     rsi = vbt.RSI.run(open_price, window=window, wtype=wtype)
...     entries = rsi.rsi_crossed_below(lower_th)
...     exits = rsi.rsi_crossed_above(upper_th)
...     pf = vbt.Portfolio.from_signals(
...         close=close_price, 
...         entries=entries, 
...         exits=exits,
...         size=100,
...         size_type='value',
...         init_cash='auto')
...     return pf.stats([
...         'total_return', 
...         'total_trades', 
...         'win_rate', 
...         'expectancy'
...     ])

>>> test_rsi()
Total Return [%]    71.335425
Total Trades                8
Win Rate [%]        71.428571
Expectancy          13.691111
dtype: object

>>> test_rsi(lower_th=20, upper_th=80)
Total Return [%]    6.652287
Total Trades               2
Win Rate [%]            50.0
Expectancy          3.737274
dtype: object
```

!!! note
    We removed the signal cleaning step because it is not necessary when signals are
    passed to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
    (which automatically cleans the signals for you).

By increasing the upper threshold to 80% and lowering the lower threshold to 20%, the
number of trades dropped to just 2 because crossing these thresholds becomes much less likely.
You can also see that the total return fell to roughly 7%—not a good sign. But how do we actually
know if this negative result means our strategy is bad and not just a result of pure luck?
Testing only one parameter combination from a large set is usually just guessing.

Let's generate multiple parameter combinations for the thresholds, simulate them, and
concatenate their statistics for further analysis:

```pycon
>>> lower_ths = range(20, 31)  # (1)!
>>> upper_ths = range(70, 81)  # (2)!
>>> th_combs = list(product(lower_ths, upper_ths))  # (3)!
>>> len(th_combs)
121

>>> comb_stats = [
...     test_rsi(lower_th=lower_th, upper_th=upper_th)
...     for lower_th, upper_th in th_combs
... ]  # (4)!
```

1. 20, 21, ..., 30.

2. 70, 71, ..., 80.

3. Generate all combinations between `lower_ths` and `upper_ths`, also known as a
[Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product).

4. Iterate over each combination and compute its statistics using a
[list comprehension](https://realpython.com/list-comprehension-python/).
This creates a list of Series.

We just simulated 121 different combinations of upper and lower thresholds
and stored their statistics in a list. To analyze this list,
we need to convert it to a DataFrame first, with metrics arranged as columns:

```pycon
>>> comb_stats_df = pd.DataFrame(comb_stats)
>>> comb_stats_df
      Total Return [%]  Total Trades  Win Rate [%]  Expectancy
0           24.369550             3     66.666667   10.606342
1           37.380341             3     66.666667   16.203667
2           34.560194             3     66.666667   14.981187
3           31.090080             3     66.666667   13.833710
4           31.090080             3     66.666667   13.833710
..                ...           ...           ...         ...
116         51.074571             6     80.000000   18.978193
117         62.853840             6     80.000000   21.334047
118         40.685579             5     75.000000   21.125494
119         -5.990835             4     66.666667   13.119897
120        -10.315159             4     66.666667   11.678455

[121 rows x 4 columns]
```

But how do we know which row corresponds to which parameter combination?
We will create a [MultiIndex](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)
with two levels, `lower_th` and `upper_th`, and set it as the index of `comb_stats_df`:

```pycon
>>> comb_stats_df.index = pd.MultiIndex.from_tuples(
...     th_combs, 
...     names=['lower_th', 'upper_th'])
>>> comb_stats_df
                    Total Return [%]  Total Trades  Win Rate [%]  Expectancy
lower_th upper_th                                                          
20       70               24.369550             3     66.666667   10.606342
          71               37.380341             3     66.666667   16.203667
          72               34.560194             3     66.666667   14.981187
          73               31.090080             3     66.666667   13.833710
          74               31.090080             3     66.666667   13.833710
...                             ...           ...           ...         ...
30       76               51.074571             6     80.000000   18.978193
          77               62.853840             6     80.000000   21.334047
          78               40.685579             5     75.000000   21.125494
          79               -5.990835             4     66.666667   13.119897
          80              -10.315159             4     66.666667   11.678455

[121 rows x 4 columns]
```

Much better! Now we can analyze each part of the retrieved information in different ways.
Since we have the same number of lower and upper thresholds, let's create a heatmap
with the X axis representing lower thresholds, the Y axis representing upper thresholds,
and the color bar reflecting the expectancy:

```pycon
>>> comb_stats_df['Expectancy'].vbt.heatmap().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/heatmap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/heatmap.dark.svg#only-dark){: .iimg loading=lazy }

Now we can explore entire regions of parameter combinations that yield positive or negative results.

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/#using-columns
parent: https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi/#multiple-backtests
name: Using columns
type: heading 3
---

### Using columns

As you may have read in the documentation, VBT is designed for processing multidimensional data.
It is built around the idea that you can represent each asset, period, parameter
combination, or backtest generally as a column in a two-dimensional array.

Instead of calculating everything inside a loop (which isn't wrong, but is often slower than a vectorized
solution), we can update our code to accept parameters as arrays.
A function that takes such an array will automatically convert multiple parameters into
multiple columns. One major benefit of this approach is that we do not have to collect results,
place them into a list, and convert them into a DataFrame—VBT handles it all!

First, define the parameters that we want to test:

```pycon
>>> windows = list(range(8, 21))
>>> wtypes = ["simple", "exp", "wilder"]
>>> lower_ths = list(range(20, 31))
>>> upper_ths = list(range(70, 81))
```

Instead of applying `itertools.product`, we will instruct various parts of our pipeline to create
a product instead, so that we can observe how each part affects the column hierarchy.

The RSI part is easy: we can pass `param_product=True` to build a product of `windows` and `wtypes`
and run the calculation over each column in `open_price`:

```pycon
>>> rsi = vbt.RSI.run(
...     open_price, 
...     window=windows, 
...     wtype=wtypes, 
...     param_product=True)
>>> rsi.rsi.columns
MultiIndex([( 8, 'simple'),
            ( 8,    'exp'),
            ( 8, 'wilder'),
            ...
            (20, 'simple'),
            (20,    'exp'),
            (20, 'wilder')],
            names=['rsi_window', 'rsi_wtype'])
```

We can see that [RSI](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI) has added
two levels to the column hierarchy: `rsi_window` and `rsi_wtype`. These are similar to what we
created manually for thresholds in [Using for-loop](#using-for-loop). There are now 39
columns in total, which equals `len(open_price.columns)` x `len(windows)` x `len(wtypes)`.

The next step is crossovers. Unlike indicators, crossovers are regular functions that take
any array-like object, broadcast it to the shape of `rsi`, and then search for crossovers.
The broadcasting step uses [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast),
which is a powerful function for bringing multiple arrays into a single shape
(learn more about broadcasting in the documentation).

In our case, we want to build a product of `lower_ths`, `upper_th_index`, and all columns in `rsi`.
Since `rsi_crossed_below` and `rsi_crossed_above` are two different functions,
we need to build a product of the threshold values manually, then instruct each
crossover function to combine them with every column in `rsi`:

```pycon
>>> lower_ths_prod, upper_ths_prod = zip(*product(lower_ths, upper_ths))
>>> len(lower_ths_prod)  # (1)!
121
>>> len(upper_ths_prod)
121

>>> lower_th_index = vbt.Param(lower_ths_prod, name='lower_th')  # (2)!
>>> entries = rsi.rsi_crossed_below(lower_th_index)
>>> entries.columns
MultiIndex([(20,  8, 'simple'),
            (20,  8,    'exp'),
            (20,  8, 'wilder'),
            ...
            (30, 20, 'simple'),
            (30, 20,    'exp'),
            (30, 20, 'wilder')],
            names=['lower_th', 'rsi_window', 'rsi_wtype'], length=4719)

>>> upper_th_index = vbt.Param(upper_ths_prod, name='upper_th')
>>> exits = rsi.rsi_crossed_above(upper_th_index)
>>> exits.columns
MultiIndex([(70,  8, 'simple'),
            (70,  8,    'exp'),
            (70,  8, 'wilder'),
            ...
            (80, 20, 'simple'),
            (80, 20,    'exp'),
            (80, 20, 'wilder')],
            names=['upper_th', 'rsi_window', 'rsi_wtype'], length=4719)
```

1. The first value in `lower_ths_prod` is paired with the first value in `upper_ths_prod`,
the second with the second, and so on—121 combinations in total.

2. Convert thresholds to `vbt.Param` to tell [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast)
that we want to build a product with the columns in `rsi`.

We have produced over 4719 columns—impressive! Did you notice, though, that `entries` and `exits`
now have different columns? `entries` has `lower_th` as one of its column levels, while `exits` has
`upper_th`. How can we pass differently labeled arrays (including `close_price`, which has
only one column) to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)?

No worries! VBT knows how to merge this information. Let's see how it works:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     close=close_price, 
...     entries=entries, 
...     exits=exits,
...     size=100,
...     size_type='value',
...     init_cash='auto'
... )
>>> pf
<vectorbtpro.portfolio.base.Portfolio at 0x7f9c415ed5c0>

>>> stats_df = pf.stats([
...     'total_return', 
...     'total_trades', 
...     'win_rate', 
...     'expectancy'
... ], agg_func=None)  # (1)!
>>> stats_df
                                        Total Return [%]  Total Trades  \
lower_th upper_th rsi_window rsi_wtype                                   
20       70       8          simple           -25.285842            31   
                              exp               -7.939736            29   
                              wilder            61.979801            11   
...                                                  ...           ...   
                  20         simple           -59.159157             4   
                              exp               -3.331163             8   
                              wilder            31.479482             3   

                                        Win Rate [%]  Expectancy  
lower_th upper_th rsi_window rsi_wtype                            
20       70       8          simple        51.612903   -1.224523  
                              exp           58.620690   -0.307862  
                              wilder        72.727273    5.634527  
...                                              ...         ...  
                  20         simple        33.333333  -16.159733  
                              exp           57.142857    7.032204  
                              wilder        50.000000   38.861607  

[4719 rows x 4 columns]
```

1. By default, [StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)
takes the mean of all columns and returns a Series. Here, we want to disable the aggregation
function and stack all Series into one large DataFrame.

Congratulations! We just backtested 4719 parameter combinations in less than a second :zap:

!!! important
    Even though we gained impressive performance, we need to be careful not to fill up all available RAM
    with our wide arrays. You can check the size of any
    [Pickleable](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable)
    instance using [Pickleable.getsize](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.getsize).
    For example, to print the total size of our portfolio in a human-readable format:

    ```pycon
    >>> print(pf.getsize())
    9.4 MB
    ```

    Although the portfolio holds about 10 MB of compressed data, it must generate many arrays,
    such as portfolio value, with the shape of the number of timestamps x parameter combinations:

    ```pycon
    >>> np.prod(pf.wrapper.shape) * 8 / 1024 / 1024
    65.27364349365234
    ```

    You can see that each floating array uses 65 MB of memory. By creating a dozen such arrays
    (which is often a worst-case scenario), memory consumption can quickly reach 1 GB.

One way to analyze the produced statistics is to use Pandas. For example,
to calculate the mean expectancy for each `rsi_window`:

```pycon
>>> stats_df['Expectancy'].groupby('rsi_window').mean()
rsi_window
8      0.154425
9      0.064130
10    -0.915478
11    -0.523294
12     0.742266
13     3.898482
14     4.414367
15     6.916872
16     8.915225
17    12.204188
18    12.897135
19    14.508950
20    16.429515
Name: Expectancy, dtype: float64
```

The longer the RSI window, the higher the mean expectancy.

Now, let's display the top 5 parameter combinations:

```pycon
>>> stats_df.sort_values(by='Expectancy', ascending=False).head()
                                        Total Return [%]  Total Trades  \
lower_th upper_th rsi_window rsi_wtype                                   
22       80       20         wilder           187.478208             2   
21       80       20         wilder           187.478208             2   
26       80       20         wilder           152.087039             3   
23       80       20         wilder           187.478208             2   
25       80       20         wilder           201.297495             3   

                                        Win Rate [%]  Expectancy  
lower_th upper_th rsi_window rsi_wtype                            
22       80       20         wilder            100.0   93.739104  
21       80       20         wilder            100.0   93.739104  
26       80       20         wilder            100.0   93.739104  
23       80       20         wilder            100.0   93.739104  
25       80       20         wilder            100.0   93.739104  
```

To analyze any specific combination with VBT, you can select it from the portfolio
just like you would select a column in a regular Pandas DataFrame. Let's plot the equity
of the most successful combination:

```pycon
>>> pf[(22, 80, 20, "wilder")].plot_value().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/value.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/value.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    Instead of selecting a column from a portfolio, which creates a new portfolio containing only
    that column, you can also check if the method you want to call supports the argument `column`
    and pass your column as this argument. For example, you could also use
    `pf.plot_value(column=(22, 80, 20, "wilder"))`.

Even though, in theory, the best combination doubles our money, it is still inferior to
simply holding Bitcoin—our basic RSI strategy cannot beat the market :anger:

But even if it could, there is more to do than just searching for the right parameters:
we need to at least (cross-)validate the strategy. You can also see how the strategy
performs on other assets. Curious how? Just expand `open_price` and `close_price`
to include multiple assets. Each example will work out of the box!

```pycon
>>> data = vbt.BinanceData.pull(['BTCUSDT', 'ETHUSDT'])
```

[=100% "100%"]{: .candystripe .candystripe-animate }

Your homework is to run the examples on this data.

The final columns will look like this:

```pycon
MultiIndex([(20, 70,  8, 'simple', 'BTCUSDT'),
            (20, 70,  8, 'simple', 'ETHUSDT'),
            (20, 70,  8,    'exp', 'BTCUSDT'),
            ...
            (30, 80, 20,    'exp', 'ETHUSDT'),
            (30, 80, 20, 'wilder', 'BTCUSDT'),
            (30, 80, 20, 'wilder', 'ETHUSDT')],
            names=['lower_th', 'upper_th', 'rsi_window', 'rsi_wtype', 'symbol'], length=9438)
```

We can see that the column hierarchy now contains another level, `symbol`, which represents the asset.
Let's visualize the distribution of expectancy across both assets:

```pycon
>>> eth_mask = stats_df.index.get_level_values('symbol') == 'ETHUSDT'
>>> btc_mask = stats_df.index.get_level_values('symbol') == 'BTCUSDT'
>>> pd.DataFrame({
...     'ETHUSDT': stats_df[eth_mask]['Expectancy'].values,
...     'BTCUSDT': stats_df[btc_mask]['Expectancy'].values
... }).vbt.histplot(xaxis=dict(title="Expectancy")).show()  # (1)!
```

1. Uses [GenericAccessor.histplot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.histplot)

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/histplot.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/basic-rsi/histplot.dark.svg#only-dark){: .iimg loading=lazy }

ETH appears to react more aggressively to our strategy on average than BTC. This could be due to the market's
greater volatility, a different structure, or just pure randomness.

One of the main takeaways from this analysis is that with strategies that have simple and
explainable mechanics, we can attempt to explain the mechanics of the market itself. Not only can we
use this to improve ourselves and design better indicators, but we can also use this information as input
to ML models, which are better at finding patterns than humans. The possibilities are endless!

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
children:
  - https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/streaming/
  - https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/multithreading/
  - https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/pipelines/
name: SuperFast SuperTrend
type: page
icon: material-lightning-bolt-outline
---

# SuperFast SuperTrend

While Python is slower than many compiled languages, it is easy to use and extremely versatile.
For many users, especially in the data science field, Python's practicality outweighs concerns about
speed. It serves as a Swiss Army knife for programmers and researchers alike.

Unfortunately for quants, Python can become a significant bottleneck when working with large datasets.
To address this, an entire ecosystem of scientific packages—such as NumPy and Pandas—exists. These
packages are highly optimized for performance, with critical code paths often written in Cython or C.
They primarily operate on arrays, providing a consistent interface for efficient data processing.

This approach is especially valuable when creating indicators that can be expressed as a set of
vectorized operations, such as [OBV](https://www.investopedia.com/terms/o/onbalancevolume.asp).
Even for non-vectorized operations like the exponentially weighted moving average (EMA), which is
used in many indicators such as [MACD](https://www.investopedia.com/terms/m/macd.asp),
implementations are written in compiled languages and exposed as ready-to-use Python functions.
However, some indicators are challenging or even impossible to develop solely with standard array
operations because they introduce path dependency, where today's decisions depend on those made
yesterday. SuperTrend is one such indicator.

In this example, you will learn how to design and implement a SuperTrend indicator, and step by step
optimize it for exceptional performance using [TA-Lib](https://github.com/mrjbq7/ta-lib) and
[Numba](http://numba.pydata.org/). We will also backtest the new indicator across a range of parameters
using VBT (PRO).

## Data

The first step is always obtaining the right data. Specifically, we need enough data to benchmark
different SuperTrend implementations. Let's pull 2 years of hourly Bitcoin and Ethereum data from
Binance using VBT's [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData) class:

```pycon
>>> from vectorbtpro import *

>>> data = vbt.BinanceData.pull(
...     ['BTCUSDT', 'ETHUSDT'], 
...     start='2020-01-01 UTC',
...     end='2022-01-01 UTC',
...     timeframe='1h'
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

[=100% "Period 36/36"]{: .candystripe .candystripe-animate }

Fetching data for both symbols took about 80 seconds to complete. Since Binance, like any
other exchange, will not return all the data at once, VBT first requested the maximum
amount of data starting from January 1, 2020, and then gradually collected the remaining data,
while respecting Binance's API rate limits. In total, this resulted in 36 requests per symbol.
Finally, VBT aligned both symbols in case their indexes or columns were different and made the
final index timezone-aware (in UTC).

To avoid repeatedly hitting the Binance servers every time we start a new Python session, we should
save the downloaded data locally using either VBT's
[Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv) or
[Data.to_hdf](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_hdf):

```pycon
>>> data.to_hdf('my_data.h5')
```

We can then easily access the saved data using [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData):

```pycon
>>> data = vbt.HDFData.pull('my_data.h5')
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

!!! tip
    We can access any of the symbols in an HDF file using regular path expressions.
    For example, the same as above: `vbt.HDFData.pull(['my_data.h5/BTCUSDT', 'my_data.h5/ETHUSDT'])`.

Once we have the data, let's take a quick look at what is inside. To get any of the stored
DataFrames, use the [Data.data](https://vectorbt.pro/pvt_41531c19/api/data/custom/#vectorbtpro.data.base.Data.data) dictionary,
where each DataFrame is keyed by symbol:

```pycon
>>> data.data['BTCUSDT'].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 17514 entries, 2019-12-31 23:00:00+00:00 to 2021-12-31 22:00:00+00:00
Data columns (total 10 columns):
  #   Column              Non-Null Count  Dtype              
---  ------              --------------  -----              
  0   Open                17514 non-null  float64            
  1   High                17514 non-null  float64            
  2   Low                 17514 non-null  float64            
  3   Close               17514 non-null  float64            
  4   Volume              17514 non-null  float64            
  5   Close time          17514 non-null  datetime64[ns, UTC]
  6   Quote volume        17514 non-null  float64            
  7   Number of trades    17514 non-null  int64              
  8   Taker base volume   17514 non-null  float64            
  9   Taker quote volume  17514 non-null  float64            
dtypes: datetime64[ns, UTC](1), float64(8), int64(1)
memory usage: 2.0 MB
```

We can also get an overview of all the symbols collected:

```pycon
>>> data.stats()
Start                   2020-01-01 00:00:00+00:00
End                     2021-12-31 23:00:00+00:00
Period                                      17513
Total Symbols                                   2
Last Index: BTCUSDT     2021-12-31 23:00:00+00:00
Last Index: ETHUSDT     2021-12-31 23:00:00+00:00
Null Counts: BTCUSDT                            0
Null Counts: ETHUSDT                            0
Name: agg_stats, dtype: object
```

Each symbol has 17513 data points with no NaNs—perfect!

If you have ever worked with VBT, you know that VBT prefers data with symbols as columns
(one per backtest) instead of features as columns. Since SuperTrend depends on the high, low, and
close price, let's get those three features as separate DataFrames using
[Data.get](https://vectorbt.pro/pvt_41531c19/api/data/custom/#vectorbtpro.data.base.Data.get):

```pycon
>>> high = data.get('High')
>>> low = data.get('Low')
>>> close = data.get('Close')

>>> close
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2020-01-01 00:00:00+00:00   7177.02   128.87
2020-01-01 01:00:00+00:00   7216.27   130.64
2020-01-01 02:00:00+00:00   7242.85   130.85
2020-01-01 03:00:00+00:00   7225.01   130.20
2020-01-01 04:00:00+00:00   7217.27   130.20
...                             ...      ...
2021-12-31 19:00:00+00:00  45728.28  3626.27
2021-12-31 20:00:00+00:00  45879.24  3645.04
2021-12-31 21:00:00+00:00  46333.86  3688.41
2021-12-31 22:00:00+00:00  46303.99  3681.80
2021-12-31 23:00:00+00:00  46216.93  3676.23

[17513 rows x 2 columns]
```

!!! tip
    To get a column for a specific symbol as a Series, use `data.get('Close', 'BTCUSDT')`.

We are all set to design our first SuperTrend indicator!

## Design

SuperTrend is a trend-following indicator that uses the Average True Range
([ATR](https://en.wikipedia.org/wiki/Average_true_range)) and the
[median price](https://www.incrediblecharts.com/indicators/median_price.php) to define a set of
upper and lower bands. The idea is straightforward: when the close price crosses above the upper
band, the asset is considered to be entering an uptrend, which signals a buy. When the close price
crosses below the lower band, the asset is considered to have exited the uptrend, which signals a sell.

While the concept is simple, the calculation process is a bit more complex:

```plaintext
BASIC UPPERBAND = (HIGH + LOW) / 2 + Multiplier * ATR
BASIC LOWERBAND = (HIGH + LOW) / 2 - Multiplier * ATR

FINAL UPPERBAND = IF (Current BASICUPPERBAND < Previous FINAL UPPERBAND) or (Previous Close > Previous FINAL UPPERBAND)
                  THEN Current BASIC UPPERBAND
                  ELSE Previous FINAL UPPERBAND
FINAL LOWERBAND = IF (Current BASIC LOWERBAND > Previous FINAL LOWERBAND) or (Previous Close < Previous FINAL LOWERBAND)
                  THEN Current BASIC LOWERBAND 
                  ELSE Previous FINAL LOWERBAND
                  
SUPERTREND      = IF (Previous SUPERTREND == Previous FINAL UPPERBAND) and (Current Close <= Current FINAL UPPERBAND)) 
                  THEN Current FINAL UPPERBAND
                  ELIF (Previous SUPERTREND == Previous FINAL UPPERBAND) and (Current Close > Current FINAL UPPERBAND) 
                  THEN Current FINAL LOWERBAND
                  ELIF (Previous SUPERTREND == Previous FINAL LOWERBAND) and (Current Close >= Current FINAL LOWERBAND) 
                  THEN Current FINAL LOWERBAND
                  ELIF (Previous SUPERTREND == Previous FINAL LOWERBAND) and (Current Close < Current FINAL LOWERBAND) 
                  THEN Current FINAL UPPERBAND
```

Even though the basic bands can easily be computed using standard tools, you will likely get a
headache when trying to calculate the final bands. Most open-source solutions agree on using a
basic Python for-loop to write the array elements one at a time. But is this approach scalable? We
are here to find out!

### Pandas

[Pandas](https://github.com/pandas-dev/pandas) is a fast, powerful, flexible, and easy-to-use open
source data analysis and manipulation tool. Since it is the go-to library for data processing in
Python, let's write our first implementation using only Pandas. This implementation will take one
column and one combination of parameters, and return four arrays: one for the SuperTrend (`trend`),
one for the direction (`dir_`), one for the uptrend (`long`), and one for the downtrend (`short`).
We will also split the implementation into five parts for readability and so we can optimize any
component at any time:

1. Calculation of the median price - `get_med_price`.

2. Calculation of the ATR - `get_atr`.

3. Calculation of the basic bands - `get_basic_bands`.

4. Calculation of the final bands - `get_final_bands`.

5. Putting all parts together - `supertrend`.

```pycon
>>> def get_med_price(high, low):
...     return (high + low) / 2

>>> def get_atr(high, low, close, period):
...     tr0 = abs(high - low)
...     tr1 = abs(high - close.shift())
...     tr2 = abs(low - close.shift())
...     tr = pd.concat((tr0, tr1, tr2), axis=1).max(axis=1)  # (1)!
...     atr = tr.ewm(
...         alpha=1 / period, 
...         adjust=False, 
...         min_periods=period).mean()  # (2)!
...     return atr

>>> def get_basic_bands(med_price, atr, multiplier):
...     matr = multiplier * atr
...     upper = med_price + matr
...     lower = med_price - matr
...     return upper, lower

>>> def get_final_bands(close, upper, lower):  # (3)!
...     trend = pd.Series(np.full(close.shape, np.nan), index=close.index)
...     dir_ = pd.Series(np.full(close.shape, 1), index=close.index)
...     long = pd.Series(np.full(close.shape, np.nan), index=close.index)
...     short = pd.Series(np.full(close.shape, np.nan), index=close.index)
... 
...     for i in range(1, close.shape[0]):  # (4)!
...         if close.iloc[i] > upper.iloc[i - 1]:
...             dir_.iloc[i] = 1
...         elif close.iloc[i] < lower.iloc[i - 1]:
...             dir_.iloc[i] = -1
...         else:
...             dir_.iloc[i] = dir_.iloc[i - 1]
...             if dir_.iloc[i] > 0 and lower.iloc[i] < lower.iloc[i - 1]:
...                 lower.iloc[i] = lower.iloc[i - 1]
...             if dir_.iloc[i] < 0 and upper.iloc[i] > upper.iloc[i - 1]:
...                 upper.iloc[i] = upper.iloc[i - 1]
... 
...         if dir_.iloc[i] > 0:
...             trend.iloc[i] = long.iloc[i] = lower.iloc[i]
...         else:
...             trend.iloc[i] = short.iloc[i] = upper.iloc[i]
...             
...     return trend, dir_, long, short

>>> def supertrend(high, low, close, period=7, multiplier=3):
...     med_price = get_med_price(high, low)
...     atr = get_atr(high, low, close, period)
...     upper, lower = get_basic_bands(med_price, atr, multiplier)
...     return get_final_bands(close, upper, lower)
```

1. Take the maximum across three arrays at each time step.

2. Wilder's moving average instead of the standard exponential moving average.

3. This function was heavily inspired by the implementation found in [Pandas TA](https://pypi.org/project/pandas-ta/).

4. As in the real world, iterate over the entire data one step at a time.

Let's run the `supertrend` function on the `BTCUSDT` symbol:

```pycon
>>> supert, superd, superl, supers = supertrend(
...     high['BTCUSDT'], 
...     low['BTCUSDT'], 
...     close['BTCUSDT']
... )

>>> supert
Open time
2020-01-01 00:00:00+00:00             NaN
2020-01-01 01:00:00+00:00             NaN
2020-01-01 02:00:00+00:00             NaN
                                      ...
2021-12-31 21:00:00+00:00    47608.346563
2021-12-31 22:00:00+00:00    47608.346563
2021-12-31 23:00:00+00:00    47608.346563
Length: 17513, dtype: float64

>>> superd  # (1)!
Open time
2020-01-01 00:00:00+00:00    1
2020-01-01 01:00:00+00:00    1
2020-01-01 02:00:00+00:00    1
                            ...
2021-12-31 21:00:00+00:00   -1
2021-12-31 22:00:00+00:00   -1
2021-12-31 23:00:00+00:00   -1
Length: 17513, dtype: int64

>>> superl  # (2)!
Open time
2020-01-01 00:00:00+00:00   NaN
2020-01-01 01:00:00+00:00   NaN
2020-01-01 02:00:00+00:00   NaN
                            ...
2021-12-31 21:00:00+00:00   NaN
2021-12-31 22:00:00+00:00   NaN
2021-12-31 23:00:00+00:00   NaN
Length: 17513, dtype: float64

>>> supers  # (3)!
Open time
2020-01-01 00:00:00+00:00             NaN
2020-01-01 01:00:00+00:00             NaN
2020-01-01 02:00:00+00:00             NaN
                                      ...
2021-12-31 21:00:00+00:00    47608.346563
2021-12-31 22:00:00+00:00    47608.346563
2021-12-31 23:00:00+00:00    47608.346563
Length: 17513, dtype: float64
```

1. 1 = uptrend, -1 = downtrend.

2. Any positive value = uptrend, NaN = downtrend or unknown.

3. Any positive value = downtrend, NaN = uptrend or unknown.

If you print out the head of the `supert` Series using `supert.head(10)`, you will notice that
the first 6 data points are all NaN. This is because ATR's rolling period is 7, so the first 6
computed windows have incomplete data.

A graph is worth 1,000 words. Let's plot the first month of data (January 2020):

```pycon
>>> date_range = slice('2020-01-01', '2020-02-01')
>>> fig = close.loc[date_range, 'BTCUSDT'].rename('Close').vbt.plot()  # (1)!
>>> supers.loc[date_range].rename('Short').vbt.plot(fig=fig)
>>> superl.loc[date_range].rename('Long').vbt.plot(fig=fig)
>>> fig.show()
```

1. Using [GenericAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.plot).

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/pandas.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/pandas.dark.svg#only-dark){: .iimg loading=lazy }

We have generated and visualized the SuperTrend values, but what about performance?
Can we already get our overfitting machine running with thousands of parameter combinations?
Not so fast. As you might have guessed, the `supertrend` function takes some time to compute:

```pycon
>>> %%timeit
>>> supertrend(high['BTCUSDT'], low['BTCUSDT'], close['BTCUSDT'])
2.15 s ± 19.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

Ouch! Running 1000 backtests would take roughly 33 minutes.

Let's see what Pandas TA has to say about this:

```pycon
>>> SUPERTREND = vbt.pandas_ta('SUPERTREND')  # (1)!

>>> %%timeit
>>> SUPERTREND.run(high['BTCUSDT'], low['BTCUSDT'], close['BTCUSDT'])
784 ms ± 14.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. [IndicatorFactory.from_pandas_ta](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_pandas_ta),
or the `vbt.pandas_ta` shortcut, is VBT's function to wrap any Pandas TA indicator so that
it can take multiple columns and parameter combinations.

That's a 3x speedup, mainly because Pandas TA uses ATR from TA-Lib.

Is this now acceptable? Of course not :anger: Can we do better than this? Absolutely!

### NumPy + Numba = :heart:

Pandas excels at manipulating heterogeneous tabular data, but is it really suitable for indicators?
You might have noticed that even though we used Pandas, none of the operations in our newly defined
functions use index or column labels. Additionally, most indicators take, manipulate, and return
arrays of the same dimensions and shape. This makes indicator development a purely algebraic
problem that can be broken down into multiple vectorized steps or solved on a per-element basis
(or both!). Since Pandas extends NumPy, and NumPy is considered a faster (though lower-level)
package, let's adapt our logic to NumPy arrays instead.

Both `get_med_price` and `get_basic_bands` use simple arithmetic operations like addition and
multiplication, which can be performed on both Pandas and NumPy arrays and require no further
changes. What about `get_atr` and `get_final_bands`? The former can be re-implemented using NumPy
and VBT's own set of Numba-compiled functions:

```pycon
>>> def get_atr_np(high, low, close, period):
...     shifted_close = vbt.nb.fshift_1d_nb(close)  # (1)!
...     tr0 = np.abs(high - low)
...     tr1 = np.abs(high - shifted_close)
...     tr2 = np.abs(low - shifted_close)
...     tr = np.column_stack((tr0, tr1, tr2)).max(axis=1)  # (2)!
...     atr = vbt.nb.wwm_mean_1d_nb(tr, period)  # (3)!
...     return atr
```

1. Uses [fshift_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.fshift_1d_nb), which shifts
one-dimensional data by one to `n` elements forward.

2. Like Pandas, this concatenates three arrays as columns and finds the maximum at each time step.

3. Uses [wwm_mean_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.wwm_mean_1d_nb),
which calculates Wilder's exponentially weighted moving average on one-dimensional data.

The latter, on the other hand, is an iterative algorithm. While this isn't ideal for NumPy, it's
a perfect use case for Numba, which can easily run for-loops at machine code speed:

```pycon
>>> @njit
... def get_final_bands_nb(close, upper, lower):  # (1)!
...     trend = np.full(close.shape, np.nan)  # (2)!
...     dir_ = np.full(close.shape, 1)
...     long = np.full(close.shape, np.nan)
...     short = np.full(close.shape, np.nan)
... 
...     for i in range(1, close.shape[0]):
...         if close[i] > upper[i - 1]:  # (3)!
...             dir_[i] = 1
...         elif close[i] < lower[i - 1]:
...             dir_[i] = -1
...         else:
...             dir_[i] = dir_[i - 1]
...             if dir_[i] > 0 and lower[i] < lower[i - 1]:
...                 lower[i] = lower[i - 1]
...             if dir_[i] < 0 and upper[i] > upper[i - 1]:
...                 upper[i] = upper[i - 1]
... 
...         if dir_[i] > 0:
...             trend[i] = long[i] = lower[i]
...         else:
...             trend[i] = short[i] = upper[i]
...             
...     return trend, dir_, long, short
```

1. The `nb` suffix is commonly used in VBT for Numba-compiled functions.

2. Removed use of `pd.Series` everywhere.

3. Replaced all `iloc` usages.

If you look at the function above, you will see that 1) it is regular Python code that can run even
without the `@njit` decorator, and 2) it's almost identical to the Pandas implementation, except
each `iloc[...]` is replaced by `[...]`. We can write a simple Python function that works on
constants and NumPy arrays, and Numba will try to make it **much** faster—fully automatically.
Isn't that impressive?

Let's look at the result of this refactoring:

```pycon
>>> def faster_supertrend(high, low, close, period=7, multiplier=3):
...     med_price = get_med_price(high, low)
...     atr = get_atr_np(high, low, close, period)
...     upper, lower = get_basic_bands(med_price, atr, multiplier)
...     return get_final_bands_nb(close, upper, lower)

>>> supert, superd, superl, supers = faster_supertrend(
...     high['BTCUSDT'].values,  # (1)!
...     low['BTCUSDT'].values, 
...     close['BTCUSDT'].values
... )

>>> supert
array([          nan,           nan,           nan, ..., 47608.3465635,
        47608.3465635, 47608.3465635])
>>> superd
array([ 1,  1,  1, ..., -1, -1, -1])

>>> superl
array([nan, nan, nan, ..., nan, nan, nan])

>>> supers
array([          nan,           nan,           nan, ..., 47608.3465635,
        47608.3465635, 47608.3465635])
```

1. Access the NumPy array of any Pandas object using `values`.

!!! info
    When calling a Numba-decorated function for the first time, execution may take longer due to compilation.

As expected, these are arrays similar to the ones returned by the `supertrend` function,
just without any labels. To attach labels, simply do:

```pycon
>>> pd.Series(supert, index=close.index)
Open time
2020-01-01 00:00:00+00:00             NaN
2020-01-01 01:00:00+00:00             NaN
2020-01-01 02:00:00+00:00             NaN
                                      ...
2021-12-31 21:00:00+00:00    47608.346563
2021-12-31 22:00:00+00:00    47608.346563
2021-12-31 23:00:00+00:00    47608.346563
Length: 17513, dtype: float64
```

Curious how much our code has improved in performance? Here is the answer:

```pycon
%%timeit
>>> faster_supertrend(
...     high['BTCUSDT'].values, 
...     low['BTCUSDT'].values,
...     close['BTCUSDT'].values
... )
1.11 ms ± 7.05 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

That's a 780x speedup over a typical Pandas TA run :smiling_imp:

### NumPy + Numba + TA-Lib = :zap:

If you think this result can't be improved, you may not have tried working with TA-Lib.
Even though TA-Lib does not provide a SuperTrend indicator, we can still use its highly
optimized functions for intermediate calculations. Specifically, instead of reinventing
the wheel by implementing median price and ATR from scratch, we can use the
`MEDPRICE` and `ATR` TA-Lib functions. They provide two major advantages over our custom implementations:

1. Single pass through the data.

2. No Numba compilation overhead.

```pycon
>>> import talib

>>> def faster_supertrend_talib(high, low, close, period=7, multiplier=3):
...     avg_price = talib.MEDPRICE(high, low)  # (1)!
...     atr = talib.ATR(high, low, close, period)
...     upper, lower = get_basic_bands(avg_price, atr, multiplier)
...     return get_final_bands_nb(close, upper, lower)

>>> faster_supertrend_talib(
...     high['BTCUSDT'].values, 
...     low['BTCUSDT'].values, 
...     close['BTCUSDT'].values
... )
(array([          nan,           nan,           nan, ..., 47608.3465635,
        47608.3465635, 47608.3465635]),
  array([ 1,  1,  1, ..., -1, -1, -1]),
  array([nan, nan, nan, ..., nan, nan, nan]),
  array([          nan,           nan,           nan, ..., 47608.3465635,
        47608.3465635, 47608.3465635]))

>>> %%timeit
>>> faster_supertrend_talib(
...     high['BTCUSDT'].values, 
...     low['BTCUSDT'].values, 
...     close['BTCUSDT'].values
... )
253 µs ± 815 ns per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

1. See [TA-Lib Functions](https://github.com/TA-Lib/ta-lib/blob/main/docs/functions.md).

That's another 4x improvement. By the time another trader processes a single column of data, we
could have processed about three thousand columns. Admittedly, the speed of our indicator is
getting ridiculously high :smile:

## Indicator factory

Let's pause here and consider: why do we need such high performance?

This is where parameter optimization becomes important. The two parameters we have, `period` and
`multiplier`, are the default values commonly used in technical analysis. However, what makes
these values universal, and how do we know there are not better values for the markets we are in?
Imagine having a pipeline that can backtest hundreds or even thousands of parameter combinations,
uncovering configurations and market regimes that, on average, correlate better.

[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) is
VBT's own powerhouse for making any indicator function fully parametrizable. To better
understand what this means, let's supercharge the `faster_supertrend_talib` function:

```pycon
>>> SuperTrend = vbt.IF(
...     class_name='SuperTrend',
...     short_name='st',
...     input_names=['high', 'low', 'close'],
...     param_names=['period', 'multiplier'],
...     output_names=['supert', 'superd', 'superl', 'supers']
... ).with_apply_func(
...     faster_supertrend_talib, 
...     takes_1d=True,  # (1)!
...     period=7,  # (2)!
...     multiplier=3
... )
```

1. Our function only accepts one-dimensional arrays.

2. Default parameter values.

The indicator factory is a class that can generate what are known as indicator classes. You can 
think of it as a conveyor belt that takes the specification of your indicator function and produces 
a standalone Python class for running that function in a very flexible way. In our example, when we
called `vbt.IF(...)`, it internally created the indicator class `SuperTrend`. Once we supplied
`faster_supertrend_talib` to [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func),
it attached a `SuperTrend.run` method for running the indicator. Let's try it out!

```pycon
>>> vbt.phelp(SuperTrend.run)  # (1)!
SuperTrend.run(
    high,
    low,
    close,
    period=Default(value=7),
    multiplier=Default(value=3),
    short_name='st',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `SuperTrend` indicator.
    

    * Inputs: `high`, `low`, `close`

    * Parameters: `period`, `multiplier`

    * Outputs: `supert`, `superd`, `superl`, `supers`
    
    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.
    
    Other keyword arguments are passed to `SuperTrend.run_pipeline`.

>>> st = SuperTrend.run(high, low, close)
>>> st.supert
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2020-01-01 00:00:00+00:00           NaN          NaN
2020-01-01 01:00:00+00:00           NaN          NaN
2020-01-01 02:00:00+00:00           NaN          NaN
...                                 ...          ...
2021-12-31 21:00:00+00:00  47608.346563  3770.258246
2021-12-31 22:00:00+00:00  47608.346563  3770.258246
2021-12-31 23:00:00+00:00  47608.346563  3770.258246

[17513 rows x 2 columns]
```

1. Use [phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp)
to see what arguments are accepted by `SuperTrend.run`.

Notice how our SuperTrend indicator easily accepted two-dimensional Pandas arrays, even though
the underlying function can only process one-dimensional NumPy arrays. Not only did it compute the
SuperTrend on each column, but it also conveniently converted the resulting arrays back into Pandas
format. Now, how does this affect performance?

```pycon
>>> %%timeit
>>> SuperTrend.run(high, low, close)
2 ms ± 130 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

Not much! With all the pre- and post-processing involved, the indicator still needs only about
one millisecond to process one column (that is, 17k data points).

### Expressions

If you feel that calling `vbt.IF(...)` and manually providing `input_names`, `param_names`, and other
information is too much work, VBT has a solution. Our `faster_supertrend_talib`
function acts as a black box to the indicator factory, so the factory cannot inspect it or
automatically obtain the required information. However, it could easily do so if we converted
`faster_supertrend_talib` into an [expression](https://realpython.com/python-eval-function/)!

Expressions are plain strings that can be evaluated as Python code. By passing such a string to
[IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr),
the factory can look inside, parse the specification, and generate a full-featured indicator class.

!!! tip
    Instance methods prefixed with `with` (such as `with_apply_func`) require you to provide the
    specification manually, while class methods prefixed with `from` (such as `from_expr`)
    can parse this information automatically.

Here is an expression for `faster_supertrend_talib`:

```pycon
>>> expr = """
... SuperTrend[st]:
... medprice = @talib_medprice(high, low)
... atr = @talib_atr(high, low, close, @p_period)
... upper, lower = get_basic_bands(medprice, atr, @p_multiplier)
... supert, superd, superl, supers = get_final_bands(close, upper, lower)
... supert, superd, superl, supers
... """
```

By using annotations with `@`, we tell the factory how to treat specific variables. For example,
any variable with the prefix `@talib` gets replaced by the respective TA-Lib function that has
been enhanced for broadcasting and multidimensionality. You can also see that parameters are
annotated with `@p`, while inputs and outputs are left unannotated. The factory can determine
that `high` is the high price and, by the last line, that there are four outputs.

For more examples, see the documentation on expression parsing.

```pycon
>>> SuperTrend = vbt.IF.from_expr(
...     expr, 
...     takes_1d=True,
...     get_basic_bands=get_basic_bands,  # (1)!
...     get_final_bands=get_final_bands_nb,
...     period=7, 
...     multiplier=3
... )

>>> st = SuperTrend.run(high, low, close)
>>> st.supert
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2020-01-01 00:00:00+00:00           NaN          NaN
2020-01-01 01:00:00+00:00           NaN          NaN
2020-01-01 02:00:00+00:00           NaN          NaN
...                                 ...          ...
2021-12-31 21:00:00+00:00  47608.346563  3770.258246
2021-12-31 22:00:00+00:00  47608.346563  3770.258246
2021-12-31 23:00:00+00:00  47608.346563  3770.258246

[17513 rows x 2 columns]

>>> %%timeit
>>> SuperTrend.run(high, low, close)
2.35 ms ± 81.3 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

1. Expressions do not have access to the current global variables to avoid side effects,
so we need to specify the objects that are unknown to the expression.

By the way, this is exactly how WorldQuant's Alphas are implemented in VBT.
Never stop loving Python for the magic it enables :sparkles:

### Plotting

Remember how we previously plotted SuperTrend? We had to manually select the date range
from each output array and add it to the plot by passing the figure around. Let's
subclass `SuperTrend` and define a `plot` method that handles all this for us:

```pycon
>>> class SuperTrend(SuperTrend):
...     def plot(self, 
...              column=None,  # (1)!
...              close_kwargs=None,  # (2)!
...              superl_kwargs=None,
...              supers_kwargs=None,
...              fig=None,  # (3)!
...              **layout_kwargs):  # (4)!
...         close_kwargs = close_kwargs if close_kwargs else {}
...         superl_kwargs = superl_kwargs if superl_kwargs else {}
...         supers_kwargs = supers_kwargs if supers_kwargs else {}
...         
...         close = self.select_col_from_obj(self.close, column).rename('Close')
...         supers = self.select_col_from_obj(self.supers, column).rename('Short')
...         superl = self.select_col_from_obj(self.superl, column).rename('Long')
...         
...         fig = close.vbt.plot(fig=fig, **close_kwargs, **layout_kwargs)  # (5)!
...         supers.vbt.plot(fig=fig, **supers_kwargs)
...         superl.vbt.plot(fig=fig, **superl_kwargs)
...         
...         return fig
```

1. We can plot only one column of data at a time.

2. Keyword arguments with the suffix `kwargs` can be used to configure the trace,
for example, to change the line color.

3. We can pass our own figure.

4. Any additional keyword argument passed to this method can be used to configure
the figure layout.

5. Update the layout only once.

How do we choose the date range to plot? It is simple: the indicator factory made
`SuperTrend` indexable just like any regular Pandas object! Let's plot the same
date range and symbol, but change the color palette slightly:

```pycon
>>> st = SuperTrend.run(high, low, close)
>>> st.loc[date_range, 'BTCUSDT'].plot(
...     superl_kwargs=dict(trace_kwargs=dict(line_color='limegreen')),
...     supers_kwargs=dict(trace_kwargs=dict(line_color='red'))
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/indicator.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/indicator.dark.svg#only-dark){: .iimg loading=lazy }

Beautiful!

## Backtesting

Backtesting is usually the simplest step in VBT: convert the indicator values into
two signal arrays, `entries` and `exits`, and pass them to
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
To make the test more realistic, let's make a few adjustments. Since we calculate SuperTrend
values based on the current close price and VBT executes orders immediately, we will shift
the execution of the signals forward by one tick:

```pycon
>>> entries = (~st.superl.isnull()).vbt.signals.fshift()  # (1)!
>>> exits = (~st.supers.isnull()).vbt.signals.fshift()
```

1. Using [SignalsAccessor.fshift](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.fshift).

We will also apply a commission of 0.1%:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     close=close, 
...     entries=entries, 
...     exits=exits, 
...     fees=0.001, 
...     freq='1h'
... )
```

Now we have a portfolio with two columns that can be analyzed using numerous built-in tools.
For example, let's calculate and display the statistics for the `ETHUSDT` symbol:

```pycon
>>> pf['ETHUSDT'].stats()
Start                         2020-01-01 00:00:00+00:00
End                           2021-12-31 23:00:00+00:00
Period                                729 days 17:00:00
Start Value                                       100.0
Min Value                                     98.469385
Max Value                                   1805.987865
End Value                                   1135.272383
Total Return [%]                            1035.272383
Benchmark Return [%]                        2752.665477
Total Time Exposure [%]                       51.750128
Max Gross Exposure [%]                            100.0
Max Drawdown [%]                               37.39953
Max Drawdown Duration                  85 days 09:00:00
Total Orders                                        348
Total Fees Paid                              272.755758
Total Trades                                        174
Win Rate [%]                                  43.103448
Best Trade [%]                                33.286985
Worst Trade [%]                              -13.783496
Avg Winning Trade [%]                          7.815551
Avg Losing Trade [%]                          -3.021041
Avg Winning Trade Duration              3 days 06:43:12
Avg Losing Trade Duration     1 days 07:54:32.727272727
Profit Factor                                  1.390947
Expectancy                                     5.949841
Sharpe Ratio                                   2.258501
Calmar Ratio                                   6.320363
Omega Ratio                                    1.103525
Sortino Ratio                                   3.27869
Name: ETHUSDT, dtype: object
```

### Optimization

Optimization in VBT can be performed in two ways: iteratively and column-wise.

The first method uses a simple loop that goes through every combination of the strategy's parameters
and runs the entire logic for each. You need to manually generate a suitable parameter grid and
concatenate the results for analysis. On the plus side, you can use [Hyperopt](http://hyperopt.github.io/hyperopt/)
and other tools that operate on a per-iteration basis.

The second method is natively supported by VBT and involves stacking columns. If you have
2 symbols and 5 parameters, VBT will create 10 columns in total: one for each combination
of symbol and parameter. It will backtest each column individually without leaving Numba (this is
one reason most functions in VBT are designed to process two-dimensional data). This approach
not only offers significant performance benefits for small to medium-sized data, but also
enables parallelization with Numba and provides results in a Pandas-friendly format.

Let's test the period values `4, 5, ..., 20`, and the multiplier values `2, 2.1, 2.2, ..., 4`,
which will produce 336 parameter combinations in total. Since our indicator is now parameterized,
we can pass these two parameter arrays directly to the `SuperTrend.run` method and instruct it to
perform the [Cartesian product](https://en.wikipedia.org/wiki/Cartesian_product) using the
`param_product=True` flag:

```pycon
>>> periods = np.arange(4, 20)
>>> multipliers = np.arange(20, 41) / 10  # (1)!

>>> st = SuperTrend.run(
...     high, low, close, 
...     period=periods, 
...     multiplier=multipliers,
...     param_product=True
... )
```

1. Using `np.arange(2, 4.1, 0.1)` produces `3.000000000000001` instead of `3.0`,
which can make indexing harder later.

[=100% "Iteration 672/672"]{: .candystripe .candystripe-animate }

The indicator completed 672 iterations, 336 per symbol. Let's see the columns that have been stacked:

```pycon
>>> st.wrapper.columns
MultiIndex([( 4, 2.0, 'BTCUSDT'),
            ( 4, 2.0, 'ETHUSDT'),
            ( 4, 2.1, 'BTCUSDT'),
            ( 4, 2.1, 'ETHUSDT'),
            ( 4, 2.2, 'BTCUSDT'),
            ...
            (19, 3.8, 'ETHUSDT'),
            (19, 3.9, 'BTCUSDT'),
            (19, 3.9, 'ETHUSDT'),
            (19, 4.0, 'BTCUSDT'),
            (19, 4.0, 'ETHUSDT')],
            names=['st_period', 'st_multiplier', 'symbol'], length=672)
```

Each DataFrame now has 672 columns. Let's plot the latest combination by specifying
the column as a regular tuple:

```pycon
>>> st.loc[date_range, (19, 4, 'ETHUSDT')].plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/optimization.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/optimization.dark.svg#only-dark){: .iimg loading=lazy }

When stacking a large number of columns, make sure you are not running out of RAM.
You can print the size of any pickleable object in VBT using the
[Pickleable.getsize](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.getsize) method:

```pycon
>>> print(st.getsize())
377.6 MB
```

You can manually calculate this as follows (without inputs and parameters):

```pycon
>>> output_size = st.wrapper.shape[0] * st.wrapper.shape[1]
>>> n_outputs = 4
>>> data_type_size = 8
>>> input_size * n_outputs * data_type_size / 1024 / 1024
359.173828125
```

!!! tip
    To reduce the memory footprint, change the `get_final_bands_nb` function to produce the output
    arrays with lower floating point accuracy, such as `np.float32` or even `np.float16`.

The backtesting part remains the same, regardless of the number of columns:

```pycon
>>> entries = (~st.superl.isnull()).vbt.signals.fshift()
>>> exits = (~st.supers.isnull()).vbt.signals.fshift()

>>> pf = vbt.Portfolio.from_signals(
...     close=close, 
...     entries=entries, 
...     exits=exits, 
...     fees=0.001, 
...     freq='1h'
... )
```

Instead of calculating all the statistics for each combination, let's plot a heatmap of their
Sharpe values, with periods laid out horizontally and multipliers vertically.
Since we have an extra column level for symbols, we will make it a slider:

```pycon
>>> pf.sharpe_ratio.vbt.heatmap(
...     x_level='st_period', 
...     y_level='st_multiplier',
...     slider_level='symbol'
... )
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/heatmap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/heatmap.dark.svg#only-dark){: .iimg loading=lazy }

Now we have a clear overview of which parameter regions performed well during
the backtesting period, yay! :partying_face:

!!! tip
    To see how these Sharpe values compare to holding:

    ```pycon
    >>> vbt.Portfolio.from_holding(close, freq='1h').sharpe_ratio
    symbol
    BTCUSDT    1.561447
    ETHUSDT    2.170813
    Name: sharpe_ratio, dtype: float64
    ```

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/superfast-supertrend/index.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/SuperTrend.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/streaming/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/
name: Streaming
type: page
icon: material-lightning-bolt-outline
---

# Streaming

If you look at the implementation of `faster_supertrend`, you will see that it repeatedly iterates
over the entire input data: multiple times in `get_med_price`, several times in `get_atr`, several
times in `get_basic_bands`, and once in `get_final_bands_nb`. This modular computational approach
works when we have access to the full dataset and only need the final result. However, if we work
with a data stream where each new element requires running the whole indicator pipeline just to append
one SuperTrend value, performance will degrade significantly.

VBT solves this issue by using a range of accumulators—Numba-compiled functions that process
the data per time step and generate a single indicator value. An accumulator receives an input state,
which contains all the required information collected so far, and returns an output state containing
the current indicator value along with all variables needed for the next time step. To keep memory
usage low, an input state generally contains only constants, not arrays. Since both input and output
states are simple [named tuples](https://realpython.com/python-namedtuple/), we can store them at any
point and resume later from where we left off.

Consider the accumulator for the rolling mean: [rolling_mean_acc_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_mean_acc_nb).
It takes an input state of type [RollMeanAIS](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.RollMeanAIS)
and returns an output state of type [RollMeanAOS](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.RollMeanAOS).
The essential information needed by this accumulator includes the window length and the sum of the
elements in the window (since the sum divided by the count gives the arithmetic mean).

The function [rolling_mean_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_mean_1d_nb)
implements a simple for-loop that goes through each element, and for every step:

1. Creates the input state and passes it to the accumulator.

2. Receives the output state and updates local variables and the output array.

```mermaid
flowchart TD;
    RollMeanAIS -->|"1."| rolling_mean_acc_nb;
    rolling_mean_acc_nb -->|"2."| RollMeanAOS;
    RollMeanAOS -->|"3."| IndicatorValue;
    RollMeanAOS -.->|"4. (next step)"| RollMeanAIS;
```

(Reload the page if the diagram does not appear.)

Can we turn our SuperTrend function into a [one-pass function](https://en.wikipedia.org/wiki/One-pass_algorithm)?
Yes! Most operations in the SuperTrend calculation—including the median price, TR, basic bands, and
final bands—can easily be performed one element at a time:

```pycon
>>> @njit(nogil=True)  # (1)!
... def get_tr_one_nb(high, low, prev_close):
...     tr0 = abs(high - low)
...     tr1 = abs(high - prev_close)
...     tr2 = abs(low - prev_close)
...     if np.isnan(tr0) or np.isnan(tr1) or np.isnan(tr2):
...         tr = np.nan
...     else:
...         tr = max(tr0, tr1, tr2)
...     return tr

>>> @njit(nogil=True)
... def get_med_price_one_nb(high, low):
...     return (high + low) / 2

>>> @njit(nogil=True)
... def get_basic_bands_one_nb(high, low, atr, multiplier):
...     med_price = get_med_price_one_nb(high, low)
...     matr = multiplier * atr
...     upper = med_price + matr
...     lower = med_price - matr
...     return upper, lower
    
>>> @njit(nogil=True)
... def get_final_bands_one_nb(close, upper, lower, 
...                            prev_upper, prev_lower, prev_dir_):
...     if close > prev_upper:
...         dir_ = 1
...     elif close < prev_lower:
...         dir_ = -1
...     else:
...         dir_ = prev_dir_
...         if dir_ > 0 and lower < prev_lower:
...             lower = prev_lower
...         if dir_ < 0 and upper > prev_upper:
...             upper = prev_upper
... 
...     if dir_ > 0:
...         trend = long = lower
...         short = np.nan
...     else:
...         trend = short = upper
...         long = np.nan
...     return upper, lower, trend, dir_, long, short
```

1. Set `nogil=True` to release the GIL and allow for multithreading later.

Calculating the ATR is slightly more complex since it relies on applying Wilder's EMA to the TR.
Luckily, VBT provides an accumulator for this as well! :star_struck:

First, let's define the input and output states needed for our new accumulator:

```pycon
>>> class SuperTrendAIS(tp.NamedTuple):
...     i: int
...     high: float
...     low: float
...     close: float
...     prev_close: float
...     prev_upper: float
...     prev_lower: float
...     prev_dir_: float
...     nobs: int
...     weighted_avg: float
...     old_wt: float
...     period: int
...     multiplier: float
    
>>> class SuperTrendAOS(tp.NamedTuple):
...     nobs: int
...     weighted_avg: float
...     old_wt: float
...     upper: float
...     lower: float
...     trend: float
...     dir_: float
...     long: float
...     short: float
```

As you can see, all fields in both states are constants. The fields from `i` to `prev_dir_` and
`multiplier` in the input state are used by our own accumulator, while `nobs`, `weighted_avg`,
`old_wt`, and `period` are required by the EMA accumulator—
[ewm_mean_acc_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.ewm_mean_acc_nb).
The output state contains the input fields updated by the accumulator and additional fields that may
be useful to the user.

Now, let's assemble everything by creating a SuperTrend accumulator:

```pycon
>>> @njit(nogil=True)
... def superfast_supertrend_acc_nb(in_state):
...     # (1)!
...     i = in_state.i
...     high = in_state.high
...     low = in_state.low
...     close = in_state.close
...     prev_close = in_state.prev_close
...     prev_upper = in_state.prev_upper
...     prev_lower = in_state.prev_lower
...     prev_dir_ = in_state.prev_dir_
...     nobs = in_state.nobs
...     weighted_avg = in_state.weighted_avg
...     old_wt = in_state.old_wt
...     period = in_state.period
...     multiplier = in_state.multiplier
...     
...     # (2)!
...     tr = get_tr_one_nb(high, low, prev_close)
... 
...     # (3)!
...     alpha = vbt.nb.alpha_from_wilder_nb(period)  # (4)!
...     ewm_mean_in_state = vbt.nb.EWMMeanAIS(
...         i=i,
...         value=tr,
...         old_wt=old_wt,
...         weighted_avg=weighted_avg,
...         nobs=nobs,
...         alpha=alpha,
...         minp=period,
...         adjust=False
...     )
...     ewm_mean_out_state = vbt.nb.ewm_mean_acc_nb(ewm_mean_in_state)
...     atr = ewm_mean_out_state.value
...     
...     # (5)!
...     upper, lower = get_basic_bands_one_nb(high, low, atr, multiplier)
...     
...     # (6)!
...     if i == 0:
...         trend, dir_, long, short = np.nan, 1, np.nan, np.nan
...     else:
...         upper, lower, trend, dir_, long, short = get_final_bands_one_nb(
...             close, upper, lower, prev_upper, prev_lower, prev_dir_)
...             
...     # (7)!
...     return SuperTrendAOS(
...         nobs=ewm_mean_out_state.nobs,
...         weighted_avg=ewm_mean_out_state.weighted_avg,
...         old_wt=ewm_mean_out_state.old_wt,
...         upper=upper,
...         lower=lower,
...         trend=trend,
...         dir_=dir_,
...         long=long,
...         short=short
...     )
```

1. Unpack the input state into local variables.

2. Calculate the current TR value.

3. Calculate the current ATR value using the EMA accumulator.

4. Get the smoothing factor [alpha](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.ewm.html)
from the period according to Wilder.

5. Calculate the current basic SuperTrend bands.

6. Calculate the current final SuperTrend bands.

7. Pack the local variables into an output state.

That's all! We can now use this accumulator in any streaming function.

Let's write a function similar to `faster_supertrend` that computes the SuperTrend values by
iterating over the data **only once**:

```pycon
>>> @njit(nogil=True)
... def superfast_supertrend_nb(high, low, close, period=7, multiplier=3):
...     trend = np.empty(close.shape, dtype=float_)  # (1)!
...     dir_ = np.empty(close.shape, dtype=int_)
...     long = np.empty(close.shape, dtype=float_)
...     short = np.empty(close.shape, dtype=float_)
...     
...     if close.shape[0] == 0:
...         return trend, dir_, long, short
... 
...     # (2)!
...     nobs = 0
...     old_wt = 1.
...     weighted_avg = np.nan
...     prev_upper = np.nan
...     prev_lower = np.nan
... 
...     for i in range(close.shape[0]):  # (3)!
...         in_state = SuperTrendAIS(
...             i=i,
...             high=high[i],
...             low=low[i],
...             close=close[i],
...             prev_close=close[i - 1] if i > 0 else np.nan,
...             prev_upper=prev_upper,
...             prev_lower=prev_lower,
...             prev_dir_=dir_[i - 1] if i > 0 else 1,
...             nobs=nobs,
...             weighted_avg=weighted_avg,
...             old_wt=old_wt,
...             period=period,
...             multiplier=multiplier
...         )
...         
...         out_state = superfast_supertrend_acc_nb(in_state)
...         
...         nobs = out_state.nobs
...         weighted_avg = out_state.weighted_avg
...         old_wt = out_state.old_wt
...         prev_upper = out_state.upper
...         prev_lower = out_state.lower
...         trend[i] = out_state.trend
...         dir_[i] = out_state.dir_
...         long[i] = out_state.long
...         short[i] = out_state.short
...         
...     return trend, dir_, long, short
```

1. No need to use `np.full` since we are overwriting all elements anyway.

2. Initial values for some fields of the input state.

3. Only one for-loop.

After implementing a streaming algorithm, you should always compare its results with a
(semi-)vectorized solution that you trust:

```pycon
>>> superfast_out = superfast_supertrend_nb(
...     high['BTCUSDT'].values,
...     low['BTCUSDT'].values,
...     close['BTCUSDT'].values
... )

>>> faster_out = faster_supertrend(
...     high['BTCUSDT'].values,
...     low['BTCUSDT'].values,
...     close['BTCUSDT'].values
... )

>>> np.testing.assert_allclose(superfast_out[0], faster_out[0])
>>> np.testing.assert_allclose(superfast_out[1], faster_out[1])
>>> np.testing.assert_allclose(superfast_out[2], faster_out[2])
>>> np.testing.assert_allclose(superfast_out[3], faster_out[3])
```

There is nothing quite as satisfying as seeing no errors :wink:

!!! note
    If you compare the results to `faster_supertrend_talib`, the outputs will differ because TA-Lib
    uses a slightly different EMA in its ATR implementation.

What about speed? You can check it yourself:

```pycon
>>> %%timeit
>>> superfast_supertrend_nb(
...     high['BTCUSDT'].values,
...     low['BTCUSDT'].values,
...     close['BTCUSDT'].values
... )
183 µs ± 1.22 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)
```

Our algorithm is 40% faster than `faster_supertrend_talib` and can comfortably compete with similar
implementations in C or other compiled languages. Neat!

!!! tip
    To avoid recompiling every time you start a new Python session, add the `cache=True`
    flag to the `@njit` decorator. Make sure you define the cacheable function in a Python
    file, not in a Jupyter Notebook cell.

We have just reached the performance ceiling—let's move on to parameter optimization.

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/superfast-supertrend/streaming.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/SuperTrend.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/multithreading/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/
name: Multithreading
type: page
icon: material-lightning-bolt-outline
---

# Multithreading

A fully Numba-compiled indicator function provides a major benefit: support for multithreading. But what
exactly is multithreading, and how does it differ from multiprocessing?

Multithreading allows a single process to run multiple threads **concurrently** while sharing the same CPU
and memory. However, due to the [GIL](https://realpython.com/python-gil/) in Python, not all tasks can
take advantage of multithreading. The GIL permits only one thread to execute at a time, even on systems
with multiple CPU cores. This means that another thread can execute only when one thread is idle.

To address the GIL's limitation, the typical solution is multiprocessing, where multiple processes are used
instead of threads. Each Python process has its own interpreter and memory space. However, it is not easy to
share the same array between processes, and processes consume significantly more system resources than
threads. For example, VBT takes 2-3 seconds to import—are you willing to wait that long for every
single process? This extra waiting time can feel like an eternity compared to our superfast streaming
function.

Do not worry, though. Compiled code called by the Python interpreter can release the GIL and run across 
multiple threads at the same time. Libraries such as NumPy and Pandas release the GIL automatically, while
Numba requires the `nogil=True` flag (which we have already set above).

```pycon
>>> SuperTrend = vbt.IF(
...     class_name='SuperTrend',
...     short_name='st',
...     input_names=['high', 'low', 'close'],
...     param_names=['period', 'multiplier'],
...     output_names=['supert', 'superd', 'superl', 'supers']
... ).with_apply_func(
...     superfast_supertrend_nb, 
...     takes_1d=True,
...     period=7, 
...     multiplier=3
... )
```

The indicator factory detects that `superfast_supertrend_nb` is Numba-compiled and dynamically creates
another Numba-compiled function. This new function selects one parameter combination at each time step
and calls our `superfast_supertrend_nb`. By default, it also ensures that this selection function releases
the GIL.

Now, let's benchmark this indicator using 336 parameter combinations per symbol:

```pycon
>>> %%timeit
>>> SuperTrend.run(
...     high, low, close, 
...     period=periods, 
...     multiplier=multipliers,
...     param_product=True,
...     execute_kwargs=dict(show_progress=False)
... )
269 ms ± 72.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

Each iteration takes about 270 / 336 / 2 = 400 microseconds, which is twice as slow as 
`superfast_supertrend_nb` by itself. This slowdown happens because the indicator also needs to concatenate
the generated columns of each output into a single array, which is an expensive step.

Now, let's repeat the same test with multithreading enabled:

```pycon
>>> %%timeit
>>> SuperTrend.run(
...     high, low, close, 
...     period=periods, 
...     multiplier=multipliers,
...     param_product=True,
...     execute_kwargs=dict(
...         engine='dask',  # (1)!
...         chunk_len='auto',  # (2)!
...         show_progress=False  # (3)!
...     )
... )
147 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. Use [Dask](https://dask.org/) as the execution engine.

2. Divide the entire parameter grid into chunks, each matching the number of cores in the system.

3. It is best not to show the progress bar when benchmarking.

This command splits all parameter combinations into chunks. Each chunk contains as many combinations as
there are CPU cores, so that each combination in the chunk can be executed concurrently. The chunks are run
sequentially. This way, we only execute at most `n` combinations at a time, ensuring we do not create more
threads than necessary.

As shown, this approach leads to a 2x speedup.

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/superfast-supertrend/multithreading.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/SuperTrend.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/pipelines/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend/
name: Pipelines
type: page
icon: material-lightning-bolt-outline
---

# Pipelines

Often, we need to consider not just the performance of deployed indicators, but also the overall health
of the backtesting pipeline. An ultrafast indicator is not very helpful if the primary bottleneck is the
portfolio simulator itself.

Let's create a simple pipeline that accepts input data and strategy parameters, and returns the Sharpe
ratio for each symbol and parameter combination:

```pycon
>>> def pipeline(data, period=7, multiplier=3):
...     high = data.get('High')
...     low = data.get('Low')
...     close = data.get('Close')
...     st = SuperTrend.run(
...         high, 
...         low, 
...         close, 
...         period=period, 
...         multiplier=multiplier
...     )
...     entries = (~st.superl.isnull()).vbt.signals.fshift()
...     exits = (~st.supers.isnull()).vbt.signals.fshift()
...     pf = vbt.Portfolio.from_signals(
...         close, 
...         entries=entries, 
...         exits=exits, 
...         fees=0.001,
...         save_returns=True,  # (1)!
...         max_order_records=0,  # (2)!
...         freq='1h'
...     )
...     return pf.sharpe_ratio  # (3)!

>>> pipeline(data)
st_period  st_multiplier  symbol 
7          3              BTCUSDT    1.521221
                          ETHUSDT    2.258501
Name: sharpe_ratio, dtype: float64

>>> %%timeit
>>> pipeline(data)
32.5 ms ± 1.12 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

1. Pre-calculate the returns.

2. Do not generate order records since we only need the returns.

3. [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) can automatically
recognize that the returns were pre-calculated and use them instead of computing them from scratch.

The indicator takes roughly 3 milliseconds for both columns, or about 10% of the total execution time.
The remaining 90% is spent selecting the data, creating `entries` and `exits`, running the simulation,
and calculating the Sharpe ratios.

As you might expect, most of the processing occurs during the simulation: VBT must update
the cash balance, group value, and other metrics at each time step to maintain the trading
environment. After the simulation, it processes the data one or more times to reconstruct the
attributes required for various statistics, such as cash flow, cash, asset flow, assets, asset value,
portfolio value, and finally, returns. If all this information were stored during the simulation, memory
would be exhausted quickly. Fortunately, we can avoid this reconstruction phase by pre-computing
the returns, as shown above.

Now, what do you think the execution time would be if you ran the same pipeline 336 times?
Would you guess 10 seconds?

```pycon
>>> op_tree = (product, periods, multipliers)
>>> period_product, multiplier_product = vbt.generate_param_combs(op_tree)  # (1)!
>>> period_product = np.asarray(period_product)
>>> multiplier_product = np.asarray(multiplier_product)

>>> %%timeit
>>> pipeline(data, period_product, multiplier_product)
2.38 s ± 142 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. Using [generate_param_combs](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.generate_param_combs).

That's only 1/5 of your estimate. This is because VBT can process wide arrays efficiently.
However, as you begin testing thousands of parameter combinations, performance will start to drop.
Generally, stacking many columns at once consumes much more memory than looping through them,
and once your system uses all available memory, it may switch to swap memory, which is much slower
than RAM. How do we solve this?

### Chunked pipeline

To avoid memory issues, let's make our pipeline chunkable. Chunking in VBT allows you to split
arguments (in this case, parameter combinations) so that only one group of argument values is passed
to the pipeline function at a time. This is done using the
[chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked) decorator:

```pycon
>>> chunked_pipeline = vbt.chunked(
...     size=vbt.LenSizer(arg_query='period', single_type=int),  # (1)!
...     arg_take_spec=dict(  # (2)!
...         data=None,  # (3)!
...         period=vbt.ChunkSlicer(),  # (4)!
...         multiplier=vbt.ChunkSlicer()
...     ),
...     merge_func=lambda x: pd.concat(x).sort_index()  # (5)!
... )(pipeline)
```

1. Get the total number of values to be chunked by measuring the length of `period`
using [LenSizer](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.LenSizer). If `period`
is an integer, it is treated as a single value.

2. Specification for splitting any argument passed to the `pipeline` function.

3. Pass the data as-is (without splitting).

4. Slice the values of each parameter into chunks using
[ChunkSlicer](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ChunkSlicer).

5. Each chunk returns a Series with Sharpe values. Concatenate the list of Series returned by
all chunks using Pandas and sort the resulting Series.

The returned `chunked_pipeline` function has the same signature (accepted arguments and their order)
as the original `pipeline`, but now it can internally split all arguments thanks to the chunking
specification we provided. It measures the number of elements in `period` and, by default, creates
the same number of chunks as there are CPU cores. Each chunk receives the same input data as `data`
(only a reference, not a copy!), and a slice of the values in `period` and `multiplier`. After
processing all chunks, it merges their results using Pandas, giving you the same output as if all
parameter combinations were processed at once. Impressive, right?

Let's test the chunked pipeline on a single combination:

```pycon
st_period  st_multiplier  symbol 
7          3              BTCUSDT    1.521221
                          ETHUSDT    2.258501
Name: sharpe_ratio, dtype: float64
```

We get the same results as with `pipeline`, which is expected.
How about multiple combinations? Let's run `chunked_pipeline` on 4 combinations,
split into 2 chunks, and also display the progress bar:

```pycon
>>> chunked_pipeline(
...     data, 
...     period_product[:4], 
...     multiplier_product[:4],
...     _n_chunks=2,  # (1)!
... )
```

1. Any argument passed to the [chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked) decorator
can be overridden at runtime by using the same argument name prefixed with an underscore `_`.

[=100% "Chunk 2/2"]{: .candystripe .candystripe-animate }

```pycon
st_period  st_multiplier  symbol 
4          2.0            BTCUSDT    0.451699
                          ETHUSDT    1.391032
            2.1            BTCUSDT    0.495387
                          ETHUSDT    1.134741
            2.2            BTCUSDT    0.985946
                          ETHUSDT    0.955616
            2.3            BTCUSDT    1.193179
                          ETHUSDT    1.307505
Name: sharpe_ratio, dtype: float64
```

How can we check if the arguments were split correctly?

```pycon
>>> chunk_meta, tasks = chunked_pipeline(
...     data, 
...     period_product[:4], 
...     multiplier_product[:4],
...     _n_chunks=2,
...     _return_raw_chunks=True
... )

>>> chunk_meta  # (1)!
[ChunkMeta(uuid='0882b000-52ab-4694-bb7c-341a9370937b', idx=0, start=0, end=2, indices=None),
  ChunkMeta(uuid='1d5a74d9-d517-437d-a20a-4580f601a280', idx=1, start=2, end=4, indices=None)]

>>> list(tasks)  # (2)!
[(<function __main__.pipeline(data, period=7, multiplier=3)>,
  (<vectorbtpro.data.custom.hdf.HDFData at 0x7f7b30509a60>,
    array([4, 4]),
    array([2., 2.1])),
  {}),
  (<function __main__.pipeline(data, period=7, multiplier=3)>,
  (<vectorbtpro.data.custom.hdf.HDFData at 0x7f7b30509a60>,
    array([4, 4]),
    array([2.2, 2.3])),
  {})]
```

1. Generated chunks of type [ChunkMeta](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ChunkMeta).
Arguments `period` and `multiplier` were sliced based on `start` and `end`.

2. Each chunk receives the same data but a different set of parameter combinations.

The first chunk contains the combinations `(4, 2.0)` and `(4, 2.1)`. The second chunk contains
the combinations `(4, 2.2)` and `(4, 2.3)`.

Here is how long it takes to run all parameter combinations:

```pycon
>>> %%timeit
>>> chunked_pipeline(data, period_product, multiplier_product)
2.33 s ± 50.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

We do not observe any performance increase because there is no multiprocessing or multithreading
occurring, but chunking is mainly about memory savings and maintaining memory health.
However, do not overdo it! Looping over all parameter combinations and processing just one at a time
is much slower because VBT cannot leverage multidimensionality:

```pycon
>>> %%timeit
>>> chunked_pipeline(data, period_product, multiplier_product, _chunk_len=1)
11.4 s ± 965 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

What is best? Usually, something in between. The default values are generally sufficient.

## Numba pipeline

Wouldn't it be great if we could parallelize our pipeline just like we did with the indicator?
Unfortunately, our Python code cannot achieve concurrency because it holds the GIL. But remember,
Numba can release the GIL to enable multithreading! If only we could write the entire pipeline in
Numba... Let's do it!

```pycon
>>> @njit(nogil=True)
... def pipeline_nb(high, low, close, 
...                 periods=np.asarray([7]),  # (1)!
...                 multipliers=np.asarray([3]), 
...                 ann_factor=365):
...
...     # (2)!
...     sharpe = np.empty(periods.size * close.shape[1], dtype=float_)
...     long_entries = np.empty(close.shape, dtype=np.bool_)
...     long_exits = np.empty(close.shape, dtype=np.bool_)
...     group_lens = np.full(close.shape[1], 1)
...     init_cash = 100.
...     fees = 0.001
...     k = 0
...     
...     for i in range(periods.size):
...         for col in range(close.shape[1]):
...             _, _, superl, supers = superfast_supertrend_nb(  # (3)!
...                 high[:, col], 
...                 low[:, col], 
...                 close[:, col], 
...                 periods[i], 
...                 multipliers[i]
...             )
...             long_entries[:, col] = vbt.nb.fshift_1d_nb(  # (4)!
...                 ~np.isnan(superl), 
...                 fill_value=False
...             )
...             long_exits[:, col] = vbt.nb.fshift_1d_nb(
...                 ~np.isnan(supers), 
...                 fill_value=False
...             )
...             
...         sim_out = vbt.pf_nb.from_signals_nb(  # (5)!
...             target_shape=close.shape,
...             group_lens=group_lens,
...             init_cash=init_cash,
...             high=high,
...             low=low,
...             close=close,
...             long_entries=long_entries,
...             long_exits=long_exits,
...             fees=fees,
...             save_returns=True
...         )
...         returns = sim_out.in_outputs.returns
...         _sharpe = vbt.ret_nb.sharpe_ratio_nb(returns, ann_factor, ddof=1)  # (6)!
...         sharpe[k:k + close.shape[1]] = _sharpe  # (7)!
...         k += close.shape[1]
...         
...     return sharpe
```

1. When using Numba, arguments cannot be both scalars and arrays.
You need to select one data type, so we switch to using arrays only.

2. Since many arrays have fixed shapes, create them outside the loop.

3. Run `superfast_supertrend_nb` on each column.

4. Shift the signals one tick forward.

5. Use [from_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signals_nb).

6. Use [sharpe_ratio_nb](https://vectorbt.pro/pvt_41531c19/api/returns/nb/#vectorbtpro.returns.nb.sharpe_ratio_nb).

7. The output array `sharpe` stores one element per column and parameter combination.

Running this pipeline on one parameter combination yields two familiar Sharpe values,
one for each column in `high`, `low`, and `close`:

```pycon
>>> ann_factor = vbt.pd_acc.returns.get_ann_factor(freq='1h')  # (1)!
>>> pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     ann_factor=ann_factor
... )
array([1.521221, 2.25850084])

>>> %%timeit
>>> pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     ann_factor=ann_factor
... )
3.13 ms ± 544 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

1. Get the annualization factor using [ReturnsAccessor.get_ann_factor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.get_ann_factor).

One iteration of `pipeline_nb` is already 10x faster than one iteration of `pipeline`.

The next step is to create a chunked pipeline. Since the returned values are no longer Pandas Series,
we cannot simply join them and expect everything to work. We need to manually concatenate them and
build a multi-level index for further analysis.

```pycon
>>> def merge_func(arrs, ann_args, input_columns):  # (1)!
...     arr = np.concatenate(arrs)
...     param_index = vbt.stack_indexes((  # (2)!
...         pd.Index(ann_args['periods']['value'], name='st_period'),
...         pd.Index(ann_args['multipliers']['value'], name='st_multiplier')
...     ))
...     index = vbt.combine_indexes((  # (3)!
...         param_index,
...         input_columns
...     ))
...     return pd.Series(arr, index=index)  # (4)!

>>> nb_chunked = vbt.chunked(
...     size=vbt.ArraySizer(arg_query='periods', axis=0),  # (5)!
...     arg_take_spec=dict(
...         high=None,  # (6)!
...         low=None,
...         close=None,
...         periods=vbt.ArraySlicer(axis=0),
...         multipliers=vbt.ArraySlicer(axis=0),
...         ann_factor=None
...     ),
...     merge_func=merge_func,
...     merge_kwargs=dict(
...         ann_args=vbt.Rep("ann_args")
...     )
... )
>>> chunked_pipeline_nb = nb_chunked(pipeline_nb)
```

1. The merging function takes a list of one-dimensional NumPy arrays returned by the pipeline,
one per chunk. With `merge_kwargs`, we also request the annotated arguments to access the passed
parameter values. Additionally, to combine parameter labels with column labels, we need
the column labels specified at each run.

2. Stack the parameter labels since all parameter values at the same position form a
parameter combination.

3. Build the product of the parameter labels and symbol labels, since each parameter combination
is applied to each symbol.

4. Return the final array as a Series.

5. We do not provide a `single_type` argument anymore to accept only arrays.

6. We need to pass three two-dimensional NumPy arrays instead of `data`, which has a complex
Python type not recognized by Numba.

Let's test the pipeline on four parameter combinations, as we did with the previous pipeline:

```pycon
>>> chunked_pipeline_nb(
...     high.values, 
...     low.values,
...     close.values,
...     periods=period_product[:4], 
...     multipliers=multiplier_product[:4],
...     ann_factor=ann_factor,
...     _n_chunks=2,
...     _merge_kwargs=dict(input_columns=close.columns)
... )
```

[=100% "Chunk 2/2"]{: .candystripe .candystripe-animate }

```pycon
st_period  st_multiplier  symbol 
4          2.0            BTCUSDT    0.451699
                          ETHUSDT    1.391032
            2.1            BTCUSDT    0.495387
                          ETHUSDT    1.134741
            2.2            BTCUSDT    0.985946
                          ETHUSDT    0.955616
            2.3            BTCUSDT    1.193179
                          ETHUSDT    1.307505
dtype: float64
```

We instantly recognize the values produced by the previous pipeline. Additionally, if you run the
code, you will notice that `chunked_pipeline_nb` averages 50 iterations per second,
compared to 15 per second with `chunked_pipeline`—a remarkable performance jump. But there is more:
let's benchmark this pipeline without and with parallelization enabled.

```pycon
>>> %%timeit
>>> chunked_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _merge_kwargs=dict(input_columns=close.columns)
... )
894 ms ± 14.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %%timeit
>>> chunked_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _execute_kwargs=dict(engine="dask"),
...     _merge_kwargs=dict(input_columns=close.columns)
... )
217 ms ± 4.67 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

We just processed 12 million data points in 217 milliseconds, and that is in Python! :sunglasses:

## Contextualized pipeline

Performance is not always the only concern. Imagine a scenario where we need to know the Sharpe
ratio at every single time step to guide our trading decisions. This is not possible with the pipelines
we wrote above, because we have introduced a path dependency—the current Sharpe now directly depends
on the previous Sharpe.

For such cases, VBT introduces the concept of a custom order function—a regular callback that can
be executed at any point during the simulation. It receives the surrounding simulation context, such as
the current cash balance, and decides whether to issue an order. Order functions are not the only
callbacks in VBT: there is a wide range of callbacks that can be called at specific checkpoints
during runtime. Using these callbacks usually comes with a noticeable performance hit (though still very
fast), but they enable event-driven backtesting in a package otherwise focused on array operations.

To maximize simulation speed, we will calculate both the SuperTrend and the Sharpe ratio in a single
pass. This way, we will not only know both values at each time step, but also maintain full control over
how they are calculated.

### Streaming Sharpe

We have already created a streaming SuperTrend indicator, but what about Sharpe? Turning Sharpe ratio
calculation into a one-pass algorithm involves building an accumulator, which is fairly straightforward
since the Sharpe ratio mainly depends on the rolling mean and standard deviation. We can create an
accumulator by simply combining two other accumulators:
[rolling_mean_acc_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_mean_acc_nb)
and [rolling_std_acc_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_std_acc_nb).
If you followed along with the streaming SuperTrend implementation, you should have no trouble
understanding the code below:

```pycon
>>> class RollSharpeAIS(tp.NamedTuple):
...     i: int
...     ret: float
...     pre_window_ret: float
...     cumsum: float
...     cumsum_sq: float
...     nancnt: int
...     window: int
...     minp: tp.Optional[int]
...     ddof: int
...     ann_factor: float
    
>>> class RollSharpeAOS(tp.NamedTuple):
...     cumsum: float
...     cumsum_sq: float
...     nancnt: int
...     value: float

>>> @njit(nogil=True)
... def rolling_sharpe_acc_nb(in_state):
...     # (1)!
...     mean_in_state = vbt.nb.RollMeanAIS(
...         i=in_state.i,
...         value=in_state.ret,
...         pre_window_value=in_state.pre_window_ret,
...         cumsum=in_state.cumsum,
...         nancnt=in_state.nancnt,
...         window=in_state.window,
...         minp=in_state.minp
...     )
...     mean_out_state = vbt.nb.rolling_mean_acc_nb(mean_in_state)
...     
...     # (2)!
...     std_in_state = vbt.nb.RollStdAIS(
...         i=in_state.i,
...         value=in_state.ret,
...         pre_window_value=in_state.pre_window_ret,
...         cumsum=in_state.cumsum,
...         cumsum_sq=in_state.cumsum_sq,
...         nancnt=in_state.nancnt,
...         window=in_state.window,
...         minp=in_state.minp,
...         ddof=in_state.ddof
...     )
...     std_out_state = vbt.nb.rolling_std_acc_nb(std_in_state)
...     
...     # (3)!
...     mean = mean_out_state.value
...     std = std_out_state.value
...     if std == 0:
...         sharpe = np.nan
...     else:
...         sharpe = mean / std * np.sqrt(in_state.ann_factor)
...
...     # (4)!
...     return RollSharpeAOS(
...         cumsum=std_out_state.cumsum,
...         cumsum_sq=std_out_state.cumsum_sq,
...         nancnt=std_out_state.nancnt,
...         value=sharpe
...     )
```

1. Calculate the mean of the window.

2. Calculate the standard deviation of the window.

3. Calculate the Sharpe ratio of the window.

4. Return the output state to be used in the next iteration.

To ensure the calculation above is correct, let's create a simple function
`rolling_sharpe_ratio_nb` that computes the rolling Sharpe ratio using our accumulator. We will compare
its output to another function with a completely different implementation:
[ReturnsAccessor.rolling_sharpe_ratio](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.rolling_sharpe_ratio).

```pycon
>>> @njit(nogil=True)
... def rolling_sharpe_ratio_nb(returns, window, minp=None, ddof=0, ann_factor=365):
...     if window is None:
...         window = returns.shape[0]  # (1)!
...     if minp is None:
...         minp = window  # (2)!
...     out = np.empty(returns.shape, dtype=float_)
...     
...     if returns.shape[0] == 0:
...         return out
... 
...     cumsum = 0.
...     cumsum_sq = 0.
...     nancnt = 0
... 
...     for i in range(returns.shape[0]):
...         in_state = RollSharpeAIS(
...             i=i,
...             ret=returns[i],
...             pre_window_ret=returns[i - window] if i - window >= 0 else np.nan,
...             cumsum=cumsum,
...             cumsum_sq=cumsum_sq,
...             nancnt=nancnt,
...             window=window,
...             minp=minp,
...             ddof=ddof,
...             ann_factor=ann_factor
...         )
...         
...         out_state = rolling_sharpe_acc_nb(in_state)
...         
...         cumsum = out_state.cumsum
...         cumsum_sq = out_state.cumsum_sq
...         nancnt = out_state.nancnt
...         out[i] = out_state.value
...         
...     return out

>>> ann_factor = vbt.pd_acc.returns.get_ann_factor(freq='1h')  # (3)!

>>> returns = close['BTCUSDT'].vbt.to_returns()  # (4)!

>>> np.testing.assert_allclose(
...     rolling_sharpe_ratio_nb(
...         returns=returns.values, 
...         window=10, 
...         ddof=1,  # (5)!
...         ann_factor=ann_factor),
...     returns.vbt.returns(freq='1h').rolling_sharpe_ratio(10).values
... )
```

1. A window size of `None` makes the window expanding.

2. A minimum number of observations of `None` defaults to the window size.

3. Get the annualization factor using [ReturnsAccessor.get_ann_factor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.get_ann_factor).

4. Calculate the returns using [GenericAccessor.to_returns](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.to_returns).

5. The default `ddof` is `1` in accessors (like Pandas) and `0` in Numba-compiled functions (like NumPy).

Everything checks out—both functions return identical arrays!

### Callbacks

In a contextualized simulation using [from_order_func_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_order_func/#vectorbtpro.portfolio.nb.from_order_func.from_order_func_nb),
we can use several callbacks to define our logic. The simulator receives a shape
`target_shape` and iterates over the columns and rows of this shape in a specific way. Think of
this shape as a two-dimensional array where columns represent assets (denoted as `col`) and rows
represent time steps (denoted as `i`). For each element in this shape, we call an order function.
This is similar to live trading: 1 trade on BTC and 0 trades on ETH yesterday, 0 trades on BTC and
0 trades on ETH today, and so on.

| Date               | BTCUSDT (`col=0`) | ETHUSDT (`col=1`) |
|--------------------|-------------------|-------------------|
| 2020-01-01 (`i=0`) | 1                 | 0                 |
| 2020-01-02 (`i=1`) | 0                 | 1                 |
| ...                | ...               | ...               |
| today (`i=n`)      | 1                 | 1                 |

Since the simulator now works on multiple columns, any information we need to manage for the
streaming SuperTrend, such as `cumsum` and `cumsum_sq`, should be maintained per column. That means
we are moving from scalars to one-dimensional arrays. Why use arrays and not lists, dicts, or tuples?
Arrays are faster than lists and dicts, they can be modified (unlike tuples), and they are native
data structures in Numba, making them ideal to hold and update data.

In traditional backtesting, we typically store our variables, such as arrays, on the instance we are
working with. But in a VBT simulation, we do not have classes and instances (Numba does support
jitted classes, but these are too heavy-weight). The only way to pass any information is by having
a callback return it as a tuple, which can then be consumed by other callbacks down the execution
stack. As you can imagine, managing large tuples is not very intuitive. The best solution is to
create a named tuple, which acts as a container (also called "memory") that works seamlessly with Numba.
We can then conveniently access any array by its name.

Where do we define this memory? At the start of each simulation, the simulator calls the
`pre_sim_func_nb` callback, which is simply a regular preprocessing function invoked before the main
simulation. Whatever this function returns is passed to other callbacks. Sounds like the perfect place,
right?

```pycon
>>> class Memory(tp.NamedTuple):  # (1)!
...     nobs: tp.Array1d
...     old_wt: tp.Array1d
...     weighted_avg: tp.Array1d
...     prev_upper: tp.Array1d
...     prev_lower: tp.Array1d
...     prev_dir_: tp.Array1d
...     cumsum: tp.Array1d
...     cumsum_sq: tp.Array1d
...     nancnt: tp.Array1d
...     was_entry: tp.Array1d
...     was_exit: tp.Array1d

>>> @njit(nogil=True)
... def pre_sim_func_nb(c):
...     memory = Memory(  # (2)!
...         nobs=np.full(c.target_shape[1], 0, dtype=int_),
...         old_wt=np.full(c.target_shape[1], 1., dtype=float_),
...         weighted_avg=np.full(c.target_shape[1], np.nan, dtype=float_),
...         prev_upper=np.full(c.target_shape[1], np.nan, dtype=float_),
...         prev_lower=np.full(c.target_shape[1], np.nan, dtype=float_),
...         prev_dir_=np.full(c.target_shape[1], np.nan, dtype=float_),
...         cumsum=np.full(c.target_shape[1], 0., dtype=float_),
...         cumsum_sq=np.full(c.target_shape[1], 0., dtype=float_),
...         nancnt=np.full(c.target_shape[1], 0, dtype=int_),
...         was_entry=np.full(c.target_shape[1], False, dtype=np.bool_),
...         was_exit=np.full(c.target_shape[1], False, dtype=np.bool_)
...     )
...     return (memory,)
```

1. Container for our variables.

2. Initialize variables and place them into the container. Notice that all variables have
the same length as the number of columns—`target_shape[1]`.

The memory returned by the simulation pre-processing function is automatically prepended to
the arguments of every other callback, unless callbacks higher in the call chain choose
not to and limit memory access. Now, let's write the main part of our simulation—
the order function. This function takes the current context, the memory, and the parameter values
set by the user, computes the current SuperTrend values, and uses them to decide whether
to enter or exit a position. This signal is stored in memory and is executed only at the next
time step (as intended):

```pycon
>>> @njit(nogil=True)
... def order_func_nb(c, memory, period, multiplier):
...     # (1)!
...     is_entry = memory.was_entry[c.col]
...     is_exit = memory.was_exit[c.col]
...
...     # (2)!
...     in_state = SuperTrendAIS(
...         i=c.i,
...         high=c.high[c.i, c.col],
...         low=c.low[c.i, c.col],
...         close=c.close[c.i, c.col],
...         prev_close=c.close[c.i - 1, c.col] if c.i > 0 else np.nan,
...         prev_upper=memory.prev_upper[c.col],
...         prev_lower=memory.prev_lower[c.col],
...         prev_dir_=memory.prev_dir_[c.col],
...         nobs=memory.nobs[c.col],
...         weighted_avg=memory.weighted_avg[c.col],
...         old_wt=memory.old_wt[c.col],
...         period=period,
...         multiplier=multiplier
...     )
... 
...     # (3)!
...     out_state = superfast_supertrend_acc_nb(in_state)
... 
...     # (4)!
...     memory.nobs[c.col] = out_state.nobs
...     memory.weighted_avg[c.col] = out_state.weighted_avg
...     memory.old_wt[c.col] = out_state.old_wt
...     memory.prev_upper[c.col] = out_state.upper
...     memory.prev_lower[c.col] = out_state.lower
...     memory.prev_dir_[c.col] = out_state.dir_
...     memory.was_entry[c.col] = not np.isnan(out_state.long)
...     memory.was_exit[c.col] = not np.isnan(out_state.short)
...     
...     # (5)!
...     in_position = c.position_now > 0
...     if is_entry and not in_position:
...         size = np.inf
...     elif is_exit and in_position:
...         size = -np.inf
...     else:
...         size = np.nan
...     return vbt.pf_nb.order_nb(
...         size=size, 
...         direction=vbt.pf_enums.Direction.LongOnly,
...         fees=0.001
...     )
```

1. Access the signals generated at the previous time step (shifting).

2. Build the input state for `superfast_supertrend_acc_nb` from the context and memory.

3. Run the SuperTrend accumulator for this step.

4. Write the returned state back into memory.

5. Decide whether to buy (`np.inf`), sell (`-np.inf`), or do nothing (`np.nan`).

!!! tip
    If the execution time needs to be shifted by more than one tick, consider creating a complete
    array for the `long` and `short` values returned by `superfast_supertrend_acc_nb`, and access
    any previous element in those arrays to generate a signal.

    If an order decision (such as `is_entry`) is based on a value from an array (such as
    `memory.was_entry`), temporarily store the element in a constant variable—Numba prefers it.

The last callback is the segment post-processing function. A segment is a group of columns
at a single time step, primarily used for managing orders of assets that share the same capital or are linked
in other ways. Since our portfolio is not grouped, each column (`BTCUSDT` and `ETHUSDT`) forms its
own group, so each segment contains just one column. After all columns in a segment are processed,
the simulator updates the current group value and the return. The return is then used by our callback
to calculate the Sharpe ratio:

```pycon
>>> @njit(nogil=True)
... def post_segment_func_nb(c, memory, ann_factor):
...     for col in range(c.from_col, c.to_col):  # (1)!
...         in_state = RollSharpeAIS(
...             i=c.i,
...             ret=c.last_return[col],  # (2)!
...             pre_window_ret=np.nan,
...             cumsum=memory.cumsum[col],
...             cumsum_sq=memory.cumsum_sq[col],
...             nancnt=memory.nancnt[col],
...             window=c.i + 1,  # (3)!
...             minp=0,
...             ddof=1,
...             ann_factor=ann_factor
...         )
...         out_state = rolling_sharpe_acc_nb(in_state)
...         memory.cumsum[col] = out_state.cumsum
...         memory.cumsum_sq[col] = out_state.cumsum_sq
...         memory.nancnt[col] = out_state.nancnt
...         c.in_outputs.sharpe[col] = out_state.value  # (4)!
```

1. Iterate over all columns in the current segment. In our case, each segment contains just one column.

2. Get the return effective at the end of the current time step.

3. Use an expanding window.

4. Write the Sharpe value to the in-place output array defined below.

Here is a short illustration of how the functions call each other:

```mermaid
flowchart TD;
    id1["pre_sim_func_nb"]
    
    id1 -->|"memory"| id2;
    id1 -->|"memory"| id8;
    
    subgraph " "
    id2["order_func_nb"]
    id3["superfast_supertrend_acc_nb"]
    id4["SuperTrend"]
    id5["Signal"]
    id6["Order"]
    id2 -->|"calls"| id3;
    id3 -->|"calculates"| id4;
    id4 -->|"generates"| id5;
    id5 -->|"issues"| id6;
    
    id7["Return"]
    id6 -->|"updates"| id7;
    id7 -->|"consumed by"| id8;
    
    id8["post_segment_func_nb"]
    id9["rolling_sharpe_acc_nb"]
    id10["Sharpe"]
    id8 -->|"calls"| id9;
    id9 -->|"calculates"| id10;
    end
```

(Reload the page if the diagram does not appear.)

!!! info
    The operation flow inside the rectangle is executed at each time step.

### Pipeline

Let's put all the parts together and define our super-flexible pipeline:

```pycon
>>> class InOutputs(tp.NamedTuple):  # (1)!
...     sharpe: tp.Array1d

>>> @njit(nogil=True)
... def ctx_pipeline_nb(high, low, close, 
...                     periods=np.asarray([7]), 
...                     multipliers=np.asarray([3]), 
...                     ann_factor=365):
...
...     in_outputs = InOutputs(sharpe=np.empty(close.shape[1], dtype=float_))
...     sharpe = np.empty(periods.size * close.shape[1], dtype=float_)
...     group_lens = np.full(close.shape[1], 1)
...     init_cash = 100.
...     k = 0
...     
...     for i in range(periods.size):
...         sim_out = vbt.pf_nb.from_order_func_nb(
...             target_shape=close.shape,
...             group_lens=group_lens,
...             cash_sharing=False,
...             init_cash=init_cash,
...             pre_sim_func_nb=pre_sim_func_nb,
...             order_func_nb=order_func_nb,
...             order_args=(periods[i], multipliers[i]),
...             post_segment_func_nb=post_segment_func_nb,
...             post_segment_args=(ann_factor,),
...             high=high,
...             low=low,
...             close=close,
...             in_outputs=in_outputs,
...             fill_pos_info=False,  # (2)!
...             max_order_records=0  # (3)!
...         )
...         sharpe[k:k + close.shape[1]] = in_outputs.sharpe
...         k += close.shape[1]
...         
...     return sharpe
```

1. A named tuple containing any array you want to write during the simulation and access 
afterwards.

2. Disables filling position records to speed up the simulation.

3. Disables filling order records to speed up the simulation.

This function has the same signature as `pipeline_nb` and, conveniently, produces the same results!

```pycon
>>> ctx_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     ann_factor=ann_factor
... )
array([1.521221, 2.25850084])

>>> chunked_ctx_pipeline_nb = nb_chunked(ctx_pipeline_nb)
>>> chunked_ctx_pipeline_nb(
...     high.values, 
...     low.values,
...     close.values,
...     periods=period_product[:4], 
...     multipliers=multiplier_product[:4],
...     ann_factor=ann_factor,
...     _n_chunks=2,
...     _merge_kwargs=dict(input_columns=close.columns)
... )
```

[=100% "Chunk 2/2"]{: .candystripe .candystripe-animate }

```pycon
st_period  st_multiplier  symbol 
4          2.0            BTCUSDT    0.451699
                          ETHUSDT    1.391032
            2.1            BTCUSDT    0.495387
                          ETHUSDT    1.134741
            2.2            BTCUSDT    0.985946
                          ETHUSDT    0.955616
            2.3            BTCUSDT    1.193179
                          ETHUSDT    1.307505
dtype: float64
```

However, compared to the previous pipeline, this one is several times slower:

```pycon
>>> %%timeit
>>> chunked_ctx_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _merge_kwargs=dict(input_columns=close.columns)
... )
6.4 s ± 45.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %%timeit
>>> chunked_ctx_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _execute_kwargs=dict(engine="dask"),
...     _merge_kwargs=dict(input_columns=close.columns)
... )
1.38 s ± 26.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

We sacrificed a bit of performance for complete flexibility. But even with this setup, event-driven 
backtesting of a **full grid** of parameter combinations with VBT is as fast as a **single**
SuperTrend calculation with Pandas :sweat_smile:

## Bonus: Own simulator

If your top priority is maximum performance, you have a perfect streaming algorithm, and you know 
exactly how to compute your desired metrics on the fly, you can skip simulation with the 
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) class entirely. Instead, 
define the core logic using a set of primitive, pure, and ultra-fast Numba-compiled for-loops. With 
this approach, you can directly use VBT's core features for handling order execution:

```pycon
>>> @njit(nogil=True)
... def raw_pipeline_nb(high, low, close, 
...                     periods=np.array([7]), 
...                     multipliers=np.array([3]), 
...                     ann_factor=365):
...     out = np.empty(periods.size * close.shape[1], dtype=float_)  # (1)!
...     
...     if close.shape[0] == 0:
...         return out
... 
...     for k in range(len(periods)):  # (2)!
...         
...         for col in range(close.shape[1]):  # (3)!
...             # (4)!
...             nobs = 0
...             old_wt = 1.
...             weighted_avg = np.nan
...             prev_close_ = np.nan
...             prev_upper = np.nan
...             prev_lower = np.nan
...             prev_dir_ = 1
...             cumsum = 0.
...             cumsum_sq = 0.
...             nancnt = 0
...             was_entry = False
...             was_exit = False
... 
...             # (5)!
...             init_cash = 100.
...             cash = init_cash
...             position = 0.
...             debt = 0.
...             locked_cash = 0.
...             free_cash = init_cash
...             val_price = np.nan
...             value = init_cash
...             prev_value = init_cash
...             return_ = 0.
... 
...             for i in range(close.shape[0]):  # (6)!
...                 # (7)!
...                 is_entry = was_entry
...                 is_exit = was_exit
... 
...                 st_in_state = SuperTrendAIS(
...                     i=i,
...                     high=high[i, col],
...                     low=low[i, col],
...                     close=close[i, col],
...                     prev_close=prev_close_,
...                     prev_upper=prev_upper,
...                     prev_lower=prev_lower,
...                     prev_dir_=prev_dir_,
...                     nobs=nobs,
...                     weighted_avg=weighted_avg,
...                     old_wt=old_wt,
...                     period=periods[k],
...                     multiplier=multipliers[k]
...                 )
... 
...                 st_out_state = superfast_supertrend_acc_nb(st_in_state)
... 
...                 nobs = st_out_state.nobs
...                 weighted_avg = st_out_state.weighted_avg
...                 old_wt = st_out_state.old_wt
...                 prev_close_ = close[i, col]
...                 prev_upper = st_out_state.upper
...                 prev_lower = st_out_state.lower
...                 prev_dir_ = st_out_state.dir_
...                 was_entry = not np.isnan(st_out_state.long)
...                 was_exit = not np.isnan(st_out_state.short)
... 
...                 if is_entry and position == 0:
...                     size = np.inf
...                 elif is_exit and position > 0:
...                     size = -np.inf
...                 else:
...                     size = np.nan
... 
...                 # (8)!
...                 val_price = close[i, col]
...                 value = cash + position * val_price
...                 if not np.isnan(size):
...                     exec_state = vbt.pf_enums.ExecState(  # (9)!
...                         cash=cash,
...                         position=position,
...                         debt=debt,
...                         locked_cash=locked_cash,
...                         free_cash=free_cash,
...                         val_price=val_price,
...                         value=value
...                     )
...                     price_area = vbt.pf_enums.PriceArea(  # (10)!
...                         open=np.nan,
...                         high=high[i, col],
...                         low=low[i, col],
...                         close=close[i, col]
...                     )
...                     order = vbt.pf_nb.order_nb(  # (11)!
...                         size=size, 
...                         direction=vbt.pf_enums.Direction.LongOnly,
...                         fees=0.001
...                     )
...                     _, new_exec_state = vbt.pf_nb.execute_order_nb(  # (12)!
...                         exec_state, order, price_area)
...                     cash, position, debt, locked_cash, free_cash, val_price, value = new_exec_state
... 
...                 value = cash + position * val_price
...                 return_ = vbt.ret_nb.get_return_nb(prev_value, value)  # (13)!
...                 prev_value = value
... 
...                 # (14)!
...                 sharpe_in_state = RollSharpeAIS(
...                     i=i,
...                     ret=return_,
...                     pre_window_ret=np.nan,
...                     cumsum=cumsum,
...                     cumsum_sq=cumsum_sq,
...                     nancnt=nancnt,
...                     window=i + 1,
...                     minp=0,
...                     ddof=1,
...                     ann_factor=ann_factor
...                 )
...                 sharpe_out_state = rolling_sharpe_acc_nb(sharpe_in_state)
...                 cumsum = sharpe_out_state.cumsum
...                 cumsum_sq = sharpe_out_state.cumsum_sq
...                 nancnt = sharpe_out_state.nancnt
...                 sharpe = sharpe_out_state.value
... 
...             out[k * close.shape[1] + col] = sharpe  # (15)!
...         
...     return out
```

1. The number of Sharpe ratios matches the number of parameter combinations times the number of columns.

2. Iterate over each parameter combination (period and multiplier).

3. Iterate over each column (asset).

4. Define the initial values for the SuperTrend and Sharpe accumulators.

5. Set the initial state of the backtesting environment, such as the initial cash balance.

6. Iterate over each row (time step).

7. Compute the SuperTrend values at this time step, generate a signal from them, and convert the signal into an order size.

8. Build and execute an order, then update the return at the end of this time step.

9. Pack the current environment state into an instance of [ExecState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ExecState).

10. Pack the current price bounds into an instance of [PriceArea](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceArea).

11. Get an [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order) 
instance using [order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.order_nb).

12. Execute the order using [execute_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.execute_order_nb),
which returns an [OrderResult](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderResult) 
and the updated backtesting environment as an [AccountState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState).

13. Return is calculated as the percentage change from the previous value to the current value.

14. Calculate the Sharpe ratio based on the updated return.

15. Store the Sharpe ratio from the latest time step in the output array.

We have just built our own simulator, optimized for one specific task. Unsurprisingly,
its speed is incredible! :exploding_head:

```pycon
>>> chunked_raw_pipeline_nb = nb_chunked(raw_pipeline_nb)

>>> %%timeit
>>> chunked_raw_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _merge_kwargs=dict(input_columns=close.columns)
... )
225 ms ± 464 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %%timeit
>>> chunked_raw_pipeline_nb(
...     high.values, 
...     low.values, 
...     close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _execute_kwargs=dict(engine="dask"),
...     _merge_kwargs=dict(input_columns=close.columns)
... )
54 ms ± 1.13 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

The reason for the 20x jump in performance compared to the previous pipeline, even though both 
process the data in a streaming manner, is that VBT must prepare contexts for all callbacks 
(even those that do nothing by default) and calculate all possible metrics that the user might need. 
Additionally, Numba does not perform well with complex relationships between objects that are shared 
or passed between multiple functions, making it quite challenging to design an efficient order 
function.

The best part of the pipeline above is its outstanding memory efficiency. For example, you can roll 
100 one-year periods over the entire dataset (a total of 1,752,000 input data points), backtest the 
full parameter grid on each period, and create an animated GIF of the entire process—all in just 15 
seconds!

First, let's split the full period into sub-periods:

```pycon
>>> range_len = int(vbt.timedelta('365d') / vbt.timedelta('1h'))  # (1)!
>>> splitter = vbt.Splitter.from_n_rolling(  # (2)!
...     high.index, 
...     n=100, 
...     length=range_len
... )

>>> roll_high = splitter.take(high, into="reset_stacked")  # (3)!
>>> roll_low = splitter.take(low, into="reset_stacked")
>>> roll_close = splitter.take(close, into="reset_stacked")
>>> roll_close.columns
MultiIndex([( 0, 'BTCUSDT'),
            ( 0, 'ETHUSDT'),
            ( 1, 'BTCUSDT'),
            ( 1, 'ETHUSDT'),
            ( 2, 'BTCUSDT'),
            ...
            (97, 'ETHUSDT'),
            (98, 'BTCUSDT'),
            (98, 'ETHUSDT'),
            (99, 'BTCUSDT'),
            (99, 'ETHUSDT')],
            names=['split', 'symbol'], length=200)
            
>>> range_indexes = splitter.take(high.index)  # (4)!
>>> range_indexes[0]
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 01:00:00+00:00',
                '2020-01-01 02:00:00+00:00', '2020-01-01 03:00:00+00:00',
                '2020-01-01 04:00:00+00:00', '2020-01-01 05:00:00+00:00',
                ...
                '2020-12-31 12:00:00+00:00', '2020-12-31 13:00:00+00:00',
                '2020-12-31 14:00:00+00:00', '2020-12-31 15:00:00+00:00',
                '2020-12-31 16:00:00+00:00', '2020-12-31 17:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='split_0', length=8760, freq=None)
```

1. Get the number of hours in a year.

2. Create an instance of [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).

3. Split the DataFrame into chunks and stack those chunks into a single DataFrame by resetting its index.
We reset the index to avoid producing many NaNs.

4. If you need the index of each split, split it the same way as above, but instead of stacking,
store it in a Series indexed by the split label.

Next, generate the Sharpe values:

```pycon
>>> sharpe_ratios = chunked_raw_pipeline_nb(
...     roll_high.values, 
...     roll_low.values,
...     roll_close.values,
...     periods=period_product, 
...     multipliers=multiplier_product,
...     ann_factor=ann_factor,
...     _execute_kwargs=dict(engine="dask"),
...     _merge_kwargs=dict(input_columns=roll_close.columns)  # (1)!
... )

>>> sharpe_ratios
st_period  st_multiplier  split  symbol 
4          2.0            0      BTCUSDT    1.751331
                                  ETHUSDT    2.479750
                          1      BTCUSDT    1.847095
                                  ETHUSDT    2.736193
                          2      BTCUSDT    1.739149
                                                  ...
19         4.0            97     ETHUSDT    1.503001
                          98     BTCUSDT    0.954932
                                  ETHUSDT    1.204134
                          99     BTCUSDT    0.818209
                                  ETHUSDT    1.191223
Length: 67200, dtype: float64
```

1. Make sure to pass the correct columns.

That's 67,200 backtests in one second :dash:

When creating a heatmap for each sub-period, we use the Sharpe ratio for holding during that period 
as the midpoint of the color scale. Any blue-tinted point indicates that the parameter combination 
performed better than the market, while a red-tinted point means it performed worse. To do this, we 
need the Sharpe ratio for holding:

```pycon
>>> pf_hold = vbt.Portfolio.from_holding(roll_close, freq='1h')
>>> sharpe_ratios_hold = pf_hold.sharpe_ratio

>>> sharpe_ratios_hold
split  symbol 
0      BTCUSDT    2.229122
        ETHUSDT    2.370132
1      BTCUSDT    2.298050
        ETHUSDT    2.611722
2      BTCUSDT    2.351417
                    ...   
97     ETHUSDT    2.315863
98     BTCUSDT    1.124489
        ETHUSDT    2.114297
99     BTCUSDT    0.975638
        ETHUSDT    2.008839
Name: sharpe_ratio, Length: 200, dtype: float64
```

!!! info
    Notice that this multi-index lists no parameter combinations: the performance of holding 
    does not depend on our indicator in any way.

Next, let's create a function that plots a sub-period:

```pycon
>>> def plot_subperiod_sharpe(index, 
...                           sharpe_ratios, 
...                           sharpe_ratios_hold, 
...                           range_indexes, 
...                           symbol):
...     split = index[0]
...     sharpe_ratios = sharpe_ratios.xs(  # (1)!
...         symbol, 
...         level='symbol', 
...         drop_level=True)
...     sharpe_ratios = sharpe_ratios.xs(  # (2)!
...         split, 
...         level='split', 
...         drop_level=True)
...     start_date = range_indexes[split][0]
...     end_date = range_indexes[split][-1]
...     return sharpe_ratios.vbt.heatmap(
...         x_level='st_period', 
...         y_level='st_multiplier',
...         title="{} - {}".format(  # (3)!
...             start_date.strftime("%d %b, %Y %H:%M:%S"),
...             end_date.strftime("%d %b, %Y %H:%M:%S")
...         ),
...         trace_kwargs=dict(  # (4)!
...             zmin=sharpe_ratios.min(),
...             zmid=sharpe_ratios_hold[(split, symbol)],
...             zmax=sharpe_ratios.max(),
...             colorscale='Spectral'
...         )
...     )
```

1. Select the symbol (only one can be plotted at a time).

2. Select the sub-period by its split index.

3. Add the start and end dates of the sub-period to the title.

4. Set up the heatmap's color scale.

Finally, use [save_animation](https://vectorbt.pro/pvt_41531c19/api/utils/image_/#vectorbtpro.utils.image_.save_animation)
to iterate over each split index, plot the heatmap for each sub-period, and append it as a 
PNG image to the GIF file:

```pycon
>>> fname = 'raw_pipeline.gif'
>>> level_idx = sharpe_ratios.index.names.index('split')
>>> split_indices = sharpe_ratios.index.levels[level_idx]

>>> vbt.save_animation(
...     fname,
...     split_indices, 
...     plot_subperiod_sharpe,  # (1)!
...     sharpe_ratios,  # (2)!
...     sharpe_ratios_hold,
...     range_indexes,
...     'BTCUSDT',
...     delta=1,  # (3)!
...     fps=7,
...     writer_kwargs=dict(loop=0)  # (4)!
... )
```

1. Plotting function.

2. Variable arguments passed to the plotting function.

3. Keyword arguments for configuring the GIF.

4. Loop the animation indefinitely.

[=100% "Heatmap 100/100"]{: .candystripe .candystripe-animate }

```pycon
>>> from IPython.display import Image, display

>>> with open(fname,'rb') as f:
...     display(Image(data=f.read(), format='png'))
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/raw_pipeline.light.gif#only-light){: .iimg loading=lazy style="width:600px" }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/supertrend/raw_pipeline.dark.gif#only-dark){: .iimg loading=lazy style="width:600px" }

Any color bluer than yellow beats the market. Just avoid picking values at the bottom :wink:

## Summary

We have covered a lot of ground, so let's review what we have learned so far.

A pipeline is a process that takes data and transforms it into insights. This process can 
be implemented using various pipeline designs, and you can always rely on VBT 
throughout the development of each one.

The easiest-to-use class of pipelines in VBT uses two main components: the indicator (such as 
`SuperTrend`) and the simulator (such as [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)).
Both components can be developed, modified, and run independently. This modular design provides 
high flexibility when signals and order execution are not path-dependent. The only major drawback 
is high memory usage, which can only be addressed by chunking—splitting data and/or parameters into 
groups that are processed sequentially (looped) or in parallel. Chunking also allows you to use all 
cores with multiprocessing and multithreading. However, multithreading only works when the 
entire pipeline is Numba-compiled and can release the GIL. Fortunately, VBT provides many 
utilities that work within Numba, so do not be afraid to write your whole pipeline using Numba—it 
is easier than it sounds!

Once signals become dependent on previous trades, both components must be combined into a 
monolithic workflow. These workflows are possible using contextualized simulation, such as with 
[Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func),
which lets you inject custom trading logic directly into the simulator using callbacks and contexts.
This approach is very similar to the event-driven backtesting style found in many frameworks, 
such as backtrader. The key difference is in how information is stored and managed, which is done 
using named tuples and arrays rather than classes and variables. These pipelines offer the 
greatest flexibility but are significantly slower than modular ones (though they still outperform 
most other backtesting software). For maximum performance, you can switch to a lower-level API 
and implement the simulator yourself. Does that sound intimidating? It should not, because 
every simulator is simply a combination of regular for-loops and order management commands.

Ultimately, you should choose the design that fits your needs best. There is no point in spending 
days creating a perfect pipeline if it only saves you five minutes in the end, right? But you will 
learn how to design efficient Python algorithms that can compete with 
top-tier algo-trading systems written in Java.

As always, happy coding :heart:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/superfast-supertrend/pipelines.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/SuperTrend.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/generation/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/
name: Generation
type: page
icon: material-broadcast
---

# Generation

Signals add an extra layer of abstraction on top of orders. Instead of specifying every detail about 
what needs to be ordered at each timestamp, you can define what a typical order should look like and 
then decide when to issue such an order. In VBT, this timing decision is made using signals, 
which are represented as boolean masks: `True` stands for "order" and `False` means "no order". The 
meaning of each signal can be changed either statically or dynamically, depending on the current 
simulation state. For example, you can instruct the simulator to ignore an "order" signal if you are 
already in the market, which is not possible with the "from-orders" method alone. Because VBT 
emphasizes data science, it is much easier to compare multiple strategies with the same trading 
conditions but different signal permutations (for example, order timings and directions). This 
approach reduces errors and leads to more fair experiments.

Since buying and selling are constant activities, it is ideal to include the order direction within 
each signal as well. However, because booleans only have two states, they cannot represent the three 
possibilities: "order to buy", "order to sell", and "no order". As a result, signals are typically 
distributed across two or more boolean arrays, each representing a different decision dimension. The 
most common way to define signals is by using two direction-unaware arrays: :one: entries and :two: 
exits. The meaning of these two arrays changes based on the direction, which is specified using a 
separate variable. For instance, when only the long direction is enabled, an entry signal opens a 
new long position and an exit signal closes it. When both directions are enabled, an entry signal 
opens a new long position, and an exit signal reverses it to open a short one. For more precise 
control over whether to reverse a position or simply close it, you can define four 
direction-aware arrays: :one: long entries, :two: long exits, :three: short entries, and :four: 
short exits. This method provides the greatest flexibility.

For example, to open a long position, close it, open a short position, and then reverse it, the 
signals would look like this:

| Long entry | Long exit | Short entry | Short exit |
|------------|-----------|-------------|------------|
| True       | False     | False       | False      |
| False      | True      | False       | False      |
| False      | False     | True        | False      |
| True       | False     | False       | False      |

The same strategy can also be defined using an entry signal, an exit signal, and a direction:

| Entry      | Exit       | Direction       |
|------------|------------|-----------------|
| True       | False      | Long only       |
| False      | True       | Long only       |
| True/False | False/True | Short only/Both |
| True       | False      | Long only/Both  |

!!! info
    Direction-unaware signals can be easily converted to direction-aware signals:

    * True, True, Long only :material-arrow-right: True, True, False, False.

    * True, True, Short only :material-arrow-right: False, False, True, True.

    * True, True, Both :material-arrow-right: True, False, True, False.

    However, direction-aware signals cannot be converted to direction-unaware signals
    if both directions are enabled and there is an exit signal present:

    * False, True, False, True :material-arrow-right: :question:

    Therefore, it is important to carefully evaluate which conditions you are interested in
    before generating signals.

You might wonder why not use an integer data type, where a positive number means "order to buy," a 
negative number means "order to sell," and zero means "no order," as is done in backtrader. Boolean 
arrays are much easier for users to generate and maintain. In addition, a boolean NumPy array uses 
eight times less memory than a 64-bit signed integer NumPy array. Boolean masks are also much more 
convenient to combine and analyze than integer arrays. For example, you can combine two masks with 
the logical OR (`|` in NumPy) operator, or sum the elements in a mask to get the number of signals. 
Since booleans are a subtype of integers, they behave just like regular integers in most math 
expressions.

## Comparison

Properly generating signals can sometimes be much more challenging than simulating them. This is 
because you need to consider not only the distribution of signals but also how they interact 
across multiple boolean arrays. For example, setting both an entry and exit at the same 
timestamp will effectively cancel both. That's why VBT offers many functions and techniques 
to support you in this process.

Signal generation usually begins by comparing two or more numeric arrays. Remember, when 
comparing entire arrays, you iterate over each row and column (that is, each element) 
in a vectorized way and compare their scalar values at each element. Essentially, you run the 
same comparison operation on every single element across all the arrays being compared. Let's 
look at an example using Bollinger Bands for two different assets. At each timestamp, 
we will place a signal whenever the low price is below the lower band, expecting the price to 
reverse back to its rolling mean:

```pycon
>>> from vectorbtpro import *

>>> data = vbt.BinanceData.pull(
...     ["BTCUSDT", "ETHUSDT"], 
...     start="2021-01-01",
...     end="2022-01-01"
... )
>>> data.get("Low")
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2021-01-01 00:00:00+00:00  28624.57   714.29
2021-01-02 00:00:00+00:00  28946.53   714.91
2021-01-03 00:00:00+00:00  31962.99   768.71
...                             ...      ...
2021-12-29 00:00:00+00:00  46096.99  3604.20
2021-12-30 00:00:00+00:00  45900.00  3585.00
2021-12-31 00:00:00+00:00  45678.00  3622.29

[365 rows x 2 columns]

>>> bb = vbt.talib("BBANDS").run(
...     data.get("Close"),
...     timeperiod=vbt.Default(14),  # (1)!
...     nbdevup=vbt.Default(2),
...     nbdevdn=vbt.Default(2)
... )
>>> bb.lowerband  # (2)!
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2021-01-01 00:00:00+00:00           NaN          NaN
2021-01-02 00:00:00+00:00           NaN          NaN
2021-01-03 00:00:00+00:00           NaN          NaN
...                                 ...          ...
2021-12-29 00:00:00+00:00  44943.280004  3712.190071
2021-12-30 00:00:00+00:00  44861.191483  3662.827659
2021-12-31 00:00:00+00:00  44882.974796  3617.893036

[365 rows x 2 columns]

>>> mask = data.get("Low") < bb.lowerband  # (3)!
>>> mask
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-01-01 00:00:00+00:00    False    False
2021-01-02 00:00:00+00:00    False    False
2021-01-03 00:00:00+00:00    False    False
...                            ...      ...
2021-12-29 00:00:00+00:00    False     True
2021-12-30 00:00:00+00:00    False     True
2021-12-31 00:00:00+00:00    False    False

[365 rows x 2 columns]

>>> mask.sum()  # (4)!
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64
```

1. Wrap each parameter with [Default](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Default)
to hide its column level. Alternatively, you can pass a list of parameters to hide with `hide_params`.

2. Get the lower band as a Pandas object. You can list all output names of an indicator using `bb.output_names`.

3. Compare two numeric arrays element-wise.

4. Get the number of signals in each column for a better overview.

This operation has created a mask that is true whenever the low price dips below the lower band. 
Such an array can already be used in a simulation! But let's see what happens when we try to compare 
the lower band generated for multiple combinations of the (upper and lower) multipliers:

```pycon
>>> bb_mult = vbt.talib("BBANDS").run(
...     data.get("Close"),
...     timeperiod=vbt.Default(14),
...     nbdevup=[2, 3],
...     nbdevdn=[2, 3]  # (1)!
... )
>>> mask = data.get("Low") < bb_mult.lowerband
ValueError: Can only compare identically-labeled DataFrame objects
```

1. Two parameter combinations: (14, 2, 2) and (14, 3, 3)

The issue here is that Pandas cannot compare DataFrames with different columns. The left
DataFrame contains the columns `BTCUSDT` and `ETHUSDT`, while the right DataFrame from the 
Bollinger Bands indicator now has the columns `(2, 2, BTCUSDT)`, `(2, 2, ETHUSDT)`, 
`(3, 3, BTCUSDT)`, and `(3, 3, ETHUSDT)`. So, what is the solution? Right, VBT! By appending 
`vbt` to the _left_ operand, you are comparing the accessor object of type 
[BaseAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor) instead of the 
DataFrame itself. This triggers the so-called [magic method](https://rszalski.github.io/magicmethods/)
`__lt__` of that accessor. This method takes the DataFrame under the accessor and the DataFrame on 
the right and combines them using [BaseAccessor.combine](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine)
and [numpy.less](https://numpy.org/doc/stable/reference/generated/numpy.less.html) as the `combine_func`.
As a result, the shapes and indexes of both DataFrames are broadcasted using VBT's powerful 
broadcasting mechanism, which overcomes the Pandas limitation.

As a result, VBT compares `(2, 2, BTCUSDT)` and `(3, 3, BTCUSDT)` only with `BTCUSDT`,
and `(2, 2, ETHUSDT)` and `(3, 3, ETHUSDT)` only with `ETHUSDT`, leveraging NumPy for performance!

```pycon
>>> mask = data.get("Low").vbt < bb_mult.lowerband  # (1)!
>>> mask
bbands_nbdevup                          2               3
bbands_nbdevdn                          2               3
symbol                    BTCUSDT ETHUSDT BTCUSDT ETHUSDT
Open time                                                
2021-01-01 00:00:00+00:00   False   False   False   False
2021-01-02 00:00:00+00:00   False   False   False   False
2021-01-03 00:00:00+00:00   False   False   False   False
...                           ...     ...     ...     ...
2021-12-29 00:00:00+00:00   False    True   False   False
2021-12-30 00:00:00+00:00   False    True   False   False
2021-12-31 00:00:00+00:00   False   False   False   False

[365 rows x 4 columns]

>>> mask.sum()
bbands_nbdevup  bbands_nbdevdn  symbol 
2               2               BTCUSDT    53
                                ETHUSDT    48
3               3               BTCUSDT    10
                                ETHUSDT     9
dtype: int64
```

1. `vbt` must always be called on the left operand.

!!! note
    For VBT to compare shapes that are not broadcastable, both DataFrames must 
    share at least one column level in common, such as `symbol` in the example above.

As you may remember from the indicator documentation, each indicator provides 
several helper methods for comparison: `{name}_above`, `{name}_equal`, and 
`{name}_below`, which essentially do the same as what we did above:

```pycon
>>> mask = bb_mult.lowerband_above(data.get("Low"))  # (1)!
>>> mask.sum()
bbands_nbdevup  bbands_nbdevdn  symbol 
2               2               BTCUSDT    53
                                ETHUSDT    48
3               3               BTCUSDT    10
                                ETHUSDT     9
dtype: int64
```

1. Our indicator does not have an input or output for the low price, so we need
to reverse the comparison order and check whether the lower band is above the low price.

### Thresholds

To compare a numeric array against two or more scalar thresholds as parameter combinations, we can use
the same approach by either appending `vbt` or by calling the method
[BaseAccessor.combine](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine). Let's
calculate the bandwidth of our single-combination indicator, which is the upper band minus the lower band
divided by the middle band, and check whether it is higher than two different thresholds:

```pycon
>>> bandwidth = (bb.upperband - bb.lowerband) / bb.middleband

>>> mask = bandwidth.vbt > vbt.Param([0.15, 0.3], name="threshold")  # (1)!
>>> mask.sum()
threshold  symbol 
0.15       BTCUSDT    253
            ETHUSDT    316
0.30       BTCUSDT     65
            ETHUSDT    136
dtype: int64

>>> mask = bandwidth.vbt.combine(
...     [0.15, 0.3],  # (2)!
...     combine_func=np.greater, 
...     keys=pd.Index([0.15, 0.3], name="threshold")  # (3)!
... )
>>> mask.sum()
threshold  symbol 
0.15       BTCUSDT    253
            ETHUSDT    316
0.30       BTCUSDT     65
            ETHUSDT    136
dtype: int64
```

1. Passes both objects to [BaseAccessor.combine](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine),
broadcasts them using [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast),
and combines them using [numpy.greater](https://numpy.org/doc/stable/reference/generated/numpy.greater.html).
With broadcasting, each value in `vbt.Param` is combined with each column in the array. This works only
for scalars.

2. When passing a list, the DataFrame is compared to each item in the list.

3. The `keys` argument appends a column level describing the items in the list.

The latest example also works with arrays instead of scalars. Alternatively, we can use
[pandas.concat](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to manually stack the
results of any comparison and treat them as separate combinations:

```pycon
>>> mask = pd.concat(
...     (bandwidth > 0.15, bandwidth > 0.3), 
...     keys=pd.Index([0.15, 0.3], name="threshold"), 
...     axis=1
... )
>>> mask.sum()
threshold  symbol 
0.15       BTCUSDT    253
            ETHUSDT    316
0.30       BTCUSDT     65
            ETHUSDT    136
dtype: int64
```

### Crossovers

So far, we have explored basic vectorized comparison operations, but one operation appears
disproportionately often in technical analysis: crossovers. A crossover refers to a situation where two
time series cross each other. There are two ways to identify crossovers: naive and native. The naive
approach compares both time series in a vectorized way and then selects the first `True` value from each
"partition" of `True` values. In VBT's terminology, a partition for signal processing is a consecutive
sequence of `True` values produced by the comparison. While we already know how to perform the
comparison, the second step can be accomplished with the accessor for signals:
[SignalsAccessor](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor), which can be
accessed via the attribute `vbt.signals` on any Pandas object.

Specifically, we will use the method [SignalsAccessor.first](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first),
which takes a mask, assigns a rank to each `True` value within each partition using
[SignalsAccessor.pos_rank](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.pos_rank)
(numbered from 0 to the length of the respective partition), and then keeps only those `True` values
that have rank 0. Let's get the crossovers where the low price dips below the lower band:

```pycon
>>> low_below_lband = data.get("Low") < bb.lowerband
>>> mask = low_below_lband.vbt.signals.first()
>>> mask.sum()
symbol
BTCUSDT    21
ETHUSDT    20
dtype: int64
```

To confirm the operation was successful, let's plot the `BTCUSDT` column of both time series
using [GenericAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.plot)
and the generated signals with [SignalsSRAccessor.plot_as_markers](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_markers):

```pycon
>>> btc_low = data.get("Low", "BTCUSDT").rename("Low")  # (1)!
>>> btc_lowerband = bb.lowerband["BTCUSDT"].rename("Lower Band")
>>> btc_mask = mask["BTCUSDT"].rename("Signals")

>>> fig = btc_low.vbt.plot()  # (2)!
>>> btc_lowerband.vbt.plot(fig=fig)
>>> btc_mask.vbt.signals.plot_as_markers(
...     y=btc_low, 
...     trace_kwargs=dict(
...         marker=dict(
...             color="#DFFF00"
...         )
...     ),
...     fig=fig
... )  # (3)!
>>> fig.show()
```

1. Rename the column for the legend.

2. The first plotting method returns a figure, which must be passed to each subsequent plotting method.

3. We are using `btc_low` as the Y-values at which to place the markers.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/crossovers.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/crossovers.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    To wait for a confirmation, use [SignalsAccessor.nth](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.nth)
    to select the n-th signal in each partition.

However, there is a catch: if the first low value is already below the first lower band value, it will also
produce a crossover signal. To address this, we need to pass `after_false=True`, which will discard the first
partition if there is no `False` value beforehand.

```pycon
>>> mask = low_below_lband.vbt.signals.first(after_false=True)
>>> mask.sum()
symbol
BTCUSDT    21
ETHUSDT    20
dtype: int64
```

Here's another catch: if the first values in the indicator are NaN, which results in `False`
values in the mask, and the first value after the last NaN yields `True`, the `after_false` argument
becomes ineffective. To resolve this, we need to manually set those positions in the mask to `True`.
Let's illustrate this with sample data:

```pycon
>>> sample_low = pd.Series([10, 9, 8, 9, 8])
>>> sample_lband = pd.Series([np.nan, np.nan, 9, 8, 9])
>>> sample_mask = sample_low < sample_lband
>>> sample_mask.vbt.signals.first(after_false=True)  # (1)!
0    False
1    False
2     True
3    False
4     True
dtype: bool

>>> sample_mask[sample_lband.ffill().isnull()] = True  # (2)!
>>> sample_mask.vbt.signals.first(after_false=True)
0    False
1    False
2    False
3    False
4     True
dtype: bool
```

1. The first crossover should not occur because we do not know what happens at those NaN values,
while the second crossover is valid.

2. Forward fill the indicator values to identify only the initial NaN values, and set the
mask to `True` at those positions to make `after_false` effective again.

Alternatively, we can remove the buffer, perform the operation, and then add the buffer back:

```pycon
>>> buffer = sample_lband.ffill().isnull().sum(axis=0).max()  # (1)!
>>> buffer
2

>>> sample_buf_mask = sample_low.iloc[buffer:] < sample_lband.iloc[buffer:]
>>> sample_buf_mask = sample_buf_mask.vbt.signals.first(after_false=True)
>>> sample_mask = sample_low.vbt.wrapper.fill(False)
>>> sample_mask.loc[sample_buf_mask.index] = sample_buf_mask
>>> sample_mask
0    False
1    False
2    False
3    False
4     True
dtype: bool
```

1. Find the maximum length of each first consecutive series of NaN values across all columns.

!!! info
    We can apply the buffer-exclusive approach described above to almost any operation in VBT.

But here comes another issue: what if our data contains gaps and we encounter a NaN in the middle
of a partition? In this case, we should set the second part of the partition to `False`, because
forward-filling that NaN value would make waiting for a confirmation difficult. However, performing
many operations on larger arrays just to find crossovers can be quite resource-intensive. Fortunately,
VBT uses its own Numba-compiled function [crossed_above_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.crossed_above_nb)
to find crossovers iteratively, which is the second, native method. To use this function,
we can use the methods [GenericAccessor.crossed_above](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_above)
and [GenericAccessor.crossed_below](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_below),
accessible via the `vbt` attribute on any Pandas object:

```pycon
>>> mask = data.get("Low").vbt.crossed_below(bb.lowerband, wait=1)  # (1)!
>>> mask.sum()
symbol
BTCUSDT    15
ETHUSDT    11
dtype: int64
```

1. Waits one bar for confirmation.

!!! info
    If the time series crosses back during the confirmation period `wait`, the signal will not be set.
    To set the signal anyway, use forward shifting.

As with other comparison methods, each indicator comes with helper methods `{name}_crossed_above` and
`{name}_crossed_below` for generating the crossover masks:

```pycon
>>> mask = bb.lowerband_crossed_above(data.get("Low"), wait=1)
>>> mask.sum()
symbol
BTCUSDT    15
ETHUSDT    11
dtype: int64
```

## Logical operators

Once we have generated two or more masks (conditions), we can combine them into a single mask using
logical operators. Common logical operators include:

* _AND_ (`&` or [numpy.logical_and](https://numpy.org/doc/stable/reference/generated/numpy.logical_and.html)):
for each element, returns True whenever all the conditions are True.

* _OR_ (`|` or [numpy.logical_or](https://numpy.org/doc/stable/reference/generated/numpy.logical_or.html)):
for each element, returns True whenever any of the conditions are True.

* _NOT_ (`~` or [numpy.logical_not](https://numpy.org/doc/stable/reference/generated/numpy.logical_not.html)):
for each element, returns True whenever the condition is False.

* _XOR_ (`^` or [numpy.logical_xor](https://numpy.org/doc/stable/reference/generated/numpy.logical_xor.html)):
for each element, returns True whenever only one of the conditions is True.

!!! note
    Do not use `and`, `or`, or `not` on arrays. These only work on single boolean values.
    For example, instead of `mask1 and mask2`, use `mask1 & mask2`. Instead of `mask1 or mask2`,
    use `mask1 | mask2`. Instead of `not mask`, use `~mask`.

For example, let's combine four conditions for a signal: the low price dips below the lower band _AND_
the bandwidth is above some threshold (a downward breakout while expanding), _OR_ the high price rises
above the upper band _AND_ the bandwidth is below some threshold (an upward breakout while squeezing):

```pycon
>>> cond1 = data.get("Low") < bb.lowerband
>>> cond2 = bandwidth > 0.3
>>> cond3 = data.get("High") > bb.upperband
>>> cond4 = bandwidth < 0.15

>>> mask = (cond1 & cond2) | (cond3 & cond4)
>>> mask.sum()
symbol
BTCUSDT    25
ETHUSDT    13
dtype: int64
```

To test multiple thresholds and broadcast exclusively using VBT:

```pycon
>>> cond1 = data.get("Low").vbt < bb.lowerband
>>> cond2 = bandwidth.vbt > vbt.Param([0.3, 0.3, 0.4, 0.4], name="cond2_th")  # (1)!
>>> cond3 = data.get("High").vbt > bb.upperband
>>> cond4 = bandwidth.vbt < vbt.Param([0.1, 0.2, 0.1, 0.2], name="cond4_th")  # (2)!

>>> mask = (cond1.vbt & cond2).vbt | (cond3.vbt & cond4)  # (3)!
>>> mask.sum()
cond2_th  cond4_th  symbol 
0.3       0.1       BTCUSDT    11
                    ETHUSDT    10
          0.2       BTCUSDT    28
                    ETHUSDT    27
0.4       0.1       BTCUSDT     9
                    ETHUSDT     5
          0.2       BTCUSDT    26
                    ETHUSDT    22
dtype: int64
```

1. Tests two thresholds in the second condition, repeating them so that each is compared with both thresholds
in the fourth condition.

2. Tests two thresholds in the fourth condition, tiling them so that each is compared with both thresholds
in the second condition.

3. Notice that `vbt` is appended to each operand on the left.
Adding it to any operand on the right is redundant.

### Cartesian product

Combining two or more arrays using a Cartesian product is a bit more complex because each array
has the column level `symbol`, which should not be combined with itself. Here is the trick:
First, convert the columns of each array to their integer positions. Then, split each position array
into "blocks" (smaller arrays). Blocks will be combined with each other, but the positions within each
block will not; that is, each block serves as a parameter combination. Then, combine all blocks using
a combinatorial function of your choice (see [itertools](https://docs.python.org/3/library/itertools.html)
for various options, or [generate_param_combs](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.generate_param_combs)),
and finally, flatten each array with blocks and use it for column selection. While this may sound complex,
it is not difficult to implement!

```pycon
>>> cond1 = data.get("Low").vbt < bb.lowerband
>>> cond2 = bandwidth.vbt > vbt.Param([0.3, 0.4], name="cond2_th")  # (1)!
>>> cond3 = data.get("High").vbt > bb.upperband
>>> cond4 = bandwidth.vbt < vbt.Param([0.1, 0.2], name="cond4_th")

>>> i1 = np.split(np.arange(len(cond1.columns)), len(cond1.columns) // 2)  # (2)!
>>> i2 = np.split(np.arange(len(cond2.columns)), len(cond2.columns) // 2)
>>> i3 = np.split(np.arange(len(cond3.columns)), len(cond3.columns) // 2)
>>> i4 = np.split(np.arange(len(cond4.columns)), len(cond4.columns) // 2)

>>> i1
[array([0, 1])]
>>> i2
[array([0, 1]), array([2, 3])]
>>> i3
[array([0, 1])]
>>> i4
[array([0, 1]), array([2, 3])]

>>> i1, i2, i3, i4 = zip(*product(i1, i2, i3, i4))  # (3)!

>>> i1
(array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]))
>>> i2
(array([0, 1]), array([0, 1]), array([2, 3]), array([2, 3]))
>>> i3
(array([0, 1]), array([0, 1]), array([0, 1]), array([0, 1]))
>>> i4
(array([0, 1]), array([2, 3]), array([0, 1]), array([2, 3]))

>>> i1 = np.asarray(i1).flatten()  # (4)!
>>> i2 = np.asarray(i2).flatten()
>>> i3 = np.asarray(i3).flatten()
>>> i4 = np.asarray(i4).flatten()

>>> i1
[0 1 0 1 0 1 0 1]
>>> i2
[0 1 0 1 2 3 2 3]
>>> i3
[0 1 0 1 0 1 0 1]
>>> i4
[0 1 2 3 0 1 2 3]

>>> cond1 = cond1.iloc[:, i1]  # (5)!
>>> cond2 = cond2.iloc[:, i2]
>>> cond3 = cond3.iloc[:, i3]
>>> cond4 = cond4.iloc[:, i4]

>>> mask = (cond1.vbt & cond2).vbt | (cond3.vbt & cond4)  # (6)!
>>> mask.sum()
cond2_th  cond4_th  symbol 
0.3       0.1       BTCUSDT    11
                    ETHUSDT    10
          0.2       BTCUSDT    28
                    ETHUSDT    27
0.4       0.1       BTCUSDT     9
                    ETHUSDT     5
          0.2       BTCUSDT    26
                    ETHUSDT    22
dtype: int64
```

1. Each DataFrame should contain only unique parameter combinations (and columns in general).

2. Split each position array into blocks with symbols.

3. Combine all blocks (parameter combinations) using the Cartesian product.

4. Flatten each array with blocks to get an array that can be used for indexing.

5. Use the final positions to select the columns from each DataFrame so that each DataFrame has the
same number of columns.

6. All columns now have the same length but still different labels :material-arrow-right:
let VBT's broadcaster handle the alignment.

In newer versions of VBT, the same effect can be achieved with a single call to
[BaseAccessor.cross](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.cross):

```pycon
>>> cond1 = data.get("Low").vbt < bb.lowerband
>>> cond2 = bandwidth.vbt > vbt.Param([0.3, 0.4], name="cond2_th")
>>> cond3 = data.get("High").vbt > bb.upperband
>>> cond4 = bandwidth.vbt < vbt.Param([0.1, 0.2], name="cond4_th")

>>> cond1, cond2, cond3, cond4 = vbt.pd_acc.x(cond1, cond2, cond3, cond4)  # (1)!
>>> mask = (cond1.vbt & cond2).vbt | (cond3.vbt & cond4)
```

1. `x` is an alias for `cross`. Another option: `cond1.vbt.x(cond2, cond3, cond4)`.

A simpler and less error-prone approach is to build an indicator that handles parameter
combinations for us :grin:

For this, we will write an indicator expression similar to the code we wrote for a single
parameter combination, and use [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr)
to auto-build an indicator by parsing that expression. The logic, including the specification of
all inputs, parameters, and outputs, is contained within the expression itself. We will use the annotation
`@res_talib_bbands` to resolve the inputs and parameters expected by TA-Lib's `BBANDS`
indicator, and "copy" them to our indicator by also adding the prefix `talib` to the parameter names.
Then, we perform the usual signal generation logic, substituting the custom parameters `cond2_th`
and `cond4_th` with their individual values, and return the entire result as an output `mask`
with the proper annotation.

```pycon
>>> MaskGenerator = vbt.IF.from_expr("""
... upperband, middleband, lowerband = @res_talib_bbands
... bandwidth = (upperband - lowerband) / middleband
... cond1 = low < lowerband
... cond2 = bandwidth > @p_cond2_th
... cond3 = high > upperband
... cond4 = bandwidth < @p_cond4_th
... @out_mask:(cond1 & cond2) | (cond3 & cond4)
... """)

>>> vbt.phelp(MaskGenerator.run, incl_doc=False)  # (1)!
Indicator.run(
    high,
    low,
    close,
    cond2_th,
    cond4_th,
    bbands_timeperiod=Default(value=5),
    bbands_nbdevup=Default(value=2),
    bbands_nbdevdn=Default(value=2),
    bbands_matype=Default(value=0),
    bbands_timeframe=Default(value=None),
    short_name='custom',
    hide_params=None,
    hide_default=True,
    **kwargs
)

>>> mask_generator = MaskGenerator.run(
...     high=data.get("High"),
...     low=data.get("Low"),
...     close=data.get("Close"),
...     cond2_th=[0.3, 0.4],
...     cond4_th=[0.1, 0.2],
...     bbands_timeperiod=vbt.Default(14),
...     param_product=True
... )  # (2)!
>>> mask_generator.mask.sum()
custom_cond2_th  custom_cond4_th  symbol 
0.3              0.1              BTCUSDT    11
                                  ETHUSDT    10
                  0.2              BTCUSDT    28
                                  ETHUSDT    27
0.4              0.1              BTCUSDT     9
                                  ETHUSDT     5
                  0.2              BTCUSDT    26
                                  ETHUSDT    22
dtype: int64
```

1. See the arguments that the run method expects.

2. Run the indicator on the Cartesian product of all parameters.

!!! info
    Even though the indicator factory includes "indicator" in its name, we can use it to generate
    signals just as easily. This is because signals are simply boolean arrays that always match the
    shape of the input.

## Shifting

To compare the current value to any previous (not future!) value, we can use forward shifting.
You can also use this to shift the final mask to delay order execution. For example, let's generate
a signal whenever the low price dips below the lower band AND the bandwidth change (that is, the
difference between the current and previous bandwidth) is positive:

```pycon
>>> cond1 = data.get("Low") < bb.lowerband
>>> cond2 = bandwidth > bandwidth.shift(1)  # (1)!

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    42
ETHUSDT    39
dtype: int64
```

1. The shifted array contains the previous values.

!!! important
    Never attempt to shift backwards, as this can lead to look-ahead bias! Use either a positive number in
    [DataFrame.shift](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.shift.html),
    or VBT's accessor method [GenericAccessor.fshift](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.fshift).

Another way to shift observations is by selecting the first observation within a rolling window.
This is especially useful when the rolling window has a variable size based on a frequency.
Let's do the same logic as above, but determine the bandwidth change relative to one week ago,
instead of yesterday:

```pycon
>>> cond2 = bandwidth > bandwidth.rolling("7d").apply(lambda x: x[0])

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    33
ETHUSDT    28
dtype: int64
```

!!! tip
    Prefer variable windows over fixed ones if your data contains gaps.

The approach above is a step in the right direction, but it introduces two possible issues:
all windows will be either 6 days long or less, and the rolling application of a custom Python function
in Pandas is not very efficient. The first issue can be solved by rolling a window of 8 days and
checking that the first observation is exactly 7 days before the current timestamp:

```pycon
>>> def exactly_ago(sr):  # (1)!
...     if sr.index[0] == sr.index[-1] - vbt.timedelta("7d"):
...         return sr.iloc[0]
...     return np.nan

>>> cond_7d_ago = bandwidth.rolling("8d").apply(exactly_ago, raw=False)
>>> cond2 = bandwidth > cond_7d_ago

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    29
ETHUSDT    26
dtype: int64
```

1. By passing `raw=False`, the input will be a Pandas Series.

The second issue can be solved by looping with Numba. The main challenge, however, is solving
both issues at once, since we want to access the timestamp of the first observation.
This requires working on a Pandas Series rather than a NumPy array, but Numba cannot operate on Pandas
Series :expressionless:

Therefore, we can use VBT's accessor method [GenericAccessor.rolling_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_apply),
which provides two modes: regular and meta. The regular mode rolls over the data of a Pandas object
like Pandas does but does not provide information about the current window :no_good:
The meta mode rolls over the __metadata__ of a Pandas object, so we can easily select data
from any array corresponding to the current window :ok_hand:

```pycon
>>> @njit
... def exactly_ago_meta_nb(from_i, to_i, col, index, freq, arr):  # (1)!
...     if index[from_i] == index[to_i - 1] - freq:  # (2)!
...         return arr[from_i, col]  # (3)!
...     return np.nan

>>> cond_7d_ago = vbt.pd_acc.rolling_apply(
...     "8d",
...     exactly_ago_meta_nb,
...     bandwidth.index.values,  # (4)!
...     vbt.timedelta("7d").to_timedelta64(),
...     vbt.to_2d_array(bandwidth),
...     wrapper=bandwidth.vbt.wrapper  # (5)!
... )
>>> cond2 = bandwidth > cond_7d_ago

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    29
ETHUSDT    26
dtype: int64
```

1. The meta function must take three arguments: the current window start index, window end index, and column.

2. The window end index is exclusive, so use `to_i - 1` to get the last index in the window.

3. Use the current column index to select the column. Make sure all arrays are two-dimensional.

4. Here are our three user-defined arguments.

5. The wrapper contains the metadata needed for iterating and building the final Pandas object.

If this approach feels intimidating, there is an even simpler method:
[GenericAccessor.ago](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.ago), which
can forward-shift the array by any delta:

```pycon
>>> cond2 = bandwidth > bandwidth.vbt.ago("7d")

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    29
ETHUSDT    26
dtype: int64

>>> bandwidth.iloc[-8]
symbol
BTCUSDT    0.125477
ETHUSDT    0.096458
Name: 2021-12-24 00:00:00+00:00, dtype: float64

>>> bandwidth.vbt.ago("7d").iloc[-1]
symbol
BTCUSDT    0.125477
ETHUSDT    0.096458
Name: 2021-12-31 00:00:00+00:00, dtype: float64
```

!!! tip
    This method returns exact matches. If there is no exact match, the value will be NaN.
    To return the previous index value instead, pass `method="ffill"`. The method also accepts a sequence
    of deltas that can be applied per element.

## Truth value testing

What if we want to test whether a certain condition was met within a period of time in the past?
To do this, we need to create an expanding or rolling window and use truth value testing with
[numpy.any](https://numpy.org/doc/stable/reference/generated/numpy.any.html) or
[numpy.all](https://numpy.org/doc/stable/reference/generated/numpy.all.html) within this window.
Because Pandas does not implement rolling aggregation using `any` and `all`, we need to be
more creative and treat booleans as integers: use `max` for logical OR and `min` for logical AND.
Also, remember to cast the resulting array to a boolean data type to generate a valid mask.

Let's generate a signal whenever the low price goes below the lower band AND there was a downward
crossover of the close price with the middle band in the past 5 candles:

```pycon
>>> cond2 = data.get("Close").vbt.crossed_below(bb.middleband)
>>> cond2 = cond2.rolling(5, min_periods=1).max().astype(bool)

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64
```

!!! note
    Be cautious when setting `min_periods` to a higher number and converting to a boolean data type:
    each NaN will become `True`. Always replace NaNs with zeros before casting.

If the window size is fixed, you can also use [GenericAccessor.rolling_any](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_any)
and [GenericAccessor.rolling_all](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_all),
which are tailored for rolling truth value testing:

```pycon
>>> cond2 = data.get("Close").vbt.crossed_below(bb.middleband)
>>> cond2 = cond2.vbt.rolling_any(5)

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64
```

Another way to perform the same rolling operations is by using the accessor method
[GenericAccessor.rolling_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_apply)
and setting `reduce_func_nb` to "any" or "all". Use the argument `wrap_kwargs`
to instruct VBT to fill NaNs with `False` and change the data type. This method also allows
passing flexible windows. For example, let's roll a window of 5 days:

```pycon
>>> cond2 = data.get("Close").vbt.crossed_below(bb.middleband)
>>> cond2 = cond2.vbt.rolling_apply(
...     "5d", "any",  # (1)!
...     minp=1, 
...     wrap_kwargs=dict(fillna=0, dtype=bool)
... )

>>> mask = cond1 & cond2
>>> mask.sum()
symbol
BTCUSDT    36
ETHUSDT    28
dtype: int64
```

1. "any" is translated to [any_reduce_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/apply_reduce/#vectorbtpro.generic.nb.apply_reduce.any_reduce_nb).

Let's try something more complex: check whether the bandwidth contracted to 10% or less at any time
within a month using an expanding window, then reset the window at the start of the next month.
This approach uses the first timestamp of each month as a time anchor for our condition. We will
use VBT's resampling logic, which lets us aggregate values by mapping any source index
(anchor points) to any target index (our index).

```pycon
>>> anchor_points = data.wrapper.get_index_points(  # (1)!
...     every="M", 
...     start=0,  # (2)!
...     exact_start=True
... )
>>> anchor_points
array([  0,  31,  59,  90, 120, 151, 181, 212, 243, 273, 304, 334])

>>> left_bound = np.full(len(data.wrapper.index), np.nan)  # (3)!
>>> left_bound[anchor_points] = anchor_points
>>> left_bound = vbt.nb.ffill_1d_nb(left_bound).astype(int_)
>>> left_bound = bandwidth.index[left_bound]
>>> left_bound
DatetimeIndex(['2021-01-01 00:00:00+00:00', '2021-01-01 00:00:00+00:00',
                '2021-01-01 00:00:00+00:00', '2021-01-01 00:00:00+00:00',
                '2021-01-01 00:00:00+00:00', '2021-01-01 00:00:00+00:00',
                ...
                '2021-12-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00',
                '2021-12-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00',
                '2021-12-01 00:00:00+00:00', '2021-12-01 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', ...)

>>> right_bound = data.wrapper.index  # (4)!
>>> right_bound
DatetimeIndex(['2021-01-01 00:00:00+00:00', '2021-01-02 00:00:00+00:00',
                '2021-01-03 00:00:00+00:00', '2021-01-04 00:00:00+00:00',
                '2021-01-05 00:00:00+00:00', '2021-01-06 00:00:00+00:00',
                ...
                '2021-12-26 00:00:00+00:00', '2021-12-27 00:00:00+00:00',
                '2021-12-28 00:00:00+00:00', '2021-12-29 00:00:00+00:00',
                '2021-12-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', ...)

>>> mask = (bandwidth <= 0.1).vbt.resample_between_bounds(  # (5)!
...     left_bound, 
...     right_bound,
...     "any",
...     closed_lbound=True,  # (6)!
...     closed_rbound=True,
...     wrap_kwargs=dict(fillna=0, dtype=bool)
... )
>>> mask.index = right_bound
>>> mask.astype(int).vbt.ts_heatmap().show()
```

1. Find the position of the first timestamp in each month using
[ArrayWrapper.get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges).

2. Remove this and the next line if you do not want to include the first month, in case it is incomplete.

3. The left bound of each window is the corresponding anchor point (start of each month).
Create an integer array the same size as our index, fill the anchor points at their positions,
and forward fill them.

4. The right bound of each window is the current index.

5. Use [GenericAccessor.resample_between_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_between_bounds)
to map the mask values to their windows and aggregate each window using the "any" operation.

6. Make both bounds inclusive to include every single timestamp in the month.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ts_heatmap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ts_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

You can see how the signal for the bandwidth reaching the 10% mark propagates through each month,
and then the calculation resets and repeats.

## Periodically

To set signals at regular intervals, such as at 18:00 each Tuesday, we have several options.
One approach is to compare different attributes of the source and target datetime.
For example, to get the timestamps for each Tuesday, you can compare
[pandas.DatetimeIndex.weekday](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.weekday.html#pandas.DatetimeIndex.weekday)
to 1 (Monday is 0 and Sunday is 6):

```pycon
>>> min_data = vbt.BinanceData.pull(  # (1)!
...     ["BTCUSDT", "ETHUSDT"], 
...     start="2021-01-01 UTC",  # (2)!
...     end="2021-02-01 UTC",
...     timeframe="1h"
... )
>>> index = min_data.wrapper.index
>>> tuesday_index = index[index.weekday == 1]
>>> tuesday_index
DatetimeIndex(['2021-01-05 00:00:00+00:00', '2021-01-05 01:00:00+00:00',
                '2021-01-05 02:00:00+00:00', '2021-01-05 03:00:00+00:00',
                '2021-01-05 04:00:00+00:00', '2021-01-05 05:00:00+00:00',
                ...
                '2021-01-26 18:00:00+00:00', '2021-01-26 19:00:00+00:00',
                '2021-01-26 20:00:00+00:00', '2021-01-26 21:00:00+00:00',
                '2021-01-26 22:00:00+00:00', '2021-01-26 23:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Fetch hourly data for a better illustration.

2. Remember to use the UTC timezone for crypto data.

Next, we can select only those timestamps that occur at a specific time:

```pycon
>>> tuesday_1800_index = tuesday_index[tuesday_index.hour == 18]
>>> tuesday_1800_index
DatetimeIndex(['2021-01-05 18:00:00+00:00', '2021-01-12 18:00:00+00:00',
                '2021-01-19 18:00:00+00:00', '2021-01-26 18:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

Since each attribute comparison produces a mask, you can obtain your signals using logical operations.
For example, to get the timestamps corresponding to each Tuesday at 17:30, compare the weekday to Tuesday
AND the hour to 17 AND the minute to 30:

```pycon
>>> tuesday_1730_index = index[
...     (index.weekday == 1) & 
...     (index.hour == 17) & 
...     (index.minute == 30)
... ]
>>> tuesday_1730_index
DatetimeIndex([], dtype='datetime64[ns, UTC]', name='Open time', freq='H')
```

As we can see, combining these conditions did not produce any exact matches because our index is hourly.
But what if we want to get the previous or next timestamp when there is no exact match?
In that case, the approach above will not work. Instead, we can use the function
[pandas.Index.get_indexer](https://pandas.pydata.org/docs/reference/api/pandas.Index.get_indexer.html),
which takes an array of index labels and searches for their corresponding positions in the index.
For example, let's get the position of January 7th in our index:

```pycon
>>> index.get_indexer([vbt.timestamp("2021-01-07", tz=index.tz)])  # (1)!
array([144])
```

1. Be sure to provide the timezone if the index is timezone-aware!

If we look for an index that does not exist, it returns `-1`:

```pycon
>>> index.get_indexer([vbt.timestamp("2021-01-07 17:30:00", tz=index.tz)]) 
array([-1])
```

!!! warning
    Do not use the result directly for indexing if there is a chance of no match. For example,
    if any positions returned are `-1` and you use them to select timestamps, the position will be replaced
    by the latest timestamp in the index.

To get either the exact match or the previous one, pass `method='ffill'`. To get the next one, pass
`method='bfill'`:

```pycon
>>> index[index.get_indexer(
...     [vbt.timestamp("2021-01-07 17:30:00", tz=index.tz)],
...     method="ffill"
... )]
DatetimeIndex(['2021-01-07 17:00:00+00:00'], ...)

>>> index[index.get_indexer(
...     [vbt.timestamp("2021-01-07 17:30:00", tz=index.tz)],
...     method="bfill"
... )]
DatetimeIndex(['2021-01-07 18:00:00+00:00'], ...)
```

Returning to our example, we first need to generate the target index for our query to search in the
source index. Use [pandas.date_range](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html)
to get the timestamp for each Tuesday at midnight, then add a timedelta of 17 hours and 30 minutes.
Next, convert the target index to positions (row indices) where our signals will be placed.
Then, extract the Pandas symbol wrapper from our data instance and use it to create a new mask
with the same number of columns as our symbols. Finally, set `True` at the generated positions
of that mask:

```pycon
>>> each_tuesday = vbt.date_range(index[0], index[-1], freq="tuesday")  # (1)!
>>> each_tuesday_1730 = each_tuesday + pd.Timedelta(hours=17, minutes=30)  # (2)!
>>> each_tuesday_1730
DatetimeIndex(['2021-01-05 17:30:00+00:00', '2021-01-12 17:30:00+00:00',
                '2021-01-19 17:30:00+00:00', '2021-01-26 17:30:00+00:00'],
              dtype='datetime64[ns, UTC]', freq=None)

>>> positions = index.get_indexer(each_tuesday_1730, method="bfill")

>>> min_symbol_wrapper = min_data.get_symbol_wrapper()  # (3)!
>>> mask = min_symbol_wrapper.fill(False)  # (4)!
>>> mask.iloc[positions] = True  # (5)!
>>> mask.sum()
symbol
BTCUSDT    4
ETHUSDT    4
dtype: int64
```

1. The timezone is included in both timestamps and will be used automatically.

2. Adding a timedelta to a datetime creates a new datetime.

3. Use [Data.get_symbol_wrapper](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get_symbol_wrapper)
to get a Pandas wrapper with symbols as columns.

4. Use [ArrayWrapper.fill](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.fill)
to create an array the same shape as the wrapper and fill it with `False` values.

5. Positions are integer row indices, so use `iloc` to select the elements at those rows.

Let's confirm that all signals match 18:00 on Tuesday, which is the first date after the requested
17:30 on Tuesday in an hourly index:

```pycon
>>> mask[mask.any(axis=1)].index.strftime("%A %T")  # (1)!
Index(['Tuesday 18:00:00', 'Tuesday 18:00:00', 'Tuesday 18:00:00',
        'Tuesday 18:00:00'],
      dtype='object', name='Open time')
```

1. Details for the string format can be found [here](https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior).

The solution above is only needed if you know just a single time boundary. For example,
if you want 17:30 on Tuesday or later, you know only the left boundary, while the right boundary is infinity
(or there may be no data after this datetime at all). When both time boundaries are known,
you can easily use the first approach and combine it with VBT's signal selection mechanism.
For example, place a signal at 17:00 on Tuesday or later, but not later than 17:00 on Wednesday.
This means placing signals from the left boundary up to the right boundary,
then selecting the first signal in that partition:

```pycon
>>> tuesday_after_1700 = (index.weekday == 1) & (index.hour >= 17)
>>> wednesday_before_1700 = (index.weekday == 2) & (index.hour < 17)
>>> main_cond = tuesday_after_1700 | wednesday_before_1700
>>> mask = min_symbol_wrapper.fill(False)
>>> mask[main_cond] = True
>>> mask = mask.vbt.signals.first()
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Tuesday 17:00:00', 'Tuesday 17:00:00', 'Tuesday 17:00:00',
        'Tuesday 17:00:00'],
      dtype='object', name='Open time')
```

The third and final approach is the VBT way :heart_on_fire:

This method uses the two accessor methods [BaseAccessor.set](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set)
and [BaseAccessor.set_between](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set_between),
which let you set elements of an array conditionally in a more intuitive way.

Place a signal at 17:30 each Tuesday or later:

```pycon
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set(
...     True, 
...     every="tuesday", 
...     at_time="17:30", 
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Tuesday 18:00:00', 'Tuesday 18:00:00', 'Tuesday 18:00:00',
        'Tuesday 18:00:00'],
      dtype='object', name='Open time')
```

Place a signal after 18:00 each Tuesday (exclusive):

```pycon
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set(
...     True, 
...     every="tuesday", 
...     at_time="18:00", 
...     add_delta=pd.Timedelta(nanoseconds=1),  # (1)!
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Tuesday 19:00:00', 'Tuesday 19:00:00', 'Tuesday 19:00:00',
        'Tuesday 19:00:00'],
      dtype='object', name='Open time')
```

1. Add a nanosecond to exclude 18:00.

Fill signals between 12:00 each Monday and 17:00 each Tuesday:

```pycon
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set_between(
...     True, 
...     every="monday", 
...     start_time="12:00", 
...     end_time="17:00", 
...     add_end_delta=pd.Timedelta(days=1),  # (1)!
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index.strftime("%A %T")
Index(['Monday 12:00:00', 'Monday 13:00:00', 'Monday 14:00:00',
        'Monday 15:00:00', 'Monday 16:00:00', 'Monday 17:00:00',
        'Monday 18:00:00', 'Monday 19:00:00', 'Monday 20:00:00',
        ...
        'Tuesday 10:00:00', 'Tuesday 11:00:00', 'Tuesday 12:00:00',
        'Tuesday 13:00:00', 'Tuesday 14:00:00', 'Tuesday 15:00:00',
        'Tuesday 16:00:00'],
      dtype='object', name='Open time', length=116)
```

1. Add a day to reach Wednesday.

Place a signal exactly at midnight on January 7th, 2021:

```pycon
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set(
...     True, 
...     on="January 7th 2021 UTC",  # (1)!
...     indexer_method=None,  # (2)!
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index
DatetimeIndex(['2021-01-07 00:00:00+00:00'], ...)
```

1. Human-readable datetime strings are accepted.

2. The default method is `bfill`, which selects the next timestamp if there is no exact match.

Fill signals between 12:00 on January 1st/7th and 12:00 on January 2nd/8th, 2021:

```pycon
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set_between(
...     True, 
...     start=["2021-01-01 12:00:00", "2021-01-07 12:00:00"],  # (1)!
...     end=["2021-01-02 12:00:00", "2021-01-08 12:00:00"],
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index
DatetimeIndex(['2021-01-01 12:00:00+00:00', '2021-01-01 13:00:00+00:00',
                '2021-01-01 14:00:00+00:00', '2021-01-01 15:00:00+00:00',
                '2021-01-01 16:00:00+00:00', '2021-01-01 17:00:00+00:00',
                ...
                '2021-01-08 06:00:00+00:00', '2021-01-08 07:00:00+00:00',
                '2021-01-08 08:00:00+00:00', '2021-01-08 09:00:00+00:00',
                '2021-01-08 10:00:00+00:00', '2021-01-08 11:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. The first range is built from the first element in `start` and `end`, and the second range
is built from the second element. You can also use human-readable datetime strings,
but those take extra time to map (to enable, see [settings.datetime](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.datetime)).

Fill signals in the first two hours of each week:

```pycon
>>> mask = min_symbol_wrapper.fill(False)
>>> mask.vbt.set_between(
...     True, 
...     every="monday",
...     split_every=False,  # (1)!
...     add_end_delta="2h",
...     inplace=True
... )
>>> mask[mask.any(axis=1)].index
DatetimeIndex(['2021-01-04 00:00:00+00:00', '2021-01-04 01:00:00+00:00',
                '2021-01-11 00:00:00+00:00', '2021-01-11 01:00:00+00:00',
                '2021-01-18 00:00:00+00:00', '2021-01-18 01:00:00+00:00',
                '2021-01-25 00:00:00+00:00', '2021-01-25 01:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Otherwise, ranges will be built from each pair of week starts.

See the API documentation for more examples.

## Iteratively

With Numba, we can write iterative logic that is just as fast as vectorized counterparts.
But which approach is better? There is no definitive answer. Overall, using vectors is often more
effective and more user-friendly because it removes the need to loop over data and automates
many mechanisms related to indexes and columns. Consider how powerful broadcasting is,
and how many extra lines of code it would take to implement the same thing iteratively.
Numba also does not allow us to work with labels or complex data types, only with numeric data,
which means you need skills and creativity to design efficient algorithms.

Most vectorized functions, as well as non-vectorized but compiled functions,
are specifically tailored for one particular task and perform it reliably. When writing your own
loop, however, you are responsible for fully implementing the required logic. Vectors are like Lego
bricks that let you easily build even the most impressive creations, while custom loops require
you to learn how to manufacture each Lego brick first :bricks:

Nevertheless, most of the functionality in VBT is powered by loops, not vectors—it should really
be called loopbt :grimacing: The main reason is simple: most operations cannot be performed
using vectors because they may introduce path dependencies, need complex data structures,
require intermediate calculations or data buffers, call third-party functions periodically,
or all of these at once. Efficiency is also a factor: you can design algorithms that loop over the data
[only once](https://en.wikipedia.org/wiki/One-pass_algorithm), whereas the same logic with vectors
may read the whole dataset a dozen times. This also applies to memory usage! Lastly,
defining and running a strategy at each time step is exactly how you would do it in the real world
(and in any other backtesting framework), and as traders, we should aim to mimic real-world
behavior as closely as possible.

Enough with theory! Let's implement the first example from [Logical operators](#logical-operators)
using a custom loop. Unless your signals rely on multiple assets or another column grouping,
you should always start with just one column:

```pycon
>>> @njit  # (1)!
... def generate_mask_1d_nb(  # (2)!
...     high, low,  # (3)!
...     uband, mband, lband,  # (4)!
...     cond2_th, cond4_th  # (5)!
... ):
...     out = np.full(high.shape, False)  # (6)!
...     
...     for i in range(high.shape[0]):  # (7)!
...         # (8)!
...         bandwidth = (uband[i] - lband[i]) / mband[i]
...         cond1 = low[i] < lband[i]
...         cond2 = bandwidth > cond2_th
...         cond3 = high[i] > uband[i]
...         cond4 = bandwidth < cond4_th
...         signal = (cond1 and cond2) or (cond3 and cond4)  # (9)!
...         out[i] = signal  # (10)!
...         
...     return out

>>> mask = generate_mask_1d_nb(
...     data.get("High")["BTCUSDT"].values,  # (11)!
...     data.get("Low")["BTCUSDT"].values,
...     bb.upperband["BTCUSDT"].values,
...     bb.middleband["BTCUSDT"].values,
...     bb.lowerband["BTCUSDT"].values,
...     0.30,
...     0.15
... )
>>> symbol_wrapper = data.get_symbol_wrapper()
>>> mask = symbol_wrapper["BTCUSDT"].wrap(mask)  # (12)!
>>> mask.sum()
25
```

1. Remember to use `@njit` to Numba-compile the function.

2. A good convention is to use the `nb` suffix for Numba-compiled functions, and `1d_nb` for those that
work on just one column of data.

3. These are the data arrays required for our logic.

4. These are the Bollinger Bands arrays needed by our logic.

5. Thresholds. These can also be set as keyword arguments with default values.

6. Create a boolean array matching the shape of your data arrays, filling it with the default value
`False` (meaning no signal).

7. Iterate over the rows, each representing a timestamp.

8. This is where the main logic goes. Perform all operations one element at a time instead of on arrays,
which is great for memory efficiency.

9. When working with single values, use `and` instead of `&`, `or` instead of `|`, and `not` instead of `~`.

10. Write the current signal to the array.

11. Because we are using a one-dimensional Numba-compiled function, we must pass one-dimensional NumPy arrays.

12. Since the mask is generated only for the `BTCUSDT` symbol, select that column from the wrapper and wrap
the array.

We now have the same number of signals as before—magic!

To make the function work on multiple columns, you can write another Numba-compiled function that loops
over the columns and calls `generate_mask_1d_nb` for each one:

```pycon
>>> @njit
... def generate_mask_nb(  # (1)!
...     high, low,
...     uband, mband, lband,
...     cond2_th, cond4_th
... ):
...     out = np.empty(high.shape, dtype=np.bool_)  # (2)!
...     
...     for col in range(high.shape[1]):  # (3)!
...         out[:, col] = generate_mask_1d_nb(  # (4)!
...             high[:, col], low[:, col],
...             uband[:, col], mband[:, col], lband[:, col],
...             cond2_th, cond4_th
...         )
...         
...     return out

>>> mask = generate_mask_nb(
...     vbt.to_2d_array(data.get("High")),  # (5)!
...     vbt.to_2d_array(data.get("Low")),
...     vbt.to_2d_array(bb.upperband),
...     vbt.to_2d_array(bb.middleband),
...     vbt.to_2d_array(bb.lowerband),
...     0.30,
...     0.15
... )
>>> mask = symbol_wrapper.wrap(mask)
>>> mask.sum()
symbol
BTCUSDT    25
ETHUSDT    13
dtype: int64
```

1. Remove `1d` from the name if the function takes two-dimensional arrays.

2. Create an empty boolean array to hold the results from `generate_mask_1d_nb`.
If you use `np.empty` instead of `np.full`, be sure to override every value. Otherwise,
unevaluated elements will remain uninitialized and be essentially garbage.

3. Iterate over the columns, which represent assets.

4. Select the current column from each array, and call the one-dimensional function.

5. Use [to_2d_array](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.to_2d_array)
to cast any array to two dimensions and convert it to NumPy.

A more "vectorbtonic" approach is to create a stand-alone indicator where you specify
the function, the required data, and what it returns. The indicator factory then takes care of everything else!

```pycon
>>> MaskGenerator = vbt.IF(  # (1)!
...     input_names=["high", "low", "uband", "mband", "lband"],
...     param_names=["cond2_th", "cond4_th"],
...     output_names=["mask"]
... ).with_apply_func(generate_mask_1d_nb, takes_1d=True)  # (2)!
>>> mask_generator = MaskGenerator.run(  # (3)!
...     data.get("High"),
...     data.get("Low"),
...     bb.upperband,
...     bb.middleband,
...     bb.lowerband,
...     [0.3, 0.4],
...     [0.1, 0.2],
...     param_product=True  # (4)!
... )
>>> mask_generator.mask.sum()
custom_cond2_th  custom_cond4_th  symbol 
0.3              0.1              BTCUSDT    11
                                  ETHUSDT    10
                  0.2              BTCUSDT    28
                                  ETHUSDT    27
0.4              0.1              BTCUSDT     9
                                  ETHUSDT     5
                  0.2              BTCUSDT    26
                                  ETHUSDT    22
dtype: int64
```

1. Create the indicator's interface and specify the apply function.

2. You could also use `generate_mask_nb` with `takes_1d=False`.

3. Run the indicator. Notice you do not need to worry about the correct type and shape of each array!

4. Test the Cartesian product of different threshold combinations.

How about shifting and testing truth values? Simple use cases like fixed shifts and windows are
easy to implement. Below, we compare the current value to the value a certain number of steps earlier:

```pycon
>>> @njit
... def value_ago_1d_nb(arr, ago):
...     out = np.empty(arr.shape, dtype=float_)  # (1)!
...     for i in range(out.shape[0]):
...         if i - ago >= 0:  # (2)!
...             out[i] = arr[i - ago]
...         else:
...             out[i] = np.nan  # (3)!
...     return out

>>> arr = np.array([1, 2, 3])
>>> value_ago_1d_nb(arr, 1)
array([nan, 1., 2.])
```

1. Use `np.empty` only if you guarantee that each array element will be set. Also, when your data involves
NaN values, set the dtype to float!

2. Before accessing a previous element, make sure it is within the bounds of the array.

3. If the previous element is outside the array, set its value to NaN.

!!! important
    Always check that the element you access is within array bounds.
    Unless you have enabled `NUMBA_BOUNDSCHECK`, Numba will not raise an error if you try to
    access a non-existent element. Instead, it will silently continue the calculation, and your kernel
    may eventually crash. If this happens, restart your kernel, disable Numba, or enable bounds
    checking to locate the bug and rerun your function.

Here is how you can test if any condition was true within a fixed window (a variable time interval):

```pycon
>>> @njit
... def any_in_window_1d_nb(arr, window):
...     out = np.empty(arr.shape, dtype=np.bool_)  # (1)!
...     for i in range(out.shape[0]):
...         from_i = max(0, i + 1 - window)  # (2)!
...         to_i = i + 1  # (3)!
...         out[i] = np.any(arr[from_i:to_i])  # (4)!
...     return out

>>> arr = np.array([False, True, True, False, False])
>>> any_in_window_1d_nb(arr, 2)
array([False, True, True, True, False])
```

1. You can use a boolean empty array if no NaN values are present.

2. Get the left bound of the window.

3. Get the right bound of the window.

4. Use these bounds to select all elements in the window and check if any of them is `True`.

Once dates and times are involved, such as when you want to compare the current value
to the value exactly 5 days earlier, it is better to pre-calculate as many intermediate
steps as possible. However, you can also work directly with a datetime-like index in Numba.
Here is how you can test if any condition was true inside a variable window (a fixed time interval):

```pycon
>>> @njit
... def any_in_var_window_1d_nb(arr, index, freq):  # (1)!
...     out = np.empty(arr.shape, dtype=np.bool_)
...     from_i = 0
...     for i in range(out.shape[0]):
...         if index[from_i] <= index[i] - freq:  # (2)!
...             for j in range(from_i + 1, index.shape[0]):  # (3)!
...                 if index[j] > index[i] - freq:
...                     from_i = j
...                     break  # (4)!
...         to_i = i + 1
...         out[i] = np.any(arr[from_i:to_i])
...     return out

>>> arr = np.array([False, True, True, False, False])
>>> index = vbt.date_range("2020", freq="5min", periods=len(arr)).values  # (5)!
>>> freq = vbt.timedelta("10min").to_timedelta64()  # (6)!
>>> any_in_var_window_1d_nb(arr, index, freq)
array([False, True, True, True, False])
```

1. Use an array of type `np.datetime64` as the index and a constant of type `np.timedelta64` as the frequency.

2. Test if the previous left bound is still valid, meaning it falls within the current time interval.

3. If not, find the new left bound with a separate loop over the index.

4. Once found, set `from_i` to the new left bound and exit the second loop.

5. Create an index with a 5-minute timeframe and convert it to a NumPy datetime array.

6. Define a frequency of 10 minutes and convert it to a NumPy timedelta value.

!!! tip
    In general, it is easier to design iterative functions using regular Python and only compile
    them with Numba after sufficient testing, since debugging is simpler in Python than in Numba.

Keep in mind that Numba (and VBT) supports many more features for processing numeric data
than for datetime or timedelta data. Fortunately, you can safely convert datetime and timedelta data
to integers outside of Numba, and most functions will keep working as before:

```pycon
>>> any_in_var_window_1d_nb(arr, vbt.dt.to_ns(index), vbt.dt.to_ns(freq))
array([False, True, True, True, False])
```

Why does this work? By converting datetime or timedelta to an integer, we get the total number
of nanoseconds that represent the object. For a datetime, this value is the number of nanoseconds
since the [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time), which is 00:00:00 UTC on January 1, 1970:

```pycon
>>> vbt.dt.to_ns(index)  # (1)!
array([1577836800000000000, 1577837100000000000, 1577837400000000000,
        1577837700000000000, 1577838000000000000])
        
>>> vbt.dt.to_ns(index - np.datetime64(0, "ns")) # (2)!
array([1577836800000000000, 1577837100000000000, 1577837400000000000,
        1577837700000000000, 1577838000000000000])

>>> vbt.dt.to_ns(freq)  # (3)!
600000000000

>>> vbt.dt.to_ns(freq) / 1000 / 1000 / 1000 / 60  # (4)!
10.0
```

1. The index as a timedelta in nanoseconds after 1970.

2. This is the same as converting a datetime to a timedelta by subtracting the Unix Epoch.

3. Frequency as a timedelta in nanoseconds.

4. Convert nanoseconds into minutes.

## Generators

Writing custom loops is powerful and enjoyable, but VBT also provides functions that can
make things easier—especially for generating signals. The most flexible helper function is the Numba-compiled
[generate_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.generate_nb) and its accessor class method
[SignalsAccessor.generate](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate).
These take a target shape, initialize a boolean output array filled with `False` values, then
iterate over columns. For each column, they call a "placement function", which is a regular UDF
that modifies the mask in place. After making changes, the placement function should return either
the position of the last placed signal or `-1` if there is no signal.

All context information for the current iteration is passed via a context of type
[GenEnContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.GenEnContext). This context provides
the current output mask segment to modify, the inclusive range start, exclusive range end,
the column, and the full output mask. This lets you make patches wherever needed. With this
approach, VBT manages the array setup and column loops, and helps you select the correct
subset of output data to modify.

Let's place a signal at 17:00 (UTC) every Tuesday:

```pycon
>>> @njit
... def place_func_nb(c, index):  # (1)!
...     last_i = -1  # (2)!
...     for out_i in range(len(c.out)):  # (3)!
...         i = c.from_i + out_i  # (4)!
...         weekday = vbt.dt_nb.weekday_nb(index[i])  # (5)!
...         hour = vbt.dt_nb.hour_nb(index[i])
...         if weekday == 1 and hour == 17:  # (6)!
...             c.out[out_i] = True  # (7)!
...             last_i = out_i
...     return last_i  # (8)!

>>> mask = vbt.pd_acc.signals.generate(  # (9)!
...     symbol_wrapper.shape,  # (10)!
...     place_func_nb,
...     vbt.dt.to_ns(symbol_wrapper.index),  # (11)!
...     wrapper=symbol_wrapper  # (12)!
... )
>>> mask.sum()
symbol
BTCUSDT    0
ETHUSDT    0
dtype: int64
```

1. The placement function receives a context and any other user-defined arguments.

2. Create a variable to track the position of the last placed signal (if any).

3. Iterate over the output array with a local index.

4. Get the global index to retrieve the current timestamp. For example, if the array segment
has 3 elements (`len(out)`) and the start index is 2 (`from_i`), then the first element
is at position 2 (`i`) and the last at position 5.

5. [dt_nb](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_nb/) provides Numba-compiled functions for working with datetimes
and timedeltas, but usually expects integer representations!

6. Check if the current timestamp is 17:00 on a Tuesday.

7. If it is, place a signal using the local index; otherwise, continue to the next timestamp.

8. Make sure the returned index is local, not global.

9. Call [SignalsAccessor.generate](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate)
as a class method of [SignalsAccessor](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor).

10. Provide the shape to iterate over.

11. Provide the index in integer form (total number of nanoseconds).

12. Provide the wrapper to wrap the resulting NumPy array.

!!! info
    Segments in [generate_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.generate_nb)
    are always full columns.

But, since our index is a daily index, there will not be any signal. Instead, let's place a signal
at the next available timestamp:

```pycon
>>> @njit
... def place_func_nb(c, index):
...     last_i = -1
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         weekday = vbt.dt_nb.weekday_nb(index[i])
...         hour = vbt.dt_nb.hour_nb(index[i])
...         if weekday == 1 and hour == 17:
...             c.out[out_i] = True
...             last_i = out_i
...         else:
...             past_target_midnight = vbt.dt_nb.past_weekday_nb(index[i], 1)  # (1)!
...             past_target = past_target_midnight + 17 * vbt.dt_nb.h_ns  # (2)!
...             if (i > 0 and index[i - 1] < past_target) and \
...                 index[i] > past_target:  # (3)!
...                 c.out[out_i] = True
...                 last_i = out_i
...     return last_i

>>> mask = vbt.pd_acc.signals.generate(
...     symbol_wrapper.shape,
...     place_func_nb,
...     vbt.dt.to_ns(symbol_wrapper.index),
...     wrapper=symbol_wrapper
... )
>>> mask.sum()
symbol
BTCUSDT    52
ETHUSDT    52
dtype: int64

>>> mask.index[mask.any(axis=1)].strftime('%A %m/%d/%Y')  # (4)!
Index(['Thursday 01/07/2021', ..., 'Thursday 12/30/2021'],
      dtype='object', name='Open time')
```

1. Get the timestamp of midnight on the previous Tuesday.

2. Get the timestamp for 17:00 on the previous Tuesday.

3. Check if the previous timestamp was before and the current timestamp is after the target time.

4. All generated signals should occur on Wednesdays.

The key point in the code above is that all datetime logic is handled using simple integers!

!!! important
    When converting to integer format, each datetime object's timezone is effectively set to UTC.
    So, make sure any value compared to the UTC timestamp is also in UTC.

But what if you want to support multiple parameter combinations? We cannot use the above function
with the indicator factory because it does not look like an apply function. Fortunately, VBT
offers a dedicated indicator factory subclass for signal generation:
[SignalFactory](https://vectorbt.pro/pvt_41531c19/api/signals/factory/#vectorbtpro.signals.factory.SignalFactory).
This class supports several generation modes, set by the `mode` argument of type
[FactoryMode](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.FactoryMode). In this case, the mode is
`FactoryMode.Entries` because our function generates signals based only on the target shape, not on
other signal arrays. The signal factory can accept any combination of inputs, parameters, and in-place outputs
to build the skeleton of our new indicator class.

The signal factory includes the class method [SignalFactory.with_place_func](https://vectorbt.pro/pvt_41531c19/api/signals/factory/#vectorbtpro.signals.factory.SignalFactory.with_place_func),
which is similar to the familiar `from_apply_func`. It takes a placement function and
generates a custom function that performs all the necessary pre- and post-processing for
[generate_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.generate_nb).
(Note that other modes may use different generation functions.) This custom function, for example,
prepares arguments and assigns them to the correct positions in the placement function call.
It is then passed to [IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func).
As a result, you get an indicator class with a `run` method you can use with any custom
shape or parameter grid. Convenient, right?

Let's parameterize the exact-match placement function with two parameters: weekday and hour.

```pycon
>>> @njit
... def place_func_nb(c, weekday, hour, index):  # (1)!
...     last_i = -1
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         weekday_now = vbt.dt_nb.weekday_nb(index[i])
...         hour_now = vbt.dt_nb.hour_nb(index[i])
...         if weekday_now == weekday and hour_now == hour:
...             c.out[out_i] = True
...             last_i = out_i
...     return last_i

>>> EntryGenerator = vbt.SignalFactory(
...     mode="entries",
...     param_names=["weekday", "hour"]
... ).with_place_func(
...     entry_place_func_nb=place_func_nb,  # (2)!
...     entry_settings=dict(  # (3)!
...         pass_params=["weekday", "hour"],
...     ),
...     var_args=True  # (4)!
... )
>>> entry_generator = EntryGenerator.run(
...     symbol_wrapper.shape,  # (5)!
...     1, 
...     [0, 17],  # (6)!
...     vbt.dt.to_ns(symbol_wrapper.index),  # (7)!
...     input_index=symbol_wrapper.index,  # (8)!
...     input_columns=symbol_wrapper.columns
... )
>>> entry_generator.entries.sum()
custom_weekday  custom_hour  symbol 
2               0            BTCUSDT    52
                              ETHUSDT    52
                17           BTCUSDT     0
                              ETHUSDT     0
dtype: int64
```

1. Each indicator function must first accept the input shape, then (optionally)
inputs, in-place outputs, parameters, and finally any additional arguments.

2. In `FactoryMode.Entries`, provide your placement function as `entry_place_func_nb`.

3. Provide settings to ensure specific inputs, in-place outputs, and parameters are passed to
the placement function. If omitted, nothing will be passed!

4. Enable variable arguments to allow passing the index as an extra positional argument.

5. Input shape must be given if neither inputs nor in-place outputs provide it.

6. Test two time combinations: 00:00 and 17:00.

7. Possible thanks to `var_args`.

8. Pass Pandas metadata for wrapping output arrays.

!!! note
    The `FactoryMode.Entries` mode does not require you to generate only entry signals during simulation.
    You can produce any mask, including exits, as long as they do not depend on entries.

The indicator function matches all midnight times but not afternoon times, as expected,
since our index is daily and only contains midnight times. You can easily plot the indicator
with the attached `plot` method, which knows how to visualize each mode:

```pycon
>>> entry_generator.plot(column=(1, 0, "BTCUSDT")).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/signal_factory.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/signal_factory.dark.svg#only-dark){: .iimg loading=lazy }

### Exits

After creating the position entry mask, we need to determine the position exit mask. When exits are
independent of entries, we can use the generator introduced above. However, in some cases, the exit
logic is entirely dependent on the entry signal. For example, a stop loss exit signal only exists
because of the entry signal that triggered the stop loss. There is also no guarantee that each entry
will have a corresponding exit. Therefore, this mode should only be used when entries do not depend
on exits, but exits depend on entries. In this case, generation is performed using the Numba-compiled
function [generate_ex_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.generate_ex_nb) and its accessor
instance method [SignalsAccessor.generate_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_exits).
The provided context is now of type [GenExContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.GenExContext)
and includes the input mask along with other generator-related arguments.

The generator receives an entry mask array and, for each column, visits every entry signal,
calling a UDF to place one or more exit signals that follow it. Remember how we accepted `from_i`
and `to_i` in the placement functions above? The previous mode always passed `0` as `from_i` and
`len(index)` as `to_i` because we could set our signals anywhere in the entire column.
Here, `from_i` is usually the position immediately after the previous entry, while `to_i` is
typically the index of the next entry, which limits our decision range to the segment between each
pair of entries.

!!! warning
    Be aware that knowing the position of the next entry signal may introduce look-ahead bias.
    Use it only for iteration purposes, and never set data based on `to_i`!

Let's generate an entry each quarter and an exit at the following date:

```pycon
>>> @njit
... def exit_place_func_nb(c):
...     c.out[0] = True  # (1)!
...     return 0

>>> entries = symbol_wrapper.fill(False)
>>> entries.vbt.set(True, every="Q", inplace=True)
>>> entries.index[entries.any(axis=1)]
DatetimeIndex(['2021-03-31 00:00:00+00:00', '2021-06-30 00:00:00+00:00',
                '2021-09-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
              
>>> exits = entries.vbt.signals.generate_exits(exit_place_func_nb)  # (2)!
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-04-01 00:00:00+00:00', '2021-07-01 00:00:00+00:00',
                '2021-10-01 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Place an exit at the first position in the segment.

2. Unlike [SignalsAccessor.generate](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate),
this method is bound to a Pandas object containing entries and uses its metadata.

We can adjust the distance to the entry signal using the `wait` parameter, which defaults to 1.
Let's instruct VBT to begin each segment at the same timestamp as the entry:

```pycon
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     wait=0
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-03-31 00:00:00+00:00', '2021-06-30 00:00:00+00:00',
                '2021-09-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

Next, let's implement a variable waiting time based on a frequency. We will wait exactly 7 days
before placing an exit:

```pycon
>>> @njit
... def exit_place_func_nb(c, index, wait_td):
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if index[i] >= index[c.from_i] + wait_td:  # (1)!
...             return out_i  # (2)!
...     return -1

>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),  # (3)!
...     vbt.dt.to_ns(vbt.timedelta("7d")),
...     wait=0
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-04-07 00:00:00+00:00', '2021-07-07 00:00:00+00:00',
                '2021-10-07 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Check whether enough time has passed since the entry. For `c.from_i`
to correspond to the entry's index, set `wait` to zero.

2. By returning the index, the generator will automatically set the respective element to True.

3. Additional arguments are provided here.

But what happens to the exit for an entry if the next entry occurs less than 7 days later?
Will an exit still be placed? No!

```pycon
>>> entries = symbol_wrapper.fill(False)
>>> entries.vbt.set(True, every="5d", inplace=True)
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),
...     vbt.dt.to_ns(vbt.timedelta("7d")),
...     wait=0
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex([], dtype='datetime64[ns, UTC]', name='Open time', freq='D')
```

By default, each segment is limited to the space between two surrounding entries.
To make this segment infinite, you can disable `until_next`:

```pycon
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),
...     vbt.dt.to_ns(vbt.timedelta("7d")),
...     wait=0,
...     until_next=False
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-01-08 00:00:00+00:00', '2021-01-13 00:00:00+00:00',
                '2021-01-18 00:00:00+00:00', '2021-01-23 00:00:00+00:00',
                '2021-01-28 00:00:00+00:00', '2021-02-02 00:00:00+00:00',
                ...
                '2021-12-04 00:00:00+00:00', '2021-12-09 00:00:00+00:00',
                '2021-12-14 00:00:00+00:00', '2021-12-19 00:00:00+00:00',
                '2021-12-24 00:00:00+00:00', '2021-12-29 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

!!! note
    In this case, it may be impossible to determine which exit belongs to which entry.
    Two or more entries may generate an exit at the same timestamp, so be careful!

In this example, the generated signals follow the sequence: `entry1`, `entry2`, `exit1`, `entry3`,
`exit2`, and so on. If you pass these signals to the simulator, it will execute `entry1` and ignore
`entry2` because there was no prior exit—you are still in the market. It will then properly execute
`exit1`. Afterward, it will open a new position with `entry3` and immediately close it with `exit2`,
which was originally intended for `entry2` (but that entry was ignored). To prevent this,
enable `skip_until_exit` to avoid processing any new entry signal that comes before an exit for a
previous entry signal. This setup matches the simulation order.

```pycon
>>> exits = entries.vbt.signals.generate_exits(
...     exit_place_func_nb,
...     vbt.dt.to_ns(entries.index),
...     vbt.dt.to_ns(vbt.timedelta("7d")),
...     wait=0,
...     until_next=False,
...     skip_until_exit=True
... )
>>> exits.index[exits.any(axis=1)]
DatetimeIndex(['2021-01-08 00:00:00+00:00', '2021-01-18 00:00:00+00:00',
                '2021-01-28 00:00:00+00:00', '2021-02-07 00:00:00+00:00',
                '2021-02-17 00:00:00+00:00', '2021-02-27 00:00:00+00:00',
                ...
                '2021-11-04 00:00:00+00:00', '2021-11-14 00:00:00+00:00',
                '2021-11-24 00:00:00+00:00', '2021-12-04 00:00:00+00:00',
                '2021-12-14 00:00:00+00:00', '2021-12-24 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

!!! note
    Always make sure to use `skip_until_exit` together with `until_next` set to False.

Finally, to make the process parametrizable, use `FactoryMode.Exits` as the mode and provide any
supporting information with the prefix `exit`:

```pycon
>>> @njit
... def exit_place_func_nb(c, wait_td, index):  # (1)!
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if index[i] >= index[c.from_i] + wait_td:
...             return out_i
...     return -1

>>> ExitGenerator = vbt.SignalFactory(
...     mode="exits",
...     param_names=["wait_td"]
... ).with_place_func(
...     exit_place_func_nb=exit_place_func_nb,
...     exit_settings=dict(
...         pass_params=["wait_td"],
...     ),
...     var_args=True,
...     wait=0,  # (2)!
...     until_next=False,
...     skip_until_exit=True,
...     param_settings=dict(  # (3)!
...         wait_td=dict(
...             post_index_func=lambda x: x.map(lambda y: str(vbt.timedelta(y)))
...         )
...     ),
... )
>>> exit_generator = ExitGenerator.run(
...     entries,  # (4)!
...     [
...         vbt.timedelta("3d").to_timedelta64(),  # (5)!
...         vbt.timedelta("7d").to_timedelta64()
...     ],
...     symbol_wrapper.index.values
... )
>>> exit_generator.exits.sum()
custom_wait_td   symbol 
3 days 00:00:00  BTCUSDT    73
                  ETHUSDT    73
7 days 00:00:00  BTCUSDT    36
                  ETHUSDT    36
dtype: int64
```

1. Remember to switch the order of the parameters and user-defined arguments.

2. Parameters for the generator can be passed as regular keyword arguments, including to the `run` method.

3. Convert the index level with timedeltas to strings so you can easily select different values.
The `post_index_func` function must take the original index and return a new index.

4. There is no need to provide the input shape or Pandas metadata because both can be derived
from the entries array.

5. Test two timedelta combinations. Before passing, convert the index and timedeltas to NumPy.
Here, you can use datetimes and timedeltas directly, without converting them to integers.

If desired, you can remove redundant entries:

```pycon
>>> new_entries = exit_generator.entries.vbt.signals.first(  # (1)!
...     reset_by=exit_generator.exits,  # (2)!
...     allow_gaps=True,  # (3)!
... )
>>> new_entries.index[new_entries[("7 days 00:00:00", "BTCUSDT")]]
DatetimeIndex(['2021-01-01 00:00:00+00:00', '2021-01-11 00:00:00+00:00',
                '2021-01-21 00:00:00+00:00', '2021-01-31 00:00:00+00:00',
                '2021-02-10 00:00:00+00:00', '2021-02-20 00:00:00+00:00',
                ...
                '2021-11-17 00:00:00+00:00', '2021-11-27 00:00:00+00:00',
                '2021-12-07 00:00:00+00:00', '2021-12-17 00:00:00+00:00',
                '2021-12-27 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Use [SignalsAccessor.first](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first)
to select the first `True` value in each partition.

2. Reset the partition when an exit signal is found.

3. The partition may contain `False` values.

After this step, each exit is guaranteed to occur after the entry it was generated for.

### Both

Instead of generating entry and exit signals separately, we can merge them. This approach is
especially useful when an exit depends on an entry and an entry also depends on an exit.
This kind of logic can be implemented using the Numba-compiled function
[generate_enex_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.generate_enex_nb) and its accessor class
method [SignalsAccessor.generate_both](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_both).
The generation process works as follows: first, two empty output masks are created for entries and
exits. Then, for each column, the entry placement function is called to generate one or more entry
signals. The generator then finds the position of the last generated entry signal and calls the exit
placement function on the segment that follows that entry. Next, the entry placement function runs
again. This cycle repeats until the column is completely traversed. The provided context is of type
[GenEnExContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.GenEnExContext) and includes all
relevant information for the current turn and iteration.

Let's demonstrate the full capability of this method by placing an entry when the price falls
below one threshold, and an exit when the price rises above another threshold. The signals are
generated strictly one after another, and both the entry and exit prices are set to the close price.

```pycon
>>> @njit
... def entry_place_func_nb(c, low, close, th):
...     if c.from_i == 0:  # (1)!
...         c.out[0] = True
...         return 0
...     exit_i = c.from_i - c.wait  # (2)!
...     exit_price = close[exit_i, c.col]  # (3)!
...     hit_price = exit_price * (1 - th)
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if low[i, c.col] <= hit_price:  # (4)!
...             return out_i
...     return -1

>>> @njit
... def exit_place_func_nb(c, high, close, th):  # (5)!
...     entry_i = c.from_i - c.wait
...     entry_price = close[entry_i, c.col]
...     hit_price = entry_price * (1 + th)
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if high[i, c.col] >= hit_price:
...             return out_i
...     return -1

>>> entries, exits = vbt.pd_acc.signals.generate_both(  # (6)!
...     symbol_wrapper.shape,
...     entry_place_func_nb=entry_place_func_nb,
...     entry_place_args=(vbt.Rep("low"), vbt.Rep("close"), 0.1),  # (7)!
...     exit_place_func_nb=exit_place_func_nb,
...     exit_place_args=(vbt.Rep("high"), vbt.Rep("close"), 0.2),
...     wrapper=symbol_wrapper,
...     broadcast_named_args=dict(  # (8)!
...         high=data.get("High"),
...         low=data.get("Low"),
...         close=data.get("Close")
...     ),
...     broadcast_kwargs=dict(post_func=np.asarray)  # (9)!
... )

>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> entries["BTCUSDT"].vbt.signals.plot_as_entries(
...     y=data.get("Close", "BTCUSDT"), fig=fig)
>>> exits["BTCUSDT"].vbt.signals.plot_as_exits(
...     y=data.get("Close", "BTCUSDT"), fig=fig)
>>> fig.show()  # (10)!
```

1. Place the first entry signal at the initial timestamp, since there is no previous signal
for the threshold comparison.

2. Find the (global) index of the most recent opposite signal using `c.from_i - c.wait`.

3. Apply the percentage threshold to the initial close price.

4. Place an entry signal if the low price falls below the threshold.

5. For the exit placement function, note two key differences:
an exit is guaranteed to have a preceding entry, and the threshold is now above
the entry close price and must be crossed upward by the high price.

6. The accessor method is a class method, as it does not depend on any existing array
and only requires the target shape.

7. Distribute three price arrays across two functions: high, low, and close.
Use VBT's [Rep](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.Rep) template to substitute
names with their broadcasted arrays.

8. Broadcast all arrays to match the target shape.

9. Make sure to convert the broadcasted arrays to NumPy.

10. Plot the OHLC price data, entries, and exits.
Set the OHLC chart to be more transparent so the signals are easier to see.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/both.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/both.dark.svg#only-dark){: .iimg loading=lazy }

To parameterize this logic, use `FactoryMode.Both` as the mode. Since the functions require input
arrays that broadcast to the input shape, VBT will not require you to provide the input shape,
as it will automatically determine it from the input arrays:

```pycon
>>> BothGenerator = vbt.SignalFactory(
...     mode="both",
...     input_names=["high", "low", "close"],
...     param_names=["entry_th", "exit_th"]
... ).with_place_func(
...     entry_place_func_nb=entry_place_func_nb,
...     entry_settings=dict(
...         pass_inputs=["low", "close"],
...         pass_params=["entry_th"],
...     ),
...     exit_place_func_nb=exit_place_func_nb,
...     exit_settings=dict(
...         pass_inputs=["high", "close"],
...         pass_params=["exit_th"],
...     )
... )
>>> both_generator = BothGenerator.run(
...     data.get("High"),
...     data.get("Low"),
...     data.get("Close"),
...     [0.1, 0.2],
...     [0.2, 0.3],
...     param_product=True
... )
>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> both_generator.plot(
...     column=(0.1, 0.3, "BTCUSDT"), 
...     entry_y=data.get("Close", "BTCUSDT"), 
...     exit_y=data.get("Close", "BTCUSDT"), 
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/both2.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/both2.dark.svg#only-dark){: .iimg loading=lazy }

### Chained exits

In VBT, a chain refers to a special sequence of entry and exit signals where each exit
follows exactly one entry, and each entry (except the first one) follows exactly one exit.
This structure makes it simple to match each exit to its corresponding entry and vice versa.
The previous example is a perfect illustration of a chain, as each signal from threshold crossing
depends only on the most recent opposite signal. Now, suppose you have already generated an array of
entries, and each of those entries should be valid only if there was a prior exit. Otherwise,
the entry should be ignored. This scenario is very similar to using `FactoryMode.Exits` with
`skip_until_exit` enabled and `until_next` disabled.

However, the `FactoryMode.Chain` mode offers the following: it uses the
[generate_enex_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.generate_enex_nb) generator with the entry
placement function [first_place_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.first_place_nb) to select
only the first entry signal after each exit, along with any custom exit placement function.
At the end, you receive two arrays: cleaned entries (often called `new_entries`) and exits (`exits`).

It is important to note that entries and exits generated during the signal generation phase do not
need to be used as entries and exits for simulation. For example, you might generate entry signals
from a moving average crossover, where each signal mimics a limit order, and use an exit placement
function to generate signals for filling those limit orders. As a result, these newly generated
signals can be used as actual entries in your simulation. If a new "entry" signal occurs before the
previous "exit" signal, it will be ignored. You can also track the fill price with another array.

```pycon
>>> @njit
... def exit_place_func_nb(c, low, request_price, fill_price_out):
...     entry_req_price = request_price[c.from_i - c.wait, c.col]  # (1)!
...     for out_i in range(len(c.out)):
...         i = c.from_i + out_i
...         if low[i, c.col] <= entry_req_price:  # (2)!
...             fill_price_out[i, c.col] = entry_req_price
...             return out_i
...     return -1

>>> ChainGenerator = vbt.SignalFactory(
...     mode="chain",
...     input_names=["low", "request_price"],
...     in_output_names=["fill_price_out"]
... ).with_place_func(  # (3)!
...     exit_place_func_nb=exit_place_func_nb,
...     exit_settings=dict(
...         pass_inputs=["low", "request_price"],
...         pass_in_outputs=["fill_price_out"],
...     ),
...     fill_price_out=np.nan  # (4)!
... )

>>> fast_ma = vbt.talib("SMA").run(
...     data.get("Close"), 
...     vbt.Default(10), 
...     short_name="fast_ma"
... )
>>> slow_ma = vbt.talib("SMA").run(
...     data.get("Close"), 
...     vbt.Default(20), 
...     short_name="slow_ma"
... )
>>> entries = fast_ma.real_crossed_above(slow_ma)  # (5)!
>>> entries.sum()
symbol
BTCUSDT    10
ETHUSDT     8
dtype: int64

>>> chain_generator = ChainGenerator.run(
...     entries,
...     data.get("Low"),
...     data.get("Close") * (1 - 0.1)  # (6)!
... )
>>> request_mask = chain_generator.new_entries  # (7)!
>>> request_mask.sum()
symbol
BTCUSDT    4
ETHUSDT    5
dtype: int64

>>> request_price = chain_generator.request_price  # (8)!
>>> request_price[request_mask.any(axis=1)]
symbol                       BTCUSDT   ETHUSDT
Open time                                     
2021-02-04 00:00:00+00:00  33242.994  1436.103
2021-03-11 00:00:00+00:00  51995.844  1643.202
2021-04-02 00:00:00+00:00  53055.009  1920.321
2021-06-07 00:00:00+00:00  30197.511  2332.845
2021-06-15 00:00:00+00:00  36129.636  2289.186
2021-07-05 00:00:00+00:00  30321.126  1976.877
2021-07-06 00:00:00+00:00  30798.009  2090.250
2021-07-27 00:00:00+00:00  35512.083  2069.541

>>> fill_mask = chain_generator.exits  # (9)!
>>> fill_mask.sum()
symbol
BTCUSDT    3
ETHUSDT    4
dtype: int64

>>> fill_price = chain_generator.fill_price_out  # (10)!
>>> fill_price[fill_mask.any(axis=1)]
symbol                       BTCUSDT   ETHUSDT
Open time                                     
2021-03-24 00:00:00+00:00        NaN  1643.202
2021-05-19 00:00:00+00:00  33242.994  1920.321
2021-06-08 00:00:00+00:00        NaN  2332.845
2021-06-18 00:00:00+00:00  36129.636       NaN
2021-07-13 00:00:00+00:00        NaN  1976.877
2021-07-19 00:00:00+00:00  30798.009       NaN
```

1. Retrieve the limit price assigned at the entry.

2. Check if this price has been reached. If so, record the signal along with the fill price.

3. The entry function and its related settings are automatically populated by VBT.

4. If you do not specify a default, VBT will create the in-place output array using `np.empty`.
If you provide a default, VBT will use `np.full` to create the array and fill it with the default value.
We need the second option because our function does not overwrite every element.

5. Generate a signal array for limit order requests based on moving average crossovers.

6. Use a limit price set at 10% below the close price.

7. New entries contain filtered order request signals. Many signals are ignored because
they appear before the previous signal's limit price could be reached.

8. This is the input array used as the limit order price. Use the request mask
to obtain the price for each request order.

9. Exits contain signals for orders that have been filled, generated by the exit placement function.
Each fill signal matches a corresponding request signal.

10. This output array contains the order fill prices. Use the fill mask
to obtain the price for each filled order.

For example, the first limit order for `BTCUSDT` was requested on `2021-02-04` and filled on `2021-05-19`.
The first limit order for `ETHUSDT` was requested on `2021-03-11` and filled on `2021-03-24`.
To simulate this process, you can pass `fill_mask` as the entries or order size, and `fill_price` as the order price.

!!! tip
    If you want each new limit order to replace any pending one instead of ignoring pending orders, use
    `FactoryMode.Exits` and then select the last input signal before each output signal.

## Preset generators

A wide range of preset signal generators is available [here](https://vectorbt.pro/pvt_41531c19/api/signals/generators/),
which use the modes discussed above. Preset indicators are designed for specific tasks and are ready
to use without any need for custom placement functions. Their naming follows a clear convention:

* Plain generators have no suffix.

* Exit generators use the suffix `X`.

* Both generators use the suffix `NX`.

* Chain exit generators use the suffix `CX`.

### Random

Do you dislike randomness in trading? There is one important case where randomness is actually
welcome: trading strategy benchmarking. For example, comparing one RSI configuration to another is
not always meaningful, since both strategy instances might be inherently suboptimal. Choosing one
could simply mean picking the lesser of two evils. Random signals, however, open up a whole new
universe of unexplored strategies. Generating enough random signal permutations on a market can
reveal the underlying structure and behavior of that market, and may help determine whether your
trading strategy is driven by a true edge or simply by chance.

There are two types of random signal generation: count-based and probability-based. The count-based
method takes a target number of signals, `n`, to place within a certain period and guarantees to
place this number unless the period is too short. The probability-based method takes a probability,
`prob`, of placing a signal at each timestamp. If the probability is too high, a signal may be placed
at every timestamp; if too low, no signals may be placed. Both types can be used with the same
accessor methods: [SignalsAccessor.generate_random](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_random)
to spread entry signals across an entire column, [SignalsAccessor.generate_random_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_random_exits)
to distribute exit signals after each entry and before the next entry, and [SignalsAccessor.generate_random_both](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_random_both)
to chain entry and exit signals one after the other.

!!! warning
    Generating an exact number of signals may introduce look-ahead bias because it involves knowledge
    of the next opposite signal or the end of the column. Use this with care, and only when the
    placement of the last signal is predetermined, such as when trading on a per-month basis.

Let's generate a signal on average once every 10 timestamps:

```pycon
>>> btcusdt_wrapper = symbol_wrapper["BTCUSDT"]
>>> mask = vbt.pd_acc.signals.generate_random(
...     btcusdt_wrapper.shape,
...     prob=1 / 10,
...     wrapper=btcusdt_wrapper,
...     seed=42  # (1)!
... )
>>> mask_index = mask.index[mask]
>>> (mask_index[1:] - mask_index[:-1]).mean()  # (2)!
Timedelta('8 days 03:20:55.813953488')
```

1. Make the calculation deterministic.

2. Calculate the average distance between two neighboring signals in the mask.

!!! note
    The more signals you generate, the closer the average neighbor distance will be to the target average.

Now, let's generate exactly one signal each week. To do this, we will generate an "entry" signal on
each Monday and an "exit" signal as our target signal. This avoids look-ahead bias because we have
already defined the bounds of the generation period.

```pycon
>>> monday_mask = btcusdt_wrapper.fill(False)
>>> monday_mask.vbt.set(True, every="monday", inplace=True)  # (1)!
>>> mask = monday_mask.vbt.signals.generate_random_exits(wait=0)  # (2)!
>>> mask_index = mask.index[mask]
>>> mask_index.strftime("%W %A")  # (3)!
Index(['01 Tuesday', '02 Wednesday', '03 Wednesday', '04 Friday', '05 Friday',
        '06 Tuesday', '07 Thursday', '08 Tuesday', '09 Friday', '10 Saturday',
        '11 Friday', '12 Saturday', '13 Monday', '14 Friday', '15 Monday',
        ...
        '41 Wednesday', '42 Friday', '43 Thursday', '44 Sunday', '45 Sunday',
        '46 Sunday', '47 Saturday', '48 Saturday', '49 Tuesday', '50 Thursday',
        '51 Sunday', '52 Tuesday'],
      dtype='object', name='Open time')
```

1. Set `True` on each Monday using [BaseAccessor.set](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set).

2. Generate exactly one signal after the previous Monday and before the next one. With `wait=0`, we
allow placing a signal immediately after Monday midnight.

3. Print the week number and weekday of each signal.

To parameterize the number of signals and the probability, you can use indicators with the prefixes
`RAND` and `RPROB`, respectively. A powerful feature of these indicators is that they can accept
parameters as array-like objects. This means you can pass `n` per column and `prob` per column, per
row, or even per element in the target shape.

Let's use [RPROB](https://vectorbt.pro/pvt_41531c19/api/signals/generators/rprob/#vectorbtpro.signals.generators.rprob.RPROB) to
gradually generate more signals over time. We will start with a probability of 0% and end at 100%,
placing a signal at each timestamp:

```pycon
>>> prob = np.linspace(0, 1, len(symbol_wrapper.index))  # (1)!
>>> rprob = vbt.RPROB.run(
...     symbol_wrapper.shape,  # (2)!
...     vbt.Default(vbt.to_2d_pr_array(prob)),  # (3)!
...     seed=42,
...     input_index=symbol_wrapper.index,
...     input_columns=symbol_wrapper.columns
... )
>>> rprob.entries.astype(int).vbt.ts_heatmap().show()  # (4)!
```

1. Use [numpy.linspace](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)
to fill the probability values between 0 and 1.

2. Provide the input shape since the indicator does not take input arrays.

3. Use [to_2d_pr_array](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.to_2d_pr_array) to
expand the array to two dimensions so each value matches a row rather than a column
(see broadcasting rules). Wrap this with [Default](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Default)
to hide the parameter.

4. Plot the final distribution.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/rprob.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/rprob.dark.svg#only-dark){: .iimg loading=lazy }

To try multiple values, you can supply them as a list. Let's show that a fixed probability of 50%
yields, on average, the same number of signals as a probability that increases from 0% to 100%,
though the distributions are quite different:

```pycon
>>> rprob = vbt.RPROB.run(
...     symbol_wrapper.shape,
...     [0.5, vbt.to_2d_pr_array(prob)],
...     seed=42,
...     input_index=symbol_wrapper.index,
...     input_columns=symbol_wrapper.columns
... )
>>> rprob.entries.sum()
rprob_prob  symbol 
0.5         BTCUSDT    176
            ETHUSDT    187
array_0     BTCUSDT    183
            ETHUSDT    178
dtype: int64
```

### Stops

Stop signals are an important element of signal development because they allow you to propagate a
stop condition through time. VBT provides two main stop signal generators: a basic
generator that compares a single time series to any stop condition, and a specialized generator
that works with candlestick data and stop order conditions that are common in trading.

The first type can be used via the Numba-compiled function [stop_place_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.stop_place_nb)
and its accessor instance method [SignalsAccessor.generate_stop_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_stop_exits).
There are also indicator classes [STX](https://vectorbt.pro/pvt_41531c19/api/signals/generators/stx/#vectorbtpro.signals.generators.stx.STX)
and [STCX](https://vectorbt.pro/pvt_41531c19/api/signals/generators/stcx/#vectorbtpro.signals.generators.stcx.STCX) that make the
stop parameterizable. Let's use the accessor method to generate take profit (TP) signals.
This requires four inputs: entry signals (`entries`), the entry price for the stop (`entry_ts`),
the high price (`ts`), and the actual stop(s) in % to compare against the high price (`stop`).
Use the crossover entries generated previously. Run the method in chained exits mode to ensure
VBT waits for an exit and removes any entry signals that appear before.

```pycon
>>> new_entries, exits = entries.vbt.signals.generate_stop_exits(
...     data.get("Close"),
...     data.get("High"),
...     stop=0.1,
...     chain=True
... )
>>> new_entries[new_entries.any(axis=1)]
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-04 00:00:00+00:00     True    False
2021-03-10 00:00:00+00:00     True    False
...
2021-11-07 00:00:00+00:00     True    False
2021-12-02 00:00:00+00:00    False     True

>>> exits[exits.any(axis=1)]
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-06 00:00:00+00:00     True    False
2021-03-13 00:00:00+00:00     True    False
...
2021-10-15 00:00:00+00:00    False     True
2021-10-19 00:00:00+00:00     True    False
```

But how do we determine the stop price? Fortunately, the Numba-compiled function also accepts a
(required) in-place output array `stop_ts` that will be filled with the stop price of each exit. By default,
VBT assumes you are not interested in this array, so to save memory,
it creates an empty (uninitialized) array, passes it to Numba, and deletes it afterward. To
have it return the array, you should pass an empty dictionary `out_dict`, where the accessor
method can place the array. When you provide an `out_dict`, VBT will create a full (initialized)
array with `np.nan`, pass it to Numba, and store it in the dictionary:

```pycon
>>> out_dict = {}
>>> new_entries, exits = entries.vbt.signals.generate_stop_exits(
...     data.get("Close"),
...     data.get("High"),
...     stop=0.1,
...     chain=True,
...     out_dict=out_dict
... )
>>> out_dict["stop_ts"][exits.any(axis=1)]
symbol                       BTCUSDT   ETHUSDT
Open time                                     
2021-02-06 00:00:00+00:00  40630.326       NaN
2021-03-13 00:00:00+00:00  61436.749       NaN
...
2021-10-15 00:00:00+00:00        NaN  3866.797
2021-10-19 00:00:00+00:00  63179.721       NaN
```

!!! tip
    You can also pass your own (pre-created) `stop_ts` inside `out_dict`, and
    VBT will override only the elements corresponding to the exits.

You can do the same with the corresponding indicator class. Let's try something different:
test two trailing stop loss (TSL) parameters, where the condition follows the high price upward
and is triggered once the low price crosses the stop value downward. The high price can be specified
with the argument `follow_ts`. The entry price will be the open price (even though we generated entries
using the close price, let's assume this situation for now), so we will also allow placing
the first signal at the entry bar by setting `wait` to zero:

```pycon
>>> stcx = vbt.STCX.run(  # (1)!
...     entries,
...     data.get("Open"),
...     ts=data.get("Low"),
...     follow_ts=data.get("High"),
...     stop=-0.1,  # (2)!
...     trailing=[False, True],  # (3)!
...     wait=0  # (4)!
... )
>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> stcx.plot(
...     column=(-0.1, True, "BTCUSDT"), 
...     entry_y="entry_ts",  # (5)!
...     exit_y="stop_ts", 
...     fig=fig
... )
>>> fig.show()
```

1. Indicators usually assume you want all optional arrays, so you do not need to
pass anything. If you are not interested, pass `stop_ts=None` to save some RAM.

2. For a downward crossover condition, specify a negative stop value.

3. Test both SL and TSL.

4. Set waiting time to zero only if the entry price is the open price.

5. When plotting, you can provide `y` as an attribute of the indicator instance.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/stcx.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/stcx.dark.svg#only-dark){: .iimg loading=lazy }

!!! note
    Waiting time cannot be higher than 1. If waiting time is 0, `entry_ts` should be the
    first value in the bar. If waiting time is 1, `entry_ts` should be the last value in the bar;
    otherwise, the stop could have also been hit at the first bar.

    By setting waiting time to zero, you may have an entry and an exit on the same bar.
    Multiple orders on the same bar can only be handled using a flexible order function
    or by converting signals directly into order records. When passed directly to
    [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
    any conflicting signals will be ignored.

If you want to place only SL, TSL, TP, and TTP orders, a more complete approach is to
use the full OHLC data, as utilized by the Numba-compiled function
[ohlc_stop_place_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.ohlc_stop_place_nb),
the accessor instance method [SignalsAccessor.generate_ohlc_stop_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_ohlc_stop_exits),
and the corresponding indicator classes [OHLCSTX](https://vectorbt.pro/pvt_41531c19/api/signals/generators/ohlcstx/#vectorbtpro.signals.generators.ohlcstx.OHLCSTX)
and [OHLCSTCX](https://vectorbt.pro/pvt_41531c19/api/signals/generators/ohlcstcx/#vectorbtpro.signals.generators.ohlcstcx.OHLCSTCX).
The main advantage of this approach is that you can check for all stop order conditions at the same time.

Let's generate signals based on a
[stop loss and trailing stop loss combo](https://www.investopedia.com/articles/trading/08/trailing-stop-loss.asp)
of 10% and 15%, respectively:

```pycon
>>> ohlcstcx = vbt.OHLCSTCX.run(
...     entries,
...     data.get("Close"),  # (1)!
...     data.get("Open"),  # (2)!
...     data.get("High"),
...     data.get("Low"),
...     data.get("Close"),
...     sl_stop=vbt.Default(0.1),  # (3)!
...     tsl_stop=vbt.Default(0.15),
...     is_entry_open=False  # (4)!
... )
>>> ohlcstcx.plot(column=("BTCUSDT")).show()  # (5)!
```

1. Entry price. Here, we use the close price since the entries array was generated using it.

2. OHLC.

3. Stop parameters.

4. Entry price is the close price. Enable this flag if the entry price is the open price.

5. The indicator instance automatically plots the candlestick data.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx.dark.svg#only-dark){: .iimg loading=lazy }

Keep in mind that there is no intra-candle data. If there was a large price fluctuation in
both directions, we could not determine whether SL was triggered before TP or vice versa.
Certain assumptions must be made:

* If a stop is hit before the open price, the stop price becomes the current open price.
This is especially true for `wait=1` and `is_entry_open=True`.

* Trailing stop can only be based on the previous high/low price, not the current one.

* We pessimistically assume that any SL is triggered before any TP.

A common tricky situation occurs when the entry price is the open price and you are waiting one bar.
For example, what happens if the condition is met during the waiting period? You cannot place 
an exit signal at the entry bar. Instead, the function waits until the next bar and checks 
whether the condition is still valid for the open price. If it is, the signal is placed with the stop
price set to the open price. If not, the function simply waits for the next opportunity.
If the stop is trailing, the target price will update as usual at the entry timestamp.
To avoid logical bugs, it is recommended to use the close price as the entry price when `wait` is one bar
(which is the default).

When working with multiple stop types simultaneously, you may want to know which exact type
was triggered. This information is stored in the `stop_type` array (machine-readable)
and the `stop_type_readable` array (human-readable):

```pycon
>>> ohlcstcx.stop_type_readable[ohlcstcx.exits.any(axis=1)]
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-22 00:00:00+00:00     TSL    None
2021-03-23 00:00:00+00:00    None     TSL
2021-03-24 00:00:00+00:00     TSL    None
2021-04-18 00:00:00+00:00      SL     TSL
2021-05-12 00:00:00+00:00      SL    None
2021-06-08 00:00:00+00:00    None      SL
2021-06-18 00:00:00+00:00      SL    None
2021-07-09 00:00:00+00:00    None     TSL
2021-07-19 00:00:00+00:00      SL    None
2021-09-07 00:00:00+00:00     TSL     TSL
2021-11-16 00:00:00+00:00     TSL     TSL
2021-12-03 00:00:00+00:00    None      SL
2021-12-29 00:00:00+00:00    None      SL
2021-12-31 00:00:00+00:00      SL    None
```

All stop types are listed in the enumerated type [StopType](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.StopType).

Both stop signal generation modes are very flexible with inputs. For example, if any element in 
the `ts` and `follow_ts` arrays is NaN (default), it will be replaced by the corresponding element in `entry_ts`.
If only an element in `follow_ts` is NaN, it will be replaced 
by the minimum or maximum (depending on the sign of the stop value) of the element in both other arrays.
Similarly, in the second mode, you can provide only `entry_price`, and VBT will auto-populate 
the open price if `is_entry_open` is enabled, or the close price otherwise. Without `high`, 
VBT will use the maximum of `open` and `close`. Generally, you are not required to provide 
every piece of information except for the entry price, but it is best to provide as much 
detail as you can for better decision-making and to closely simulate real-world conditions.

For example, let's run the same as above but specify only the entry price:

```pycon
>>> ohlcstcx = vbt.OHLCSTCX.run(
...     entries,
...     data.get("Close"),
...     sl_stop=vbt.Default(0.1),
...     tsl_stop=vbt.Default(0.15),
...     is_entry_open=False
... )
>>> ohlcstcx.plot(column=("BTCUSDT")).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx2.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx2.dark.svg#only-dark){: .iimg loading=lazy }

The same flexibility applies to parameters: just like the probability parameter 
in random signal generators, you can pass each parameter as an array, such as one element per row, 
column, or even element. Let's treat every second entry as a short entry and reverse the [trailing
take profit](https://capitalise.ai/trailing-take-profit-manage-your-risk-while-locking-the-profits/)
(TTP) logic for it:

```pycon
>>> entry_pos_rank = entries.vbt.signals.pos_rank(allow_gaps=True)  # (1)!
>>> short_entries = (entry_pos_rank >= 0) & (entry_pos_rank % 2 == 1)  # (2)!

>>> ohlcstcx = vbt.OHLCSTCX.run(
...     entries,
...     data.get("Close"),
...     data.get("Open"),
...     data.get("High"),
...     data.get("Low"),
...     data.get("Close"),
...     tsl_th=vbt.Default(0.2),  # (3)!
...     tsl_stop=vbt.Default(0.1),
...     reverse=vbt.Default(short_entries),  # (4)!
...     is_entry_open=False
... )
>>> ohlcstcx.plot(column=("BTCUSDT")).show()
```

1. Rank each entry signal by its position among all entry signals using 
[SignalsAccessor.pos_rank](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.pos_rank).

2. Select only those entries whose position is odd, meaning those not divisible by 2.
These will be our short entries.

3. TTP information consists of two parts: a take profit threshold (`tsl_th`) that needs to be crossed upward,
and a trailing stop loss (`tsl_stop`) that is enabled once the threshold is crossed.

4. The short entry mask becomes our reversal mask.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx3.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx3.dark.svg#only-dark){: .iimg loading=lazy }

You can then split both final arrays into four direction-aware arrays for simulation:

```pycon
>>> long_entries = ohlcstcx.new_entries.vbt & (~short_entries)  # (1)!
>>> long_exits = ohlcstcx.exits.vbt.signals.first_after(long_entries)  # (2)!
>>> short_entries = ohlcstcx.new_entries.vbt & short_entries
>>> short_exits = ohlcstcx.exits.vbt.signals.first_after(short_entries)

>>> fig = data.plot(
...     symbol="BTCUSDT", 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> long_entries["BTCUSDT"].vbt.signals.plot_as_entries(
...     ohlcstcx.entry_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="limegreen"), name="Long entries"), 
...     fig=fig
... )
>>> long_exits["BTCUSDT"].vbt.signals.plot_as_exits(
...     ohlcstcx.stop_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="orange"), name="Long exits"),
...     fig=fig
... )
>>> short_entries["BTCUSDT"].vbt.signals.plot_as_entries(
...     ohlcstcx.entry_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="magenta"), name="Short entries"),
...     fig=fig
... )
>>> short_exits["BTCUSDT"].vbt.signals.plot_as_exits(
...     ohlcstcx.stop_price["BTCUSDT"],
...     trace_kwargs=dict(marker=dict(color="red"), name="Short exits"),
...     fig=fig
... )
>>> fig.show()
```

1. We cannot use the original `entries` array since the generator ignored some entries
that came too early. But since new entries retain their positions, we can identify
long signals by inverting the short array and combining it with the new entry array using the _AND_ operation.

2. A useful feature of the chained exits mode is that each exit always follows its entry directly—
there are no other entries in between, so we can use [SignalsAccessor.first_after](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.first_after)
for this task.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx4.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/ohlcstcx4.dark.svg#only-dark){: .iimg loading=lazy }

It seems like all trades are winners, thanks to a range-bound but still volatile market :four_leaf_clover:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/signal-development/generation.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/SignalDevelopment.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/pre-analysis/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/
name: Pre-analysis
type: page
icon: material-broadcast
---

# Pre-analysis

Pre-analysis occurs before simulation and allows you to examine generated signal data, select
specific signals—such as by removing duplicates—and analyze the distribution of signal data to
identify potential issues with your trading strategy. Since signals are often conditionally linked to
neighboring signals and can involve cross-timestamp dependencies, performing this analysis using only
Pandas or other data science tools is not straightforward in a fully vectorized way. Fortunately,
VBT manages much of this complexity for you :muscle:

## Ranking

Ideally, signals with opposite signs follow each other, making it easy to connect them. However, in
practice, things can get messy: you might encounter entire partitions of signals with the same sign
(for example, multiple `True` values in a row without any `False` between them), or some signals may
have no opposing signal at all. In these situations, you typically filter out signals that should
not be executed before passing them to the simulator. For instance, when comparing one time series
to another, you may consider the first signal in each partition the most important (the main
signal), while the subsequent signals are less important because they arrive too late. This
difference in importance means you need to review each signal and decide if it is worth keeping.

Instead of writing your own loop for this, you can use ranking, which is a powerful approach to
quantify signal positions. Ranking assigns each signal a number that exists only within a defined
context. For example, you might assign `1` to the first signal of each partition and `0` to others.
Selecting the main signal then becomes as simple as comparing the entire mask to `1`, which is
another advantage of using mask arrays over integer arrays. In VBT, ranking is handled by the
Numba-compiled function [rank_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.rank_nb) and its accessor
method [SignalsAccessor.rank](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.rank).
These take a mask and call a UDF `rank_func_nb` at each signal in the mask, passing a context of
type [RankContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.RankContext), along with any
optional arguments via `rank_args`.

For example, here is a ranker that performs the process just described:

```pycon
>>> @njit
>>> def rank_func_nb(c):
...     if c.sig_in_part_cnt == 1:  # (1)!
...         return 1
...     return 0

>>> sample_mask = pd.Series([True, True, False, True, True])
>>> ranked = sample_mask.vbt.signals.rank(rank_func_nb)
>>> ranked
0    1
1    0
2   -1
3    1
4    0
dtype: int64
```

1. Get the number of signals in the current partition up to this point, including the current one.

As you can see, it assigns `1` to each primary signal and `0` to each secondary signal. The ranking
function also uses `-1` for all `False` values, which is a reserved number. To select the first
signal of each partition, simply compare for equality with `1`:

```pycon
>>> ranked == 1
0     True
1    False
2    False
3     True
4    False
dtype: bool
```

!!! tip
    This approach is very similar to how [SignalsAccessor.first](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first) works.

To call your UDF only on `True` values that follow a `False` value, use `after_false`. This is
especially useful in crossover calculations, since you often want to avoid assigning a signal during
an initial period when a time series is already above or below another time series.

```pycon
>>> ranked = sample_mask.vbt.signals.rank(
...     rank_func_nb, 
...     after_false=True
... )
>>> ranked == 1
0    False
1    False
2    False
3     True
4    False
dtype: bool
```

Another advantage of this approach is that you can specify another mask—a resetter—whose signal
resets partitions in the main mask. For example, suppose you have entry and exit arrays. To select
the first entry between each pair of exits, specify the entry array as the main mask and the exit
array as the resetting mask. This way, only valid signals are processed, and those before the first
resetting signal are ignored.

```pycon
>>> sample_entries = pd.Series([True, True, True, True, True])
>>> sample_exits = pd.Series([False, False, True, False, False])
>>> ranked = sample_entries.vbt.signals.rank(
...     rank_func_nb, 
...     reset_by=sample_exits
... )
>>> ranked == 1
0     True
1    False
2    False
3     True
4    False
dtype: bool
```

!!! info
    As you may have noticed, the partition is effectively reset at the timestamp right after the 
    resetting signal. This is because, when an entry and exit happen at the same timestamp, the entry
    is assumed to come first and should belong to the previous partition. To make VBT assume that
    the main signal comes after the resetting signal (such as when exits are the main mask and entries are
    the resetter), pass `wait=0`.

To prevent any entry signal from appearing before the first exit signal, you can use `after_reset`:

```pycon
>>> ranked = sample_entries.vbt.signals.rank(
...     rank_func_nb, 
...     reset_by=sample_exits,
...     after_reset=True
... )
>>> ranked == 1
0    False
1    False
2    False
3     True
4    False
dtype: bool
```

### Preset rankers

Writing your own ranking functions can be enjoyable, but there are two preset rankers that cover most
use cases: [sig_pos_rank_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.sig_pos_rank_nb) for ranking
signals, and [part_pos_rank_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.part_pos_rank_nb) for
ranking entire partitions. These are used by the accessor methods
[SignalsAccessor.pos_rank](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.pos_rank)
and [SignalsAccessor.partition_pos_rank](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.partition_pos_rank)
respectively. Both methods assign ranks starting at zero.

The first method assigns each signal a rank based on its position within the current partition
(`allow_gaps=False`) or globally (`allow_gaps=True`):

```pycon
>>> sample_mask = pd.Series([True, True, False, True, True])
>>> ranked = sample_mask.vbt.signals.pos_rank()
>>> ranked
0    0
1    1
2   -1
3    0
4    1
dtype: int64

>>> ranked == 1  # (1)!
0    False
1     True
2    False
3    False
4     True
dtype: bool

>>> ranked = sample_mask.vbt.signals.pos_rank(allow_gaps=True)
>>> ranked
0    0
1    1
2   -1
3    2
4    3
dtype: int64

>>> (ranked > -1) & (ranked % 2 == 1)  # (2)!
0    False
1     True
2    False
3    False
4     True
dtype: bool
```

1. Select every second signal in each partition.

2. Select every second signal globally.

The second method assigns each signal a rank based on its partition's position, making it easy to
select entire partitions of signals:

```pycon
>>> ranked = sample_mask.vbt.signals.partition_pos_rank()
>>> ranked
0    0
1    0
2   -1
3    1
4    1
dtype: int64

>>> ranked == 1  # (1)!
0    False
1    False
2    False
3     True
4     True
dtype: bool
```

1. Select the second partition.

Additionally, there are accessor methods that handle the comparison for you:
[SignalsAccessor.first](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first),
[SignalsAccessor.nth](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.nth),
[SignalsAccessor.from_nth](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.from_nth),
and [SignalsAccessor.to_nth](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.to_nth).
All of these use the signal position ranker (the first method), and each method has a version with
the suffix `after`, such as
[SignalsAccessor.to_nth_after](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.to_nth_after),
which works the same way but only after each resetting signal and with `allow_gaps` enabled.

Why does this matter? Because now you can, for example, compare one time series to another and
select the first signal after a specified number of confirmations. For example, returning to our
Bollinger Bands example with two conditions, let's see how many signals remain if we require zero,
one, or two confirmations:

```pycon
>>> entry_cond1 = data.get("Low") < bb.lowerband
>>> entry_cond2 = bandwidth > 0.3
>>> entry_cond3 = data.get("High") > bb.upperband
>>> entry_cond4 = bandwidth < 0.15
>>> entries = (entry_cond1 & entry_cond2) | (entry_cond3 & entry_cond4)

>>> entries.vbt.signals.from_nth(0).sum()
symbol
BTCUSDT    25
ETHUSDT    13
dtype: int64

>>> entries.vbt.signals.from_nth(1).sum()
symbol
BTCUSDT    14
ETHUSDT     5
dtype: int64

>>> entries.vbt.signals.from_nth(2).sum()
symbol
BTCUSDT    6
ETHUSDT    2
dtype: int64
```

Now, let's generate exit signals from the opposite conditions:

```pycon
>>> exit_cond1 = data.get("High") > bb.upperband
>>> exit_cond2 = bandwidth > 0.3
>>> exit_cond3 = data.get("Low") < bb.lowerband
>>> exit_cond4 = bandwidth < 0.15
>>> exits = (exit_cond1 & exit_cond2) | (exit_cond3 & exit_cond4)
```

What is the maximum number of exit signals after each entry signal?

```pycon
>>> exits.vbt.signals.pos_rank_after(entries, reset_wait=0).max() + 1  # (1)!
symbol
BTCUSDT     9
ETHUSDT    11
dtype: int64
```

1. The count is the maximum rank plus one, since ranks start at zero. We also assume that an entry
signal comes before an exit signal when both occur at the same timestamp by passing `reset_wait=0`.

Conversely, what is the maximum number of entry signals after each exit signal?

```pycon
>>> entries.vbt.signals.pos_rank_after(exits).max() + 1
symbol
BTCUSDT    11
ETHUSDT     7
dtype: int64
```

Get the timestamps and ranks of exit signals with the highest rank after each entry signal:

```pycon
>>> ranked = exits.vbt.signals.pos_rank_after(entries, reset_wait=0)
>>> highest_ranked = ranked == ranked.max()
>>> ranked[highest_ranked.any(axis=1)]
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-05-12 00:00:00+00:00       -1       10
2021-07-28 00:00:00+00:00        8       -1
```

Are there any exit signals before the first entry signal, and if so, how many?

```pycon
>>> exits_after = exits.vbt.signals.from_nth_after(0, entries, reset_wait=0)
>>> (exits ^ exits_after).sum()  # (1)!
symbol
BTCUSDT    10
ETHUSDT     4
dtype: int64
```

1. Use the _XOR_ operation to keep only the signals that are in either `exits` or `exits_after`.
Since all signals in `exits_after` are guaranteed to also be present in `exits`, the result will
contain only those signals that are in `exits` but not in `exits_after`.

### Mapped ranks

To take your ranking analysis a step further, you can use the `as_mapped` flag in
[SignalsAccessor.rank](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.rank)
to have VBT generate a mapped array of ranks instead of a Series or DataFrame of integers.
Mapped arrays do not store `-1` values and work directly with zero and positive ranks, reducing
data size while still allowing you to compute various metrics per column or by group. For example,
suppose both symbols belong to one portfolio and you want to aggregate their statistics. Let's
compare the bandwidth against multiple threshold combinations and return the maximum rank across both
symbol columns for each combination:

```pycon
>>> mask = bandwidth.vbt > vbt.Param(np.arange(1, 10) / 10, name="bw_th")
>>> mapped_ranks = mask.vbt.signals.pos_rank(as_mapped=True)
>>> mapped_ranks.max(group_by=vbt.ExceptLevel("symbol"))  # (1)!
bw_th
0.1    237.0
0.2     50.0
0.3     19.0
0.4     12.0
0.5     10.0
0.6      8.0
0.7      5.0
0.8      2.0
0.9      NaN
Name: max, dtype: float64
```

1. Use [ExceptLevel](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.base.indexes.ExceptLevel)
to aggregate by every column level except for the symbol level.

## Cleaning

Cleaning is the process of removing signals that should not be converted into orders. Since we are
mainly interested in having one signal to open a position and another to close or reverse it,
we want a signal sequence where signals of opposite signs appear one after another, forming a
[chain](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/generation/#chained-exits). Unless you want to accumulate
orders using the `accumulate` argument in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
only the first signal will be executed anyway. Removing redundant signals is simple with
[SignalsAccessor.first_after](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first_after).
Below, we select the first exit signal after each entry signal and the first entry signal after
each exit signal (in this specific order!):

```pycon
>>> new_exits = exits.vbt.signals.first_after(entries, reset_wait=0)
>>> new_entries = entries.vbt.signals.first_after(exits)
```

Let's visualize the selected signals:

```pycon
>>> symbol = "ETHUSDT"
>>> fig = data.plot(
...     symbol=symbol, 
...     ohlc_trace_kwargs=dict(opacity=0.5), 
...     plot_volume=False
... )
>>> entries[symbol].vbt.signals.plot_as_entries(
...     y=data.get("Close", symbol), fig=fig)
>>> exits[symbol].vbt.signals.plot_as_exits(
...     y=data.get("Close", symbol), fig=fig)
>>> new_entries[symbol].vbt.signals.plot_as_entry_marks(
...     y=data.get("Close", symbol), fig=fig, 
...     trace_kwargs=dict(name="New entries"))
>>> new_exits[symbol].vbt.signals.plot_as_exit_marks(
...     y=data.get("Close", symbol), fig=fig, 
...     trace_kwargs=dict(name="New exits"))
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/cleaning.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/signal-dev/cleaning.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    To allow the first exit signal before the first entry signal, pass `after_reset=False`.
    To __require__ the first exit signal to come before the first entry signal, reverse the order
    of the `first_after` calls.

However, there is an even simpler method: [SignalsAccessor.clean](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.clean),
which does the same as above but in a single pass over the entire signal dataset:

```pycon
>>> new_entries, new_exits = entries.vbt.signals.clean(exits)
```

This method also provides several convenient arguments to control the cleaning process. By default,
it assumes that entry signals are executed before exit signals (use `reverse_order` to change this).
It also removes all entry and exit signals that occur at the same time (use `keep_conflicts` to keep them),
and ensures that an entry appears first (use `force_first` to disable this). For more complex cleaning,
a custom loop is required. If you do not provide the second mask (`exits` in this case), it will
simply select the first signal in each partition.

## Duration

Besides ranks, we can also analyze duration! For example, we might want to know the average, minimum,
and maximum distance between pairs of neighboring signals in a mask. While extracting such information
is usually straightforward, the real challenge is representing it: often, we want to know not only the
distance, but also the index of the first and last signal. Mapped arrays are not enough, since they
typically represent a single data feature. The solution is to use
[Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges) records—the core class
for analyzing time-bound processes such as positions and drawdowns! Here, we can mark one signal as
the range's start and another as the end, and analyze various metrics regarding the distance between
them :triangular_ruler:

To get range records for a single mask, use the Numba-compiled function
[between_ranges_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.between_ranges_nb) and its accessor
method [SignalsAccessor.between_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.between_ranges).
Let's map each pair of neighboring signals in `entries` to a range:

```pycon
>>> ranges = entries.vbt.signals.between_ranges()
>>> ranges.records
    id  col  start_row  end_row  status
0    0    0         99      100       1
1    1    0        100      101       1
2    2    0        101      102       1
...
33   9    1        173      242       1
34  10    1        242      286       1
35  11    1        286      313       1
```

!!! tip
    To print records in a more readable format, use `records_readable`.

Here, `col` is the column index, `start_idx` is the index of the left signal, `end_row`
is the index of the right signal, and `status` (of type
[RangeStatus](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.RangeStatus)) is always `RangeStatus.Closed`.
You can access each of these fields as regular attributes and get an analyzable mapped array in return.
For example, here is how to get the index of the first signal in each column:

```pycon
>>> ranges.start_idx.min(wrap_kwargs=dict(to_index=True))
symbol
BTCUSDT   2021-04-10 00:00:00+00:00
ETHUSDT   2021-02-25 00:00:00+00:00
Name: min, dtype: datetime64[ns, UTC]
```

Similarly, you can get the duration as a mapped array through the `duration` attribute.
Let's describe the duration in each column:

```pycon
>>> ranges.duration.describe(wrap_kwargs=dict(to_timedelta=True))
symbol                    BTCUSDT                    ETHUSDT
mean             10 days 21:00:00           21 days 12:00:00
std    22 days 18:47:41.748587504 28 days 19:32:48.777556028
min               1 days 00:00:00            1 days 00:00:00
25%               1 days 00:00:00            1 days 00:00:00
50%               1 days 00:00:00            2 days 00:00:00
75%               2 days 06:00:00           32 days 18:00:00
max              89 days 00:00:00           80 days 00:00:00
```

We see that at least 50% of entry signals in column `BTCUSDT` are consecutive
(one bar = one day), while the average duration between two signals is 10 days. We also
see that signals in `ETHUSDT` are more sparse. The longest duration with no signals is
90 days for `BTCUSDT` and 80 days for `ETHUSDT`.

When working with two masks, such as entry and exit signals, we are often more interested in
the space between signals in each mask rather than within a single mask. This is done using a
mapping procedure that goes through each signal in the first mask (the "source mask") and looks for
one or more following signals in the second mask (the "target mask"), stopping at the next signal
in the source mask. This is implemented by the Numba-compiled function
[between_two_ranges_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.between_two_ranges_nb). The accessor
method remains the same as above: [SignalsAccessor.between_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.between_ranges),
which switches to the two-mask mode if the `target` argument is provided. For example, let's get the
average distance from each entry signal to its next exit signal before and after cleaning:

```pycon
>>> ranges = entries.vbt.signals.between_ranges(target=exits)
>>> ranges.avg_duration
symbol
BTCUSDT   46 days 00:51:25.714285714
ETHUSDT   38 days 18:51:25.714285714
Name: avg_duration, dtype: timedelta64[ns]

>>> new_ranges = new_entries.vbt.signals.between_ranges(target=new_exits)
>>> new_ranges.avg_duration  # (1)!
symbol
BTCUSDT   43 days 00:00:00
ETHUSDT   23 days 12:00:00
Name: avg_duration, dtype: timedelta64[ns]
```

1. The average duration decreased because we selected the first exit signal during cleaning.

!!! info
    If two signals happen at the same time, the signal from the source mask is assumed to come first.

Since an exit signal can follow many entry signals, we can also reverse the mapping order
by specifying a many-to-one relationship with `relation="manyone"`, and get the average distance from
each exit to any of its preceding entry signals:

```pycon
>>> ranges = entries.vbt.signals.between_ranges(target=exits, relation="manyone")
>>> ranges.avg_duration
symbol
BTCUSDT   37 days 14:10:54.545454545
ETHUSDT   22 days 01:50:46.153846153
Name: avg_duration, dtype: timedelta64[ns]

>>> new_ranges = new_entries.vbt.signals.between_ranges(target=new_exits, relation="manyone")
>>> new_ranges.avg_duration
symbol
BTCUSDT   43 days 00:00:00
ETHUSDT   23 days 12:00:00
Name: avg_duration, dtype: timedelta64[ns]
```

We can see that the cleaning process worked as intended because the average distance from each entry
to its exit signal and vice versa is now the same.

Recall that a partition is simply a sequence of `True` values with no `False` values in between.
The same mapping approach can measure the length of entire partitions of signals: take the first and
last signal of a partition and map them to a range record. This is enabled by the Numba-compiled
function [partition_ranges_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.partition_ranges_nb)
and its accessor method [SignalsAccessor.partition_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.partition_ranges).
Let's extract the number of entry signal partitions and their length distribution before and after cleaning:

```pycon
>>> ranges = entries.vbt.signals.partition_ranges()
>>> ranges.duration.describe()
symbol    BTCUSDT   ETHUSDT
count   11.000000  8.000000
mean     2.272727  1.625000
std      1.190874  0.916125
min      1.000000  1.000000
25%      1.500000  1.000000
50%      2.000000  1.000000
75%      3.000000  2.250000
max      5.000000  3.000000

>>> new_ranges = new_entries.vbt.signals.partition_ranges()
>>> new_ranges.duration.describe()
symbol  BTCUSDT  ETHUSDT
count       4.0      4.0
mean        1.0      1.0
std         0.0      0.0
min         1.0      1.0
25%         1.0      1.0
50%         1.0      1.0
75%         1.0      1.0
max         1.0      1.0
```

We see there are 11 partitions in column `BTCUSDT`, with at least 50% of them
consisting of two or more signals. What does this mean? It means that whenever our strategy
generates an entry, this entry signal remains valid for 2 or more days at least 50% of the time.
After cleaning, we can see that many partitions that were located between two exit
signals have been removed, and now each partition is exactly one signal long (that is, just the first signal).
We also note that our strategy is more active in the `BTCUSDT` market compared to the `ETHUSDT` market.

We can also measure the pairwise distance between partitions, not just the partitions themselves!
Let's derive the distribution of the distance between the last signal of one partition and the first signal
of the next partition using the range records generated by the accessor method
[SignalsAccessor.between_partition_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.between_partition_ranges),
which is based on the Numba-compiled function [between_partition_ranges_nb](https://vectorbt.pro/pvt_41531c19/api/signals/nb/#vectorbtpro.signals.nb.between_partition_ranges_nb):

```pycon
>>> ranges = entries.vbt.signals.between_partition_ranges()
>>> ranges.duration.describe(wrap_kwargs=dict(to_timedelta=True))
symbol                    BTCUSDT                    ETHUSDT
mean             24 days 16:48:00 36 days 03:25:42.857142857
std    31 days 00:33:47.619615945 30 days 08:40:17.723113570
min               2 days 00:00:00            2 days 00:00:00
25%               2 days 00:00:00           14 days 12:00:00
50%               6 days 12:00:00           29 days 00:00:00
75%              40 days 06:00:00           56 days 12:00:00
max              89 days 00:00:00           80 days 00:00:00
```

Now we can more easily analyze how many periods in a row our strategy marks as "do not order".
Here, the average streak without a signal in the `ETHUSDT` column is 36 days.

## Overview

If we want a quick summary of what is happening in our signal arrays, we can compute
a variety of metrics and display them together using the base method
[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats),
which is overridden by the accessor [SignalsAccessor](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor)
and designed specifically for signal data:

```pycon
>>> entries.vbt.signals.stats(column="BTCUSDT")
Start                         2021-01-01 00:00:00+00:00
End                           2021-12-31 00:00:00+00:00
Period                                365 days 00:00:00
Total                                                25
Rate [%]                                       6.849315
First Index                   2021-04-10 00:00:00+00:00
Last Index                    2021-12-27 00:00:00+00:00
Norm Avg Index [-1, 1]                         0.159121
Distance: Min                           1 days 00:00:00
Distance: Median                        1 days 00:00:00
Distance: Max                          89 days 00:00:00
Total Partitions                                     11
Partition Rate [%]                                 44.0
Partition Length: Min                   1 days 00:00:00
Partition Length: Median                2 days 00:00:00
Partition Length: Max                   5 days 00:00:00
Partition Distance: Min                 2 days 00:00:00
Partition Distance: Median              6 days 12:00:00
Partition Distance: Max                89 days 00:00:00
Name: BTCUSDT, dtype: object
```

!!! note
    If you do not provide a column, the method will compute the mean across all columns.

Here is what these metrics mean. The signal mask starts on January 1, 2021, and ends on December 31, 2021.
The entire period covers 365 days. There are 25 signals in our mask, which is 6.85% of 365 (the total number of
entries). The first and last signals appear on April 10 and December 27, respectively. The index of these signals
can be found with [SignalsAccessor.nth_index](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.nth_index).
A positive normalized average index, which measures the skew of signal positions in the mask (see
[SignalsAccessor.norm_avg_index](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.norm_avg_index)),
suggests that more signals occur in the second half of the backtesting period. At least 50% of the signals
are located next to each other, while the maximum distance between any two signals is 89 days.
There are 11 signal partitions in the mask, which is less than the total number of signals, so some
partitions have two or more signals. The partition rate, which is the number of partitions divided by
the number of signals (see [SignalsAccessor.partition_rate](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.partition_rate)),
is 44%, which is between 1 / 25 = 4% (all signals in one big partition)
and 25 / 25 = 100% (each partition has only one signal). This is confirmed by the median partition
length of 2 signals. The longest streak of `True` values is 5 days. The minimum distance between
each pair of partitions is just 1 `False` value (`[True, False, True]` gives a distance of 2). The
longest streak of `False` values is 89 days.

Since our `entries` mask exists relative to our `exits` mask, we can specify the second mask
using the `other` setting:

```pycon
>>> entries.vbt.signals.stats(column="BTCUSDT", settings=dict(target=exits))
Start                         2021-01-01 00:00:00+00:00
End                           2021-12-31 00:00:00+00:00
Period                                365 days 00:00:00
Total                                                25
Rate [%]                                       6.849315
Total Overlapping                                     1  << new
Overlapping Rate [%]                           1.923077  << new
First Index                   2021-04-10 00:00:00+00:00
Last Index                    2021-12-27 00:00:00+00:00
Norm Avg Index [-1, 1]                         0.159121
Distance -> Target: Min                 0 days 00:00:00  << new
Distance -> Target: Median             49 days 00:00:00  << new
Distance -> Target: Max                66 days 00:00:00  << new
Total Partitions                                     11
Partition Rate [%]                                 44.0
Partition Length: Min                   1 days 00:00:00
Partition Length: Median                2 days 00:00:00
Partition Length: Max                   5 days 00:00:00
Partition Distance: Min                 2 days 00:00:00
Partition Distance: Median              6 days 12:00:00
Partition Distance: Max                89 days 00:00:00
Name: BTCUSDT, dtype: object
```

This adds three more metrics: the number of overlapping signals in both masks,
the overlapping rate as a percentage of the total number of signals in both masks,
and the distribution of the distance from each entry to the next exit up to the next entry.
For example, we see that only one signal occurs at the same timestamp in both masks.
This is also shown by the minimum pairwise distance of 0 days between entries and exits.
An interesting insight: at least 50% of the time, we are in the market for more than 49 days.

## Summary

Most trading strategies can be easily split into a set of primitive conditions, most of which
can be implemented and vectorized. Since each of these conditions is a simple yes-or-no question
(like "is the bandwidth below 10%?"), we can translate it into a mask: a boolean array representing
the answer at each timestamp. Combining the answers to all questions means combining the masks with
logical operations, which is both easy and highly efficient. So why not define a trading strategy
iteratively, as is done with other software? Building each mask separately gives us a unique
opportunity to analyze the answers our strategy produces and also to assess the effectiveness of
our questions. Instead of treating our trading strategy as a black box and relying only on simulation
metrics such as Sharpe, we can analyze each logical component of our strategy before
passing the whole thing to the backtester—the ultimate portal to the world of data science :mirror:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/signal-development/pre-analysis.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/SignalDevelopment.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/stop-signals/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
name: Stop signals
type: page
icon: material-select-compare
---

# Stop signals

Our goal is to use large-scale backtesting to compare the performance of trading with and without stop
loss (SL), trailing stop (TS), and take profit (TP) signals. To ensure this analysis is comprehensive,
we will conduct a large number of experiments across three dimensions: instruments, time, and
parameters.

First, we will select 10 cryptocurrencies by market capitalization (excluding stablecoins such as USDT)
and gather 3 years of their daily pricing data. Specifically, we will backtest the period from 2018 to
2021, as this range includes periods of sharp price declines (such as corrections after the all-time
high in December 2017 and during the coronavirus crisis in March 2020) as well as surges (the all-time
high in December 2020). This provides a balanced perspective. For each instrument, we will split this
period into 400 smaller, overlapping time windows, each lasting 6 months. We will run our tests on each
of these windows to account for different market conditions. For each instrument and time window, we
will generate an entry signal at the very first bar and determine an exit signal according to the stop
configuration. We will test 100 stop values, increasing by 1% increments, and compare the performance
of each one to trading randomly and holding during that specific time window. In total, we will conduct
2,000,000 tests.

!!! important
    Make sure you have at least 16 GB of free RAM available, or memory swapping enabled.

## Parameters

The first step is to define the parameters of the analysis pipeline. As discussed above, we will
backtest 3 years of pricing data, use 400 time windows, 10 cryptocurrencies, and 100 stop values. We
will also set fees and slippage to 0.25% each, and the initial capital to $100. The absolute amount
does not matter, but it must be consistent across all assets to allow for direct comparison. Feel free
to change any parameter of interest.

```pycon
>>> from vectorbtpro import *
>>> import ipywidgets

>>> seed = 42
>>> symbols = [
...     "BTC-USD", "ETH-USD", "XRP-USD", "BCH-USD", "LTC-USD", 
...     "BNB-USD", "EOS-USD", "XLM-USD", "XMR-USD", "ADA-USD"
... ]
>>> start_date = vbt.utc_timestamp("2018-01-01")
>>> end_date = vbt.utc_timestamp("2021-01-01")
>>> time_delta = end_date - start_date
>>> window_len = vbt.timedelta("180d")
>>> window_cnt = 400
>>> exit_types = ["SL", "TS", "TP", "Random", "Holding"]
>>> step = 0.01  # (1)!
>>> stops = np.arange(step, 1 + step, step)

>>> vbt.settings.wrapping["freq"] = "d"
>>> vbt.settings.plotting["layout"]["template"] = "vbt_dark"
>>> vbt.settings.portfolio["init_cash"] = 100.  # (2)!
>>> vbt.settings.portfolio["fees"] = 0.0025  # (3)!
>>> vbt.settings.portfolio["slippage"] = 0.0025  # (4)!

>>> pd.Series({
...     "Start date": start_date,
...     "End date": end_date,
...     "Time period (days)": time_delta.days,
...     "Assets": len(symbols),
...     "Window length": window_len,
...     "Windows": window_cnt,
...     "Exit types": len(exit_types),
...     "Stop values": len(stops),
...     "Tests per asset": window_cnt * len(stops) * len(exit_types),
...     "Tests per window": len(symbols) * len(stops) * len(exit_types),
...     "Tests per exit type": len(symbols) * window_cnt * len(stops),
...     "Tests per stop type and value": len(symbols) * window_cnt,
...     "Tests total": len(symbols) * window_cnt * len(stops) * len(exit_types)
... })
Start date                       2018-01-01 00:00:00+00:00
End date                         2021-01-01 00:00:00+00:00
Time period (days)                                    1096
Assets                                                  10
Window length                            180 days 00:00:00
Windows                                                400
Exit types                                               5
Stop values                                            100
Tests per asset                                     200000
Tests per window                                      5000
Tests per exit type                                 400000
Tests per stop type and value                         4000
Tests total                                        2000000
dtype: object
```

1. 1%

2. $100

3. 0.25%

4. 0.25%

Our configuration produces sample sizes with enough statistical power to analyze four variables: assets
(200k tests per asset), time (5k tests per time window), exit types (400k tests per exit type), and
stop values (4k tests per stop type and value). Similar to Tableau's approach to dimensions and
measures, we can group our performance metrics by each of these variables. However, we will mainly
focus on 5 exit types: SL exits, TS exits, TP exits, random exits, and holding exits (executed at the
last bar).

```pycon
>>> cols = ["Open", "Low", "High", "Close", "Volume"]
>>> yfdata = vbt.YFData.pull(symbols, start=start_date, end=end_date)
```

[=100% "Symbol 10/10"]{: .candystripe .candystripe-animate }

```pycon
>>> yfdata.data.keys()
dict_keys(['BTC-USD', 'ETH-USD', 'XRP-USD', 'BCH-USD', 'LTC-USD', 
            'BNB-USD', 'EOS-USD', 'XLM-USD', 'XMR-USD', 'ADA-USD'])

>>> yfdata.data["BTC-USD"].shape
(1096, 7)
```

The data instance `yfdata` contains a dictionary of OHLCV data, keyed by cryptocurrency name. Each
DataFrame contains 1096 rows (days) and 5 columns (O, H, L, C, and V). You can plot each DataFrame as
follows:

```pycon
>>> yfdata.plot(symbol="BTC-USD").show()  # (1)!
```

1. Only one symbol can be plotted at a time.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/yfdata.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/yfdata.dark.svg#only-dark){: .iimg loading=lazy }

Since assets are one of the dimensions we want to analyze, VBT expects us to combine them as
columns into a single DataFrame and label them clearly. To do this, we swap assets and features to
create a dictionary of DataFrames, now with assets as columns and keyed by feature name, such as "Open".

```pycon
>>> ohlcv = yfdata.concat()

>>> ohlcv.keys()
dict_keys(['Open', 'High', 'Low', 'Close', 
            'Volume', 'Dividends', 'Stock Splits'])

>>> ohlcv['Open'].shape
(1096, 10)
```

## Time windows

Next, we will use a 6-month sliding window over the entire time period and take 400 "snapshots" of
each price DataFrame within this window. Each snapshot is a subset of data to be backtested
independently. Like assets and other variables, snapshots also need to be stacked horizontally as
columns. As a result, we will obtain 180 rows (window length in days) and 4000 columns (10 assets x
400 windows). Each column will represent the price of one asset within one specific time window.

```pycon
>>> splitter = vbt.Splitter.from_n_rolling(  # (1)!
...     ohlcv["Open"].index, 
...     n=window_cnt,
...     length=window_len.days
... )

>>> split_ohlcv = {}
>>> for k, v in ohlcv.items():  # (2)!
...     split_ohlcv[k] = splitter.take(v, into="reset_stacked")  # (3)!
>>> print(split_ohlcv["Open"].shape)
(180, 4000)

>>> split_indexes = splitter.take(ohlcv["Open"].index)  # (4)!
>>> print(split_indexes)
split
0      DatetimeIndex(['2018-01-01 00:00:00+00:00', '2...
1      DatetimeIndex(['2018-01-03 00:00:00+00:00', '2...
2      DatetimeIndex(['2018-01-06 00:00:00+00:00', '2...
3      DatetimeIndex(['2018-01-08 00:00:00+00:00', '2...
4      DatetimeIndex(['2018-01-10 00:00:00+00:00', '2...
                              ...                        
395    DatetimeIndex(['2020-06-26 00:00:00+00:00', '2...
396    DatetimeIndex(['2020-06-28 00:00:00+00:00', '2...
397    DatetimeIndex(['2020-06-30 00:00:00+00:00', '2...
398    DatetimeIndex(['2020-07-03 00:00:00+00:00', '2...
399    DatetimeIndex(['2020-07-05 00:00:00+00:00', '2...
Length: 400, dtype: object

>>> print(split_indexes[10])  # (5)!
DatetimeIndex(['2018-01-24 00:00:00+00:00', '2018-01-25 00:00:00+00:00',
                '2018-01-26 00:00:00+00:00', '2018-01-27 00:00:00+00:00',
                '2018-01-28 00:00:00+00:00', '2018-01-29 00:00:00+00:00',
                ...
                '2018-07-17 00:00:00+00:00', '2018-07-18 00:00:00+00:00',
                '2018-07-19 00:00:00+00:00', '2018-07-20 00:00:00+00:00',
                '2018-07-21 00:00:00+00:00', '2018-07-22 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Date', length=180, freq='D')
```

1. Create an instance of [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).

2. For each DataFrame with symbols in `ohlcv`...

3. Split the DataFrame into chunks and stack those chunks into one DataFrame by resetting its index.
Resetting the index prevents the operation from producing many NaNs.

4. We also want to know the index of each split. To do this, split the index in the same way as above,
but instead of stacking, store the result in a Series indexed by split label.

5. Here is the index of the split with the label `10`.

A great feature of VBT is its use of [hierarchical indexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html)
to store valuable information for each backtest. This ensures that the column hierarchy is preserved throughout
the entire backtesting pipeline—from signal generation to performance modeling—and can be easily
extended. Currently, our columns have the following hierarchy:

```pycon
>>> split_ohlcv["Open"].columns
MultiIndex([(  0, 'BTC-USD'),
            (  0, 'ETH-USD'),
            (  0, 'XRP-USD'),
            ...
            (399, 'XLM-USD'),
            (399, 'XMR-USD'),
            (399, 'ADA-USD')],
            names=['split', 'symbol'], length=4000)
```

This multi-index captures three parameters: the symbol, the start date of the time window, and its end
date. Later, we will extend this multi-index with exit types and stop values, so that each of the 2
million tests has its own price series.

## Entry signals

Unlike most other backtesting libraries, VBT does not store signals as a signed integer array.
Instead, it splits signals into two boolean arrays—entries and exits—which makes manipulation much
easier. At the beginning of each time window, let's generate an entry signal indicating a buy order.
The DataFrame will have the same shape, index, and columns as the price DataFrame, so VBT can link
their elements together.

```pycon
>>> entries = pd.DataFrame.vbt.signals.empty_like(split_ohlcv["Open"])
>>> entries.iloc[0, :] = True

>>> entries.shape
(180, 4000)
```

## Exit signals

For each of the entry signals we created, we will generate an exit signal according to our five exit
types: SL, TS, TP, random, and holding. We will also concatenate their DataFrames into a single (huge)
DataFrame with 180 rows and 2,000,000 columns, each representing a separate backtest. Since exit
signals are boolean, their memory usage remains manageable.

Let's first generate exit signals according to stop conditions. We want to test 100 different stop
values with a 1% increment, starting from 1% and ending at 100% (that is, find the timestamp where
the price exceeds the entry price by 100%). When OHLC data is checked against these conditions, the
position closes at (or shortly after) the time the particular stop is hit.

!!! tip
    We use [OHLCSTX](https://vectorbt.pro/pvt_41531c19/api/signals/generators/ohlcstx/#vectorbtpro.signals.generators.ohlcstx.OHLCSTX)
    instead of the built-in stop-loss in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
    because we want to analyze signals before simulation. Also, constructing parameter grids is easier
    this way. For a reality check, you can run the same setup using
    [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals) alone.

```pycon
>>> sl_ohlcstx = vbt.OHLCSTX.run(
...     entries,  # (1)!
...     entry_price=split_ohlcv["Close"],  # (2)!
...     open=split_ohlcv["Open"], 
...     high=split_ohlcv["High"], 
...     low=split_ohlcv["Low"], 
...     close=split_ohlcv["Close"], 
...     sl_stop=list(stops),  # (3)!
...     stop_type=None  # (4)!
... )
>>> sl_exits = sl_ohlcstx.exits.copy()  # (5)!
>>> sl_price = sl_ohlcstx.close.copy()  # (6)!
>>> sl_price[sl_exits] = sl_ohlcstx.stop_price
>>> del sl_ohlcstx  # (7)!

>>> sl_exits.shape
(180, 400000)

>>> tsl_ohlcstx = vbt.OHLCSTX.run(
...     entries, 
...     entry_price=split_ohlcv["Close"], 
...     open=split_ohlcv["Open"], 
...     high=split_ohlcv["High"], 
...     low=split_ohlcv["Low"], 
...     close=split_ohlcv["Close"], 
...     tsl_stop=list(stops),
...     stop_type=None
... )
>>> tsl_exits = tsl_ohlcstx.exits.copy()
>>> tsl_price = tsl_ohlcstx.close.copy()
>>> tsl_price[tsl_exits] = tsl_ohlcstx.stop_price
>>> del tsl_ohlcstx

>>> tsl_exits.shape
(180, 400000)

>>> tp_ohlcstx = vbt.OHLCSTX.run(
...     entries, 
...     entry_price=split_ohlcv["Close"], 
...     open=split_ohlcv["Open"], 
...     high=split_ohlcv["High"], 
...     low=split_ohlcv["Low"], 
...     close=split_ohlcv["Close"], 
...     tp_stop=list(stops),
...     stop_type=None
... )
>>> tp_exits = tp_ohlcstx.exits.copy()
>>> tp_price = tp_ohlcstx.close.copy()
>>> tp_price[tp_exits] = tp_ohlcstx.stop_price
>>> del tp_ohlcstx

>>> tp_exits.shape
(180, 400000)
```

1. The [OHLCSTX](https://vectorbt.pro/pvt_41531c19/api/signals/generators/ohlcstx/#vectorbtpro.signals.generators.ohlcstx.OHLCSTX)
indicator is an exit generator, so the first input array is always an entry mask.

2. The second input array is the entry price. This is set to the closing price,
because we cannot execute the stop at the same bar as the entry signal anyway.

3. Providing a parameter as a list will test each value in that list across the entire input.

4. The `stop_type` argument is an "in-place output," filled with various useful info.
Since we only need the exits and stop price arrays, set it to `None` to save memory.

5. Assign output arrays to variables. Copy them so you can delete the indicator instance later.

6. The order price is the stop price at the exit time, and the closing price otherwise.

7. Delete the indicator instance to free up memory.

This extends our column hierarchy with a new column level to indicate the stop value.
We just need to make this consistent across all DataFrames:

```pycon
>>> def rename_stop_level(df):
...     return df.vbt.rename_levels({
...         "ohlcstx_sl_stop": "stop_value",
...         "ohlcstx_tsl_stop": "stop_value",
...         "ohlcstx_tp_stop": "stop_value"
...     }, strict=False)

>>> sl_exits = rename_stop_level(sl_exits)
>>> tsl_exits = rename_stop_level(tsl_exits)
>>> tp_exits = rename_stop_level(tp_exits)

>>> sl_price = rename_stop_level(sl_price)
>>> tsl_price = rename_stop_level(tsl_price)
>>> tp_price = rename_stop_level(tp_price)

>>> sl_exits.columns
MultiIndex([(0.01,   0, 'BTC-USD'),
            (0.01,   0, 'ETH-USD'),
            (0.01,   0, 'XRP-USD'),
            ...
            ( 1.0, 399, 'XLM-USD'),
            ( 1.0, 399, 'XMR-USD'),
            ( 1.0, 399, 'ADA-USD')],
            names=['stop_value', 'split', 'symbol'], length=400000)
```

A major feature of VBT is its strong emphasis on data science, allowing us to apply popular
analysis tools to nearly any part of the backtesting pipeline. For example, let's explore how the
number of exit signals depends on the stop type and value:

```pycon
>>> pd.Series({
...     "SL": sl_exits.vbt.signals.total().mean(),
...     "TS": tsl_exits.vbt.signals.total().mean(),
...     "TP": tp_exits.vbt.signals.total().mean()
... }, name="avg_num_signals")
SL    0.428585
TS    0.587100
TP    0.520042
Name: avg_num_signals, dtype: float64

>>> def groupby_stop_value(df):
...     return df.vbt.signals.total().groupby("stop_value").mean()

>>> pd.DataFrame({
...     "Stop Loss": groupby_stop_value(sl_exits),
...     "Trailing Stop": groupby_stop_value(tsl_exits),
...     "Take Profit": groupby_stop_value(tp_exits)
... }).vbt.plot(
...     xaxis_title="Stop value", 
...     yaxis_title="Avg number of signals"
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/avg_num_signals.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/avg_num_signals.dark.svg#only-dark){: .iimg loading=lazy }

We see that TS is by far the most common exit signal. The SL and TP curves track closely up to
a stop value of 50% and then diverge, with TP dominating. While it may seem that bulls are mostly
in charge, especially for larger price moves, remember that it is much easier to achieve a 50% profit
than a 50% loss, since the latter requires a 100% profit to recover. This means that negative downward
spikes tend to dominate small to medium price changes (and potentially shake out weak hands). These
are well-known dynamics in cryptocurrency markets.

To simplify our analysis going forward, we want to ensure that each column has at least one exit
signal to close the position. If a column has no exit signal, we should add one at the last
timestamp. We do this by combining the stop exits with a last-bar exit using the _OR_ rule and
selecting whichever signal comes first:

```pycon
>>> sl_exits.iloc[-1, :] = True
>>> tsl_exits.iloc[-1, :] = True
>>> tp_exits.iloc[-1, :] = True

>>> sl_exits = sl_exits.vbt.signals.first_after(entries)  # (1)!
>>> tsl_exits = tsl_exits.vbt.signals.first_after(entries)
>>> tp_exits = tp_exits.vbt.signals.first_after(entries)

>>> pd.Series({
...     "SL": sl_exits.vbt.signals.total().mean(),
...     "TS": tsl_exits.vbt.signals.total().mean(),
...     "TP": tp_exits.vbt.signals.total().mean()
... }, name="avg_num_signals")
SL    1.0
TS    1.0
TP    1.0
Name: avg_num_signals, dtype: float64
```

1. Select the first exit after each entry using [SignalsAccessor.first_after](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first_after).

Next, we will generate signals for the two remaining exit types: random and holding. These will
act as benchmarks to compare SL, TS, and TP against.

"Holding" exit signals are placed at the very last bar of each time series. In most cases, we do not
need to bother with these, since we can simply assess open positions. However, for consistency,
we want each column to have exactly one signal. Another reason is to ensure the shape and columns
match those of the stop signals, so we can concatenate all DataFrames later.

```pycon
>>> hold_exits = pd.DataFrame.vbt.signals.empty_like(sl_exits)
>>> hold_exits.iloc[-1, :] = True
>>> hold_price = vbt.broadcast_to(split_ohlcv["Close"], sl_price)

>>> hold_exits.shape
(180, 400000)
```

To generate random exit signals, simply shuffle any signal array. The only requirement is that
each column contains exactly one signal.

```pycon
>>> rand_exits = hold_exits.vbt.shuffle(seed=seed)  # (1)!
>>> rand_price = hold_price

>>> rand_exits.shape
(180, 400000)
```

1. This operation returns a new array.

The final step is to concatenate all DataFrames along the column axis and add a new column level
called `exit_type`:

```pycon
>>> exits = pd.DataFrame.vbt.concat(
...     sl_exits, 
...     tsl_exits, 
...     tp_exits, 
...     rand_exits, 
...     hold_exits, 
...     keys=pd.Index(exit_types, name="exit_type")
... )
>>> del sl_exits  # (1)!
>>> del tsl_exits
>>> del tp_exits
>>> del rand_exits
>>> del hold_exits

>>> exits.shape
(180, 2000000)

>>> price = pd.DataFrame.vbt.concat(
...     sl_price, 
...     tsl_price, 
...     tp_price, 
...     rand_price, 
...     hold_price, 
...     keys=pd.Index(exit_types, name="exit_type")
... )
>>> del sl_price
>>> del tsl_price
>>> del tp_price
>>> del rand_price
>>> del hold_price

>>> price.shape
```

1. Delete DataFrames to free up memory.

The `exits` array now contains 2,000,000 columns—one for each backtest. The column hierarchy is
complete, with one tuple of parameters per backtest.

```pycon
>>> exits.columns
MultiIndex([(     'SL', 0.01,   0, 'BTC-USD'),
            (     'SL', 0.01,   0, 'ETH-USD'),
            (     'SL', 0.01,   0, 'XRP-USD'),
            ...
            ('Holding',  1.0, 399, 'XLM-USD'),
            ('Holding',  1.0, 399, 'XMR-USD'),
            ('Holding',  1.0, 399, 'ADA-USD')],
            names=['exit_type', 'stop_value', 'split', 'symbol'], 
            length=2000000)
```

!!! warning
    One boolean array takes roughly 400 MB of RAM:

    ```pycon
    >>> print(exits.vbt.getsize())
    390.0 MB
    ```

    One floating array takes roughly 3 GB of RAM:

    ```pycon
    >>> print(price.vbt.getsize())
    2.9 GB
    ```

This setup allows us to group signals by one or more levels and easily analyze them together.
For example, let's compare different exit types and stop values by the average distance
from exit signal to entry signal (in days):

```pycon
>>> avg_distance = entries.vbt.signals.between_ranges(target=exits)\  # (1)!
...     .duration.mean()\
...     .groupby(["exit_type", "stop_value"])\
...     .mean()\
...     .unstack(level="exit_type")

>>> avg_distance.mean()
exit_type
Holding    179.000000
Random      89.432010
SL         124.686960
TP         113.887502
TS         104.159855
dtype: float64

>>> avg_distance[exit_types].vbt.plot(
...     xaxis_title="Stop value", 
...     yaxis_title="Avg distance to entry"
... ).show()
```

1. Using [SignalsAccessor.between_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.between_ranges).

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/avg_distance.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/avg_distance.dark.svg#only-dark){: .iimg loading=lazy }

This scatterplot gives a more detailed look at the distribution of exit signals.
As expected, plain holding exits occur exactly 179 days after entry (the maximum possible), while
random exits are distributed evenly across the time window and are not affected by any stop value.
We are most interested in the stop curves, which are flat, indicating high price volatility during
our timeframe. The lower the curve, the greater the likelihood of hitting a stop.
For example, a TS of 20% is hit after an average of only 30 days, while SL and TP stops would
take 72 and 81 days, respectively. But is an early exit actually beneficial?

## Simulation

Now comes the actual backtesting part:

```pycon
>>> %%time
>>> pf = vbt.Portfolio.from_signals(
...     split_ohlcv["Close"],  # (1)!
...     entries, 
...     exits, 
...     price=price
... )

>>> len(pf.orders)
3995570
CPU times: user 21.2 s, sys: 9.11 s, total: 30.3 s
Wall time: 51.5 s
```

1. We usually want to provide OHLC prices rather than just the closing price, but since OHLC was
already used to generate the signal arrays, the closing price is sufficient here.

Pretty easy, right?

The simulation took about 50 seconds on my Apple M1 and produced a total of 3,995,570 orders
ready for analysis (should be 4 million, but some price data appear to be missing).
Keep in mind that any floating array generated by the portfolio object with the same shape as our
exit signals, such as portfolio value or returns, requires 8 * 180 * 2,000,000 bytes, or almost 3 GB
of RAM. We can analyze anything from trades to Sharpe ratio, but given the size of the data,
we will focus on a fast-to-calculate metric: total return.

```pycon
>>> total_return = pf.total_return
>>> del pf  # (1)!

>>> total_return.shape
(2000000,)
```

1. Delete the portfolio instance to free up memory.

If your computer takes a long time to run the simulation, you can:

* Use [Google Colab](https://colab.research.google.com/).

* Reduce the parameter space (for example, lower stop value granularity from 1% to 2%).

* Use random search (for example, pick a random subset of columns).

* Cast to `np.float32` or lower (if supported).

* Split the exit signals into chunks and simulate each chunk separately. Just make sure each
chunk matches the shape of the price and entries arrays (and remember to delete the previous
portfolio before running a new one):

```pycon
>>> total_returns = []
>>> for i in vbt.ProgressBar(range(len(exit_types))):  # (1)!
...     exit_type_columns = exits.columns.get_level_values("exit_type")
...     chunk_mask = exit_type_columns == exit_types[i]
...     chunk_pf = vbt.Portfolio.from_signals(
...         split_ohlcv["Close"], 
...         entries, 
...         exits.loc[:, chunk_mask],  # (2)!
...         price=price.loc[:, chunk_mask]
...     )
...     total_returns.append(chunk_pf.total_return)
...     
...     del chunk_pf
...     vbt.flush()  # (3)!
    
>>> total_return = pd.concat(total_returns)

>>> total_return.shape
```

1. Iterate over exit types.

2. Select the columns that correspond to the current exit type.

3. Delete the portfolio instance and force garbage collection.

[=100% "Chunk 5/5"]{: .candystripe .candystripe-animate }

```pycon
>>> total_return = pd.concat(total_returns)

>>> total_return.shape
(2000000,)
```

This approach has similar execution time but is much easier on memory.

## Performance

The first step is always to look at the baseline distribution:

```pycon
>>> return_by_type = total_return.unstack(level="exit_type")[exit_types]

>>> return_by_type["Holding"].describe(percentiles=[])
count    400000.000000
mean          0.086025
std           0.824799
min          -0.910154
50%          -0.139127
max           6.490103
Name: Holding, dtype: float64

>>> purple_color = vbt.settings["plotting"]["color_schema"]["purple"]
>>> return_by_type["Holding"].vbt.histplot(
...     xaxis_title="Total return",
...     xaxis_tickformat=".2%",
...     yaxis_title="Count",
...     trace_kwargs=dict(marker_color=purple_color)
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/holding_histplot.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/holding_histplot.dark.svg#only-dark){: .iimg loading=lazy }

The distribution of holding performance across time windows is highly left-skewed. On one hand,
this indicates prolonged sideways and bearish regimes within our timeframe. On the other hand,
since the price of any asset can rise infinitely but only fall to zero, the distribution is naturally
denser on the left and sparser on the right. Every second return is a loss of more than 6%, but
thanks to occasional bull runs, the strategy still achieves an average profit of 9%.

Let's add other strategies to our analysis:

```pycon
>>> pd.DataFrame({
...     "Mean": return_by_type.mean(),
...     "Median": return_by_type.median(),
...     "Std": return_by_type.std(),
... })
                Mean    Median       Std
exit_type                              
SL         0.054361 -0.158458  0.764171
TS         0.057613 -0.093185  0.692137
TP         0.036844  0.077450  0.465555
Random     0.025284 -0.073612  0.575394
Holding    0.086025 -0.139127  0.824799

>>> return_by_type.vbt.boxplot(
...     trace_kwargs=dict(boxpoints=False),  # (1)!
...     yaxis_title="Total return",
...     yaxis_tickformat=".2%"
... ).show()
```

1. Plotly may be slow when drawing a huge number of outlier points.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/return_by_type.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/return_by_type.dark.svg#only-dark){: .iimg loading=lazy }

None of the strategies outperforms the baseline's average return. However, the TP strategy is the most
consistent—while it sets an upper bound that limits extreme profits (note the missing outliers),
its trade returns are less volatile and mostly positive. SL and TS are unbounded at the top because some
stops are never triggered, causing those columns to fall back to plain holding. The random strategy is
interesting, too: while its average return is lower, it ranks second after TP in terms of median return
and volatility.

To further support this picture, let's calculate each strategy's win rate:

```pycon
>>> (return_by_type > 0).mean().rename("win_rate")
exit_type
SL         0.305702
TS         0.364098
TP         0.585405
Random     0.405520
Holding    0.402250
Name: win_rate, dtype: float64
```

Almost 60% of TP trades are profitable—a stark contrast to the other strategies. However, a high
win rate does not guarantee long-term trading success if your winning trades are much smaller
than your losing ones. Therefore, let's group by stop type and value and calculate the
[expectancy](https://www.icmarkets.com/blog/reward-to-risk-win-ratio-and-expectancy/):

```pycon
>>> init_cash = vbt.settings.portfolio["init_cash"]

>>> def get_expectancy(return_by_type, level_name):
...     grouped = return_by_type.groupby(level_name, axis=0)
...     win_rate = grouped.apply(lambda x: (x > 0).mean())
...     avg_win = grouped.apply(lambda x: init_cash * x[x > 0].mean())
...     avg_win = avg_win.fillna(0)
...     avg_loss = grouped.apply(lambda x: init_cash * x[x < 0].mean())
...     avg_loss = avg_loss.fillna(0)
...     return win_rate * avg_win - (1 - win_rate) * np.abs(avg_loss)
    
>>> expectancy_by_stop = get_expectancy(return_by_type, "stop_value")

>>> expectancy_by_stop.mean()
exit_type
SL         5.436087
TS         5.761281
TP         3.684371
Random     2.362847
Holding    8.602499
dtype: float64

>>> expectancy_by_stop.vbt.plot(
...     xaxis_title="Stop value", 
...     yaxis_title="Expectancy"
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/expectancy_by_stop.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/expectancy_by_stop.dark.svg#only-dark){: .iimg loading=lazy }

Each strategy can steadily add to our account over time, with the holding strategy emerging as
the clear winner—we can expect to add nearly $9 out of every $100 invested after each
6-month holding period. The only configuration that outperforms the baseline is TS, specifically
with stop values ranging from 20% to 40%. The weakest performers are SL and TS with stop values
around 45% and 60%, which seem to be triggered at the bottoms of major corrections, making them
even worse than random exits. The TP strategy, on the other hand, outperforms the random exit
strategy once the stop value crosses 30%. In general, patience seems to pay off in cryptocurrencies.

Finally, let's see how these strategies fare under different market conditions. We will use a simplified
regime classification that divides holding returns into 20 bins and calculates each strategy's
expectancy within those bins (the last bin is excluded for chart readability). Due to the highly
skewed distribution of holding returns, we need to ensure that the bins are equal in size.

```pycon
>>> return_values = np.sort(return_by_type["Holding"].values)
>>> idxs = np.ceil(np.linspace(0, len(return_values) - 1, 21)).astype(int)
>>> bins = return_values[idxs][:-1]

>>> def bin_return(return_by_type):
...     classes = pd.cut(return_by_type["Holding"], bins=bins, right=True)
...     new_level = np.array(classes.apply(lambda x: x.right))
...     new_level = pd.Index(new_level, name="bin_right")
...     return return_by_type.vbt.add_levels(new_level, axis=0)

>>> binned_return_by_type = bin_return(return_by_type)

>>> expectancy_by_bin = get_expectancy(binned_return_by_type, "bin_right")

>>> expectancy_by_bin.vbt.plot(
...     trace_kwargs=dict(mode="lines"),
...     xaxis_title="Total return of holding",
...     xaxis_tickformat=".2%",
...     yaxis_title="Expectancy"
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/expectancy_by_bin.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/stop-signals/expectancy_by_bin.dark.svg#only-dark){: .iimg loading=lazy }

The chart above confirms the general intuition behind stop orders:
SL and TS help limit losses during downtrends, TP works well for short-term trades seeking quick gains,
and holding excels in high-growth markets. Interestingly, random exits perform poorly in sideways and
bullish periods, but they often match or outperform stop exits in bear markets.

## Bonus: Dashboard

Dashboards can be a powerful way to interact with your data.

Let's first define the components of our dashboard. We have two types of components: controls
(such as an asset dropdown) and graphs. Controls set parameters and trigger updates for the graphs.

```pycon
>>> range_starts = pd.DatetimeIndex(list(map(lambda x: x[0], split_indexes)))
>>> range_ends = pd.DatetimeIndex(list(map(lambda x: x[-1], split_indexes)))

>>> symbol_lvl = return_by_type.index.get_level_values("symbol")
>>> split_lvl = return_by_type.index.get_level_values("split")
>>> range_start_lvl = range_starts[split_lvl]
>>> range_end_lvl = range_ends[split_lvl]

>>> asset_multi_select = ipywidgets.SelectMultiple(
...     options=symbols,
...     value=symbols,
...     rows=len(symbols),
...     description="Symbols"
... )
>>> dates = np.unique(yfdata.wrapper.index)
>>> date_range_slider = ipywidgets.SelectionRangeSlider(
...     options=dates,
...     index=(0, len(dates)-1),
...     orientation="horizontal",
...     readout=False,
...     continuous_update=False
... )
>>> range_start_label = ipywidgets.Label()
>>> range_end_label = ipywidgets.Label()
>>> metric_dropdown = ipywidgets.Dropdown(
...     options=["Mean", "Median", "Win Rate", "Expectancy"],
...     value="Expectancy"
... )
>>> stop_scatter = vbt.Scatter(
...     trace_names=exit_types,
...     x_labels=stops, 
...     xaxis_title="Stop value", 
...     yaxis_title="Expectancy"
... )
>>> stop_scatter_img = ipywidgets.Image(
...     format="png",
...     width=stop_scatter.fig.layout.width,
...     height=stop_scatter.fig.layout.height
... )
>>> bin_scatter = vbt.Scatter(
...     trace_names=exit_types,
...     x_labels=expectancy_by_bin.index, 
...     trace_kwargs=dict(mode="lines"),
...     xaxis_title="Total return of holding",
...     xaxis_tickformat="%",
...     yaxis_title="Expectancy"
... )
>>> bin_scatter_img = ipywidgets.Image(
...     format="png",
...     width=bin_scatter.fig.layout.width,
...     height=bin_scatter.fig.layout.height
... )
```

The second step is to define the update function, which is triggered whenever any control 
is changed. We also call this function manually to initialize the graphs with default parameters.

```pycon
>>> def update_scatter(*args, **kwargs):
...     _symbols = asset_multi_select.value
...     _from = date_range_slider.value[0]
...     _to = date_range_slider.value[1]
...     _metric_name = metric_dropdown.value
...     
...     range_mask = (range_start_lvl >= _from) & (range_end_lvl <= _to)
...     asset_mask = symbol_lvl.isin(_symbols)
...     filt = return_by_type[range_mask & asset_mask]
...     
...     filt_binned = bin_return(filt)
...     if _metric_name == "Mean":
...         filt_metric = filt.groupby("stop_value").mean()
...         filt_bin_metric = filt_binned.groupby("bin_right").mean()
...     elif _metric_name == "Median":
...         filt_metric = filt.groupby("stop_value").median()
...         filt_bin_metric = filt_binned.groupby("bin_right").median()
...     elif _metric_name == "Win Rate":
...         filt_metric = (filt > 0).groupby("stop_value").mean()
...         filt_bin_metric = (filt_binned > 0).groupby("bin_right").mean()
...     elif _metric_name == "Expectancy":
...         filt_metric = get_expectancy(filt, "stop_value")
...         filt_bin_metric = get_expectancy(filt_binned, "bin_right")
...         
...     stop_scatter.fig.update_layout(yaxis_title=_metric_name)
...     stop_scatter.update(filt_metric)
...     stop_scatter_img.value = stop_scatter.fig.to_image(format="png")
...     
...     bin_scatter.fig.update_layout(yaxis_title=_metric_name)
...     bin_scatter.update(filt_bin_metric)
...     bin_scatter_img.value = bin_scatter.fig.to_image(format="png")
...     
...     range_start_label.value = np.datetime_as_string(
...         _from.to_datetime64(), unit="D")
...     range_end_label.value = np.datetime_as_string(
...         _to.to_datetime64(), unit="D")
    
>>> asset_multi_select.observe(update_scatter, names="value")
>>> date_range_slider.observe(update_scatter, names="value")
>>> metric_dropdown.observe(update_scatter, names="value")
>>> update_scatter()
```

In the final step, we define the dashboard layout and run it:

```pycon
>>> dashboard = ipywidgets.VBox([
...     asset_multi_select,
...     ipywidgets.HBox([
...         range_start_label,
...         date_range_slider,
...         range_end_label
...     ]),
...     metric_dropdown,
...     stop_scatter_img,
...     bin_scatter_img
... ])
>>> dashboard
```

<div class="grid cards width-eighty" markdown>

- :material-monitor-dashboard:{ .lg .middle } __Dashboard__

    ---

    Run the notebook to view the dashboard!

</div>

## Summary

Large-scale backtesting is useful for much more than just hyperparameter optimization. When used
thoughtfully, it provides a way to explore complex trading phenomena. By leveraging multidimensional
arrays, dynamic compilation, and pandas integration—as done by VBT—you can quickly gain
new insights by applying popular data science tools to each part of your backtesting pipeline.

In this example, we ran 2 million tests to see how various stop values affect stop signals' performance
and how stop signals compare to holding and random trading. The results confirm many of our earlier
beliefs about stop signals across different market conditions, but they also reveal optimal
configurations that may have worked well in recent years of trading cryptocurrencies.

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/stop-signals.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/StopSignals.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
children:
  - https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/alignment/
  - https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/aggregation/
name: MTF analysis
type: page
icon: material-chart-timeline
---

# MTF analysis

??? youtube "Multiple Timeframe Analysis on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/L6g3mslaTwE?si=viqfxQjMRjXSBGy7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

Focusing on a single time frame can cause us to miss the broader trend, overlook important support and
resistance levels, and ignore high-probability entry and stop points. By observing the same pair across
different time frames (or time compressions), we can identify the overall movement of an asset
([the trend is your friend](https://www.investopedia.com/articles/forex/05/050505.asp)) and spot key
chart patterns. In fact, all technical indicators can yield different results depending on the time
frame, and combining these results can provide a more complete view of the market we are trading in.

## Resampling

Since VBT works with time series, the main operation that allows us to switch between different
time frames is called _resampling_. There are two types of resampling: upsampling and downsampling.

[Upsampling](https://en.wikipedia.org/wiki/Upsampling) converts a time series to a shorter time frame
that has a higher frequency, such as changing daily prices to hourly prices. The "up" prefix means
there is an increase in the number of data points. This operation does not result in any information
loss because no data is removed, just re-indexed: the value for each day appears at the very first
hour in the upsampled array, while all other hours have NaN. By forward-filling those NaN values, you
can compare any daily time series with an hourly time series!

```mermaid
flowchart LR;
    subgraph daily [ ]
    id1["2020-01-01"]
    end
    subgraph hourly [ ]
    id2["2020-01-01 00:00:00"]
    id3["2020-01-01 01:00:00"]
    id4["..."]
    id5["2020-01-01 23:00:00"]
    
    id1 -->|"value"| id2;
    id1 -->|"NaN"| id3;
    id1 -->|"NaN"| id4;
    id1 -->|"NaN"| id5;
    end
```

(Reload the page if the diagram does not appear.)

[Downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)), on the other hand,
converts a time series to a longer time frame with a lower frequency, such as turning hourly prices
into daily prices. The "down" prefix means there is a decrease in the number of data points. Unlike
upsampling, downsampling **causes information loss** because multiple pieces of data are aggregated
into one. That's why time frames are also called time compressions. Even though we lose some
information, we can now see a broader trend!

```mermaid
flowchart LR;
    subgraph daily [ ]
    id5["2020-01-01"]
    end
    subgraph hourly [ ]
    id1["2020-01-01 00:00:00"]
    id2["2020-01-01 01:00:00"]
    id3["..."]
    id4["2020-01-01 23:00:00"]
    
    id1 -->|"value_0"| id5;
    id2 -->|"value_1"| id5;
    id3 -->|"..."| id5;
    id4 -->|"value_n"| id5;
    end
```

!!! tip
    Downsampling is similar to a moving average, as both aggregate data at each time step
    to show a broader trend.

## Data

Before pulling any data, we should ask ourselves: _"What is the shortest time frame we want to analyze?"_
Once we answer this question, we need to pull data with that exact granularity. For example, to work
with the time frames `H1` (1 hour), `H4` (4 hours), and `D1` (1 day), we need data at least as
granular as `H1`, which can later be downsampled to get the `H4` and `D1` time frames.

!!! note
    This does not work the other way around: we cannot upsample `H4` or `D1` data to generate `H1`
    because most data points would just become NaN.

```pycon
>>> from vectorbtpro import *

>>> h1_data = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="2020-01-01 UTC", 
...     end="2021-01-01 UTC",
...     timeframe="1h"
... )
```

[=100% "Period 18/18"]{: .candystripe .candystripe-animate }

Let's persist the data locally to avoid re-fetching it every time we start a new runtime:

```pycon
>>> h1_data.to_hdf()
```

We can then easily access the saved data using [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData):

```pycon
>>> h1_data = vbt.HDFData.pull("BinanceData.h5")
```

Let's look at the index of the data:

```pycon
>>> h1_data.wrapper.index  # (1)!
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 01:00:00+00:00',
                '2020-01-01 02:00:00+00:00', '2020-01-01 03:00:00+00:00',
                '2020-01-01 04:00:00+00:00', '2020-01-01 05:00:00+00:00',
                ...
                '2020-12-31 18:00:00+00:00', '2020-12-31 19:00:00+00:00',
                '2020-12-31 20:00:00+00:00', '2020-12-31 21:00:00+00:00',
                '2020-12-31 22:00:00+00:00', '2020-12-31 23:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', length=8767, freq=None)
```

1. [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data), like many other VBT classes,
holds a wrapper, which contains useful metadata about the index and columns.

As expected, the index starts at midnight on January 1 and ends at 11 PM on December 31.
But why is `freq=None`? Pandas was not able to infer the frequency because some data
points are missing. This often happens when the exchange experiences downtime. To get all
the missing indices, we can create a resampler using [Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler)
and then use [Resampler.index_difference](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.index_difference)
with `reverse=True`:

```pycon
>>> h1_resampler = h1_data.wrapper.get_resampler("1h")  # (1)!
>>> h1_resampler.index_difference(reverse=True)  # (2)!
DatetimeIndex(['2020-02-09 02:00:00+00:00', '2020-02-19 12:00:00+00:00',
                '2020-02-19 13:00:00+00:00', '2020-02-19 14:00:00+00:00',
                '2020-02-19 15:00:00+00:00', '2020-02-19 16:00:00+00:00',
                '2020-03-04 10:00:00+00:00', '2020-04-25 02:00:00+00:00',
                '2020-04-25 03:00:00+00:00', '2020-06-28 02:00:00+00:00',
                '2020-06-28 03:00:00+00:00', '2020-06-28 04:00:00+00:00',
                '2020-11-30 06:00:00+00:00', '2020-12-21 15:00:00+00:00',
                '2020-12-21 16:00:00+00:00', '2020-12-21 17:00:00+00:00',
                '2020-12-25 02:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Resampler is a mapper between the index in the wrapper (source index)
and the resampled index (target index).

2. Gets all indices in the target index that are not present in the source index.

These are the time periods when Binance was likely down. The good news is that we do not need
to set these data points to NaN because VBT handles missing indices just fine. In fact,
marking these points as missing would only bloat the data and cause problems with indicators that
do not handle missing values well, such as TA-Lib.

How can we downsample this data to `H4` and `D1`? If we check the columns stored in the data
instance, we will see familiar column names: `Open`, `High`, `Low`, `Close`, and `Volume`.

```pycon
>>> h1_data.wrapper.columns
Index(['Open', 'High', 'Low', 'Close', 'Volume', 'Close time', 'Quote volume',
        'Number of trades', 'Taker base volume', 'Taker quote volume'],
      dtype='object')
```

First, let's remove the columns that are not relevant to us right now:

```pycon
>>> h1_ohlcv_data = h1_data[["Open", "High", "Low", "Close", "Volume"]]
```

The most standard way to resample any OHLCV data is by using Pandas:

```pycon
>>> h4_ohlcv = h1_ohlcv_data.get().resample("4h").agg({  # (1)!
...     "Open": "first",
...     "High": "max",
...     "Low": "min",
...     "Close": "last",
...     "Volume": "sum"
... })
>>> h4_ohlcv
                                Open      High       Low     Close  \
Open time                                                           
2020-01-01 00:00:00+00:00   7195.24   7245.00   7175.46   7225.01   
2020-01-01 04:00:00+00:00   7225.00   7236.27   7199.11   7209.83   
2020-01-01 08:00:00+00:00   7209.83   7237.73   7180.00   7197.20   
...                             ...       ...       ...       ...   
2020-12-31 12:00:00+00:00  28910.29  28989.03  27850.00  28770.00   
2020-12-31 16:00:00+00:00  28782.01  29000.00  28311.00  28897.83   
2020-12-31 20:00:00+00:00  28897.84  29169.55  28780.00  28923.63   

                                  Volume  
Open time                                
2020-01-01 00:00:00+00:00   2833.749180  
2020-01-01 04:00:00+00:00   2061.295051  
2020-01-01 08:00:00+00:00   3166.654361  
...                                 ...  
2020-12-31 12:00:00+00:00  19597.147389  
2020-12-31 16:00:00+00:00  10279.179141  
2020-12-31 20:00:00+00:00   7875.879035  

[2196 rows x 5 columns]
```

1. See [Resampler.aggregate](https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.aggregate.html).

We can see that the time interval has increased from 1 hour to 4 hours; in other words, we have
constructed one larger bar from four smaller ones:

```pycon
>>> h1_ohlcv_data.get().iloc[:4]
                              Open     High      Low    Close      Volume
Open time                                                                
2020-01-01 00:00:00+00:00  7195.24  7196.25  7175.46  7177.02  511.814901
2020-01-01 01:00:00+00:00  7176.47  7230.00  7175.71  7216.27  883.052603
2020-01-01 02:00:00+00:00  7215.52  7244.87  7211.41  7242.85  655.156809
2020-01-01 03:00:00+00:00  7242.66  7245.00  7220.00  7225.01  783.724867

>>> h4_ohlcv.iloc[[0]]
                              Open    High      Low    Close      Volume
Open time                                                               
2020-01-01 00:00:00+00:00  7195.24  7245.0  7175.46  7225.01  2833.74918
```

Great! But, as with many things, VBT provides special methods that can perform this even more
efficiently or flexibly (and often both).

Remember how most classes in VBT subclass [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)?
This class, in turn, subclasses [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping), which
is designed to manage all Pandas objects stored in a class instance. Because it also contains Pandas
metadata like index and columns, we can use that index for resampling. Specifically, any
subclass of [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) has an abstract method
[Wrapping.resample](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.resample),
which can be overridden to resample complex VBT objects, such as instances of
[Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) and
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

Fortunately, VBT has already implemented this method in most classes that can be resampled.
Usually, it forwards most arguments and keyword arguments to
[Wrapping.get_resampler](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.get_resampler)
to build a resampler, and then applies this resampler to all Pandas objects stored in the VBT
object. For example, [Data.resample](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.resample) looks for
OHLCV columns in a data instance and resamples them automatically. But what about other columns,
such as `Number of trades`? Their resampling function can be defined in the feature config
[Data.feature_config](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.feature_config). Even better,
VBT has defined resampling functions for all columns in all remote data classes!

Let's check the feature config of [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData):

```pycon
>>> print(vbt.prettify(vbt.BinanceData.feature_config))
Config({
    'Close time': {
        'resample_func': <function BinanceData.<lambda> at 0x7fd2d60c4378>
    },
    'Quote volume': {
        'resample_func': <function BinanceData.<lambda> at 0x7fd2d60c4400>
    },
    'Number of trades': {
        'resample_func': <function BinanceData.<lambda> at 0x7fd2d60c4488>
    },
    'Taker base volume': {
        'resample_func': <function BinanceData.<lambda> at 0x7fd2d60c4510>
    },
    'Taker quote volume': {
        'resample_func': <function BinanceData.<lambda> at 0x7fd2d60c4598>
    }
})
```

Each of these lambda functions takes the Pandas object and the resampler and performs the operation
using [GenericAccessor.resample_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_apply).

!!! tip
    There is no need to define resampling functions for OHLCV columns because VBT already knows how to handle them.

Let's downsample `H1` to `H4` and `D1` with just one line of code each:

```pycon
>>> h1_data.use_feature_config_of(vbt.BinanceData) # (1)!

>>> h4_data = h1_data.resample("4h")
>>> d1_data = h1_data.resample("1d")
```

1. [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) stores information from Binance
but does not have any built-in knowledge of its layout, so we use the feature config from
[BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData). Note that this is only
necessary for data classes that are not tied to a specific column layout.

That's it!

```pycon
>>> d1_data.get().iloc[[0, -1]]  # (1)!
                                Open     High       Low     Close  \
Open time                                                          
2020-01-01 00:00:00+00:00   7195.24   7255.0   7175.15   7200.85   
2020-12-31 00:00:00+00:00  28875.55  29300.0  27850.00  28923.63   

                                  Volume  Quote volume  Trade count  \
Open time                                                            
2020-01-01 00:00:00+00:00  16792.388165  1.212145e+08       194010   
2020-12-31 00:00:00+00:00  75508.505152  2.173600e+09      1552793   

                            Taker base volume  Taker quote volume  
Open time                                                         
2020-01-01 00:00:00+00:00        8946.955535        6.459779e+07  
2020-12-31 00:00:00+00:00       36431.622080        1.049389e+09 
```

1. Get the first and last row of the downsampled data.

We can verify the resampling results by comparing them to data from the same time frame
fetched directly from Binance:

```pycon
>>> vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="2020-01-01 UTC", 
...     end="2021-01-01 UTC",
...     timeframe="1d"
... ).get().iloc[[0, -1]]
```

[=100% "100%"]{: .candystripe .candystripe-animate }

```pycon
                                Open     High       Low     Close  \
Open time                                                          
2020-01-01 00:00:00+00:00   7195.24   7255.0   7175.15   7200.85   
2020-12-31 00:00:00+00:00  28875.55  29300.0  27850.00  28923.63   

                                  Volume  Quote volume  Trade count  \
Open time                                                            
2020-01-01 00:00:00+00:00  16792.388165  1.212145e+08       194010   
2020-12-31 00:00:00+00:00  75508.505152  2.173600e+09      1552793   

                            Taker base volume  Taker quote volume  
Open time                                                         
2020-01-01 00:00:00+00:00        8946.955535        6.459779e+07  
2020-12-31 00:00:00+00:00       36431.622080        1.049389e+09
```

Our data instance has just resampled itself in exactly the same way as done by Binance :fire:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/mtf-analysis/index.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/MTFAnalysis.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/alignment/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/
name: Alignment
type: page
icon: material-chart-timeline
---

# Alignment

Comparing time series across different time frames can be challenging. For example, suppose you want 
to calculate the ratio between the close price on `H1` and `H4`.

## Pandas

Comparing both time series using Pandas produces the following results:

```pycon
>>> h1_close = h1_data.get("Close")
>>> h4_close = h4_data.get("Close")

>>> h1_close.iloc[:4]
Open time
2020-01-01 00:00:00+00:00    7177.02
2020-01-01 01:00:00+00:00    7216.27
2020-01-01 02:00:00+00:00    7242.85
2020-01-01 03:00:00+00:00    7225.01
Name: Close, dtype: float64

>>> h4_close.iloc[:1]
Open time
2020-01-01 00:00:00+00:00    7225.01
Freq: 4H, Name: Close, dtype: float64

>>> h1_h4_ratio = h1_close / h4_close
>>> h1_h4_ratio.iloc[:4]
Open time
2020-01-01 00:00:00+00:00    0.993358
2020-01-01 01:00:00+00:00         NaN
2020-01-01 02:00:00+00:00         NaN
2020-01-01 03:00:00+00:00         NaN
Name: Close, dtype: float64
```

If you think this result is correct, you are mistaken :upside_down_face:

Here, only the timestamp `2020-01-01 00:00:00` in both time series is being compared, while the other
timestamps become NaN. This is because Pandas matches values by their index labels. So far, so good.
The real issue is that each timestamp represents the **open time** and contains information up to the
next timestamp. In reality, the close price at `2020-01-01 00:00:00` occurs right before
`2020-01-01 01:00:00` in `h1_close` and right before `2020-01-01 04:00:00` in `h4_close`. This means
we are comparing information from the past with information from the future, which leads to
look-ahead bias!

```mermaid
flowchart TD;
    subgraph H4 [ ]
    id1["2020-01-01 00:00:00"]
    id2["2020-01-01 04:00:00"]
    
    id1 --o|"close_3"| id2;
    end
    subgraph H1 [ ]
    id3["2020-01-01 00:00:00"]
    id4["2020-01-01 01:00:00"]
    id5["2020-01-01 02:00:00"]
    id6["2020-01-01 03:00:00"]
    id7["2020-01-01 04:00:00"]
    
    id3 --o|"close_0"| id4;
    id4 --o|"close_1"| id5;
    id5 --o|"close_2"| id6;
    id6 --o|"close_3"| id7;
    end
```

As shown in the diagram above, we should only compare `close_3` between both time frames.
Comparing `close_0` and `close_3` will not immediately trigger an error (!), but you could run into
big issues in production and not realize why your backtesting results are so inaccurate.

To make a fairer comparison of the close price, each timestamp in `h1_close` should be compared with
the **previous** timestamp in `h4_close`:

```pycon
>>> h4_close_shifted = h4_close.shift()
>>> h1_h4_ratio = h1_close / h4_close_shifted
>>> h1_h4_ratio.iloc[:8]
Open time
2020-01-01 00:00:00+00:00         NaN
2020-01-01 01:00:00+00:00         NaN
2020-01-01 02:00:00+00:00         NaN
2020-01-01 03:00:00+00:00         NaN
2020-01-01 04:00:00+00:00    0.998929
2020-01-01 05:00:00+00:00         NaN
2020-01-01 06:00:00+00:00         NaN
2020-01-01 07:00:00+00:00         NaN
Name: Close, dtype: float64
```

This approach makes more sense, since before `2020-01-01 04:00:00`, the close price at the end of
`2020-01-01 03:00:00` is not yet available. Still, this comparison can be improved, because the
close price at `2020-01-01 03:00:00` in `h1_close` is equal to the close price at 
`2020-01-01 00:00:00` in `h4_close`. Therefore, we can safely shift the resulting series backward:

```pycon
>>> h1_h4_ratio.shift(-1).iloc[:8]
Open time
2020-01-01 00:00:00+00:00         NaN
2020-01-01 01:00:00+00:00         NaN
2020-01-01 02:00:00+00:00         NaN
2020-01-01 03:00:00+00:00    0.998929
2020-01-01 04:00:00+00:00         NaN
2020-01-01 05:00:00+00:00         NaN
2020-01-01 06:00:00+00:00         NaN
2020-01-01 07:00:00+00:00    0.998725
Name: Close, dtype: float64
```

You might wonder why there are so many NaNs, and why many values are not being compared. To address
this, we need to upsample `h4_close` to `H1` and forward-fill the NaN values:

```pycon
>>> h4_h1_close = h4_close.shift(1).resample("1h").last().shift(-1).ffill()
>>> h4_h1_close.iloc[:8]
Open time
2020-01-01 00:00:00+00:00        NaN
2020-01-01 01:00:00+00:00        NaN
2020-01-01 02:00:00+00:00        NaN
2020-01-01 03:00:00+00:00    7225.01  << first close
2020-01-01 04:00:00+00:00    7225.01
2020-01-01 05:00:00+00:00    7225.01
2020-01-01 06:00:00+00:00    7225.01
2020-01-01 07:00:00+00:00    7209.83  << second close
Freq: H, Name: Close, dtype: float64
```

!!! note
    Do not forward or backward shift when downsampling, only when upsampling.

Now, let's plot the first 16 points of both time series for validation:

```pycon
>>> fig = h1_close.rename("H1").iloc[:16].vbt.plot()
>>> h4_h1_close.rename("H4_H1").iloc[:16].vbt.plot(fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/h4_h1.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/h4_h1.dark.svg#only-dark){: .iimg loading=lazy }

As we can see, at each hour, `H4_H1` provides the latest available information from the previous
4 hours. Now, we can compare both time frames safely:

```pycon
>>> h1_h4_ratio = h1_close / h4_h1_close
>>> h1_h4_ratio
Open time
2020-01-01 00:00:00+00:00         NaN
2020-01-01 01:00:00+00:00         NaN
2020-01-01 02:00:00+00:00         NaN
2020-01-01 03:00:00+00:00    1.000000
2020-01-01 04:00:00+00:00    0.998929
...                               ...
2020-12-31 19:00:00+00:00    1.000000
2020-12-31 20:00:00+00:00    1.007920
2020-12-31 21:00:00+00:00         NaN
2020-12-31 22:00:00+00:00         NaN
2020-12-31 23:00:00+00:00         NaN
Name: Close, Length: 8784, dtype: float64
```

The same approach applies to high and low prices, since their information is also available only
at the end of each bar. The open price, however, is known at the start of each bar, so there is
no need to shift `H4` forward or backward:

```pycon
>>> h1_open = h1_data.get("Open")
>>> h4_open  = h4_data.get("Open")

>>> h1_open.iloc[:8]
Open time
2020-01-01 00:00:00+00:00    7195.24
2020-01-01 01:00:00+00:00    7176.47
2020-01-01 02:00:00+00:00    7215.52
2020-01-01 03:00:00+00:00    7242.66
2020-01-01 04:00:00+00:00    7225.00
2020-01-01 05:00:00+00:00    7217.26
2020-01-01 06:00:00+00:00    7224.24
2020-01-01 07:00:00+00:00    7225.88
Name: Open, dtype: float64

>>> h4_h1_open = h4_open.resample("1h").first().ffill()
>>> h4_h1_open.iloc[:8]
Open time
2020-01-01 00:00:00+00:00    7195.24  << first open
2020-01-01 01:00:00+00:00    7195.24
2020-01-01 02:00:00+00:00    7195.24
2020-01-01 03:00:00+00:00    7195.24
2020-01-01 04:00:00+00:00    7225.00  << second open
2020-01-01 05:00:00+00:00    7225.00
2020-01-01 06:00:00+00:00    7225.00
2020-01-01 07:00:00+00:00    7225.00
Freq: H, Name: Open, dtype: float64
```

## VBT

It may seem like getting everything right requires a lot of work. But do not worry, VBT
has you covered! To realign an array safely, use [GenericAccessor.realign_opening](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_opening)
for information available at the beginning of each bar, and [GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing)
for information that becomes available only at the end of each bar:

```pycon
>>> h4_close.vbt.realign_closing("1h")
Open time
2020-01-01 00:00:00+00:00         NaN
2020-01-01 01:00:00+00:00         NaN
2020-01-01 02:00:00+00:00         NaN
2020-01-01 03:00:00+00:00     7225.01
2020-01-01 04:00:00+00:00     7225.01
...                               ...
2020-12-31 16:00:00+00:00    28770.00
2020-12-31 17:00:00+00:00    28770.00
2020-12-31 18:00:00+00:00    28770.00
2020-12-31 19:00:00+00:00    28897.83
2020-12-31 20:00:00+00:00    28897.83
Freq: H, Name: Close, Length: 8781, dtype: float64

>>> h4_open.vbt.realign_opening("1h")
Open time
2020-01-01 00:00:00+00:00     7195.24
2020-01-01 01:00:00+00:00     7195.24
2020-01-01 02:00:00+00:00     7195.24
2020-01-01 03:00:00+00:00     7195.24
2020-01-01 04:00:00+00:00     7225.00
...                               ...
2020-12-31 16:00:00+00:00    28782.01
2020-12-31 17:00:00+00:00    28782.01
2020-12-31 18:00:00+00:00    28782.01
2020-12-31 19:00:00+00:00    28782.01
2020-12-31 20:00:00+00:00    28897.84
Freq: H, Name: Open, Length: 8781, dtype: float64
```

!!! important
    Use `realign_opening` only if the information in the array occurs exactly at the start of the bar
    (such as open price), and `realign_closing` if the information is available only after that (such
    as high, low, and close prices).

That's all—now the results above can be safely combined with any `H1` data :partying_face:

### Resampler

If you want a deeper understanding of how these two functions work, let's first explain what
a "resampler" is in VBT. A resampler is an instance of the
[Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler) class.
It simply stores a source index and frequency, along with a target index and frequency:

```pycon
>>> h4_h1_resampler = h4_close.vbt.wrapper.get_resampler("1h")  # (1)!
>>> h4_h1_resampler.source_index
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 04:00:00+00:00',
                '2020-01-01 08:00:00+00:00', '2020-01-01 12:00:00+00:00',
                '2020-01-01 16:00:00+00:00', '2020-01-01 20:00:00+00:00',
                ...
                '2020-12-31 00:00:00+00:00', '2020-12-31 04:00:00+00:00',
                '2020-12-31 08:00:00+00:00', '2020-12-31 12:00:00+00:00',
                '2020-12-31 16:00:00+00:00', '2020-12-31 20:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', length=2196, freq='4H')

>>> h4_h1_resampler.target_index
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 01:00:00+00:00',
                '2020-01-01 02:00:00+00:00', '2020-01-01 03:00:00+00:00',
                '2020-01-01 04:00:00+00:00', '2020-01-01 05:00:00+00:00',
                ...
                '2020-12-31 15:00:00+00:00', '2020-12-31 16:00:00+00:00',
                '2020-12-31 17:00:00+00:00', '2020-12-31 18:00:00+00:00',
                '2020-12-31 19:00:00+00:00', '2020-12-31 20:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', length=8781, freq='H')

>>> h4_h1_resampler.source_freq
Timedelta('0 days 04:00:00')

>>> h4_h1_resampler.target_freq
Timedelta('0 days 01:00:00')
```

1. Using [Wrapping.get_resampler](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.get_resampler)

With just these two pieces of information, we can handle most resampling tasks. All we need to do
is iterate over both indexes and track which elements in one index correspond to which in the other.
This is performed efficiently using Numba. This also means that, unlike Pandas, VBT allows
for any arbitrary target index in resampling. In fact, the [Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler)
class provides several convenient class methods to create an instance from a variety of Pandas objects
and functions. For example, to create a resampler from a [Pandas Resampler](https://pandas.pydata.org/docs/reference/resampling.html):

```pycon
>>> pd_resampler = h4_close.resample("1h")
>>> vbt.Resampler.from_pd_resampler(pd_resampler, source_freq="4h")
<vectorbtpro.base.resampling.base.Resampler at 0x7ff518c3f358>
```

Or you can use [Resampler.from_date_range](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.from_date_range)
to build a custom hourly index starting from 10 AM and ending at 10 PM:

```pycon
>>> resampler = vbt.Resampler.from_date_range(
...     source_index=h4_close.index,
...     source_freq="4h",
...     start="2020-01-01 10:00:00",
...     end="2020-01-01 22:00:00",
...     freq="1h",
... )
```

You can then pass the resampler directly to [GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing)
to fill in the latest information from the `H4` time frame:

```pycon
>>> h4_close.vbt.realign_closing(resampler)
2020-01-01 10:00:00    7209.83
2020-01-01 11:00:00    7197.20
2020-01-01 12:00:00    7197.20
2020-01-01 13:00:00    7197.20
2020-01-01 14:00:00    7197.20
2020-01-01 15:00:00    7234.19
2020-01-01 16:00:00    7234.19
2020-01-01 17:00:00    7234.19
2020-01-01 18:00:00    7234.19
2020-01-01 19:00:00    7229.48
2020-01-01 20:00:00    7229.48
2020-01-01 21:00:00    7229.48
2020-01-01 22:00:00    7229.48
Freq: H, Name: Close, dtype: float64
```

### Custom index

You can also specify your target index directly. For example, let's get the latest information
on `H4` at the beginning of each month (= downsampling). Note that if you do not specify the target
frequency, VBT will infer it from the target index. In our case, this is `MonthBegin`,
which is a [DateOffset](https://pandas.pydata.org/docs/reference/api/pandas.tseries.offsets.DateOffset.html)
type. Date offsets like this cannot be converted into the timedelta required by the underlying Numba
function. Numba only accepts numeric values and `np.timedelta64` for frequency
(see [this](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#overview) overview).
To prevent automatic frequency inference, you can set it to False. In this case, VBT calculates
the right bound for each index value by using the next index value, rather than a fixed frequency.

```pycon
>>> target_index = pd.Index([
...     "2020-01-01",
...     "2020-02-01",
...     "2020-03-01",
...     "2020-04-01",
...     "2020-05-01",
...     "2020-06-01",
...     "2020-07-01",
...     "2020-08-01",
...     "2020-09-01",
...     "2020-10-01",
...     "2020-11-01",
...     "2020-12-01",
...     "2021-01-01"
... ])
>>> resampler = vbt.Resampler(h4_close.index, target_index, target_freq=False)
>>> h4_close.vbt.realign_closing(resampler)
2020-01-01     9352.89
2020-02-01     8523.61
2020-03-01     6410.44
2020-04-01     8620.00
2020-05-01     9448.27
2020-06-01     9138.55
2020-07-01    11335.46
2020-08-01    11649.51
2020-09-01    10776.59
2020-10-01    13791.00
2020-11-01    19695.87
2020-12-01    28923.63
2021-01-01    28923.63
Name: Close, dtype: float64
```

To verify that the output is correct, let's check the close value for `2020-08-01`, which should be
the latest value for that month.

```pycon
>>> h4_close[h4_close.index < "2020-09-01"].iloc[-1]
11649.51
```

!!! tip
    Disabling the frequency is only necessary for offsets that cannot be translated to timedelta. Other
    offsets, such as those for daily data, are converted automatically without issue.

A major drawback of disabling or being unable to infer the frequency of a target index is that the
bounds of each index value become variable rather than fixed. Consider the following scenario: we want
to downsample `H4` to two dates, `2020-01-01` and `2020-02-01`, knowing these are months. If we do not
specify the target frequency, VBT uses the latest close price after each date:

```pycon
>>> target_index = pd.Index([
...     "2020-01-01",
...     "2020-02-01",
... ])
>>> resampler = vbt.Resampler(h4_close.index, target_index, target_freq=False)
>>> h4_close.vbt.realign_closing(resampler)
2020-01-01     9352.89
2020-02-01    28923.63
Name: Close, dtype: float64
```

The value for `2020-02-01` ends up being the latest value in `H4`, which is clearly not what we intended.
To set the bounds for all index values, specify the target frequency:

```pycon
>>> resampler = vbt.Resampler(h4_close.index, target_index, target_freq="30d")
>>> h4_close.vbt.realign_closing(resampler)
2020-01-01    9513.21
2020-02-01    8531.88
Name: Close, dtype: float64
```

This is better, but it is still inaccurate because a month may be more or less than 30 days. Since we
cannot use date offsets, we need to set the bounds for each index value manually. This is possible
using [GenericAccessor.realign](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign),
which is the base method for both `realign_closing` and `realign_opening` that we used above. This method
is very powerful, allowing you to specify all the necessary details manually. The main idea is simple:
return the latest available information from an array at each position in a target index.

For example, let's get the latest information from `H4` at a specific time:

```pycon
>>> h4_open.vbt.realign("2020-06-07 12:15:00")  # (1)!
9576.57

>>> h4_close.vbt.realign(
...     "2020-06-07 12:15:00",
...     source_rbound=True  # (2)!
... )
9575.59
```

1. The open price occurs at the beginning of each source bar, so `source_rbound=False` (default).

2. The high/low/close price occurs at the end of each source bar, so `source_rbound=True`.

Note that the target datetime you provide is an exact point in time when the information should become
available. Let's get the latest highest value at the beginning of two months in `target_index`:

```pycon
>>> h4_high = h4_data.get("High")
>>> h4_high.vbt.realign(
...     target_index,
...     source_rbound=True
... )
2020-01-01       NaN
2020-02-01    9430.0
Name: High, dtype: float64
```

Here, `2020-01-01` means exactly `2020-01-01 00:00:00`, when no information is available yet,
so the result is NaN. On `2020-02-01`, however, we can use the information from `2020-01-31 20:00:00`:

```pycon
>>> h4_high.index[h4_high.index < "2020-02-01"][-1]
Timestamp('2020-01-31 20:00:00+0000', tz='UTC', freq='4H')
```

To make the target index act like bars instead of precise points in time, you can enable the right
bound for it as well:

```pycon
>>> h4_high.vbt.realign(
...     target_index,
...     source_rbound=True,
...     target_rbound=True
... )
UserWarning: Using right bound of target index without frequency. Set target_freq.
2020-01-01     9430.00
2020-02-01    29169.55
Name: High, dtype: float64
```

You may notice a warning that VBT could not infer the frequency of `target_index`.
But to avoid the mistake made when setting the frequency to `30d` (since months can have different
lengths), let's specify the right bound manually instead of enabling `target_rbound`. Fortunately,
VBT provides a helpful method for this:

```pycon
>>> resampler = vbt.Resampler(h4_high.index, target_index)
>>> resampler.target_rbound_index  # (1)!
DatetimeIndex([
    '2020-01-31 23:59:59.999999999',
    '2262-04-11 23:47:16.854775807'
], dtype='datetime64[ns]', freq=None)

>>> resampler = vbt.Resampler(
...     h4_high.index,
...     target_index,
...     target_freq=pd.offsets.MonthBegin(1))
>>> resampler.target_rbound_index  # (2)!
DatetimeIndex([
    '2020-01-31 23:59:59.999999999',
    '2020-02-29 23:59:59.999999999'
], dtype='datetime64[ns]', freq=None)
```

1. Without frequency, the right bound varies and ends at the next index (or infinity).

2. With frequency, the right bound is fixed.

We can now use this right bound as the target index:

```pycon
>>> h4_high.vbt.realign(
...     resampler.replace(
...         target_index=resampler.target_rbound_index,
...         target_freq=False
...     ),  # (1)!
...     wrap_kwargs=dict(index=target_index)  # (2)!
... )
2020-01-01    9430.00
2020-02-01    8671.61
Name: High, dtype: float64
```

1. Use [Configured.replace](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.replace) to replace
information in any configured VBT object.

2. Use the original `target_index` for wrapping the returned array.

Or, you can conveniently use `target_rbound="pandas"` in
[GenericAccessor.realign](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign):

```pycon
>>> h4_high.vbt.realign(
...     target_index,
...     freq=pd.offsets.MonthBegin(1),
...     target_rbound="pandas"
... )
2020-01-01    9430.00
2020-02-01    8671.61
Name: High, dtype: float64
```

Let's validate the output using Pandas:

```pycon
>>> h4_high[h4_high.index < "2020-03-01"].resample(vbt.offset("M")).last()  # (1)!
Open time
2020-01-01 00:00:00+00:00    9430.00
2020-02-01 00:00:00+00:00    8671.61
Freq: MS, Name: High, dtype: float64
```

A clear advantage of VBT over Pandas in this situation is:

1. Being able to resample not just to frequencies, but also to custom indexes.

2. High efficiency, as VBT processes the data only once, at C-speed.

!!! info
    Why did we use `vbt.offset("M")` instead of just `"M"`? Pandas methods may interpret offsets
    differently than VBT methods. For example, Pandas treats "M" as month end, while VBT treats
    it as month start (since we are usually working with bars). As a rule of thumb: explicitly convert
    any string offset if it must be passed to a Pandas method. If the method belongs to VBT, this is
    usually not necessary.

### Numeric index

We have not yet discussed the ability to perform resampling on numeric indexes. In the example below,
we obtain the latest information at each 6th element from the `H4` time frame, which is another way
to downsample to a daily frequency (as long as there are no gaps):

```pycon
>>> resampler = vbt.Resampler(
...     source_index=np.arange(len(h4_high)),
...     target_index=np.arange(len(h4_high))[::6],
...     source_freq=1,
...     target_freq=6
... )
>>> h4_high.vbt.realign(
...     resampler,
...     source_rbound=True,
...     target_rbound=True
... )
0        7242.98
6        6985.00
12       7361.00
...          ...
2178    27410.00
2184    28996.00
2190    29169.55
Name: High, Length: 366, dtype: float64
```

Good luck doing the same thing using Pandas.

### Forward filling

By default, when upsampling or downsampling, VBT will forward fill missing values by propagating
the latest known value. This is usually desired when the final goal is to compare the resampled time series
to another time series of the same timeframe. However, this may not work well for certain more sensitive
types of time series, such as signals: repeating the same signal multiple times can distort the view of
the original timeframe, especially when upsampling. To place each value only once, you can use the
`ffill` argument. For example, let's upsample a 5-minute mask with 3 entries to a 1-minute mask with 15
entries, both with and without forward filling. We assume that the 5-minute mask itself was generated
using the close price:

```pycon
>>> min5_index = vbt.date_range(start="2020", freq="5min", periods=3)
>>> min1_index = vbt.date_range(start="2020", freq="1min", periods=15)
>>> min5_mask = pd.Series(False, index=min5_index)
>>> min5_mask.iloc[0] = True  # (1)!
>>> min5_mask.iloc[2] = True

>>> resampler = vbt.Resampler(min5_index, min1_index)
>>> min1_mask = min5_mask.vbt.realign_closing(resampler)  # (2)!
>>> min1_mask
2020-01-01 00:00:00    NaN
2020-01-01 00:01:00    NaN
2020-01-01 00:02:00    NaN
2020-01-01 00:03:00    NaN
2020-01-01 00:04:00    1.0
2020-01-01 00:05:00    1.0
2020-01-01 00:06:00    1.0
2020-01-01 00:07:00    1.0
2020-01-01 00:08:00    1.0
2020-01-01 00:09:00    0.0
2020-01-01 00:10:00    0.0
2020-01-01 00:11:00    0.0
2020-01-01 00:12:00    0.0
2020-01-01 00:13:00    0.0
2020-01-01 00:14:00    1.0
Freq: T, dtype: float64

>>> min1_mask = min5_mask.vbt.realign_closing(resampler, ffill=False)  # (3)!
>>> min1_mask
2020-01-01 00:00:00    NaN
2020-01-01 00:01:00    NaN
2020-01-01 00:02:00    NaN
2020-01-01 00:03:00    NaN
2020-01-01 00:04:00    1.0
2020-01-01 00:05:00    NaN
2020-01-01 00:06:00    NaN
2020-01-01 00:07:00    NaN
2020-01-01 00:08:00    NaN
2020-01-01 00:09:00    0.0
2020-01-01 00:10:00    NaN
2020-01-01 00:11:00    NaN
2020-01-01 00:12:00    NaN
2020-01-01 00:13:00    NaN
2020-01-01 00:14:00    1.0
Freq: T, dtype: float64

>>> min1_mask = min1_mask.fillna(False).astype(bool)  # (4)!
>>> min1_mask
2020-01-01 00:00:00    False
2020-01-01 00:01:00    False
2020-01-01 00:02:00    False
2020-01-01 00:03:00    False
2020-01-01 00:04:00     True
2020-01-01 00:05:00    False
2020-01-01 00:06:00    False
2020-01-01 00:07:00    False
2020-01-01 00:08:00    False
2020-01-01 00:09:00    False
2020-01-01 00:10:00    False
2020-01-01 00:11:00    False
2020-01-01 00:12:00    False
2020-01-01 00:13:00    False
2020-01-01 00:14:00     True
Freq: T, dtype: bool
```

1. Set the first value to `True`, the second to `False`, and the third to `True` again.

2. With forward filling.

3. Without forward filling.

4. Convert to a valid mask.

## Indicators

How can we use the resampling logic from above to construct indicators that combine multiple time
frames? It is quite straightforward:

1. Run each indicator on its specific time frame.

2. Resample the arrays to a single index (usually to the highest frequency).

3. Combine the resampled arrays.

Let's demonstrate this by calculating the crossover of two moving averages on the `H4` and `D1`
time frames. First, we will run the TA-Lib SMA indicator on the close price for both time frames:

```pycon
>>> h4_sma = vbt.talib("SMA").run(
...     h4_data.get("Close"),
...     skipna=True  # (1)!
... ).real
>>> d1_sma = vbt.talib("SMA").run(
...     d1_data.get("Close"),
...     skipna=True
... ).real

>>> h4_sma = h4_sma.ffill()  # (2)!
>>> d1_sma = d1_sma.ffill()
```

1. Some values are NaN, and since TA-Lib has issues with NaN, run the indicator only on valid values.

2. Forward fill NaN values after running the indicator (not before!).

Next, upsample `D1` to `H4` so that both indicators share the same index:

```pycon
>>> resampler = vbt.Resampler(
...     d1_sma.index,  # (1)!
...     h4_sma.index,  # (2)!
...     source_freq="1d",
...     target_freq="4h"
... )
>>> d1_h4_sma = d1_sma.vbt.realign_closing(resampler)  # (3)!
```

1. The source time frame is `D1`.

2. The target time frame is `H4` -> upsampling.

3. The SMA was run on the close price in both time frames, so use
[GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing).

Let's validate the result of resampling:

```pycon
>>> d1_sma["2020-12-30":]
Open time
2020-12-30 00:00:00+00:00    21746.412000
2020-12-31 00:00:00+00:00    22085.034333
Freq: D, Name: Close, dtype: float64

>>> d1_h4_sma["2020-12-30":]
Open time
2020-12-30 00:00:00+00:00    21440.423000
2020-12-30 04:00:00+00:00    21440.423000
2020-12-30 08:00:00+00:00    21440.423000
2020-12-30 12:00:00+00:00    21440.423000
2020-12-30 16:00:00+00:00    21440.423000
2020-12-30 20:00:00+00:00    21746.412000  << first value available
2020-12-31 00:00:00+00:00    21746.412000
2020-12-31 04:00:00+00:00    21746.412000
2020-12-31 08:00:00+00:00    21746.412000
2020-12-31 12:00:00+00:00    21746.412000
2020-12-31 16:00:00+00:00    21746.412000
2020-12-31 20:00:00+00:00    22085.034333  << second value available
Freq: 4H, Name: Close, dtype: float64
```

Finally, as usual, compare the new time series to generate entries and exits:

```pycon
>>> entries = h4_sma.vbt.crossed_above(d1_h4_sma)
>>> exits = h4_sma.vbt.crossed_below(d1_h4_sma)

>>> def plot_date_range(date_range):
...     fig = h4_sma[date_range].rename("H4").vbt.plot()
...     d1_h4_sma[date_range].rename("D1_H4").vbt.plot(fig=fig)
...     entries[date_range].rename("Entry").vbt.signals.plot_as_entries(
...         y=h4_sma[date_range], fig=fig)
...     exits[date_range].rename("Exit").vbt.signals.plot_as_exits(
...         y=h4_sma[date_range], fig=fig)
...     return fig

>>> plot_date_range(slice("2020-02-01", "2020-03-01")).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/d1_h4.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/d1_h4.dark.svg#only-dark){: .iimg loading=lazy }

If any calculation was done on the open price, you can handle this by using
[GenericAccessor.realign](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign)
directly and disabling the right bound for the affected index:

```pycon
>>> d1_open_sma = vbt.talib("SMA").run(
...     d1_data.get("Open"),  # (1)!
...     skipna=True
... ).real
>>> d1_open_sma = d1_open_sma.ffill()

>>> d1_h4_open_sma = d1_open_sma.vbt.realign(
...     resampler,
...     source_rbound=False,  # (2)!
...     target_rbound=True,  # (3)!
... )
```

1. Use the open price instead of the close price.

2. Disable the right bound for the source index (`D1`) since it contains only opening information.

3. Enable the right bound for the target index (`H4`) since it contains closing information.

Let's validate the result of resampling:

```pycon
>>> d1_open_sma["2020-12-30":]
Open time
2020-12-30 00:00:00+00:00    21440.420333
2020-12-31 00:00:00+00:00    21746.409667
Freq: D, Name: Open, dtype: float64

>>> d1_h4_open_sma["2020-12-30":]
Open time
2020-12-30 00:00:00+00:00    21440.420333  << first value available
2020-12-30 04:00:00+00:00    21440.420333
2020-12-30 08:00:00+00:00    21440.420333
2020-12-30 12:00:00+00:00    21440.420333
2020-12-30 16:00:00+00:00    21440.420333
2020-12-30 20:00:00+00:00    21440.420333
2020-12-31 00:00:00+00:00    21746.409667  << second value available
2020-12-31 04:00:00+00:00    21746.409667
2020-12-31 08:00:00+00:00    21746.409667
2020-12-31 12:00:00+00:00    21746.409667
2020-12-31 16:00:00+00:00    21746.409667
2020-12-31 20:00:00+00:00    21746.409667
Freq: 4H, Name: Open, dtype: float64
```

Now, let's try something more interesting: calculate the bandwidth of the Bollinger Bands indicator for
a set of arbitrary frequencies and combine everything into a single DataFrame.

```pycon
>>> def generate_bandwidths(freqs):
...     bandwidths = []
...     for freq in freqs:
...         close = h1_data.resample(freq).get("Close")  # (1)!
...         bbands = vbt.talib("BBANDS").run(close, skipna=True)
...         upperband = bbands.upperband.ffill()
...         middleband = bbands.middleband.ffill()
...         lowerband = bbands.lowerband.ffill()
...         bandwidth = (upperband - lowerband) / middleband
...         bandwidths.append(bandwidth.vbt.realign_closing("1h"))  # (2)!
...     df = pd.concat(bandwidths, axis=1, keys=pd.Index(freqs, name="timeframe"))  # (3)!
...     return df.ffill()  # (4)!

>>> bandwidths = generate_bandwidths(["1h", "4h", "1d", "7d"])
>>> bandwidths
timeframe                        1h        4h        1d        7d
Open time                                                        
2020-01-01 00:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 01:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 02:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 03:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 04:00:00+00:00  0.011948       NaN       NaN       NaN
...                             ...       ...       ...       ...
2020-12-31 19:00:00+00:00  0.027320  0.017939  0.134607  0.652958
2020-12-31 20:00:00+00:00  0.036515  0.017939  0.134607  0.652958
2020-12-31 21:00:00+00:00  0.025027  0.017939  0.134607  0.652958
2020-12-31 22:00:00+00:00  0.014318  0.017939  0.134607  0.652958
2020-12-31 23:00:00+00:00  0.012875  0.017939  0.134607  0.652958

[8784 rows x 4 columns]
```

1. Downsample.

2. Upsample back to `H1`.

3. Concatenate the columns from all time frames into a single DataFrame.

4. When upsampling back to `H1`, the new index may be longer than the original `H1` index.
To produce the exact same target index, use [GenericAccessor.realign](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign)
instead of [GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing) 
(this is your homework!).

We can then plot everything as a heatmap:

```pycon
>>> bandwidths.loc[:, ::-1].vbt.ts_heatmap().show()  # (1)!
```

1. Reverse the order of columns so that the lowest frequency appears at the top, and plot a heatmap
using [GenericDFAccessor.ts_heatmap](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericDFAccessor.ts_heatmap).

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/heatmap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/heatmap.dark.svg#only-dark){: .iimg loading=lazy }

We just created this awesome visualization in 10 lines of code! :guitar:

But we can make the code even shorter: each TA-Lib indicator has a `timeframe` parameter :stuck_out_tongue:

```pycon
>>> bbands = vbt.talib("BBANDS").run(
...     h1_data.get("Close"),
...     skipna=True,
...     timeframe=["1h", "4h", "1d", "7d"],
...     broadcast_kwargs=dict(wrapper_kwargs=dict(freq="1h"))  # (1)!
... )
>>> bandwidths = (bbands.upperband - bbands.lowerband) / bbands.middleband
>>> bandwidths
timeframe                        1h        4h        1d        7d
Open time                                                        
2020-01-01 00:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 01:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 02:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 03:00:00+00:00       NaN       NaN       NaN       NaN
2020-01-01 04:00:00+00:00  0.011948       NaN       NaN       NaN
...                             ...       ...       ...       ...
2020-12-31 19:00:00+00:00  0.027320  0.017939  0.134607  0.652958
2020-12-31 20:00:00+00:00  0.036515  0.017939  0.134607  0.652958
2020-12-31 21:00:00+00:00  0.025027  0.017939  0.134607  0.652958
2020-12-31 22:00:00+00:00  0.014318  0.017939  0.134607  0.652958
2020-12-31 23:00:00+00:00  0.012875  0.017939  0.134607  0.652958

[8784 rows x 4 columns]
```

1. Specify the source frequency since our data has gaps.

## Testing

As with everything in VBT, time frames are simply another dimension that can be tested
by iteration (loop over frequencies and simulate each one independently) or by stacking columns.
If you do not want to increase the data size by storing multiple time frames in a single array, use the
first approach. If you want to make decisions based on multiple time frames, or if you want to test them
from the same perspective and under the same conditions (which is necessary for a fair experiment),
and you do not have too much data to overwhelm your memory, use the second approach.

Let's demonstrate the second approach. Below, for each frequency, we compute the SMA crossover
on the open price of `H1`. We then align and concatenate all time frames and simulate them as a single
entity using the close price of `H1` and some stop loss. In this way, we can test multiple time frames by
keeping order execution as granular as possible.

```pycon
>>> def generate_signals(data, freq, fast_window, slow_window):
...     open_price = data.resample(freq).get("Open")  # (1)!
...     fast_sma = vbt.talib("SMA")\
...         .run(
...             open_price, 
...             fast_window, 
...             skipna=True, 
...             short_name="fast_sma"
...         )\
...         .real.ffill()\  # (2)!
...         .vbt.realign(data.wrapper.index)  # (3)!
...     slow_sma = vbt.talib("SMA")\
...         .run(
...             open_price, 
...             slow_window, 
...             skipna=True, 
...             short_name="slow_sma"
...         )\
...         .real.ffill()\
...         .vbt.realign(data.wrapper.index)
...     entries = fast_sma.vbt.crossed_above(slow_sma)  # (4)!
...     exits = fast_sma.vbt.crossed_below(slow_sma)
...     return entries, exits

>>> fast_window = [10, 20]  # (5)!
>>> slow_window = [20, 30]
>>> h1_entries, h1_exits = generate_signals(h1_data, "1h", fast_window, slow_window)
>>> h4_entries, h4_exits = generate_signals(h1_data, "4h", fast_window, slow_window)
>>> d1_entries, d1_exits = generate_signals(h1_data, "1d", fast_window, slow_window)

>>> entries = pd.concat(  # (6)!
...     (h1_entries, h4_entries, d1_entries), 
...     axis=1, 
...     keys=pd.Index(["1h", "4h", "1d"], name="timeframe")
... )
>>> exits = pd.concat(
...     (h1_exits, h4_exits, d1_exits), 
...     axis=1, 
...     keys=pd.Index(["1h", "4h", "1d"], name="timeframe")
... )

>>> (entries.astype(int) - exits.astype(int))\
...     .resample("1d").sum()\
...     .vbt.ts_heatmap(
...         trace_kwargs=dict(
...             colorscale=["#ef553b", "rgba(0, 0, 0, 0)", "#17becf"],
...             colorbar=dict(
...                 tickvals=[-1, 0, 1], 
...                 ticktext=["Exit", "", "Entry"]
...             )
...         )
...     ).show()  # (7)!
```

1. Downsample to the target time frame and get the open price.

2. Run the TA-Lib indicator on non-NA values and forward fill NaNs (if any) afterwards.

3. Upsample the array back to the original time frame (`H1`). Unlike the example
with Bollinger Bands, here we are resampling to the exact same index as in `h1_data`.
We also do not need to specify any bounds because we are using the open price.

4. Calculate the crossover on the upsampled data (`H1`). Remember that signals must be generated on
the most granular data.

5. Test multiple window combinations.

6. Stack the generated entries and exits from all time frames into a single DataFrame, and create
a separate column level to indicate which column belongs to which time frame.

7. Merge entries and exits so that -1 is exit, 0 is no signal, and 1 is entry. Resample
the array to daily frequency, since Plotly does not handle large amounts of data well.
Display everything as a time-series heatmap.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/bi_signals.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/bi_signals.dark.svg#only-dark){: .iimg loading=lazy }

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     h1_data,
...     entries,
...     exits,
...     sl_stop=0.1,
...     freq="1h"
... )

>>> pf.orders.count()
timeframe  fast_sma_timeperiod  slow_sma_timeperiod
1h         10                   20                     504
            20                   30                     379
4h         10                   20                     111
            20                   30                      85
1d         10                   20                      13
            20                   30                       7
Name: count, dtype: int64

>>> pf.sharpe_ratio
timeframe  fast_sma_timeperiod  slow_sma_timeperiod
1h         10                   20                     3.400095
            20                   30                     2.051091
4h         10                   20                     2.751626
            20                   30                     1.559501
1d         10                   20                     3.239846
            20                   30                     2.755367
Name: sharpe_ratio, dtype: float64
```

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/mtf-analysis/alignment.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/MTFAnalysis.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/aggregation/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/
name: Aggregation
type: page
icon: material-chart-timeline
---

# Aggregation

Aggregation is an essential step in downsampling. For example, suppose you want to determine the
maximum drawdown (MDD) for each month in your data. You can do this using several different methods
provided by VBT. The first method involves resampling the data and then manipulating the
results:

```pycon
>>> ms_data = h1_data.resample("M")  # (1)!
>>> ms_data.get("Low") / ms_data.get("High") - 1
Open time
2020-01-01 00:00:00+00:00   -0.282623
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.349883
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.222909
2020-08-01 00:00:00+00:00   -0.156360
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.264255
2020-11-01 00:00:00+00:00   -0.335702
2020-12-01 00:00:00+00:00   -0.400262
Freq: MS, dtype: float64
```

1. Note that we do not use `vbt.offset` here because the `resample` method belongs to a VBT object,
and it can correctly interpret any offset string.

You can achieve the same result by resampling only the arrays needed for the calculation:

```pycon
>>> h1_high = h1_data.get("High")
>>> h1_low = h1_data.get("Low")
>>> ms_high = h1_high.resample(vbt.offset("M")).max()
>>> ms_low = h1_low.resample(vbt.offset("M")).min()
>>> ms_low / ms_high - 1
Open time
2020-01-01 00:00:00+00:00   -0.282623
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.349883
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.222909
2020-08-01 00:00:00+00:00   -0.156360
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.264255
2020-11-01 00:00:00+00:00   -0.335702
2020-12-01 00:00:00+00:00   -0.400262
Freq: MS, dtype: float64
```

You can also use VBT's superfast [GenericAccessor.resample_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_apply),
which uses Numba:

```pycon
>>> ms_high = h1_high.vbt.resample_apply("M", vbt.nb.max_reduce_nb)
>>> ms_low = h1_low.vbt.resample_apply("M", vbt.nb.min_reduce_nb)
>>> ms_low / ms_high - 1
Open time
2020-01-01 00:00:00+00:00   -0.282623
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.349883
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.222909
2020-08-01 00:00:00+00:00   -0.156360
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.264255
2020-11-01 00:00:00+00:00   -0.335702
2020-12-01 00:00:00+00:00   -0.400262
Freq: MS, dtype: float64
```

!!! tip
    See the available reduce functions ending with `reduce_nb` in [nb.apply_reduce](https://vectorbt.pro/pvt_41531c19/api/generic/nb/apply_reduce/).
    If you cannot find a specific function, you can always write your own :wink:

## Custom index

Using rules like `"M"` is very convenient, but they may not cover every use case. Imagine a scenario
where you already have a target index you want to resample to. None of the Pandas functions provide
this level of flexibility, unless you can express your operation using
[pandas.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html).
Fortunately, VBT supports a wide range of inputs and options to make this possible.

### Using target index

The method [GenericAccessor.resample_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_apply)
offers two modes: one uses a target index (see [GenericAccessor.resample_to_index](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_to_index)),
and the other uses a Pandas resampler along with VBT's grouping mechanism (see [GenericAccessor.groupby_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.groupby_apply)).
The first mode is the default. It provides very fast performance but requires careful handling of bounds.
The second mode guarantees results that match Pandas but is considerably slower. You can enable this
mode by passing `use_groupby_apply=True` to [GenericAccessor.resample_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_apply).

The first mode works similarly to [GenericAccessor.realign](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign).
It uses the source and target indexes to aggregate all array elements located between each pair of
timestamps in the target index. This operation is performed in a single pass for maximum efficiency.
Also, similar to `realign`, you can pass a [Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler)
instance and provide your own custom index, including numeric indexes. However, unlike `realign`, there is
no argument to specify frequencies or bounds. The left and right bounds are always the previous and next
elements in the target index (or infinity). The following example demonstrates this:

```pycon
>>> target_index = pd.Index([
...     "2020-01-01",
...     "2020-02-01",
... ])
>>> h1_high.vbt.resample_to_index(
...     target_index, 
...     vbt.nb.max_reduce_nb
... )
2020-01-01     9578.0
2020-02-01    29300.0
Name: High, dtype: float64
```

!!! info
    You only need to consider this behavior when passing a custom index. Passing a frequency like `"M"`
    will produce results identical to Pandas with default arguments.

In this example, the second value takes the maximum of all values coming after `2020-02-01`, which
is not intended if you want aggregation to occur strictly per month. To fix this, add another index value
to act as the rightmost bound:

```pycon
>>> target_rbound_index = vbt.Resampler.get_rbound_index(  # (1)!
...     target_index, 
...     pd.offsets.MonthBegin(1)
... )
>>> h1_high.vbt.resample_to_index(
...     target_index.append(target_rbound_index[[-1]]), 
...     vbt.nb.max_reduce_nb
... ).iloc[:-1]
2020-01-01     9578.0
2020-02-01    10500.0
Name: High, dtype: float64

>>> h1_high[:"2020-03-01"].resample(vbt.offset("M")).max().iloc[:-1]  # (2)!
Open time
2020-01-01 00:00:00+00:00     9578.0
2020-02-01 00:00:00+00:00    10500.0
Freq: MS, Name: High, dtype: float64
```

1. Get the right bound index for `target_index` using
[Resampler.get_rbound_index](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.get_rbound_index)
(class method).

2. Validate the output using Pandas.

### Using group-by

The second mode has a completely different implementation. It creates or accepts a
[Pandas Resampler](https://pandas.pydata.org/docs/reference/resampling.html) or a
[Pandas Grouper](https://pandas.pydata.org/docs/reference/api/pandas.Grouper.html), then parses
them to build a [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper) instance.
The grouper stores a map that links each group of elements in the source index to the corresponding
elements in the target index. This map is then used in a Numba-compiled function to aggregate each group.

Enough theory! Let's perform the resampling procedure using the grouping mechanism:

```pycon
>>> pd_resampler = h1_high.resample(vbt.offset("M"))
>>> ms_high = h1_high.vbt.groupby_apply(pd_resampler, vbt.nb.max_reduce_nb)
>>> ms_low = h1_low.vbt.groupby_apply(pd_resampler, vbt.nb.min_reduce_nb)
>>> ms_low / ms_high - 1
2020-01-01 00:00:00+00:00   -0.282623
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.349883
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.222909
2020-08-01 00:00:00+00:00   -0.156360
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.264255
2020-11-01 00:00:00+00:00   -0.335702
2020-12-01 00:00:00+00:00   -0.400262
dtype: float64
```

Since parsing a resampler or grouper object from Pandas can be slow, you can provide your own
grouper to speed things up significantly. You have two options: you can provide any
`group_by` object, such as a Pandas Index, a NumPy array, or a level name in a MultiIndex,
or you can provide a [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper) instance.

Below, we aggregate the data by the month index:

```pycon
>>> h1_high.vbt.groupby_apply(h1_high.index.month, vbt.nb.max_reduce_nb)
Open time
1      9578.00
2     10500.00
3      9188.00
4      9460.00
5     10067.00
6     10380.00
7     11444.00
8     12468.00
9     12050.85
10    14100.00
11    19863.16
12    29300.00
Name: High, dtype: float64
```

This is similar to calling [pandas.DataFrame.groupby](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html):

```pycon
>>> h1_high.groupby(h1_high.index.month).max()
Open time
1      9578.00
2     10500.00
3      9188.00
4      9460.00
5     10067.00
6     10380.00
7     11444.00
8     12468.00
9     12050.85
10    14100.00
11    19863.16
12    29300.00
Name: High, dtype: float64
```

!!! tip
    Built-in functions like `max` for Pandas resampling and grouping are already optimized and perform
    on par with VBT. Consider using VBT's functions mainly when you have a custom function
    and need to use `apply` - that is where VBT really shines :sunny:

### Using bounds

As we've seen, [GenericAccessor.resample_to_index](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_to_index)
aggregates all array values that come after or before each element in the target index,
while [GenericAccessor.groupby_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.groupby_apply)
aggregates all array values that map to the same target index by binning. The first method does not allow
for gaps or custom bounds, while the second method does not support overlapping groups.
Both of these limitations are addressed by [GenericAccessor.resample_between_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_between_bounds)!

This method takes the left and right bounds of the target index and aggregates all array values that fall
between those two bounds:

```pycon
>>> target_lbound_index = pd.Index([
...     "2020-01-01",
...     "2020-02-01",
... ])
>>> target_rbound_index = pd.Index([
...     "2020-02-01",
...     "2020-03-01",
... ])
>>> h1_high.vbt.resample_between_bounds(  # (1)!
...     target_lbound_index, 
...     target_rbound_index,
...     vbt.nb.max_reduce_nb
... )
2020-01-01     9578.0
2020-02-01    10500.0
Name: High, dtype: float64
```

1. By default, the left bound is closed (included in the aggregation) and the right bound is open
(excluded from the aggregation). To change this behavior, use `closed_lbound` and `closed_rbound`.

This enables interesting possibilities, such as creating custom-sized expanding windows. Let's calculate
the highest high up to the beginning of each month:

```pycon
>>> h1_high.vbt.resample_between_bounds(
...     "2020-01-01", 
...     vbt.date_range("2020-01-02", "2021-01-01", freq="M", inclusive="both"),
...     vbt.nb.max_reduce_nb
... )  # (1)!
2020-02-01     9578.00
2020-03-01    10500.00
2020-04-01    10500.00
2020-05-01    10500.00
2020-06-01    10500.00
2020-07-01    10500.00
2020-08-01    11444.00
2020-09-01    12468.00
2020-10-01    12468.00
2020-11-01    14100.00
2020-12-01    19863.16
2021-01-01    29300.00
Freq: MS, Name: High, dtype: float64
```

1. The returned index contains the highest high before (and not including) each of the dates.

Let's validate the result:

```pycon
>>> h1_high.expanding().max().resample(vbt.offset("M")).max()
Open time
2020-01-01 00:00:00+00:00     9578.00
2020-02-01 00:00:00+00:00    10500.00
2020-03-01 00:00:00+00:00    10500.00
2020-04-01 00:00:00+00:00    10500.00
2020-05-01 00:00:00+00:00    10500.00
2020-06-01 00:00:00+00:00    10500.00
2020-07-01 00:00:00+00:00    11444.00
2020-08-01 00:00:00+00:00    12468.00
2020-09-01 00:00:00+00:00    12468.00
2020-10-01 00:00:00+00:00    14100.00
2020-11-01 00:00:00+00:00    19863.16
2020-12-01 00:00:00+00:00    29300.00
Freq: MS, Name: High, dtype: float64
```

## Meta methods

All the methods discussed above work well when the primary operation is performed on **one** array.
However, if your operation involves multiple arrays (such as `h1_high` and `h1_low` in our example), you
need to perform several resampling operations and ensure the results align correctly. A cleaner
solution is to perform a resampling operation that completes the entire calculation in a single pass,
which is optimal for performance and consistency. This can be achieved using meta methods.

Meta methods are class methods that are not tied to any specific array. They can accept, broadcast,
and combine multiple arrays of data. The good news is that most of the methods we used earlier are
also available as meta methods! Let's calculate the MDD using a single resampling operation with
[GenericAccessor.resample_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_apply):

```pycon
>>> @njit  # (1)!
... def mdd_nb(from_i, to_i, col, high, low):  # (2)!
...     peak = -np.inf
...     max_dd = 0.0
...     for i in range(from_i, to_i):
...         h = high[i, col]  # (3)!
...         if np.isfinite(h) and h > peak:
...             peak = h
...         if np.isfinite(peak):
...             l = low[i, col]
...             if np.isfinite(l):
...                 dd = l / peak - 1.0
...                 if dd < max_dd:
...                     max_dd = dd
...     return max_dd  # (4)!

>>> vbt.pd_acc.resample_apply(  # (5)!
...     'MS',
...     mdd_nb,
...     vbt.Rep('high'),  # (6)!
...     vbt.Rep('low'),
...     broadcast_named_args=dict(  # (7)!
...         high=h1_high,
...         low=h1_low
...     )
... )
Open time
2020-01-01 00:00:00+00:00   -0.104466
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.132904
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.072828
2020-08-01 00:00:00+00:00   -0.132385
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.070095
2020-11-01 00:00:00+00:00   -0.169173
2020-12-01 00:00:00+00:00   -0.116436
Freq: MS, dtype: float64
```

1. Write a regular Python function and decorate it with `@njit` to compile it with Numba.

2. Because a meta method is not bound to any array, the calculation function must accept the
*meta*data of each selection, which includes the index range `from_i` and `to_i` and the column `col`.

3. Select the relevant elements from each array using the provided metadata. Here, `h`
is the highest price recorded in the specific month, which starts at index `from_i` (inclusive)
and ends at index `to_i` (exclusive).

4. The reduction function should always return a single value.

5. `vbt.pd_acc.resample_apply` is an alias for `pd.DataFrame.vbt.resample_apply`. Remember, we are now
calling `resample_apply` as a **class** method.

6. We want to take the high and low price as additional arguments after the metadata arguments.
Pass them right after the reduction function as templates, which are resolved after broadcasting.

7. Adding arrays to this dictionary broadcasts them to the same shape and converts them to
two-dimensional NumPy arrays.

You can think of meta methods as flexible versions of regular methods. They act as micro-pipelines
that can process any number of arrays and let's select elements from those arrays as needed. If you
add a print statement in `mdd_nb` to output `from_i`, `to_i`, and `col`, you will see:

```pycon
0 744 0
744 1434 0
1434 2177 0
2177 2895 0
2895 3639 0
3639 4356 0
4356 5100 0
5100 5844 0
5844 6564 0
6564 7308 0
7308 8027 0
8027 8767 0
```

Each of these lines represents a separate call to `mdd_nb`. The first two indices in each line give the
absolute start and end indices to select from your data. Since we used `MS` as the target frequency,
`from_i` and `to_i` mark the start and end of each month. You can prove this:

```pycon
>>> h1_high.iloc[0:744]  # (1)!
Open time
2020-01-01 00:00:00+00:00    7196.25
2020-01-01 01:00:00+00:00    7230.00
2020-01-01 02:00:00+00:00    7244.87
...                              ...
2020-01-31 21:00:00+00:00    9373.85
2020-01-31 22:00:00+00:00    9430.00
2020-01-31 23:00:00+00:00    9419.96
Name: High, Length: 744, dtype: float64

>>> mdd_nb(0, 744, 0, vbt.to_2d_array(h1_high), vbt.to_2d_array(h1_low))
-0.10446592991831694
```

1. The first pair of indices corresponds to January 2020.

Here is the same example using [GenericAccessor.resample_between_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_between_bounds):

```pycon
>>> target_lbound_index = vbt.date_range("2020-01-01", "2020-12-01", freq="M", tz="UTC", inclusive="both")
>>> target_rbound_index = vbt.date_range("2020-02-01", "2021-01-01", freq="M", tz="UTC", inclusive="both")
>>> vbt.pd_acc.resample_between_bounds(
...     target_lbound_index,
...     target_rbound_index,
...     mdd_nb,
...     vbt.Rep('high'),
...     vbt.Rep('low'),
...     broadcast_named_args=dict(
...         high=h1_high,
...         low=h1_low
...     )
... )
2020-01-01 00:00:00+00:00   -0.104466
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.132904
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.072828
2020-08-01 00:00:00+00:00   -0.132385
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.070095
2020-11-01 00:00:00+00:00   -0.169173
2020-12-01 00:00:00+00:00   -0.116436
Freq: MS, dtype: float64
```

The sky is the limit when it comes to what VBT enables for your analysis :milky_way:

## Numba

Ninety percent of VBT's functionality is compiled with Numba. If you want to skip the high-level
API and dive deeper into Numba, simply consult the documentation to find the Numba-compiled function
used by the accessor function you want to work with. For example,
[GenericAccessor.resample_between_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_between_bounds)
first generates index ranges using [map_bounds_to_source_ranges_nb](https://vectorbt.pro/pvt_41531c19/api/base/resampling/nb/#vectorbtpro.base.resampling.nb.map_bounds_to_source_ranges_nb),
then uses [reduce_index_ranges_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/resample/#vectorbtpro.generic.nb.apply_reduce.reduce_index_ranges_nb)
for generic calls and [reduce_index_ranges_meta_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/resample/#vectorbtpro.generic.nb.apply_reduce.reduce_index_ranges_meta_nb)
for meta calls. Let's run the same meta function as above:

```pycon
>>> from vectorbtpro.base.resampling.nb import map_bounds_to_source_ranges_nb

>>> range_starts, range_ends = map_bounds_to_source_ranges_nb(  # (1)!
...     source_index=h1_high.index.values,
...     target_lbound_index=target_lbound_index.values,
...     target_rbound_index=target_rbound_index.values,
...     closed_lbound=True,
...     closed_rbound=False,
... )
>>> np.column_stack((range_starts, range_ends))  # (2)!
array([[   0,  744],
        [ 744, 1434],
        [1434, 2177],
        [2177, 2895],
        [2895, 3639],
        [3639, 4356],
        [4356, 5100],
        [5100, 5844],
        [5844, 6564],
        [6564, 7308],
        [7308, 8027],
        [8027, 8767]])

>>> ms_mdd_arr = vbt.nb.reduce_index_ranges_meta_nb(  # (3)!
...     1,  # (4)!
...     range_starts,
...     range_ends,
...     mdd_nb,
...     vbt.to_2d_array(h1_high),  # (5)!
...     vbt.to_2d_array(h1_low)
... )
>>> ms_mdd_arr
array([[-0.10446593],
        [-0.19571429],
        [-0.58836199],
        [-0.13290365],
        [-0.1937022 ],
        [-0.14903661],
        [-0.07282757],
        [-0.13238465],
        [-0.18470481],
        [-0.07009498],
        [-0.1691734 ],
        [-0.11643554]])
```

1. The first function iterates over each pair of bounds and returns the start and end index
in the source index for those bounds.

2. The returned indices are the same ones we printed earlier.
Use `np.column_stack` to display start and end indices side by side.

3. The second function loops through each index range and calls `mdd_nb` on it.

4. Number of columns in `h1_high` and `h1_low`.

5. Custom arrays should always be two-dimensional.

This is the fastest execution you can achieve. You can then wrap the array like this:

```pycon
>>> pd.Series(ms_mdd_arr[:, 0], index=target_lbound_index)
2020-01-01 00:00:00+00:00   -0.104466
2020-02-01 00:00:00+00:00   -0.195714
2020-03-01 00:00:00+00:00   -0.588362
2020-04-01 00:00:00+00:00   -0.132904
2020-05-01 00:00:00+00:00   -0.193702
2020-06-01 00:00:00+00:00   -0.149037
2020-07-01 00:00:00+00:00   -0.072828
2020-08-01 00:00:00+00:00   -0.132385
2020-09-01 00:00:00+00:00   -0.184705
2020-10-01 00:00:00+00:00   -0.070095
2020-11-01 00:00:00+00:00   -0.169173
2020-12-01 00:00:00+00:00   -0.116436
Freq: MS, dtype: float64
```

## Caveats

As discussed in [Alignment](https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis/alignment), each timestamp represents the open time,
and information at that timestamp occurs somewhere between this timestamp and the next one.
This is typically not an issue if you downsample to a frequency that is an integer multiple of the
source frequency. For example, let's look at downsampling two days of `H4` data to the `D1` time frame:

```pycon
>>> h4_close_2d = h4_close.iloc[:12]
>>> h4_close_2d
Open time
2020-01-01 00:00:00+00:00    7225.01
2020-01-01 04:00:00+00:00    7209.83
2020-01-01 08:00:00+00:00    7197.20
2020-01-01 12:00:00+00:00    7234.19
2020-01-01 16:00:00+00:00    7229.48
2020-01-01 20:00:00+00:00    7200.85
2020-01-02 00:00:00+00:00    7129.61
2020-01-02 04:00:00+00:00    7110.57
2020-01-02 08:00:00+00:00    7139.79
2020-01-02 12:00:00+00:00    7130.98
2020-01-02 16:00:00+00:00    6983.27
2020-01-02 20:00:00+00:00    6965.71
Freq: 4H, Name: Close, dtype: float64

>>> h4_close_2d.resample("1d").last()
Open time
2020-01-01 00:00:00+00:00    7200.85
2020-01-02 00:00:00+00:00    6965.71
Freq: D, Name: Close, dtype: float64
```

This operation is correct: `7200.85` is the last value of `2020-01-01`, and
`6965.71` is the last value of `2020-01-02`. But what happens if we change `H4` to `H5`? Not so good:

```pycon
>>> h5_close = h1_close.resample("5h").last()
>>> h5_close_2d = h5_close.iloc[:10]
>>> h5_close_2d
Open time
2020-01-01 00:00:00+00:00    7217.27
2020-01-01 05:00:00+00:00    7188.77
2020-01-01 10:00:00+00:00    7221.43
2020-01-01 15:00:00+00:00    7229.48
2020-01-01 20:00:00+00:00    7211.02
2020-01-02 01:00:00+00:00    7138.93
2020-01-02 06:00:00+00:00    7161.83
2020-01-02 11:00:00+00:00    7130.98
2020-01-02 16:00:00+00:00    6948.49
2020-01-02 21:00:00+00:00    6888.85
Freq: 5H, Name: Close, dtype: float64

>>> h5_close_2d.resample("1d").last()
Open time
2020-01-01 00:00:00+00:00    7211.02
2020-01-02 00:00:00+00:00    6888.85
Freq: D, Name: Close, dtype: float64
```

Try to spot the issue and revisit this once you have found it (or not)...

The Pandas resampler assumes that information at each timestamp occurs exactly at that timestamp,
so it selects the latest value of the first day as the last timestamp of that day—
`2020-01-01 20:00:00`. However, this approach does not work for us. The timestamp `2020-01-01 20:00:00`
contains the close price, which actually happens just before the next timestamp,
or `2020-01-02 01:00:00` on the following day. This value is not yet available at the end of the
first day. Using this information too early leads to lookahead bias and unreliable backtesting results.

This issue only arises when the target frequency is not an exact multiple of the source frequency:

```pycon
>>> vbt.timedelta("1d") % vbt.timedelta("1h")  # (1)!
Timedelta('0 days 00:00:00')

>>> vbt.timedelta("1d") % vbt.timedelta("4h")  # (2)!
Timedelta('0 days 00:00:00')

>>> vbt.timedelta("1d") % vbt.timedelta("5h")  # (3)!
Timedelta('0 days 04:00:00')
```

1. Will not cause issues.

2. Will not cause issues.

3. Needs adjustment.

Fortunately, the solution is simple: treat each timestamp as the close time instead of the open time.
Logically, the close time is the next timestamp minus one nanosecond (the smallest possible timedelta):

```pycon
>>> h5_close_time = h5_close_2d.index.shift("5h") - pd.Timedelta(nanoseconds=1)
>>> h5_close_time.name = "Close time"
>>> h5_close_2d.index = h5_close_time
>>> h5_close_2d
Close time
2020-01-01 04:59:59.999999999+00:00    7217.27
2020-01-01 09:59:59.999999999+00:00    7188.77
2020-01-01 14:59:59.999999999+00:00    7221.43
2020-01-01 19:59:59.999999999+00:00    7229.48
2020-01-02 00:59:59.999999999+00:00    7211.02
2020-01-02 05:59:59.999999999+00:00    7138.93
2020-01-02 10:59:59.999999999+00:00    7161.83
2020-01-02 15:59:59.999999999+00:00    7130.98
2020-01-02 20:59:59.999999999+00:00    6948.49
2020-01-03 01:59:59.999999999+00:00    6888.85
Freq: 5H, Name: Close, dtype: float64
```

Now, each timestamp guarantees a correct resampling operation:

```pycon
>>> h5_close_2d.resample("1d").last()
Close time
2020-01-01 00:00:00+00:00    7229.48
2020-01-02 00:00:00+00:00    6948.49
2020-01-03 00:00:00+00:00    6888.85
Freq: D, Name: Close, dtype: float64
```

!!! note
    When using close time, do not specify the right bound when resampling with VBT methods.
    For example, instead of using [GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing),
    you can now safely use [GenericAccessor.realign_opening](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_opening).

## Portfolio

When working with portfolios, you must distinguish between two time frames: the one used 
for simulation and the one used for analysis (or reconstruction). By default, both time frames
are the same. However, with a special command, you can run your trading strategy on more granular data 
and then downsample the simulated data for analysis. This approach offers two important advantages:

1. Using a shorter time frame during simulation allows you to place many more orders with greater precision.

2. Using a longer time frame during analysis reduces memory usage and speeds up processing time.

Let's simulate a simple crossover strategy on `H1` data:

```pycon
>>> fast_sma = vbt.talib("SMA").run(h1_close, timeperiod=vbt.Default(10))
>>> slow_sma = vbt.talib("SMA").run(h1_close, timeperiod=vbt.Default(20))
>>> entries = fast_sma.real_crossed_above(slow_sma.real)
>>> exits = fast_sma.real_crossed_below(slow_sma.real)

>>> pf = vbt.Portfolio.from_signals(h1_close, entries, exits)
>>> pf.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/h1_pf.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/h1_pf.dark.svg#only-dark){: .iimg loading=lazy }

Calculating portfolio returns involves reconstructing many attributes, including 
cash flow, cash, asset flow, asset value, total value, and, finally, returns. This chain of 
reconstructions can be a bottleneck if your input data (such as tick data) is extremely granular.
Fortunately, there is a brand new method, [Wrapping.resample](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.resample),
that allows you to resample VBT objects of any complexity (as long as resampling is 
possible and logically reasonable). Here, we resample the portfolio to the start of each month:

```pycon
>>> ms_pf = pf.resample("M")
>>> ms_pf.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/ms_pf.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/mtf-analysis/ms_pf.dark.svg#only-dark){: .iimg loading=lazy }

The main simulation artifacts are the close price, order records, and additional inputs such as
cash deposits and earnings. When you trigger a resampling job, the close price and these additional
inputs can be resampled easily with operations like `last` and `sum`. 

Order records, however, are more complex: they are structured NumPy arrays 
(similar to a Pandas DataFrame) that store order information in each row. The timestamp for each order 
is recorded in a separate column, allowing multiple orders at the same timestamp. 
This design makes it possible to resample such records simply by re-indexing their timestamp column
to the target index, which is done using [Resampler.map_to_target_index](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.map_to_target_index).

After resampling the artifacts, a new [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) 
instance is created, and attributes like returns are reconstructed on the new data.
This perfectly demonstrates why VBT reconstructs all attributes after simulation 
rather than during simulation, as many traditional backtesters do. 

To show that these results are trustworthy:

```pycon
>>> pf.total_return
2.735083772113918

>>> ms_pf.total_return
2.735083772113918
```

Or, by comparing the resampled returns of the original portfolio to the returns of the resampled portfolio:

```pycon
>>> (1 + pf.returns).resample(vbt.offset("M")).apply(lambda x: x.prod() - 1)
Open time
2020-01-01 00:00:00+00:00    0.150774
2020-02-01 00:00:00+00:00    0.057471
2020-03-01 00:00:00+00:00   -0.005920
2020-04-01 00:00:00+00:00    0.144156
2020-05-01 00:00:00+00:00    0.165367
2020-06-01 00:00:00+00:00   -0.015025
2020-07-01 00:00:00+00:00    0.179079
2020-08-01 00:00:00+00:00    0.084451
2020-09-01 00:00:00+00:00   -0.018819
2020-10-01 00:00:00+00:00    0.064898
2020-11-01 00:00:00+00:00    0.322020
2020-12-01 00:00:00+00:00    0.331068
Freq: MS, Name: Close, dtype: float64

>>> ms_pf.returns
Open time
2020-01-01 00:00:00+00:00    0.150774
2020-02-01 00:00:00+00:00    0.057471
2020-03-01 00:00:00+00:00   -0.005920
2020-04-01 00:00:00+00:00    0.144156
2020-05-01 00:00:00+00:00    0.165367
2020-06-01 00:00:00+00:00   -0.015025
2020-07-01 00:00:00+00:00    0.179079
2020-08-01 00:00:00+00:00    0.084451
2020-09-01 00:00:00+00:00   -0.018819
2020-10-01 00:00:00+00:00    0.064898
2020-11-01 00:00:00+00:00    0.322020
2020-12-01 00:00:00+00:00    0.331068
Freq: MS, Name: Close, dtype: float64
```

!!! tip
    Since returns are reconstructed all the way up from order records and involve 
    many other attributes, getting identical results like this shows that the entire 
    VBT implementation is algorithmically correct :smirk:

By the way, if you are curious how to aggregate those P&L values from the graph, here is how:

```pycon
>>> ms_pf.trades.pnl.to_pd(reduce_func_nb="sum")  # (1)!
Open time
2020-01-01 00:00:00+00:00     15.077357
2020-02-01 00:00:00+00:00      6.613564
2020-03-01 00:00:00+00:00     -0.113362
2020-04-01 00:00:00+00:00     16.831599
2020-05-01 00:00:00+00:00     22.888280
2020-06-01 00:00:00+00:00     -2.502485
2020-07-01 00:00:00+00:00     26.603047
2020-08-01 00:00:00+00:00     18.804921
2020-09-01 00:00:00+00:00     -6.180621
2020-10-01 00:00:00+00:00     10.133302
2020-11-01 00:00:00+00:00     35.891558
2020-12-01 00:00:00+00:00    129.461217
Freq: MS, Name: Close, dtype: float64
```

1. Extract the P&L values for all exit trade records 
(returned as a [MappedArray](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray) instance), 
and use [MappedArray.to_pd](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.to_pd)
to aggregate values mapped to the same index with `reduce_func_nb` and return the result in Pandas format.

## Summary

Remember that when working with bars, any information stored under a timestamp 
usually does not happen exactly at that moment. It happens somewhere between this 
timestamp and the next one. While this might seem basic, it drastically changes the logic of resampling, 
requiring us to be very careful to avoid lookahead bias when aligning multiple time frames. Fortunately, 
VBT provides a range of highly optimized functions that account for this and make our lives easier!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/mtf-analysis/aggregation.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/MTFAnalysis.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
children:
  - https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/integrations/
  - https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/dynamic/
name: Portfolio optimization
type: page
icon: material-chart-bar-stacked
---

# Portfolio optimization

Portfolio optimization focuses on constructing a portfolio of assets that seeks to maximize returns and
minimize risk. In this context, a portfolio refers to the distribution of an investor's assets—a weight
vector—which can be optimized for risk tolerance, expected rate of return, cost minimization, and other
objectives. This optimization can be performed regularly to reflect recent changes in market behavior.

In VBT, a portfolio is a collection of asset vectors combined into a larger array along the column
axis. By default, each of these vectors is treated as a separate backtesting instance, but you can apply a
grouping instruction to treat multiple assets as a single unit. Portfolio optimization then becomes the
process of converting a set of pricing vectors (information as input) into a set of allocation vectors
(actions as output), which can then be provided to any simulator.

Thanks to VBT's modular design (and in line with the key principles of data science), optimization and
simulation are handled separately. This enables you to analyze and filter allocation vectors even before
they are backtested. This approach is similar to the typical workflow for working with signals: 1)
generate, 2) pre-analyze, 3) simulate, and 4) post-analyze. In this example, we will cover how to complete
each of these steps for the highest informational yield.

## Data

As always, we should begin by obtaining some data. Because portfolio optimization involves working with a
group of assets, we need to fetch data for more than one symbol. Here, we will fetch one year of hourly
data for 5 different cryptocurrencies:

```pycon
>>> from vectorbtpro import *

>>> data = vbt.BinanceData.pull(
...     ["BTCUSDT", "ETHUSDT", "BNBUSDT", "XRPUSDT", "ADAUSDT"], 
...     start="2020-01-01 UTC", 
...     end="2021-01-01 UTC",
...     timeframe="1h"
... )
```

[=100% "Symbol 5/5"]{: .candystripe .candystripe-animate }

[=100% "Period 9/9"]{: .candystripe .candystripe-animate }

Let's save the data locally to avoid re-fetching it every time we start a new runtime:

```pycon
>>> data.to_hdf()

>>> data = vbt.HDFData.pull("BinanceData.h5")
```

## Allocation

In simple terms, asset allocation is the process of deciding where to invest funds in the market—it is a
horizontal vector composed of weights or amounts of assets at a specific timestamp. For example, to
allocate 50% to `BTCUSDT`, 20% to `ETHUSDT`, and distribute the remainder equally among the other assets,
the allocation vector would be `[0.5, 0.2, 0.1, 0.1, 0.1]`. Frequently, weight allocations sum to 1,
ensuring the entire stake is continuously invested, but you can also choose to invest only a portion of
your balance or specify a particular (continuous or discrete) number of assets instead of weights. Since we
generally want to allocate periodically rather than hold positions indefinitely, we also need to decide on
rebalancing timestamps.

### Manually

Let's manually generate and simulate allocations to better understand how everything works together.

#### Index points

The first step is to decide when to re-allocate. This is straightforward using
[ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points),
which converts a human-readable query into a list of index positions (also called "index points" or
"allocation points"). These positions are simple numeric indices, where `0` is the first row and
`len(index) - 1` is the last.

For example, let's convert the first day of each month into index points:

```pycon
>>> ms_points = data.wrapper.get_index_points(every="M")
>>> ms_points
array([0, 744, 1434, 2177, 2895, 3639, 4356, 5100, 5844, 6564, 7308, 8027])
```

!!! tip
    You can check the indices above using Pandas:
    
    ```pycon
    >>> data.wrapper.index.get_indexer(
    ...     pd.Series(index=data.wrapper.index).resample(vbt.offset("M")).asfreq().index, 
    ...     method="bfill"
    ... )
    array([0, 744, 1434, 2177, 2895, 3639, 4356, 5100, 5844, 6564, 7308, 8027])
    ```

We can also convert these index points back to timestamps:

```pycon
>>> data.wrapper.index[ms_points]
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-02-01 00:00:00+00:00',
                '2020-03-01 00:00:00+00:00', '2020-04-01 00:00:00+00:00',
                '2020-05-01 00:00:00+00:00', '2020-06-01 00:00:00+00:00',
                '2020-07-01 00:00:00+00:00', '2020-08-01 00:00:00+00:00',
                '2020-09-01 00:00:00+00:00', '2020-10-01 00:00:00+00:00',
                '2020-11-01 00:00:00+00:00', '2020-12-01 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

!!! note
    [ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points)
    always returns indices that can be used on the index, unless `skipna` is disabled.
    In that case, it will return `-1` wherever an index point cannot be found.

These are our [rebalancing](https://www.investopedia.com/terms/r/rebalancing.asp) timestamps!

The main advantage of this method is its flexibility. The `every` argument can be a string,
an integer, a `pd.Timedelta` object, or a `pd.DateOffset` object:

```pycon
>>> example_points = data.wrapper.get_index_points(every=24 * 30)  # (1)!
>>> data.wrapper.index[example_points]
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-31 00:00:00+00:00',
                '2020-03-01 06:00:00+00:00', '2020-03-31 07:00:00+00:00',
                '2020-04-30 09:00:00+00:00', '2020-05-30 09:00:00+00:00',
                '2020-06-29 12:00:00+00:00', '2020-07-29 12:00:00+00:00',
                '2020-08-28 12:00:00+00:00', '2020-09-27 12:00:00+00:00',
                '2020-10-27 12:00:00+00:00', '2020-11-26 12:00:00+00:00',
                '2020-12-26 17:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)

>>> date_offset = pd.offsets.WeekOfMonth(week=3, weekday=4)
>>> example_points = data.wrapper.get_index_points(  # (2)!
...     every=date_offset, 
...     add_delta=pd.Timedelta(hours=17)
... )
>>> data.wrapper.index[example_points]
DatetimeIndex(['2020-01-24 17:00:00+00:00', '2020-02-28 17:00:00+00:00',
                '2020-03-27 17:00:00+00:00', '2020-04-24 17:00:00+00:00',
                '2020-05-22 17:00:00+00:00', '2020-06-26 17:00:00+00:00',
                '2020-07-24 17:00:00+00:00', '2020-08-28 17:00:00+00:00',
                '2020-09-25 17:00:00+00:00', '2020-10-23 17:00:00+00:00',
                '2020-11-27 17:00:00+00:00', '2020-12-25 17:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Every 24 * 30 = 1440 hours.

2. At 17:00 on the last Friday of each month.

!!! tip
    Take a look at the [available date offsets](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects).

You can also use `start` and `end` as human-readable strings (thanks to
[dateparser](https://github.com/scrapinghub/dateparser)!), integers, or `pd.Timestamp` objects
to limit the date range as needed:

```pycon
>>> example_points = data.wrapper.get_index_points(
...     start="April 1st 2020",
...     every="M"
... )
>>> data.wrapper.index[example_points]
DatetimeIndex(['2020-04-01 00:00:00+00:00', '2020-05-01 00:00:00+00:00',
                '2020-06-01 00:00:00+00:00', '2020-07-01 00:00:00+00:00',
                '2020-08-01 00:00:00+00:00', '2020-09-01 00:00:00+00:00',
                '2020-10-01 00:00:00+00:00', '2020-11-01 00:00:00+00:00',
                '2020-12-01 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

Another helpful feature is providing your own dates using the `on` argument.
[ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points)
will match these dates to the index. If any date is not found, it will simply use the next date
(not the previous one, since we should not look into the future):

```pycon
>>> example_points = data.wrapper.get_index_points(
...     on=["April 1st 2020 19:45", "17 September 2020 00:01"]
... )
>>> data.wrapper.index[example_points]
DatetimeIndex([
    '2020-04-01 20:00:00+00:00', 
    '2020-09-17 01:00:00+00:00'
], dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

But let's continue with the `ms_points` generated earlier.

#### Filling

We now have our allocation index points, so it's time to fill in the actual allocations at these points.
First, let's create an empty DataFrame with symbols as columns:

```pycon
>>> symbol_wrapper = data.get_symbol_wrapper(freq="1h")  # (1)!
>>> filled_allocations = symbol_wrapper.fill()  # (2)!
>>> filled_allocations
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time                                                             
2020-01-01 00:00:00+00:00      NaN      NaN      NaN      NaN      NaN
2020-01-01 01:00:00+00:00      NaN      NaN      NaN      NaN      NaN
2020-01-01 02:00:00+00:00      NaN      NaN      NaN      NaN      NaN
...                            ...      ...      ...      ...      ...
2020-12-31 21:00:00+00:00      NaN      NaN      NaN      NaN      NaN
2020-12-31 22:00:00+00:00      NaN      NaN      NaN      NaN      NaN
2020-12-31 23:00:00+00:00      NaN      NaN      NaN      NaN      NaN

[8767 rows x 5 columns]
```

1. Using [Data.get_symbol_wrapper](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get_symbol_wrapper).

2. Using [ArrayWrapper.fill](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.fill).

Next, we need to generate allocations and assign them at their index points. In this example,
we will create allocations randomly:

```pycon
>>> np.random.seed(42)  # (1)!

>>> def random_allocate_func():
...     weights = np.random.uniform(size=symbol_wrapper.shape[1])
...     return weights / weights.sum()  # (2)!

>>> for idx in ms_points:
...     filled_allocations.iloc[idx] = random_allocate_func()

>>> allocations = filled_allocations[~filled_allocations.isnull().any(axis=1)]
>>> allocations
symbol                      ADAUSDT   BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
Open time                                                                  
2020-01-01 00:00:00+00:00  0.133197  0.338101  0.260318  0.212900  0.055485
2020-02-01 00:00:00+00:00  0.065285  0.024308  0.362501  0.251571  0.296334
2020-03-01 00:00:00+00:00  0.009284  0.437468  0.375464  0.095773  0.082010
2020-04-01 00:00:00+00:00  0.105673  0.175297  0.302353  0.248877  0.167800
2020-05-01 00:00:00+00:00  0.327909  0.074759  0.156568  0.196343  0.244421
2020-06-01 00:00:00+00:00  0.367257  0.093395  0.240527  0.277095  0.021727
2020-07-01 00:00:00+00:00  0.220313  0.061837  0.023590  0.344094  0.350166
2020-08-01 00:00:00+00:00  0.346199  0.130452  0.041828  0.293025  0.188497
2020-09-01 00:00:00+00:00  0.067065  0.272119  0.018898  0.499708  0.142210
2020-10-01 00:00:00+00:00  0.297647  0.140040  0.233647  0.245617  0.083048
2020-11-01 00:00:00+00:00  0.232128  0.185574  0.224925  0.214230  0.143143
2020-12-01 00:00:00+00:00  0.584609  0.056118  0.124283  0.028681  0.206309
```

1. Set the random seed to make the results reproducible.

2. Divide by the sum to ensure the weights add up to 1.

That's it! We can now use these weight vectors for simulation.

#### Simulation

The simulation step is simple: use the filled allocations as the size for the target percentage type,
enable grouping with cash sharing, and use a dynamic call sequence.

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=data.get("Close"),
...     size=filled_allocations,
...     size_type="targetpercent",
...     group_by=True,  # (1)!
...     cash_sharing=True,
...     call_seq="auto"  # (2)!
... )
```

1. Change this if you have multiple groups.

2. Selling before buying is important!

We can extract the actual allocations produced by the simulation:

```pycon
>>> sim_alloc = pf.get_asset_value(group_by=False).vbt / pf.value
>>> sim_alloc  # (1)!
symbol                      ADAUSDT   BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
Open time                                                                  
2020-01-01 00:00:00+00:00  0.133197  0.338101  0.260318  0.212900  0.055485
2020-01-01 01:00:00+00:00  0.132979  0.337881  0.259649  0.214099  0.055393
2020-01-01 02:00:00+00:00  0.133259  0.337934  0.259737  0.213728  0.055342
...                             ...       ...       ...       ...       ...
2020-12-31 21:00:00+00:00  0.636496  0.067686  0.188081  0.035737  0.072000
2020-12-31 22:00:00+00:00  0.634586  0.068128  0.189404  0.035930  0.071952
2020-12-31 23:00:00+00:00  0.638154  0.068205  0.187649  0.035619  0.070373

[8766 rows x 5 columns]
```

1. You can also return this using `pf.allocations`.

We can plot the allocations manually:

```pycon
>>> sim_alloc.vbt.plot(
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/actual_allocations.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/actual_allocations.dark.svg#only-dark){: .iimg loading=lazy }

Or use [Portfolio.plot_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.plot_allocations):

```pycon
>>> pf.plot_allocations().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/plot_allocations.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/plot_allocations.dark.svg#only-dark){: .iimg loading=lazy }

Without transaction costs such as commissions and slippage, the source and target allocations should
closely match at the allocation points:

```pycon
>>> np.isclose(allocations, sim_alloc.iloc[ms_points])
array([[ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True],
        [ True,  True,  True,  True,  True]])
```

### Allocation method

We have learned how to manually generate, fill, and simulate allocations. But VBT would not
be VBT if it did not provide a convenient function for this! This is where
[PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
comes into play: it offers various class methods for generating allocations.
The workings of this class are quite straightforward (despite the complex implementation): it generates
allocations and stores them in a compressed form for later analysis and simulation.

The allocation generation is managed by the class method
[PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func).
If you review the documentation for this method, you will notice that it takes the same arguments as
[ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points)
to generate index points. Then, at each of these points, it calls a user-defined allocation
function `allocate_func` to produce an allocation vector. Finally, all the returned vectors are concatenated
into a single two-dimensional NumPy array, while the index points are stored in a separate structured NumPy
array of type [AllocPoints](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/records/#vectorbtpro.portfolio.pfopt.records.AllocPoints).

Let's apply the optimizer class to `random_allocate_func`:

```pycon
>>> np.random.seed(42)

>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,  # (1)!
...     random_allocate_func,
...     every="M"  # (2)!
... )
```

1. The wrapper must contain symbols as columns.

2. If you do not provide a frequency, the function will be called at every date.

[=100% "Allocation 12/12"]{: .candystripe .candystripe-animate }

!!! tip
    There is also a convenient method [PortfolioOptimizer.from_random](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_random)
    to generate random allocations. Give it a try!

Now, let's look at the generated random allocations:

```pycon
>>> pfo.allocations  # (1)!
symbol                      ADAUSDT   BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
Open time                                                                  
2020-01-01 00:00:00+00:00  0.133197  0.338101  0.260318  0.212900  0.055485
2020-02-01 00:00:00+00:00  0.065285  0.024308  0.362501  0.251571  0.296334
2020-03-01 00:00:00+00:00  0.009284  0.437468  0.375464  0.095773  0.082010
2020-04-01 00:00:00+00:00  0.105673  0.175297  0.302353  0.248877  0.167800
2020-05-01 00:00:00+00:00  0.327909  0.074759  0.156568  0.196343  0.244421
2020-06-01 00:00:00+00:00  0.367257  0.093395  0.240527  0.277095  0.021727
2020-07-01 00:00:00+00:00  0.220313  0.061837  0.023590  0.344094  0.350166
2020-08-01 00:00:00+00:00  0.346199  0.130452  0.041828  0.293025  0.188497
2020-09-01 00:00:00+00:00  0.067065  0.272119  0.018898  0.499708  0.142210
2020-10-01 00:00:00+00:00  0.297647  0.140040  0.233647  0.245617  0.083048
2020-11-01 00:00:00+00:00  0.232128  0.185574  0.224925  0.214230  0.143143
2020-12-01 00:00:00+00:00  0.584609  0.056118  0.124283  0.028681  0.206309
```

1. See [PortfolioOptimizer.get_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.get_allocations).

We can also fill the entire array so it can be used in simulation:

```pycon
>>> pfo.filled_allocations  # (1)!
symbol                      ADAUSDT   BNBUSDT   BTCUSDT  ETHUSDT   XRPUSDT
Open time                                                                 
2020-01-01 00:00:00+00:00  0.133197  0.338101  0.260318   0.2129  0.055485
2020-01-01 01:00:00+00:00       NaN       NaN       NaN      NaN       NaN
2020-01-01 02:00:00+00:00       NaN       NaN       NaN      NaN       NaN
2020-01-01 03:00:00+00:00       NaN       NaN       NaN      NaN       NaN
...                             ...       ...       ...      ...       ...
2020-12-31 21:00:00+00:00       NaN       NaN       NaN      NaN       NaN
2020-12-31 22:00:00+00:00       NaN       NaN       NaN      NaN       NaN
2020-12-31 23:00:00+00:00       NaN       NaN       NaN      NaN       NaN

[8767 rows x 5 columns]
```

1. See [PortfolioOptimizer.fill_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.fill_allocations).

!!! note
    A row filled with NaN values means there is no allocation at that timestamp.

Since an instance of [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
stores not only the allocation vectors but also the index points themselves, you can access them
using [PortfolioOptimizer.alloc_records](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.alloc_records)
and analyze them like regular records:

```pycon
>>> pfo.alloc_records.records_readable
    Id  Group          Allocation Index
0    0  group 2020-01-01 00:00:00+00:00
1    1  group 2020-02-01 00:00:00+00:00
2    2  group 2020-03-01 00:00:00+00:00
3    3  group 2020-04-01 00:00:00+00:00
4    4  group 2020-05-01 00:00:00+00:00
5    5  group 2020-06-01 00:00:00+00:00
6    6  group 2020-07-01 00:00:00+00:00
7    7  group 2020-08-01 00:00:00+00:00
8    8  group 2020-09-01 00:00:00+00:00
9    9  group 2020-10-01 00:00:00+00:00
10  10  group 2020-11-01 00:00:00+00:00
11  11  group 2020-12-01 00:00:00+00:00
```

The allocations can be plotted easily using
[PortfolioOptimizer.plot](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.plot):

```pycon
>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/optimizer.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/optimizer.dark.svg#only-dark){: .iimg loading=lazy }

Since [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
is a subclass of [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),
we can generate statistics to describe the optimizer's current state:

```pycon
>>> pfo.stats()
Start                       2020-01-01 00:00:00+00:00
End                         2020-12-31 23:00:00+00:00
Period                              365 days 06:00:00
Total Records                                      12
Mean Allocation: ADAUSDT                     0.229714
Mean Allocation: BNBUSDT                     0.165789
Mean Allocation: BTCUSDT                     0.197075
Mean Allocation: ETHUSDT                     0.242326
Mean Allocation: XRPUSDT                     0.165096
Name: group, dtype: object
```

What about simulation? [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
includes a dedicated class method for this purpose:
[Portfolio.from_optimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_optimizer).

```pycon
>>> pf = vbt.Portfolio.from_optimizer(data, pfo, freq="1h")

>>> pf.sharpe_ratio
2.097991099869708
```

Alternatively, you can run the simulation directly from the portfolio optimizer:

```pycon
>>> pf = pfo.simulate(data, freq="1h")

>>> pf.sharpe_ratio
2.097991099869708
```

As we can see, VBT continues its modular approach to keep individual backtesting components
as independent as possible while maintaining coherence. Instead of defining all the logic within
a single backtesting module, you can divide the workflow into a set of logically separate,
isolated components, each of which can be maintained independently.

#### Once

To allocate once, you can use [PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func)
with `on=0`, or use [PortfolioOptimizer.from_initial](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_initial):

=== "from_allocate_func"

    ```pycon
    >>> def const_allocate_func(target_alloc):
    ...     return target_alloc

    >>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
    ...     symbol_wrapper,
    ...     const_allocate_func,
    ...     [0.5, 0.2, 0.1, 0.1, 0.1],
    ...     on=0
    ... )

    >>> pfo.plot().show()
    ```

=== "from_initial"

    ```pycon
    >>> pfo = vbt.PortfolioOptimizer.from_initial(
    ...     symbol_wrapper,
    ...     [0.5, 0.2, 0.1, 0.1, 0.1]
    ... )

    >>> pfo.plot().show()
    ```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/once.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/once.dark.svg#only-dark){: .iimg loading=lazy }

!!! note
    Even if the lines appear straight on the chart, this does not mean rebalancing happens at each
    timestamp. This effect is mainly due to VBT forward-filling the allocation. In reality,
    the initial allocation is preserved at the first timestamp, and then it typically begins to
    change. That's why periodic or threshold rebalancing is required to maintain the allocation
    over the entire period.

#### Custom array

If you already have an array with allocations in either compressed or filled form,
you can use [PortfolioOptimizer.from_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocations)
and [PortfolioOptimizer.from_filled_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_filled_allocations)
respectively.

Let's create a compressed array with custom allocations for each quarter:

```pycon
>>> custom_index = vbt.date_range("2020-01-01", "2021-01-01", freq="Q")
>>> custom_allocations = pd.DataFrame(
...     [
...         [0.5, 0.2, 0.1, 0.1, 0.1],
...         [0.1, 0.5, 0.2, 0.1, 0.1],
...         [0.1, 0.1, 0.5, 0.2, 0.1],
...         [0.1, 0.1, 0.1, 0.5, 0.2]
...     ],
...     index=custom_index,
...     columns=symbol_wrapper.columns
... )
```

When you pass a DataFrame, VBT automatically uses its index as the `on` argument
to apply allocations at those (or next) timestamps in the original index:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocations(
...     symbol_wrapper,
...     custom_allocations
... )
>>> pfo.allocations
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00      0.5      0.2      0.1      0.1      0.1
2020-04-01 00:00:00+00:00      0.1      0.5      0.2      0.1      0.1
2020-07-01 00:00:00+00:00      0.1      0.1      0.5      0.2      0.1
2020-10-01 00:00:00+00:00      0.1      0.1      0.1      0.5      0.2
```

However, if you pass a NumPy array, VBT will not be able to parse the dates,
so you must specify the index points manually:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocations(
...     symbol_wrapper,
...     custom_allocations.values,
...     start="2020-01-01",
...     end="2021-01-01",
...     every="Q"
... )
>>> pfo.allocations
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00      0.5      0.2      0.1      0.1      0.1
2020-04-01 00:00:00+00:00      0.1      0.5      0.2      0.1      0.1
2020-07-01 00:00:00+00:00      0.1      0.1      0.5      0.2      0.1
2020-10-01 00:00:00+00:00      0.1      0.1      0.1      0.5      0.2
```

You can also use allocations that have already been filled as input. In this case, you do not
even need to provide a wrapper. Vectorbt will extract the necessary information from the DataFrame
itself. Filled allocations are handled by treating any row where all values are NaN as empty.
Let's use the filled allocations from the previous optimizer as input to another optimizer:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_filled_allocations(
...     pfo.fill_allocations()
... )
>>> pfo.allocations
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00      0.5      0.2      0.1      0.1      0.1
2020-04-01 00:00:00+00:00      0.1      0.5      0.2      0.1      0.1
2020-07-01 00:00:00+00:00      0.1      0.1      0.5      0.2      0.1
2020-10-01 00:00:00+00:00      0.1      0.1      0.1      0.5      0.2
```

!!! tip
    You can re-run this cell as many times as you like. There is no information loss!

#### Templates

What if you want to use more complex allocation functions that require passing arguments?
One of the coolest features of VBT is templates, which act as a sort of callback.
With templates, you can instruct VBT to run small code snippets at various execution points,
typically whenever new information becomes available.

When a new index point is processed by [PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func),
VBT substitutes all templates found in `*args` and `**kwargs` using the current context
and passes them to the allocation function. The template context includes all arguments given
to the class method, plus the generated index points (`index_points`), the current iteration
index (`i`), and the specific index point (`index_point`).

To make our example more interesting, let's rotate and allocate 100% to one asset at a time:

```pycon
>>> def rotation_allocate_func(wrapper, i):
...     weights = np.full(len(wrapper.columns), 0)
...     weights[i % len(wrapper.columns)] = 1
...     return weights

>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     rotation_allocate_func,
...     vbt.Rep("wrapper"),  # (1)!
...     vbt.Rep("i"),
...     every="M"
... )

>>> pfo.plot().show()
```

1. Replace the first and second variable arguments with the wrapper and iteration index, respectively.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/templates.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/templates.dark.svg#only-dark){: .iimg loading=lazy }

You can also use evaluation templates to accomplish the same task:

```pycon
>>> def rotation_allocate_func(symbols, chosen_symbol):
...     return {s: 1 if s == chosen_symbol else 0 for s in symbols}

>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     rotation_allocate_func,
...     vbt.RepEval("wrapper.columns"),  # (1)!
...     vbt.RepEval("wrapper.columns[i % len(wrapper.columns)]"),
...     every="M"
... )

>>> pfo.allocations
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00        1        0        0        0        0
2020-02-01 00:00:00+00:00        0        1        0        0        0
2020-03-01 00:00:00+00:00        0        0        1        0        0
2020-04-01 00:00:00+00:00        0        0        0        1        0
2020-05-01 00:00:00+00:00        0        0        0        0        1
2020-06-01 00:00:00+00:00        1        0        0        0        0
2020-07-01 00:00:00+00:00        0        1        0        0        0
2020-08-01 00:00:00+00:00        0        0        1        0        0
2020-09-01 00:00:00+00:00        0        0        0        1        0
2020-10-01 00:00:00+00:00        0        0        0        0        1
2020-11-01 00:00:00+00:00        1        0        0        0        0
2020-12-01 00:00:00+00:00        0        1        0        0        0
```

1. Evaluate these expressions and use them as arguments for the function.

!!! tip
    The allocation function can return a sequence of values (one per asset), a dictionary (with assets as
    keys), or even a Pandas Series (with assets as the index). In other words, it can return anything
    that can be packed into a list and used as input for a DataFrame. If any asset key is not provided,
    its allocation will be NaN.

#### Groups

Testing a single combination of parameters can be boring, so VBT provides two different
features for parameter combinations: arguments wrapped with the [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
class and group configs. The idea behind the former is similar to what you may have seen in
[broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast): wrap a sequence of
values with this class to combine the argument with other arguments or similar parameters.
Let's implement constant-weight asset allocation with various rebalancing intervals:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     const_allocate_func,
...     [0.5, 0.2, 0.1, 0.1, 0.1],
...     every=vbt.Param(["1M", "2M", "3M"])  # (1)!
... )

>>> pf = pfo.simulate(data, freq="1h")
>>> pf.total_return
every
1M    3.716574
2M    3.435540
3M    3.516401
Name: total_return, dtype: float64
```

1. Create three groups: rebalance monthly, once every two months, and once every three months.

!!! tip
    To hide the progress bar, pass `execute_kwargs=dict(show_progress=False)`.

As you can see, VBT recognizes that the `every` argument is a parameter, so it creates
a column level named after the argument and puts it on top of the symbol columns.

Now, let's define another parameter for weights:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     const_allocate_func,
...     vbt.Param([
...         [0.5, 0.2, 0.1, 0.1, 0.1],
...         [0.2, 0.1, 0.1, 0.1, 0.5]
...     ], keys=pd.Index(["w1", "w2"], name="weights")),  # (1)!
...     every=vbt.Param(["1M", "2M", "3M"])
... )
```

1. Set a custom name for the parameter in the final column index.

This code produces 6 parameter combinations (groups):

```pycon
>>> pfo.wrapper.grouper.get_index()
MultiIndex([('1M', 'w1'),
            ('1M', 'w2'),
            ('2M', 'w1'),
            ('2M', 'w2'),
            ('3M', 'w1'),
            ('3M', 'w2')],
            names=['every', 'weights'])
```

And applies each combination to the asset columns:

```pycon
>>> pfo.wrapper.columns
MultiIndex([('1M', 'w1', 'ADAUSDT'),
            ('1M', 'w1', 'BNBUSDT'),
            ('1M', 'w1', 'BTCUSDT'),
            ...
            ('3M', 'w2', 'BTCUSDT'),
            ('3M', 'w2', 'ETHUSDT'),
            ('3M', 'w2', 'XRPUSDT')],
            names=['every', 'weights', 'symbol'])
```

To select or plot the allocations for any parameter combination,
you can use Pandas-like indexing **on groups**:

```pycon
>>> pfo[("3M", "w2")].stats()
Start                       2020-01-01 00:00:00+00:00
End                         2020-12-31 23:00:00+00:00
Period                              365 days 06:00:00
Total Records                                       4
Mean Allocation: ADAUSDT                          0.2
Mean Allocation: BNBUSDT                          0.1
Mean Allocation: BTCUSDT                          0.1
Mean Allocation: ETHUSDT                          0.1
Mean Allocation: XRPUSDT                          0.5
Name: (3M, w2), dtype: object
```

!!! note
    When plotting, instead of indexing, you can pass a group name or tuple using the `column` argument.

But what if you have more complex groups? Representing everything with parameters can become
cumbersome when arguments barely overlap. Fortunately, you can use the `group_configs` argument
to pass a list of dictionaries, each representing a group and defining its arguments.
Let's use this method for the example above:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     const_allocate_func,
...     group_configs=[
...         dict(args=([0.5, 0.2, 0.1, 0.1, 0.1],), every="1M"),
...         dict(args=([0.2, 0.1, 0.1, 0.1, 0.5],), every="2M"),
...         dict(args=([0.1, 0.1, 0.1, 0.5, 0.2],), every="3M"),
...         dict(args=([0.1, 0.1, 0.5, 0.2, 0.1],), every="1M"),
...         dict(args=([0.1, 0.5, 0.2, 0.1, 0.1],), every="2M"),
...         dict(args=([0.5, 0.2, 0.1, 0.1, 0.1],), every="3M"),
...     ]
... )
pfo.wrapper.grouper.get_index()
Int64Index([0, 1, 2, 3, 4, 5], dtype='int64', name='group_config')
```

Unlike the previous example, where VBT created two column levels for the parameters, this
produces only one, where each number is the index of a group config. Now, let's make it more fun
by creating one group with a constant allocation and another group with a random allocation!

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     const_allocate_func,
...     group_configs=[
...         dict(
...             allocate_func=const_allocate_func, 
...             args=([0.5, 0.2, 0.1, 0.1, 0.1],),
...             _name="const"  # (1)!
...         ),
...         dict(
...             allocate_func=random_allocate_func,
...             every="M",
...             _name="random"
...         ),
...     ]
... )
>>> pfo.wrapper.grouper.get_index()
Index(['const', 'random'], dtype='object', name='group_config')
```

1. Assign a name to the config with the reserved key `_name`.

You can also combine [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) instances and
group configs for maximum flexibility:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     const_allocate_func,
...     group_configs={  # (1)!
...         "const": dict(
...             allocate_func=const_allocate_func, 
...             args=([0.5, 0.2, 0.1, 0.1, 0.1],)
...         ),
...         "random": dict(
...             allocate_func=random_allocate_func,
...         ),
...     },
...     every=vbt.Param(["1M", "2M", "3M"])  # (2)!
... )
>>> pfo.wrapper.grouper.get_index()
MultiIndex([('1M',  'const'),
            ('1M', 'random'),
            ('2M',  'const'),
            ('2M', 'random'),
            ('3M',  'const'),
            ('3M', 'random')],
            names=['every', 'group_config'])
```

1. You can also provide group configs as a dictionary with configuration names as keys.

2. Make sure that any parameterized argument is not redefined in any group config.

!!! info
    The column levels for parameters are always placed above those for group configs.

#### Numba

By default, VBT iterates over index points with a standard Python for-loop. This has almost
no effect on performance if the number of allocations is low, which is typical in portfolio
optimization. This is because running the allocation function itself takes much longer than a
single loop iteration. However, when the number of iterations reaches tens of thousands,
it may be worth using Numba for iteration.

To enable Numba, set `jitted_loop` to True. In this case, index points are iterated using
[allocate_meta_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/nb/#vectorbtpro.portfolio.pfopt.nb.allocate_meta_nb),
which passes the current iteration index, the current index point, and `*args`.

!!! note
    Variable keyword arguments are not supported by Numba yet.

Let's implement the rotation example with Numba, rebalancing every day:

```pycon
>>> @njit
... def rotation_allocate_func_nb(i, idx, n_cols):
...     weights = np.full(n_cols, 0)
...     weights[i % n_cols] = 1
...     return weights

>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     rotation_allocate_func_nb,
...     vbt.RepEval("len(wrapper.columns)"),
...     every="D",
...     jitted_loop=True
... )

>>> pfo.allocations.head()
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-05 00:00:00+00:00      1.0      0.0      0.0      0.0      0.0
2020-01-12 00:00:00+00:00      0.0      1.0      0.0      0.0      0.0
2020-01-19 00:00:00+00:00      0.0      0.0      1.0      0.0      0.0
2020-01-26 00:00:00+00:00      0.0      0.0      0.0      1.0      0.0
2020-02-02 00:00:00+00:00      0.0      0.0      0.0      0.0      1.0
```

#### Distribution

For the best performance, you can also run the allocation function in a distributed way,
as long as each function call does not depend on previous calls. This is only an issue if you
are storing state in a custom variable.

If the jitted loop is disabled, VBT sends all iterations to the
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function, which is
VBT's in-house execution infrastructure. This works much like how indicator parameter
combinations are distributed. In fact, the same argument `execute_kwargs` is available to
control execution.

Let's disable the jitted loop and pass all arguments required by our Numba-compiled function
`rotation_allocate_func_nb` using templates (since the function is not called by
[allocate_meta_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/nb/#vectorbtpro.portfolio.pfopt.nb.allocate_meta_nb)
anymore!):

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     rotation_allocate_func_nb,
...     vbt.Rep("i"),
...     vbt.Rep("index_point"),
...     vbt.RepEval("len(wrapper.columns)"),
...     every="D",
...     execute_kwargs=dict(engine="dask")
... )

>>> pfo.allocations.head()
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00        1        0        0        0        0
2020-01-02 00:00:00+00:00        0        1        0        0        0
2020-01-03 00:00:00+00:00        0        0        1        0        0
2020-01-04 00:00:00+00:00        0        0        0        1        0
2020-01-05 00:00:00+00:00        0        0        0        0        1
```

There is another great option for distributing the allocation process: enable
the jitted loop with [allocate_meta_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/nb/#vectorbtpro.portfolio.pfopt.nb.allocate_meta_nb)
and use chunking. This way, you can split index points into chunks and iterate over each chunk
entirely within Numba. Control chunking with the `chunked` argument, which is resolved and
passed to [chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked).
Just remember to supply a chunking specification for all extra arguments required by the allocation
function:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     rotation_allocate_func_nb,
...     vbt.RepEval("len(wrapper.columns)"),
...     every="D",
...     jitted_loop=True,
...     chunked=dict(
...         arg_take_spec=dict(args=vbt.ArgsTaker(None)),  # (1)!
...         engine="dask"
...     )
... )
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00      1.0      0.0      0.0      0.0      0.0
2020-01-02 00:00:00+00:00      0.0      1.0      0.0      0.0      0.0
2020-01-03 00:00:00+00:00      0.0      0.0      1.0      0.0      0.0
2020-01-04 00:00:00+00:00      0.0      0.0      0.0      1.0      0.0
2020-01-05 00:00:00+00:00      0.0      0.0      0.0      0.0      1.0
```

1. The argument `n_cols` used by `rotation_allocate_func_nb` is passed as the first argument through `*args`
to [allocate_meta_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/nb/#vectorbtpro.portfolio.pfopt.nb.allocate_meta_nb).
This is why we use [ArgsTaker](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ArgsTaker) to specify
that the first argument should not be split in any way (since we are chunking rows, not columns).
Otherwise, a warning will be triggered.

If you are not tired of all these distribution options, here is another one: parallelize the iteration
internally with Numba. This can be done with the `jitted` argument, which is resolved and passed
to the `@njit` decorator of [allocate_meta_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/nb/#vectorbtpro.portfolio.pfopt.nb.allocate_meta_nb):

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     rotation_allocate_func_nb,
...     vbt.RepEval("len(wrapper.columns)"),
...     every="D",
...     jitted_loop=True,
...     jitted=dict(parallel=True)
... )

>>> pfo.allocations.head()
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00      1.0      0.0      0.0      0.0      0.0
2020-01-02 00:00:00+00:00      0.0      1.0      0.0      0.0      0.0
2020-01-03 00:00:00+00:00      0.0      0.0      1.0      0.0      0.0
2020-01-04 00:00:00+00:00      0.0      0.0      0.0      1.0      0.0
2020-01-05 00:00:00+00:00      0.0      0.0      0.0      0.0      1.0
```

#### Previous allocation

To access the allocation created in the previous step, you must disable all distribution (in other words,
run the allocation function serially) and use a temporary list or another container
to store all generated allocations. Each time the allocation function is called, it will generate
a new allocation and save it in the container, making it accessible for the next step.
Let's slightly randomize each previous allocation to create a new one:

```pycon
>>> def randomize_prev_allocate_func(i, allocations, mean, std):
...     if i == 0:
...         return allocations[0]  # (1)!
...     prev_allocation = allocations[-1]  # (2)!
...     log_returns = np.random.normal(mean, std, size=len(prev_allocation))  # (3)!
...     returns = np.exp(log_returns) - 1  # (4)!
...     new_allocation = prev_allocation * (1 + returns)  # (5)!
...     new_allocation = new_allocation / new_allocation.sum()  # (6)!
...     allocations.append(new_allocation)  # (7)!
...     return new_allocation

>>> np.random.seed(42)

>>> n_symbols = len(symbol_wrapper.columns)
>>> init_allocation = np.full(n_symbols, 1 / n_symbols)
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     randomize_prev_allocate_func,
...     i=vbt.Rep("i"),  # (8)!
...     allocations=[init_allocation],  # (9)!
...     mean=0,
...     std=0.5,
...     every="W",
...     start=0,  # (10)!
...     exact_start=True
... )

>>> pfo.plot().show()
```

1. Use the initial allocation at the very first step.

2. For later steps, access the last allocation in the list.

3. Sample random log returns from a normal distribution using the provided parameters.

4. Convert the log returns to simple returns.

5. Apply the simple returns to the previous allocation.

6. Make sure the new allocation sums to 1.

7. Add the new allocation to the list to use at the next step.

8. Instruct VBT to pass the iteration number as `i`.

9. Create a list to be shared among function calls.

10. Start exactly from the first timestamp to use the initial allocation.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/prev_allocation.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/prev_allocation.dark.svg#only-dark){: .iimg loading=lazy }

#### Current allocation

Now that you know how to access the previous allocation, how do you get the current (updated) allocation,
since it has changed over time? You can simply forward-simulate it!

```pycon
>>> def current_allocate_func(price, index_point, alloc_info):
...     prev_alloc_info = alloc_info[-1]
...     prev_index_point = prev_alloc_info["index_point"]
...     prev_allocation = prev_alloc_info["allocation"]
...     if prev_index_point is None:
...         current_allocation = prev_allocation
...     else:
...         prev_price_period = price.iloc[prev_index_point:index_point]  # (1)!
...         prev_pfo = vbt.PFO.from_initial(prev_price_period.vbt.wrapper, prev_allocation)  # (2)!
...         prev_pf = prev_pfo.simulate(prev_price_period)
...         current_allocation = prev_pf.allocations.iloc[-1]  # (3)!
...     alloc_info.append(dict(  # (4)!
...         index_point=index_point,
...         allocation=current_allocation,
...     ))
...     return current_allocation

>>> n_symbols = len(symbol_wrapper.columns)
>>> init_allocation = np.full(n_symbols, 1 / n_symbols)
>>> pfo = vbt.PortfolioOptimizer.from_allocate_func(
...     symbol_wrapper,
...     current_allocate_func,
...     price=data.get("Close"),
...     index_point=vbt.Rep("index_point"),
...     alloc_info=[dict(index_point=None, allocation=init_allocation)],  # (5)!
...     every="W",
...     start=0,
...     exact_start=True
... )
>>> pfo.plot().show()
```

1. Select the prices for the period between the previous step (inclusive) and the current step (exclusive).

2. Create a smaller optimizer that starts from the previous allocation and simulates it for that period.

3. The last allocation in the simulated portfolio is the current allocation.

4. Add the current allocation and its index to the list for the next step.

5. Tip: If you want to access this list later, define it as a separate variable.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/current_allocation.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/current_allocation.dark.svg#only-dark){: .iimg loading=lazy }

The code above accesses the previous allocation, simulates the forward return, and then uses
the last allocation of the simulated portfolio as the new one. This is equivalent to simulating
only the initial allocation :sparkles:

```pycon
>>> init_pfo = vbt.PFO.from_initial(symbol_wrapper, init_allocation)
>>> continuous_pf = pfo.simulate(data.get("Close"))
>>> index_points = symbol_wrapper.get_index_points(every="W", start=0, exact_start=True)
>>> discrete_pfo = vbt.PFO.from_allocations(symbol_wrapper, continuous_pf.allocations.iloc[index_points])
>>> discrete_pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/current_allocation.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/current_allocation.dark.svg#only-dark){: .iimg loading=lazy }

## Optimization

Periodic allocation can be useful, but it offers somewhat limited capabilities compared to what is possible.
Consider a common scenario where you want to rebalance based on a data window instead of just at fixed
points in time. If you use an allocation function, you would need to track either the previous allocation
or the lookback period. To make this process easier, VBT provides an "optimization" function that
operates over a range of timestamps.

### Index ranges

Like index points, index ranges are collections of indices, but each element represents a range of
indices instead of a single point. In VBT, index ranges are usually represented by a
two-dimensional NumPy array, where the first column contains the range start indices (inclusive), and
the second column contains the range end indices (exclusive). Just as we used
[ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points) to
turn human-readable queries into arrays of indices, we can use
[ArrayWrapper.get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges)
to create similar queries for index ranges.

Let's demonstrate how to use this method by dividing the entire period into monthly ranges:

```pycon
>>> example_ranges = data.wrapper.get_index_ranges(every="M")
>>> example_ranges[0]
array([0, 744, 1434, 2177, 2895, 3639, 4356, 5100, 5844, 6564, 7308])

>>> example_ranges[1]
array([744, 1434, 2177, 2895, 3639, 4356, 5100, 5844, 6564, 7308, 8027])
```

Here's what happened: VBT created a new datetime index with a monthly frequency and generated a
range from each pair of values in that index.

To convert each index range back to timestamps:

```pycon
>>> data.wrapper.index[example_ranges[0][0]:example_ranges[1][0]]  # (1)!
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-01 01:00:00+00:00',
                '2020-01-01 02:00:00+00:00', '2020-01-01 03:00:00+00:00',
                '2020-01-01 04:00:00+00:00', '2020-01-01 05:00:00+00:00',
                ...
                '2020-01-31 18:00:00+00:00', '2020-01-31 19:00:00+00:00',
                '2020-01-31 20:00:00+00:00', '2020-01-31 21:00:00+00:00',
                '2020-01-31 22:00:00+00:00', '2020-01-31 23:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', length=744, freq=None)
```

1. The first number (either 0 or 1) selects the bound (left or right), and the second number selects a range.

!!! important
    The right bound (second column) is always exclusive, so you should not use it for indexing
    directly, as it may point to an element beyond the index length.

We can see that the first range covers values from `2020-01-01` to `2020-01-31`, representing one month.

If you want to look back over a set period of time rather than up to the previous allocation timestamp,
you can use the `lookback_period` argument. In the following example, we generate new indices each month
while looking back over the previous 3 months:

```pycon
>>> example_ranges = data.wrapper.get_index_ranges(
...     every="M", 
...     lookback_period="3M"  # (1)!
... )

>>> def get_index_bounds(range_starts, range_ends):  # (2)!
...     for i in range(len(range_starts)):
...         start_idx = range_starts[i]  # (3)!
...         end_idx = range_ends[i]  # (4)!
...         range_index = data.wrapper.index[start_idx:end_idx]
...         yield range_index[0], range_index[-1]

>>> list(get_index_bounds(*example_ranges))
[(Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-03-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-02-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-04-30 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-03-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-05-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-04-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-06-30 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-05-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-07-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-06-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-08-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-07-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-09-30 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-08-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-10-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-09-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-11-30 23:00:00+0000', tz='UTC'))]
```

1. The lookback period can also be provided as an integer (which is multiplied by the source frequency),
a `pd.Timedelta` object, or a `pd.DateOffset` object.

2. This is a simple function that returns the first and last timestamp for each index range.

3. Inclusive.

4. Exclusive.

But what if you know the exact dates when each range should start and/or end? Unlike index points, the
`start` and `end` arguments can be collections of indices or timestamps to define the range bounds:

```pycon
>>> example_ranges = data.wrapper.get_index_ranges(
...     start=["2020-01-01", "2020-04-01", "2020-08-01"],
...     end=["2020-04-01", "2020-08-01", "2020-12-01"]
... )

>>> list(get_index_bounds(*example_ranges))
[(Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-03-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-04-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-07-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-08-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-11-30 23:00:00+0000', tz='UTC'))]
```

!!! tip
    You can mark the first timestamp as exclusive and the last timestamp as inclusive by setting
    `closed_start` to False and `closed_end` to True, respectively. Note that these settings affect
    the input, but the output always follows the _from inclusive to exclusive_ scheme.

Additionally, if either `start` or `end` is a single value, it will automatically be broadcast to match
the length of the other argument. Let's simulate the movement of an expanding window:

```pycon
>>> example_ranges = data.wrapper.get_index_ranges(
...     start="2020-01-01",
...     end=["2020-04-01", "2020-08-01", "2020-12-01"]
... )

>>> list(get_index_bounds(*example_ranges))
[(Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-03-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-07-31 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-11-30 23:00:00+0000', tz='UTC'))]
```

Another useful argument is `fixed_start`, which, when combined with `every`, can also simulate an
expanding window:

```pycon
>>> example_ranges = data.wrapper.get_index_ranges(
...     every="Q",
...     exact_start=True,  # (1)!
...     fixed_start=True
... )

>>> list(get_index_bounds(*example_ranges))
[(Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-03-30 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-06-29 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-09-29 23:00:00+0000', tz='UTC')),
  (Timestamp('2020-01-01 00:00:00+0000', tz='UTC'),
  Timestamp('2020-12-30 23:00:00+0000', tz='UTC'))]
```

1. Without this flag, the first date would be "2020-03-31" as generated by 
[pandas.date_range](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html).

### Optimization method

Similar to [PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func),
which is applied on index points, there is a class method
[PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func)
that is applied on index ranges. This method works almost identically to its counterpart, except that
each iteration calls an optimization function `optimize_func` that focuses on an index range (available
as `index_slice` through the template context). All index ranges are stored as
records of type [AllocRanges](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/records/#vectorbtpro.portfolio.pfopt.records.AllocRanges),
which is a subclass of [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges).

Let's try a simple example: allocate inversely proportional to the return of an asset. This approach
allocates more to assets that have recently performed poorly, with the expectation of buying them at a
discounted price and that they will turn bullish in the upcoming period.

```pycon
>>> def inv_rank_optimize_func(price, index_slice):
...     price_period = price.iloc[index_slice]  # (1)!
...     first_price = price_period.iloc[0]
...     last_price = price_period.iloc[-1]
...     ret = (last_price - first_price) / first_price  # (2)!
...     ranks = ret.rank(ascending=False)  # (3)!
...     return ranks / ranks.sum()  # (4)!

>>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
...     symbol_wrapper,
...     inv_rank_optimize_func,
...     data.get("Close"),
...     vbt.Rep("index_slice"),  # (5)!
...     every="M"
... )

>>> pfo.allocations
symbol                      ADAUSDT   BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
Open time                                                                  
2020-02-01 00:00:00+00:00  0.066667  0.200000  0.266667  0.133333  0.333333
2020-03-01 00:00:00+00:00  0.333333  0.133333  0.266667  0.066667  0.200000
2020-04-01 00:00:00+00:00  0.266667  0.200000  0.133333  0.333333  0.066667
2020-05-01 00:00:00+00:00  0.066667  0.200000  0.266667  0.133333  0.333333
2020-06-01 00:00:00+00:00  0.066667  0.266667  0.200000  0.133333  0.333333
2020-07-01 00:00:00+00:00  0.066667  0.266667  0.200000  0.133333  0.333333
2020-08-01 00:00:00+00:00  0.066667  0.266667  0.333333  0.133333  0.200000
2020-09-01 00:00:00+00:00  0.333333  0.133333  0.266667  0.066667  0.200000
2020-10-01 00:00:00+00:00  0.266667  0.066667  0.133333  0.333333  0.200000
2020-11-01 00:00:00+00:00  0.333333  0.266667  0.066667  0.133333  0.200000
2020-12-01 00:00:00+00:00  0.133333  0.333333  0.266667  0.200000  0.066667
```

1. Select the data within the current index range.

2. Calculate the return of each asset.

3. Calculate the inverse rank of each return.

4. Convert the ranks into weights that sum to 1.

5. Use the [Rep](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.Rep) template to instruct VBT 
to replace it with the index slice of type `slice`, which can be easily applied to any Pandas array.

To automatically select the index range from an array, we can wrap the array with
[Takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Takeable):

```pycon
>>> def inv_rank_optimize_func(price):
...     first_price = price.iloc[0]
...     last_price = price.iloc[-1]
...     ret = (last_price - first_price) / first_price
...     ranks = ret.rank(ascending=False)
...     return ranks / ranks.sum()

>>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
...     symbol_wrapper,
...     inv_rank_optimize_func,
...     vbt.Takeable(data.get("Close")),
...     every="M"
... )
```

!!! tip
    Although this approach introduces a slight overhead, it has a key advantage over the manual
    approach: VBT knows how to select an index range even when the takeable array is a Pandas object
    whose index or frequency differs from that of the optimization. This is possible thanks to VBT's
    robust resampling.

To validate the allocation array, we first need to access the index ranges over which our portfolio
optimization was performed. These are stored under the same attribute as index points:
[PortfolioOptimizer.alloc_records](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.alloc_records):

```pycon
>>> pfo.alloc_records.records_readable
    Range Id  Group               Start Index                 End Index  \
0          0  group 2020-01-01 00:00:00+00:00 2020-02-01 00:00:00+00:00   
1          1  group 2020-02-01 00:00:00+00:00 2020-03-01 00:00:00+00:00   
2          2  group 2020-03-01 00:00:00+00:00 2020-04-01 00:00:00+00:00   
3          3  group 2020-04-01 00:00:00+00:00 2020-05-01 00:00:00+00:00   
4          4  group 2020-05-01 00:00:00+00:00 2020-06-01 00:00:00+00:00   
5          5  group 2020-06-01 00:00:00+00:00 2020-07-01 00:00:00+00:00   
6          6  group 2020-07-01 00:00:00+00:00 2020-08-01 00:00:00+00:00   
7          7  group 2020-08-01 00:00:00+00:00 2020-09-01 00:00:00+00:00   
8          8  group 2020-09-01 00:00:00+00:00 2020-10-01 00:00:00+00:00   
9          9  group 2020-10-01 00:00:00+00:00 2020-11-01 00:00:00+00:00   
10        10  group 2020-11-01 00:00:00+00:00 2020-12-01 00:00:00+00:00   

            Allocation Index  Status  
0  2020-02-01 00:00:00+00:00  Closed  
1  2020-03-01 00:00:00+00:00  Closed  
2  2020-04-01 00:00:00+00:00  Closed  
3  2020-05-01 00:00:00+00:00  Closed  
4  2020-06-01 00:00:00+00:00  Closed  
5  2020-07-01 00:00:00+00:00  Closed  
6  2020-08-01 00:00:00+00:00  Closed  
7  2020-09-01 00:00:00+00:00  Closed  
8  2020-10-01 00:00:00+00:00  Closed  
9  2020-11-01 00:00:00+00:00  Closed  
10 2020-12-01 00:00:00+00:00  Closed  
```

We observe three different types of timestamps: a start (`start_idx`), an end (`end_idx`), and
an allocation timestamp (`alloc_idx`). The start and end timestamps define our index ranges,
while the allocation timestamps indicate when the allocations were actually placed. By default,
VBT places an allocation at the end of each index range. In cases where the end index
exceeds the bounds (remember that it is an excluded index), the status of the range is marked
as "Open"; otherwise, it is "Closed" (meaning we can safely use that allocation). Allocation
arrays and filled allocation arrays include only closed allocations.

!!! tip
    Use the `alloc_wait` argument to control how many ticks after the range the allocation should be
    placed. The default is `1`. Passing `0` will place the allocation at the last tick within the
    index range, which should be used with caution when optimizing based on the close price.

Let's validate the allocation generated from the first month of data:

```pycon
>>> start_idx = pfo.alloc_records.values[0]["start_idx"]
>>> end_idx = pfo.alloc_records.values[0]["end_idx"]
>>> close_period = data.get("Close").iloc[start_idx:end_idx]
>>> close_period.vbt.rebase(1).vbt.plot().show()  # (1)!
```

1. Rescale all close prices to start from 1 using [GenericAccessor.rebase](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rebase)
and plot them using [GenericAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.plot).

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/close_period.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/close_period.dark.svg#only-dark){: .iimg loading=lazy }

We see that `ADAUSDT` produced the highest return and `XRPUSDT` the lowest. This is correctly
reflected by allocating only 6% to ADAUSDT and 33% to XRPUSDT.

Storing index ranges, rather than just index points, in a
[PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
instance also enables new metrics and subplots:

```pycon
>>> pfo.stats()
Start                       2020-01-01 00:00:00+00:00
End                         2020-12-31 23:00:00+00:00
Period                              365 days 06:00:00
Total Records                                      11
Coverage                                     0.915593  << ranges cover 92%
Overlap Coverage                                  0.0  << ranges do not overlap
Mean Allocation: ADAUSDT                     0.181818
Mean Allocation: BNBUSDT                     0.212121
Mean Allocation: BTCUSDT                     0.218182
Mean Allocation: ETHUSDT                     0.163636
Mean Allocation: XRPUSDT                     0.224242
Name: group, dtype: object

>>> pfo.plots().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/plots.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/plots.dark.svg#only-dark){: .iimg loading=lazy }

In the graph above, we see not only when each re-allocation occurs, but also which index range
the re-allocation is based on.

All other features, such as [support for groups](#groups), remain identical to
[PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func).

#### Waiting

By default, when generating weights over a specific time period, the weights will be allocated at the
next available timestamp. This has some implications. For example, when calling
[PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func)
without any arguments, it will optimize over the entire time period but return no allocations,
because there is no next timestamp at which to allocate the generated weights:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
...     symbol_wrapper,
...     inv_rank_optimize_func,
...     vbt.Takeable(data.get("Close"))
... )
>>> pfo.allocations
Empty DataFrame
Columns: [BTCUSDT, ETHUSDT, BNBUSDT, XRPUSDT, ADAUSDT]
Index: []
```

The solution is to set the waiting time to zero:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
...     symbol_wrapper,
...     inv_rank_optimize_func,
...     vbt.Takeable(data.get("Close")),
...     alloc_wait=0  # (1)!
... )
>>> pfo.allocations
symbol                     BTCUSDT   ETHUSDT   BNBUSDT   XRPUSDT   ADAUSDT
Open time                                                                 
2020-12-31 23:00:00+00:00      0.2  0.066667  0.266667  0.333333  0.133333
```

1. Here.

#### Numba

Let's perform both the iteration and optimization entirely using Numba. The only difference,
compared to a Numba-compiled allocation function, is that an optimization function takes two
arguments instead of one: the range start and end index. Under the hood, the iteration and
execution are managed by [optimize_meta_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/nb/#vectorbtpro.portfolio.pfopt.nb.optimize_meta_nb).

```pycon
>>> @njit
... def inv_rank_optimize_func_nb(i, start_idx, end_idx, price):
...     price_period = price[start_idx:end_idx]
...     first_price = price_period[0]
...     last_price = price_period[-1]
...     ret = (last_price - first_price) / first_price
...     ranks = vbt.nb.rank_1d_nb(-ret)  # (1)!
...     return ranks / ranks.sum()

>>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
...     symbol_wrapper,
...     inv_rank_optimize_func_nb,
...     data.get("Close").values,  # (2)!
...     every="M",
...     jitted_loop=True
... )

>>> pfo.allocations
symbol                      ADAUSDT   BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
Open time                                                                  
2020-02-01 00:00:00+00:00  0.066667  0.200000  0.266667  0.133333  0.333333
2020-03-01 00:00:00+00:00  0.333333  0.133333  0.266667  0.066667  0.200000
2020-04-01 00:00:00+00:00  0.266667  0.200000  0.133333  0.333333  0.066667
2020-05-01 00:00:00+00:00  0.066667  0.200000  0.266667  0.133333  0.333333
2020-06-01 00:00:00+00:00  0.066667  0.266667  0.200000  0.133333  0.333333
2020-07-01 00:00:00+00:00  0.066667  0.266667  0.200000  0.133333  0.333333
2020-08-01 00:00:00+00:00  0.066667  0.266667  0.333333  0.133333  0.200000
2020-09-01 00:00:00+00:00  0.333333  0.133333  0.266667  0.066667  0.200000
2020-10-01 00:00:00+00:00  0.266667  0.066667  0.133333  0.333333  0.200000
2020-11-01 00:00:00+00:00  0.333333  0.266667  0.066667  0.133333  0.200000
2020-12-01 00:00:00+00:00  0.133333  0.333333  0.266667  0.200000  0.066667
```

1. Negate the array to calculate the inverse rank.

2. Make sure to convert any Pandas object to a NumPy array.

The adaptation to Numba is quite simple, right? :wink:

The speedup from this compilation is significant, especially when there are many re-allocation steps
and/or parameter combinations. Try it for yourself!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/portfolio-optimization/index.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/PortfolioOptimization.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/integrations/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/
name: Integrations
type: page
icon: material-chart-bar-stacked
---

# Integrations

[PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
integrates smoothly with a variety of third-party libraries.

## PyPortfolioOpt

> [PyPortfolioOpt](https://pyportfolioopt.readthedocs.io/en/latest/) is a library that implements
> portfolio optimization methods, including classical efficient frontier techniques and Black-Litterman
> allocation, as well as more recent developments such as shrinkage and Hierarchical Risk Parity. It
> also includes some experimental features, like exponentially-weighted covariance matrices.

PyPortfolioOpt offers a variety of optimization methods that are very straightforward to use. The
optimization process consists of several distinct steps, though some steps may be skipped depending on
the optimizer:

* Calculate the expected returns (mostly found in `pfopt.expected_returns`).

* Calculate the covariance matrix (mostly found in `pfopt.risk_models`).

* Initialize and configure an optimizer (mainly located in `pypfopt.efficient_frontier`, with the
base class in `pypfopt.base_optimizer`), including objectives, constraints, and the target.

* Run the optimizer to obtain the weights.

* Optionally, convert the weights into a discrete allocation.

For example, here is how to use mean-variance optimization (MVO) to maximize the Sharpe ratio:

```pycon
>>> from pypfopt.expected_returns import mean_historical_return
>>> from pypfopt.risk_models import CovarianceShrinkage
>>> from pypfopt.efficient_frontier import EfficientFrontier

>>> expected_returns = mean_historical_return(data.get("Close"))
>>> cov_matrix = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
>>> optimizer = EfficientFrontier(expected_returns, cov_matrix)
>>> weights = optimizer.max_sharpe()
>>> weights
OrderedDict([('ADAUSDT', 0.1166001117223088),
              ('BNBUSDT', 0.0),
              ('BTCUSDT', 0.0),
              ('ETHUSDT', 0.8833998882776911),
              ('XRPUSDT', 0.0)])
```

### Parsing

Thanks to the excellent work of @robertmartin8, almost the entire PyPortfolioOpt codebase (with a few
exceptions) uses consistent argument and function names. This consistency allows us to create a semantic
graph of functions that serve as inputs to other functions. Why is this important? Because the user only
needs to provide the target function (for example, `EfficientFrontier.max_sharpe`), and we can
automatically determine the entire call stack using just the pricing data! If the user provides any
additional keyword arguments, we can identify which functions in the stack accept those arguments and
pass them automatically.

For the example above, the graph looks like this:

```mermaid
flowchart TD;
    id1["User"]
    id2["mean_historical_return"]
    id3["CovarianceShrinkage.ledoit_wolf"]
    id4["EfficientFrontier"]
    
    id1 -->|"prices"| id2;
    id1 -->|"prices"| id3;
    id2 -->|"expected_returns"| id4;
    id3 -->|"cov_matrix"| id4;
```

(Reload the page if the diagram does not appear.)

This is where VBT comes into play. It implements the function
[resolve_pypfopt_func_kwargs](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.resolve_pypfopt_func_kwargs),
which accepts any PyPortfolioOpt function and resolves its arguments. When a user-provided argument
matches an argument in the function's signature, it marks that argument to be passed to the function.
Let's try this out with expected returns:

```pycon
>>> from vectorbtpro.portfolio.pfopt.base import resolve_pypfopt_func_kwargs

>>> vbt.phelp(mean_historical_return)  # (1)!
mean_historical_return(
    prices,
    returns_data=False,
    compounding=True,
    frequency=252,
    log_returns=False
):
    Calculate annualised mean (daily) historical return from input (daily) asset prices.
    Use ``compounding`` to toggle between the default geometric mean (CAGR) and the
    arithmetic mean.

>>> print(vbt.prettify(resolve_pypfopt_func_kwargs(
...     mean_historical_return, 
...     prices=data.get("Close"),  # (2)!
...     freq="1h",  # (3)!
...     year_freq="365d",
...     other_arg=100  # (4)!
... )))
{
    'prices': <pandas.core.frame.DataFrame object at 0x7f9428052c50 of shape (8767, 5)>,
    'returns_data': False,
    'compounding': True,
    'frequency': 8760.0,
    'log_returns': False
}
```

1. Shows which arguments the function accepts.

2. Present in the signature, so it will be passed.

3. Translates `freq` and `year_freq` into `frequency` (number of time periods in a year), so it will be passed.

4. Not present in the signature, so it will be ignored.

Now, let's try this with `EfficientFrontier`:

```pycon
>>> print(vbt.prettify(resolve_pypfopt_func_kwargs(
...     EfficientFrontier, 
...     prices=data.get("Close")
... )))
{
    'expected_returns': <pandas.core.series.Series object at 0x7f9479927128 of shape (5,)>,
    'cov_matrix': <pandas.core.frame.DataFrame object at 0x7f94280528d0 of shape (5, 5)>,
    'weight_bounds': (
        0,
        1
    ),
    'solver': None,
    'verbose': False,
    'solver_options': None
}
```

Here, VBT automatically resolved the `expected_returns` and `cov_matrix` arguments using
[resolve_pypfopt_expected_returns](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.resolve_pypfopt_expected_returns)
and [resolve_pypfopt_cov_matrix](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.resolve_pypfopt_cov_matrix),
respectively. If you provide these arguments manually, VBT will use them directly. You can also
specify these arguments as strings to select the function used to generate them:

```pycon
>>> print(vbt.prettify(resolve_pypfopt_func_kwargs(
...     EfficientFrontier, 
...     prices=data.get("Close"),
...     expected_returns="ema_historical_return",
...     cov_matrix="sample_cov"
... )))
{
    'expected_returns': <pandas.core.series.Series object at 0x7f9428044cf8 of shape (5,)>,
    'cov_matrix': <pandas.core.frame.DataFrame object at 0x7f942805bf60 of shape (5, 5)>,
    'weight_bounds': (
        0,
        1
    ),
    'solver': None,
    'verbose': False,
    'solver_options': None
}
```

### Auto-optimization

With the ability to parse and resolve function arguments, VBT provides the
[pypfopt_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pypfopt_optimize)
function, which takes user requirements and translates them into function calls. Using this function is
very straightforward!

```pycon
>>> vbt.pypfopt_optimize(prices=data.get("Close"))
{'ADAUSDT': 0.1166,
  'BNBUSDT': 0.0,
  'BTCUSDT': 0.0,
  'ETHUSDT': 0.8834,
  'XRPUSDT': 0.0}
```

In summary, [pypfopt_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pypfopt_optimize)
first resolves the optimizer by calling [resolve_pypfopt_optimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.resolve_pypfopt_optimizer).
This triggers a cascade of argument resolutions by [parsing arguments](#parsing), including computing
expected returns and the risk model for asset risk. It then adds objectives and constraints to the
optimizer instance. Finally, it calls the target metric method (such as `max_sharpe`) or a custom
convex or non-convex objective using the same parsing procedure as shown above. If desired,
it can also convert continuous weights into discrete ones using `DiscreteAllocation`.

Since multiple PyPortfolioOpt functions may require the same argument that needs to be pre-computed,
[pypfopt_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pypfopt_optimize) uses a
built-in caching mechanism. If any provided arguments are not used, it issues a warning (which you can
hide by setting `silence_warnings` to True) stating that the argument was not required by any function in
the call stack.

Below, we will demonstrate different optimizations using both PyPortfolioOpt and VBT. For example,
optimizing a long/short portfolio to minimize total variance:

=== "PyPortfolioOpt"

    ```pycon
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(None, S, weight_bounds=(-1, 1))
    >>> ef.min_volatility()
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', -0.01118),
                  ('BNBUSDT', 0.09695),
                  ('BTCUSDT', 0.9624),
                  ('ETHUSDT', -0.10516),
                  ('XRPUSDT', 0.05699)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     expected_returns=None,
    ...     weight_bounds=(-1, 1),
    ...     target="min_volatility"
    ... )
    {'ADAUSDT': -0.01118,
      'BNBUSDT': 0.09695,
      'BTCUSDT': 0.9624,
      'ETHUSDT': -0.10516,
      'XRPUSDT': 0.05699}
    ```

    1. `CovarianceShrinkage.ledoit_wolf` and `EfficientFrontier` are used by default.

Optimizing a portfolio to maximize the Sharpe ratio, with direction constraints:

```pycon
directions = ["long", "long", "long", "short", "short"]
direction_mapper = dict(zip(data.symbols, directions))
```

=== "PyPortfolioOpt"

    ```pycon
    >>> from pypfopt.expected_returns import capm_return
    
    >>> mu = capm_return(data.get("Close"))
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(mu, S, weight_bounds=(-1, 1))
    >>> for symbol, direction in direction_mapper.items():
    ...     idx = data.symbols.index(symbol)
    ...     if direction == "long":
    ...         ef.add_constraint(lambda w, _idx=idx: w[_idx] >= 0)
    ...     if direction == "short":
    ...         ef.add_constraint(lambda w, _idx=idx: w[_idx] <= 0)
    >>> ef.max_sharpe()
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('BTCUSDT', 0.26614),
                  ('ETHUSDT', 0.433),
                  ('BNBUSDT', 0.30086),
                  ('XRPUSDT', 0.0),
                  ('ADAUSDT', 0.0)])
    ```

=== "VBT"

    ```pycon
    >>> constraints = []
    >>> for symbol, direction in direction_mapper.items():
    ...     idx = data.symbols.index(symbol)
    ...     if direction == "long":
    ...         constraints.append(lambda w, _idx=idx: w[_idx] >= 0)
    ...     if direction == "short":
    ...         constraints.append(lambda w, _idx=idx: w[_idx] <= 0)
    >>> vbt.pypfopt_optimize(  
    ...     prices=data.get("Close"),
    ...     expected_returns="capm_return",
    ...     cov_matrix="ledoit_wolf",
    ...     target="max_sharpe",
    ...     weight_bounds=(-1, 1),
    ...     constraints=constraints,
    ... )
    {'BTCUSDT': 0.26614,
      'ETHUSDT': 0.433,
      'BNBUSDT': 0.30086,
      'XRPUSDT': 0.0,
      'ADAUSDT': 0.0}
    ```

Optimizing a portfolio to maximize the Sharpe ratio with sector constraints:

```pycon
>>> sector_mapper = {
...     "ADAUSDT": "DeFi",
...     "BNBUSDT": "DeFi",
...     "BTCUSDT": "Payment",
...     "ETHUSDT": "DeFi",
...     "XRPUSDT": "Payment"
... }
>>> sector_lower = {
...     "DeFi": 0.75
... }
>>> sector_upper = {}
```

=== "PyPortfolioOpt"

    ```pycon
    >>> mu = capm_return(data.get("Close"))
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(mu, S)
    >>> ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)
    >>> adausdt_index = ef.tickers.index("ADAUSDT")
    >>> ef.add_constraint(lambda w: w[adausdt_index] == 0.10)
    >>> ef.max_sharpe()
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.1),
                  ('BNBUSDT', 0.2772),
                  ('BTCUSDT', 0.0524),
                  ('ETHUSDT', 0.3728),
                  ('XRPUSDT', 0.1976)])
    ```

=== "VBT"

    ```pycon
    >>> adausdt_index = list(sector_mapper.keys()).index("ADAUSDT")
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     expected_returns="capm_return",
    ...     sector_mapper=sector_mapper,
    ...     sector_lower=sector_lower,
    ...     sector_upper=sector_upper,
    ...     constraints=[lambda w: w[adausdt_index] == 0.10]
    ... )
    {'ADAUSDT': 0.1,
      'BNBUSDT': 0.2772,
      'BTCUSDT': 0.0524,
      'ETHUSDT': 0.3728,
      'XRPUSDT': 0.1976}
    ```

    1. `CovarianceShrinkage.ledoit_wolf`, `EfficientFrontier`, and `EfficientFrontier.max_sharpe` 
    are used by default.

Optimizing a portfolio to maximize return for a given risk, with sector constraints and an L2
regularization objective:

=== "PyPortfolioOpt"

    ```pycon
    >>> from pypfopt.objective_functions import L2_reg
    
    >>> mu = capm_return(data.get("Close"))
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(mu, S)
    >>> ef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)
    >>> ef.add_objective(L2_reg, gamma=0.1)
    >>> ef.efficient_risk(0.15)
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.26004),
                  ('BNBUSDT', 0.24466),
                  ('BTCUSDT', 0.10778),
                  ('ETHUSDT', 0.2453),
                  ('XRPUSDT', 0.14222)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     expected_returns="capm_return",
    ...     sector_mapper=sector_mapper,
    ...     sector_lower=sector_lower,
    ...     sector_upper=sector_upper,
    ...     objectives=["L2_reg"],  # (2)!
    ...     gamma=0.1,  # (3)!
    ...     target="efficient_risk",
    ...     target_volatility=0.15  # (4)!
    ... )
    {'ADAUSDT': 0.26004,
      'BNBUSDT': 0.24466,
      'BTCUSDT': 0.10778,
      'ETHUSDT': 0.2453,
      'XRPUSDT': 0.14222}
    ```

    1. `CovarianceShrinkage.ledoit_wolf` and `EfficientFrontier` are used by default.

    2. Objectives can be provided as attributes of the `pypfopt.objective_functions` module.

    3. Recognized as an argument of the objective function.

    4. Recognized as an argument of the target metric method.

Optimizing along the mean-semivariance frontier:

=== "PyPortfolioOpt"

    ```pycon
    >>> from pypfopt import EfficientSemivariance
    >>> from pypfopt.expected_returns import returns_from_prices
    
    >>> mu = capm_return(data.get("Close"))
    >>> returns = returns_from_prices(data.get("Close"))
    >>> returns = returns.dropna()
    >>> es = EfficientSemivariance(mu, returns)
    >>> es.efficient_return(0.01)
    >>> weights = es.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.0),
                  ('BNBUSDT', 0.0),
                  ('BTCUSDT', 1.0),
                  ('ETHUSDT', 0.0),
                  ('XRPUSDT', 0.0)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(
    ...     prices=data.get("Close"),
    ...     expected_returns="capm_return",
    ...     optimizer="efficient_semivariance",  # (1)!
    ...     target="efficient_return",
    ...     target_return=0.01
    ... )
    {'ADAUSDT': 0.0, 'BNBUSDT': 0.0, 'BTCUSDT': 1, 'ETHUSDT': 0.0, 'XRPUSDT': 0.0}
    ```

    1. The second argument of `EfficientSemivariance` is `returns`, which VBT recognizes and
    automatically converts from prices.

Minimizing transaction costs:

```pycon
>>> initial_weights = np.array([1 / len(data.symbols)] * len(data.symbols))
```

=== "PyPortfolioOpt"

    ```pycon
    >>> from pypfopt.objective_functions import transaction_cost
    
    >>> mu = mean_historical_return(data.get("Close"))
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(mu, S)
    >>> ef.add_objective(transaction_cost, w_prev=initial_weights, k=0.001)
    >>> ef.add_objective(L2_reg, gamma=0.05)
    >>> ef.min_volatility()
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.16025),
                  ('BNBUSDT', 0.2),
                  ('BTCUSDT', 0.27241),
                  ('ETHUSDT', 0.2),
                  ('XRPUSDT', 0.16734)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     objectives=["transaction_cost", "L2_reg"],
    ...     w_prev=initial_weights, 
    ...     k=0.001,
    ...     gamma=0.05,
    ...     target="min_volatility"
    ... )
    {'ADAUSDT': 0.16025,
      'BNBUSDT': 0.2,
      'BTCUSDT': 0.27241,
      'ETHUSDT': 0.2,
      'XRPUSDT': 0.16734}
    ```

    1. `mean_historical_return`, `CovarianceShrinkage.ledoit_wolf`, and `EfficientFrontier` are 
    used by default.

Custom convex objective:

```pycon
>>> import cvxpy as cp

>>> def logarithmic_barrier_objective(w, cov_matrix, k=0.1):
...     log_sum = cp.sum(cp.log(w))
...     var = cp.quad_form(w, cov_matrix)
...     return var - k * log_sum
```

=== "PyPortfolioOpt"

    ```pycon
    >>> mu = mean_historical_return(data.get("Close"))
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(mu, S, weight_bounds=(0.01, 0.3))
    >>> ef.convex_objective(logarithmic_barrier_objective, cov_matrix=S, k=0.001)
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.12214),
                  ('BNBUSDT', 0.22175),
                  ('BTCUSDT', 0.3),
                  ('ETHUSDT', 0.21855),
                  ('XRPUSDT', 0.13756)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     weight_bounds=(0.01, 0.3),
    ...     k=0.001,
    ...     target=logarithmic_barrier_objective  # (2)!
    ... )
    {'ADAUSDT': 0.12214,
      'BNBUSDT': 0.22175,
      'BTCUSDT': 0.3,
      'ETHUSDT': 0.21855,
      'XRPUSDT': 0.13756}
    ```

    1. `mean_historical_return`, `CovarianceShrinkage.ledoit_wolf`, and `EfficientFrontier` are 
    used by default.

    2. The second argument of `logarithmic_barrier_objective` is `cov_matrix`, which VBT
    recognizes and calculates automatically (or retrieves from cache).

Custom non-convex objective:

```pycon
>>> def deviation_risk_parity(w, cov_matrix):
...     cov_matrix = np.asarray(cov_matrix)
...     n = cov_matrix.shape[0]
...     rp = (w * (cov_matrix @ w)) / cp.quad_form(w, cov_matrix)
...     return cp.sum_squares(rp - 1 / n).value
```

=== "PyPortfolioOpt"

    ```pycon
    >>> mu = mean_historical_return(data.get("Close"))
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> ef = EfficientFrontier(mu, S)
    >>> ef.nonconvex_objective(deviation_risk_parity, ef.cov_matrix)
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.17421),
                  ('BNBUSDT', 0.19933),
                  ('BTCUSDT', 0.2515),
                  ('ETHUSDT', 0.1981),
                  ('XRPUSDT', 0.17686)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     target=deviation_risk_parity,  # (2)!
    ...     target_is_convex=False
    ... )
    {'ADAUSDT': 0.17421,
      'BNBUSDT': 0.19933,
      'BTCUSDT': 0.2515,
      'ETHUSDT': 0.1981,
      'XRPUSDT': 0.17686}
    ```

    1. `mean_historical_return`, `CovarianceShrinkage.ledoit_wolf`, and `EfficientFrontier` are 
    used by default.

    2. The second argument of `deviation_risk_parity` is `cov_matrix`, which VBT recognizes
    and calculates automatically (or retrieves from cache).

Black-Litterman Allocation ([read more](https://pyportfolioopt.readthedocs.io/en/latest/BlackLitterman.html)):

```pycon
>>> sp500_data = vbt.YFData.pull(
...     "^GSPC", 
...     start=data.wrapper.index[0], 
...     end=data.wrapper.index[-1]
... )
>>> market_caps = data.get("Close") * data.get("Volume")
>>> viewdict = {
...     "ADAUSDT": 0.20, 
...     "BNBUSDT": -0.30, 
...     "BTCUSDT": 0, 
...     "ETHUSDT": -0.2, 
...     "XRPUSDT": 0.15
... }
```

=== "PyPortfolioOpt"

    ```pycon
    >>> from pypfopt.black_litterman import (
    ...     market_implied_risk_aversion,
    ...     market_implied_prior_returns,
    ...     BlackLittermanModel
    ... )
    
    >>> S = CovarianceShrinkage(data.get("Close")).ledoit_wolf()
    >>> delta = market_implied_risk_aversion(sp500_data.get("Close"))
    >>> prior = market_implied_prior_returns(market_caps.iloc[-1], delta, S)
    >>> bl = BlackLittermanModel(S, pi=prior, absolute_views=viewdict)
    >>> rets = bl.bl_returns()
    >>> ef = EfficientFrontier(rets, S)
    >>> ef.min_volatility()
    >>> weights = ef.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.0),
                  ('BNBUSDT', 0.06743),
                  ('BTCUSDT', 0.89462),
                  ('ETHUSDT', 0.0),
                  ('XRPUSDT', 0.03795)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(  # (1)!
    ...     prices=data.get("Close"),
    ...     expected_returns="bl_returns",  # (2)!
    ...     market_prices=sp500_data.get("Close"),
    ...     market_caps=market_caps.iloc[-1],
    ...     absolute_views=viewdict,
    ...     target="min_volatility"
    ... )
    {'ADAUSDT': 0.0,
      'BNBUSDT': 0.06743,
      'BTCUSDT': 0.89462,
      'ETHUSDT': 0.0,
      'XRPUSDT': 0.03795}
    ```

    1. `mean_historical_return` and `EfficientFrontier` are used by default.

    2. `BlackLittermanModel`, `delta`, `prior`, and `bl_returns` are all resolved automatically.

Hierarchical Risk Parity ([read more](https://pyportfolioopt.readthedocs.io/en/latest/OtherOptimizers.html#hierarchical-risk-parity-hrp)):

=== "PyPortfolioOpt"

    ```pycon
    >>> from pypfopt import HRPOpt
    
    >>> rets = returns_from_prices(data.get("Close"))
    >>> hrp = HRPOpt(rets)
    >>> hrp.optimize()
    >>> weights = hrp.clean_weights()
    >>> weights
    OrderedDict([('ADAUSDT', 0.19415),
                  ('BNBUSDT', 0.24834),
                  ('BTCUSDT', 0.22827),
                  ('ETHUSDT', 0.15217),
                  ('XRPUSDT', 0.17707)])
    ```

=== "VBT"

    ```pycon
    >>> vbt.pypfopt_optimize(
    ...     prices=data.get("Close"),  # (1)!
    ...     optimizer="hrp",
    ...     target="optimize"
    ... )
    {'ADAUSDT': 0.19415,
      'BNBUSDT': 0.24834,
      'BTCUSDT': 0.22827,
      'ETHUSDT': 0.15217,
      'XRPUSDT': 0.17707}
    ```

    1. Returns are automatically recognized by VBT and converted from prices.

### Argument groups

If you need to provide two functions with an argument that has the same name but requires different
values, pass the argument as an instance of [pfopt_func_dict](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pfopt_func_dict).
The keys should be the functions or their names, and the values should be the corresponding argument
values:

```pycon
>>> vbt.pypfopt_optimize(  
...     prices=data.get("Close"),
...     expected_returns="bl_returns",  
...     market_prices=sp500_data.get("Close"),
...     market_caps=market_caps.iloc[-1],
...     absolute_views=viewdict,
...     target="min_volatility",
...     cov_matrix=vbt.pfopt_func_dict({
...         "EfficientFrontier": "sample_cov",  # (1)!
...         "_def": "ledoit_wolf"  # (2)!
...     })
... )
{'ADAUSDT': 0.0,
  'BNBUSDT': 0.05013,
  'BTCUSDT': 0.91912,
  'ETHUSDT': 0.0,
  'XRPUSDT': 0.03075}
```

1. Use the annualized sample covariance matrix for the `EfficientFrontier` class.

2. Use the Ledoit-Wolf shrinkage estimate for all other functions.

### Periodically

So, why does VBT implement a special parser for PyPortfolioOpt instead of relying on the original,
modular API of the library?

Using a single function that addresses all requirements makes it much simpler to use as an optimization
function. For example, VBT applies sensible defaults for expected returns and other variables,
and understands where each variable should be utilized. Also, passing arbitrary keyword arguments and
having VBT distribute them across the relevant functions makes it easy to test multiple argument
combinations using groups.

Let's see this in action with [PortfolioOptimizer.from_pypfopt](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_pypfopt),
which uses [pypfopt_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pypfopt_optimize)
as its `optimize_func`. Optimize for maximum Sharpe in each week:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_pypfopt(
...     prices=data.get("Close"),
...     every="W"
... )

>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/pypfopt.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/pypfopt.dark.svg#only-dark){: .iimg loading=lazy }

Now, see how easy it is to test several values for the `target` argument:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_pypfopt(
...     prices=data.get("Close"),
...     every="W",
...     target=vbt.Param([
...         "max_sharpe", 
...         "min_volatility", 
...         "max_quadratic_utility"
...     ])
... )

>>> pfo.plot(column="min_volatility").show()  # (1)!
```

1. Only one group can be plotted. Note that `column` here refers to a group, since each
[PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
instance is always grouped.

[=100% "Group 3/3"]{: .candystripe .candystripe-animate }

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/pypfopt_groups.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/pypfopt_groups.dark.svg#only-dark){: .iimg loading=lazy }

```pycon
>>> pf = pfo.simulate(data, freq="1h")

>>> pf.sharpe_ratio
target
max_sharpe               2.779042
min_volatility           1.862926
max_quadratic_utility    2.352667
Name: sharpe_ratio, dtype: float64
```

You can see that optimizing for maximum Sharpe provides the highest out-of-sample Sharpe. Great!

#### Manually

You can also manually wrap the [pypfopt_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pypfopt_optimize)
function. This is useful, for example, when you want to preprocess the data or postprocess the weights:

```pycon
>>> def optimize_func(prices, index_slice, **kwargs):
...     period_prices = prices.iloc[index_slice]
...     return vbt.pypfopt_optimize(prices=period_prices, **kwargs)

>>> pfo = vbt.PortfolioOptimizer.from_optimize_func(
...     data.symbol_wrapper,
...     optimize_func,
...     prices=data.get("Close"),
...     index_slice=vbt.Rep("index_slice"),
...     every="W"
... )
```

## Riskfolio-Lib

> [Riskfolio-Lib](https://github.com/dcajasn/Riskfolio-Lib) is a library for quantitative strategic
> asset allocation and portfolio optimization in Python, developed in Peru 🇵🇪. Its goal is to help
> students, academics, and practitioners build investment portfolios based on mathematically complex
> models with minimal effort. It is built on top of [cvxpy](https://www.cvxpy.org/) and closely
> integrates with [pandas](https://pandas.pydata.org/) data structures.

Like PyPortfolioOpt, Riskfolio-Lib also offers a variety of portfolio optimization tools. A typical
optimization workflow follows these steps:

* Load price data and convert it into returns.

* Create and configure a new portfolio object (usually with the suffix `Portfolio`).

* Depending on the chosen model, run one or more methods (usually with the suffix `stats`)
that pre-calculate various statistics required for the optimization.

* Run the optimization method (usually with the suffix `optimization`) to obtain the weights.

For example, here is how to perform mean-variance optimization (MVO) to maximize the Sharpe ratio:

```pycon
>>> import riskfolio as rp

>>> returns = data.get("Close").vbt.to_returns()

>>> port = rp.Portfolio(returns=returns)
>>> port.assets_stats(
...     method_mu="hist",
...     method_cov="hist",
...     d=0.94
... )
>>> w = port.optimization(
...     model="Classic",
...     rm="MV",
...     obj="Sharpe",
...     rf=0,
...     l=0,
...     hist=True
... )
>>> w.T
          ADAUSDT       BNBUSDT   BTCUSDT   ETHUSDT       XRPUSDT
weights  0.207779  1.043621e-08  0.336897  0.455323  3.650466e-09
```

!!! tip
    Why does `assets_stats` not return anything? It is because it calculates `mu` and `cov` and
    updates the portfolio attributes `port.mu` and `port.cov` in place.

### Parsing

The method described above for generating a vector of weights from vectors of returns works well, but
splitting the optimization process across several function calls can make parameterization more
challenging. Ideally, we want a single function that can express any Riskfolio-Lib setup, preferably
using only keyword arguments. To create such a function that covers many cases, we need to identify the
inputs and outputs of each function and understand how these functions connect. Thanks to the consistent
naming of arguments and functions in Riskfolio-Lib (kudos to @dcajasn!), along with the
[comprehensive tutorials](https://riskfolio-lib.readthedocs.io/en/latest/examples.html), we can
determine the required order of function calls for each optimization task.

For example, the optimization method [Portfolio.optimization](https://riskfolio-lib.readthedocs.io/en/latest/portfolio.html#Portfolio.Portfolio.optimization)
with the "Classic" model requires the statistics method [Portfolio.assets_stats](https://riskfolio-lib.readthedocs.io/en/latest/portfolio.html#Portfolio.Portfolio.assets_stats)
to be called first. The "FM" model, on the other hand, requires both [Portfolio.assets_stats](https://riskfolio-lib.readthedocs.io/en/latest/portfolio.html#Portfolio.Portfolio.assets_stats)
and [Portfolio.factors_stats](https://riskfolio-lib.readthedocs.io/en/latest/portfolio.html#Portfolio.Portfolio.factors_stats).
If the user provides constraints, we need to pre-process them using the corresponding
[constraints function](https://riskfolio-lib.readthedocs.io/en/latest/constraints.html).

Once we have established the call stack, how do we assign arguments to each function? We can check
the signature of each function:

```pycon
>>> from vectorbtpro.utils.parsing import get_func_arg_names

>>> get_func_arg_names(port.assets_stats)
['method_mu', 'method_cov', 'd']
```

If the user provides an argument called `method_mu`, it should be passed to this function and any other
function that lists this argument, since it likely serves the same purpose. To determine which arguments
need to be given to a particular Riskfolio-Lib function, you can use the handy
[resolve_riskfolio_func_kwargs](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.resolve_riskfolio_func_kwargs)
function:

```pycon
>>> from vectorbtpro.portfolio.pfopt.base import resolve_riskfolio_func_kwargs

>>> resolve_riskfolio_func_kwargs(
...     port.assets_stats,
...     method_mu="hist",
...     method_cov="hist",
...     model="Classic"
... )
{'method_mu': 'hist', 'method_cov': 'hist'}
```

If you need to override any argument for a specific function only, you can provide a `func_kwargs`
dictionary with functions as keys and keyword arguments as values:

```pycon
>>> resolve_riskfolio_func_kwargs(
...     port.assets_stats,
...     method_mu="hist",
...     method_cov="hist",
...     model="Classic",
...     func_kwargs=dict(
...         assets_stats=dict(method_mu="ewma1"),
...         optimization=dict(model="BL")
...     )
... )
{'method_mu': 'ewma1', 'method_cov': 'hist'}
```

In this way, you can let VBT distribute the arguments automatically, but still have the option to
manage it manually using `func_kwargs`.

### Auto-optimization

Now that we know how to parse and resolve function arguments, VBT provides a function that can take
a single set of keyword arguments and translate them into an optimization procedure:
[riskfolio_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.riskfolio_optimize).
This function is just as easy to use as the one for PyPortfolioOpt!

```pycon
>>> vbt.riskfolio_optimize(returns)
{'ADAUSDT': 0.20777948652846492,
  'BNBUSDT': 1.0435918170753283e-08,
  'BTCUSDT': 0.33689720861500716,
  'ETHUSDT': 0.45532329077024425,
  'XRPUSDT': 3.6503655112892327e-09}
```

Under the hood, the function first determines the portfolio class using the `port_cls` argument. It
then creates a new portfolio instance, passing any keyword arguments that match the constructor method
`__init__`. Next, it identifies the optimization method from the `opt_method` argument, which is set to
`"optimization"` by default. Given the optimization method and the model (provided through the `model`
argument), it can determine which statistics methods to run beforehand and in what order. The names of
these statistics methods are stored in `stats_methods`, unless the user has already specified them. The
next step is to resolve any asset classes, constraints, and views, translating them into keyword
arguments suitable for the following functions in the call stack. For example, asset classes are
pre-processed using [resolve_asset_classes](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.resolve_asset_classes),
which allows users to pass `asset_classes` using various formats that Riskfolio-Lib does not otherwise
support. Once all keyword arguments are prepared, the function runs the statistics methods (if any),
followed by the optimization method. It then returns the weights as a dictionary, using the column names
(i.e., asset names) from the returns array as keys.

Below, we demonstrate several optimizations using both Riskfolio-Lib and VBT.
Ulcer Index Portfolio Optimization for Mean Risk
([notebook](https://nbviewer.org/github/dcajasn/Riskfolio-Lib/tree/master/examples/)):

=== "Riskfolio-Lib"

    ```pycon
    >>> port = rp.Portfolio(returns=returns)
    >>> port.assets_stats(
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     d=0.94
    ... )
    >>> w = port.optimization(
    ...     model="Classic", 
    ...     rm="UCI", 
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     hist=True
    ... )
    >>> w.T
                  ADAUSDT       BNBUSDT  BTCUSDT  ETHUSDT       XRPUSDT
    weights  4.421983e-11  1.922731e-11   0.8343   0.1657  9.143250e-11
    ```

=== "VBT"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     d=0.94,
    ...     model="Classic", 
    ...     rm="UCI", 
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     hist=True
    ... )
    {'ADAUSDT': 4.4219828299615346e-11,
      'BNBUSDT': 1.9227309961407513e-11,
      'BTCUSDT': 0.8342998038068898,
      'ETHUSDT': 0.16570019603823058,
      'XRPUSDT': 9.143250192538338e-11}
    ```

Worst Case Mean Variance Portfolio Optimization using box and elliptical uncertainty sets
([notebook](https://nbviewer.org/github/dcajasn/Riskfolio-Lib/tree/master/examples/)):

=== "Riskfolio-Lib"

    ```pycon
    >>> port = rp.Portfolio(returns=returns)
    >>> port.assets_stats(
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     d=0.94
    ... )
    >>> port.wc_stats(
    ...     box="s", 
    ...     ellip="s", 
    ...     q=0.05, 
    ...     n_sim=3000, 
    ...     window=3, 
    ...     dmu=0.1, 
    ...     dcov=0.1, 
    ...     seed=0
    ... )
    >>> w = port.wc_optimization(
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     Umu="box", 
    ...     Ucov="box"
    ... )
    >>> w.T
                  ADAUSDT       BNBUSDT   BTCUSDT   ETHUSDT       XRPUSDT
    weights  8.434620e-11  4.298850e-11  0.385894  0.614106  4.185089e-11
    ```

=== "VBT"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     opt_method="wc",  # (1)!
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     box="s", 
    ...     ellip="s", 
    ...     q=0.05, 
    ...     n_sim=3000, 
    ...     window=3, 
    ...     dmu=0.1, 
    ...     dcov=0.1, 
    ...     seed=0,
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     Umu="box", 
    ...     Ucov="box"
    ... )
    {'ADAUSDT': 8.434620227152581e-11,
      'BNBUSDT': 4.2988498616065945e-11,
      'BTCUSDT': 0.38589404919778153,
      'ETHUSDT': 0.6141059506330329,
      'XRPUSDT': 4.185089189555317e-11}
    ```

    1. Use the worst-case optimization method.

=== "VBT (using func_kwargs)"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     func_kwargs=dict(  # (1)!
    ...         assets_stats=dict(
    ...             opt_method="wc",
    ...             method_mu="hist", 
    ...             method_cov="hist"
    ...         ),
    ...         wc_stats=dict(
    ...             box="s", 
    ...             ellip="s", 
    ...             q=0.05, 
    ...             n_sim=3000, 
    ...             window=3, 
    ...             dmu=0.1, 
    ...             dcov=0.1, 
    ...             seed=0
    ...         ),
    ...         wc_optimization=dict(
    ...             obj="Sharpe", 
    ...             rf=0, 
    ...             l=0, 
    ...             Umu="box", 
    ...             Ucov="box"
    ...         )
    ...     )
    ... )
    {'ADAUSDT': 8.434620227152581e-11,
      'BNBUSDT': 4.2988498616065945e-11,
      'BTCUSDT': 0.38589404919778153,
      'ETHUSDT': 0.6141059506330329,
      'XRPUSDT': 4.185089189555317e-11}
    ```

    1. Functions listed as keys must be in the order they should be called.
    Any key ending with `stats` will be interpreted as a statistic method (one or more).
    Any key ending with `optimization` will be interpreted as an optimization function (only one).

Mean Variance Portfolio with Short Weights
([notebook](https://nbviewer.org/github/dcajasn/Riskfolio-Lib/tree/master/examples/)):

=== "Riskfolio-Lib"

    ```pycon
    >>> port = rp.Portfolio(returns=returns)
    >>> port.sht = True
    >>> port.uppersht = 0.3
    >>> port.upperlng = 1.3
    >>> port.budget = 1.0
    >>> port.assets_stats(
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     d=0.94
    ... )
    >>> w = port.optimization(
    ...     model="Classic", 
    ...     rm="MV", 
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     hist=True
    ... )
    >>> w.T
              ADAUSDT       BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
    weights  0.295482 -2.109934e-07  0.456143  0.548375 -0.299999
    ```

=== "VBT"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     sht=True,
    ...     uppersht=0.3,
    ...     upperlng=1.3,
    ...     budget=1.0,
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     d=0.94,
    ...     rm="MV", 
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     hist=True
    ... )
    {'ADAUSDT': 0.2954820993653493,
      'BNBUSDT': -2.1099344275128538e-07,
      'BTCUSDT': 0.45614303697962627,
      'ETHUSDT': 0.5483745379125106,
      'XRPUSDT': -0.2999994632634474}
    ```

Constraints on Assets ([notebook](https://nbviewer.org/github/dcajasn/Riskfolio-Lib/tree/master/examples/)):

=== "Riskfolio-Lib"

    ```pycon
    >>> port = rp.Portfolio(returns=returns)
    >>> port.assets_stats(
    ...     method_mu="hist", 
    ...     method_cov="hist",
    ...     d=0.94
    ... )
    >>> asset_classes = {"Assets": returns.columns.tolist()}
    >>> asset_classes = pd.DataFrame(asset_classes)
    >>> constraints = {  # (1)!
    ...     "Disabled": [False, False],
    ...     "Type": ["All Assets", "Assets"],
    ...     "Set": ["", ""],
    ...     "Position": ["", "BTCUSDT"],
    ...     "Sign": [">=", "<="],
    ...     'Weight': [0.1, 0.15],
    ...     "Type Relative": ["", ""],
    ...     "Relative Set": ["", ""],
    ...     "Relative": ["", ""],
    ...     "Factor": ["", ""],
    ... }
    >>> constraints = pd.DataFrame(constraints)
    >>> A, B = rp.assets_constraints(constraints, asset_classes)
    >>> port.ainequality = A
    >>> port.binequality = B
    >>> w = port.optimization(
    ...     model="Classic",
    ...     rm="MV",
    ...     obj="Sharpe",
    ...     rf=0,
    ...     l=0,
    ...     hist=True
    ... )
    >>> w.T
              ADAUSDT  BNBUSDT  BTCUSDT   ETHUSDT  XRPUSDT
    weights  0.181443      0.1     0.15  0.468557      0.1
    ```

    1. Each asset must have a weight of at least 10%. `BTCUSDT` must have a weight of at most 15%.

=== "VBT"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     constraints=[{  # (1)!
    ...         "Type": "All Assets",
    ...         "Sign": ">=",
    ...         "Weight": 0.1
    ...     }, {
    ...         "Type": "Assets",
    ...         "Position": "BTCUSDT",
    ...         "Sign": "<=",
    ...         "Weight": 0.15
    ...     }],
    ...     d=0.94,
    ...     rm="MV", 
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     hist=True
    ... )
    {'ADAUSDT': 0.181442978888792,
      'BNBUSDT': 0.10000000609450148,
      'BTCUSDT': 0.1499998352568763,
      'ETHUSDT': 0.4685571774444982,
      'XRPUSDT': 0.10000000231533206}
    ```

    1. There is no need to build a complete DataFrame; VBT will handle this for you.
    Also, there is no need to provide `asset_classes`; they are inferred from the columns.

Constraints on Asset Classes ([notebook](https://nbviewer.org/github/dcajasn/Riskfolio-Lib/tree/master/examples/)):

```pycon
>>> tags = [
...     "Smart contracts",
...     "Smart contracts",
...     "Payments",
...     "Smart contracts",
...     "Payments"
... ]
```

=== "Riskfolio-Lib"

    ```pycon
    >>> port = rp.Portfolio(returns=returns)
    >>> port.assets_stats(
    ...     method_mu="hist", 
    ...     method_cov="hist",
    ...     d=0.94
    ... )
    >>> asset_classes = {
    ...     "Assets": returns.columns.tolist(),
    ...     "Tags": tags
    ... }
    >>> asset_classes = pd.DataFrame(asset_classes)
    >>> constraints = {  # (1)!
    ...     "Disabled": [False],
    ...     "Type": ["Classes"],
    ...     "Set": ["Tags"],
    ...     "Position": ["Smart contracts"],
    ...     "Sign": [">="],
    ...     'Weight': [0.8],
    ...     "Type Relative": [""],
    ...     "Relative Set": [""],
    ...     "Relative": [""],
    ...     "Factor": [""],
    ... }
    >>> constraints = pd.DataFrame(constraints)
    >>> A, B = rp.assets_constraints(constraints, asset_classes)
    >>> port.ainequality = A
    >>> port.binequality = B
    >>> w = port.optimization(
    ...     model="Classic",
    ...     rm="MV",
    ...     obj="Sharpe",
    ...     rf=0,
    ...     l=0,
    ...     hist=True
    ... )
    >>> w.T
              ADAUSDT       BNBUSDT  BTCUSDT   ETHUSDT       XRPUSDT
    weights  0.227839  5.856725e-10      0.2  0.572161  1.852774e-10
    ```

    1. The sum of all weights in the "Smart contracts" class from the "Tags" set must be at least 80%.

=== "VBT"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     method_mu="hist", 
    ...     method_cov="hist", 
    ...     asset_classes={"Tags": tags},
    ...     constraints=[{
    ...         "Type": "Classes",
    ...         "Set": "Tags",
    ...         "Position": "Smart contracts",
    ...         "Sign": ">=",
    ...         "Weight": 0.8
    ...     }],
    ...     d=0.94,
    ...     rm="MV", 
    ...     obj="Sharpe", 
    ...     rf=0, 
    ...     l=0, 
    ...     hist=True
    ... )
    {'ADAUSDT': 0.22783907021563807,
      'BNBUSDT': 5.856745345006487e-10,
      'BTCUSDT': 0.19999999471503008,
      'ETHUSDT': 0.5721609342983793,
      'XRPUSDT': 1.852779892852209e-10}
    ```

Nested Clustered Optimization (NCO)
([notebook](https://nbviewer.org/github/dcajasn/Riskfolio-Lib/tree/master/examples/)):

=== "Riskfolio-Lib"

    ```pycon
    >>> port = rp.HCPortfolio(returns=returns)
    >>> w = port.optimization(
    ...     model="NCO",
    ...     codependence="pearson",
    ...     covariance="hist",
    ...     obj="MinRisk",
    ...     rm="MV",
    ...     rf=0,
    ...     l=2,
    ...     linkage="ward",
    ...     max_k=10,
    ...     leaf_order=True
    ... )
    >>> w.T
                  ADAUSDT  BNBUSDT   BTCUSDT       ETHUSDT   XRPUSDT
    weights  6.402583e-09  0.05898  0.911545  3.331509e-09  0.029475
    ```

=== "VBT"

    ```pycon
    >>> vbt.riskfolio_optimize(
    ...     returns,
    ...     port_cls="HCPortfolio",  # (1)!
    ...     model="NCO",
    ...     codependence="pearson",
    ...     covariance="hist",
    ...     obj="MinRisk",
    ...     rm="MV",
    ...     rf=0,
    ...     l=2,
    ...     linkage="ward",
    ...     max_k=10,
    ...     leaf_order=True
    ... )
    {'ADAUSDT': 6.402581338827853e-09,
      'BNBUSDT': 0.05897978986842499,
      'BTCUSDT': 0.9115447868616637,
      'ETHUSDT': 3.3315084935084894e-09,
      'XRPUSDT': 0.029475413535821407}
    ```

    1. Specify the name of the portfolio class.

!!! note
    If you receive the message "The problem doesn't have a solution with actual input parameters"
    when using the "MOSEK" solver, make sure you have installed and activated [MOSEK](https://www.mosek.com/).
    You can also try using "ECOS".

### Periodically

As mentioned earlier, having a single function to handle everything is not only easier to use, 
but its main advantage is that it can be parameterized and leveraged for rebalancing with 
[PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer).
Specifically, the optimization function shown above is used by the method 
[PortfolioOptimizer.from_riskfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_riskfolio),
which calls it on a periodic basis. Let's demonstrate its flexibility by optimizing for 
maximum Sharpe in the previous week:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_riskfolio(
...     returns=returns,
...     every="W"
... )

>>> pfo.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/riskfolio.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/riskfolio.dark.svg#only-dark){: .iimg loading=lazy }

What about parameters? We can wrap any argument, even nested ones, with 
[Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) to test multiple parameter combinations.
For example, let's test different maximum `BTCUSDT` weights to ensure constraints behave as expected:

```pycon
>>> pfo = vbt.PortfolioOptimizer.from_riskfolio(
...     returns=returns,
...     constraints=[{
...         "Type": "Assets",
...         "Position": "BTCUSDT",
...         "Sign": "<=",
...         "Weight": vbt.Param([0.1, 0.2, 0.3], name="BTCUSDT_maxw")  # (1)!
...     }],
...     every="W",
...     param_search_kwargs=dict(incl_types=list)  # (2)!
... )
```

1. Parameters can be defined in any nested structure.

2. For VBT to search for parameters within nested structures, you need to whitelist them first.

[=100% "Group 3/3"]{: .candystripe .candystripe-animate }

```pycon
>>> pfo.allocations.groupby("BTCUSDT_maxw").max()
symbol        ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT
BTCUSDT_maxw                                             
0.1               1.0      1.0      0.1      1.0      1.0
0.2               1.0      1.0      0.2      1.0      1.0
0.3               1.0      1.0      0.3      1.0      1.0
```

It works perfectly :ok_hand:

## Universal portfolios

!!! note
    To install this package, first install vectorbtpro, and then install universal-portfolios,
    not both together. Because its dependency versions are quite strict, you may want to ignore
    its dependencies altogether by running `pip install -U universal-portfolios --no-deps`.

> The purpose of [Universal Portfolios](https://github.com/Marigold/universal-portfolios)
> is to bring together different Online Portfolio Selection (OLPS) algorithms and provide
> unified tools for analyzing them.

Unlike PyPortfolioOpt, which generates weights for a specific time range, OLPS aims to select
portfolio weights for each period to maximize final wealth. This means the generated weights
always have the same shape as the original array.

Let's look at the uniform allocation (UCRP):

```pycon
>>> from universal import tools, algos

>>> with vbt.WarningsFiltered():  # (1)!
...     algo = algos.CRP()
...     algo_result = algo.run(data.get("Close"))

>>> algo_result.weights
symbol                     ADAUSDT  BNBUSDT  BTCUSDT  ETHUSDT  XRPUSDT  CASH
Open time                                                                   
2020-01-01 00:00:00+00:00      0.2      0.2      0.2      0.2      0.2   0.0
2020-01-01 01:00:00+00:00      0.2      0.2      0.2      0.2      0.2   0.0
2020-01-01 02:00:00+00:00      0.2      0.2      0.2      0.2      0.2   0.0
...                            ...      ...      ...      ...      ...   ...
2020-12-31 21:00:00+00:00      0.2      0.2      0.2      0.2      0.2   0.0
2020-12-31 22:00:00+00:00      0.2      0.2      0.2      0.2      0.2   0.0
2020-12-31 23:00:00+00:00      0.2      0.2      0.2      0.2      0.2   0.0

[8767 rows x 6 columns]
```

1. You may see some deprecation warnings from Pandas; prepend this line to ignore them.

As you can see, Universal Portfolios generates and allocates weights at every single timestamp,
which is not realistic in practice since rebalancing that frequently is unsustainable unless
the data frequency is low. Also, iterating over this amount of data with this library is usually
**quite slow**.

To address this, we should downsample the pricing array to a longer time frame, then upsample
back to the original index. Let's try this with the `DynamicCRP` algorithm by first downsampling
to a daily frequency:

```pycon
>>> with vbt.WarningsFiltered():
...     algo = algos.DynamicCRP(
...         n=30,
...         min_history=7,
...         metric='sharpe',
...         alpha=0.01
...     )
...     algo_result = algo.run(data.get("Close").resample("D").last())
...     down_weights = algo_result.weights

>>> down_weights
symbol                          ADAUSDT       BNBUSDT   BTCUSDT       ETHUSDT  \
Open time
2020-01-01 00:00:00+00:00  2.000000e-01  2.000000e-01  0.200000  2.000000e-01
2020-01-02 00:00:00+00:00  2.000000e-01  2.000000e-01  0.200000  2.000000e-01
2020-01-03 00:00:00+00:00  2.000000e-01  2.000000e-01  0.200000  2.000000e-01
...                                 ...           ...       ...           ...
2020-12-29 00:00:00+00:00  8.475716e-09  8.176270e-09  0.664594  8.274986e-09
2020-12-30 00:00:00+00:00  0.000000e+00  0.000000e+00  0.656068  0.000000e+00
2020-12-31 00:00:00+00:00  0.000000e+00  0.000000e+00  0.655105  0.000000e+00

symbol                          XRPUSDT
Open time
2020-01-01 00:00:00+00:00  2.000000e-01
2020-01-02 00:00:00+00:00  2.000000e-01
2020-01-03 00:00:00+00:00  2.000000e-01
...                                 ...
2020-12-29 00:00:00+00:00  9.004152e-09
2020-12-30 00:00:00+00:00  0.000000e+00
2020-12-31 00:00:00+00:00  0.000000e+00

[366 rows x 5 columns]
```

Notice that the calculation still takes quite a bit of time, even though we have reduced the total
number of reallocation timestamps by a factor of 24.

Next, let's bring these weights back to the original time frame:

```pycon
>>> weights = down_weights.vbt.realign(
...     data.wrapper.index,
...     freq="1h",
...     source_rbound=True,  # (1)!
...     target_rbound=True,
...     ffill=False  # (2)!
... )
>>> weights
symbol                     ADAUSDT  BNBUSDT   BTCUSDT  ETHUSDT  XRPUSDT
Open time
2020-01-01 00:00:00+00:00      NaN      NaN       NaN      NaN      NaN
2020-01-01 01:00:00+00:00      NaN      NaN       NaN      NaN      NaN
2020-01-01 02:00:00+00:00      NaN      NaN       NaN      NaN      NaN
...                            ...      ...       ...      ...      ...
2020-12-31 21:00:00+00:00      NaN      NaN       NaN      NaN      NaN
2020-12-31 22:00:00+00:00      NaN      NaN       NaN      NaN      NaN
2020-12-31 23:00:00+00:00      0.0      0.0  0.655105      0.0      0.0

[8766 rows x 5 columns]
```

1. We are working with the close price, so set the right bound of each bar.

2. Do not forward-fill to avoid rebalancing at every timestamp.

This array is now ready to be used in a simulation.

To simplify this workflow, VBT provides the class method
[PortfolioOptimizer.from_universal_algo](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_universal_algo),
which runs the full simulation with Universal Portfolios and, once finished, selects allocations
at specific dates from the resulting DataFrame. By default, it selects timestamps with non-NA,
non-repeating weights. The method requires an algorithm (`algo`) and the pricing data (`S`) as input.
The algorithm can be provided in several forms: as the name or instance of the algorithm class
(which must be a subclass of `universal.algo.Algo`), or as the result of the algorithm
(a `universal.result.AlgoResult` object).

Let's run the same algorithm as above, but now using [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer).
We will also try several values for `n`:

```pycon
>>> with vbt.WarningsFiltered():
...     down_pfo = vbt.PortfolioOptimizer.from_universal_algo(
...         "DynamicCRP",
...         data.get("Close").resample("D").last(),
...         n=vbt.Param([7, 14, 30, 90]),
...         min_history=7,
...         metric='sharpe',
...         alpha=0.01
...     )

>>> down_pfo.plot(column=90).show()
```

[=100% "Group 4/4"]{: .candystripe .candystripe-animate }

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/universal.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/universal.dark.svg#only-dark){: .iimg loading=lazy }

You can upsample the optimizer back to the original time frame by creating an instance of
[Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler) and passing it
to [PortfolioOptimizer.resample](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.resample):

```pycon
>>> resampler = vbt.Resampler(
...     down_pfo.wrapper.index,
...     data.wrapper.index,
...     target_freq="1h"
... )
>>> pfo = down_pfo.resample(resampler)
```

!!! note
    An allocation made at the end of a daily bar will be placed at the end of the first hourly bar on
    that day. This may not be desired if the allocation uses information from that daily bar. To
    address this, calculate and use the right bounds of both indexes with
    [Resampler.get_rbound_index](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.get_rbound_index).

Finally, use the new optimizer in a simulation:

```pycon
>>> pf = pfo.simulate(data, freq="1h")

>>> pf.sharpe_ratio
n
7     2.913174
14    3.456085
30    3.276883
90    2.176654
Name: sharpe_ratio, dtype: float64
```

### Custom algorithm

Let's create a mean-reversion algorithm using Universal Portfolios. The idea is that stocks that have
performed poorly will revert to the mean and may achieve higher returns than those trading above their mean.

```pycon
>>> from universal.algo import Algo

>>> class MeanReversion(Algo):
...     PRICE_TYPE = 'log'
...     
...     def __init__(self, n):
...         self.n = n
...         super().__init__(min_history=n)
...     
...     def init_weights(self, cols):
...         return pd.Series(np.zeros(len(cols)), cols)
...     
...     def step(self, x, last_b, history):
...         ma = history.iloc[-self.n:].mean()
...         delta = x - ma
...         w = np.maximum(-delta, 0.)
...         return w / sum(w)

>>> with vbt.WarningsFiltered():
...     pfo = vbt.PortfolioOptimizer.from_universal_algo(
...         MeanReversion,
...         data.get("Close").resample("D").last(),  # (1)!
...         n=30,  # (2)!
...         every="W"  # (3)!
...     )

>>> pfo.plot().show()
```

1. Data frequency is daily.

2. Window length (in days).

3. Allocate at the beginning of each week.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/universal_custom.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/universal_custom.dark.svg#only-dark){: .iimg loading=lazy }

Now it's your turn: try creating and implementing a simple optimization strategy that could work in the
real world. You may be surprised by how complex and fascinating some strategies can become after starting
with something very basic :slightly_smiling_face:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/portfolio-optimization/integrations.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/PortfolioOptimization.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/dynamic/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization/
name: Dynamic
type: page
icon: material-chart-bar-stacked
---

# Dynamic

Up to this point, all allocation and optimization functions have relied strictly on external information,
such as pricing data, and have not controlled actual execution. But what if you want to rebalance based on
conditions within the current trading environment? For example, to perform threshold rebalancing,
you need to know the current portfolio value. This creates a path-dependent problem that can only be
handled with a custom order function.

Let's backtest threshold rebalancing—a portfolio management strategy used to maintain a set of
desired allocations without allowing asset weights to drift too far. We will build a template
pipeline that accepts any user-defined, Numba-compiled allocation function. When one of the
individual portfolio constituents moves outside its desired allocation bounds, the entire portfolio
is rebalanced to realign with the target allocations.

Here is a general template:

```pycon
>>> GroupMemory = namedtuple("GroupMemory", [  # (1)!
...     "target_alloc", 
...     "size_type",
...     "direction",
...     "order_value_out"
... ])

>>> @njit
... def pre_group_func_nb(c):  # (2)!
...     group_memory = GroupMemory(
...         target_alloc=np.full(c.group_len, np.nan),  # (3)!
...         size_type=np.full(c.group_len, vbt.pf_enums.SizeType.TargetPercent),  # (4)!
...         direction=np.full(c.group_len, vbt.pf_enums.Direction.Both),
...         order_value_out=np.full(c.group_len, np.nan)  # (5)!
...     )
...     return group_memory,

>>> @njit
... def pre_segment_func_nb(  # (6)!
...     c, 
...     group_memory,  # (7)!
...     min_history,  # (8)!
...     threshold,  # (9)!
...     allocate_func_nb,  # (10)!
...     *args
... ):
...     should_rebalance = False
...     
...     if c.i >= min_history:
...         in_position = False
...         for col in range(c.from_col, c.to_col):
...             if c.last_position[col] != 0:
...                 in_position = True
...                 break
...                 
...         if not in_position:
...             should_rebalance = True
...         else:
...             curr_value = c.last_value[c.group]
...             for group_col in range(c.group_len):
...                 col = c.from_col + group_col
...                 curr_position = c.last_position[col]
...                 curr_price = c.last_val_price[col]
...                 curr_alloc = curr_position * curr_price / curr_value
...                 curr_threshold = vbt.pf_nb.select_from_col_nb(c, col, threshold)  # (11)!
...                 alloc_diff = curr_alloc - group_memory.target_alloc[group_col]
...                 
...                 if abs(alloc_diff) >= curr_threshold:
...                     should_rebalance = True
...                     break
...                     
...     if should_rebalance:
...         allocate_func_nb(c, group_memory, *args)  # (12)!
...         vbt.pf_nb.sort_call_seq_1d_nb(  # (13)!
...             c, 
...             group_memory.target_alloc, 
...             group_memory.size_type, 
...             group_memory.direction, 
...             group_memory.order_value_out
...         )
...         
...     return group_memory, should_rebalance

>>> @njit
... def order_func_nb(  # (14)!
...     c, 
...     group_memory,  # (15)! 
...     should_rebalance, 
...     price,
...     fees
... ):
...     if not should_rebalance:
...         return vbt.pf_nb.order_nothing_nb()
...     
...     group_col = c.col - c.from_col  # (16)!
...     return vbt.pf_nb.order_nb(
...         size=group_memory.target_alloc[group_col], 
...         price=vbt.pf_nb.select_nb(c, price),
...         size_type=group_memory.size_type[group_col],
...         direction=group_memory.direction[group_col],
...         fees=vbt.pf_nb.select_nb(c, fees)
...     )
```

1. Create a named tuple to store variables shared between contexts within each portfolio group.

2. Initialize arrays in each portfolio group.

3. Array for the target allocation per asset. This array is updated at each rebalancing step.

4. Arrays for size type and direction per asset. These default to target percentage and both directions,
but you can modify them within `allocate_func_nb`.

5. Temporary array needed for sorting by order value.

6. Rebalancing takes place in a pre-segment function. Remember, a segment is a group of assets at a
specific timestamp, and in this function we decide whether to rebalance the entire group.

7. Arguments returned by `pre_group_func_nb` are passed right after the context.

8. Indicates when to place the first allocation.

9. Threshold array, which can be a constant, or provided by timestamp, asset, or element.

10. Allocation function that should accept the context, the group memory, and `*args`.

11. Use this function since segment contexts do not have column information.

12. The allocation function should update arrays in the group context and return nothing (`None`).

13. Sort the current call sequence by order value to execute sell orders before buy orders (to release
funds early). Use [sort_call_seq_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_order_func/#vectorbtpro.portfolio.nb.from_order_func.sort_call_seq_nb)
and [sort_call_seq_1d_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_order_func/#vectorbtpro.portfolio.nb.from_order_func.sort_call_seq_1d_nb)
when passing two-dimensional and one-dimensional arrays, respectively.

14. For each asset in the current group (sorted by order value), VBT will call this order
function.

15. Again, arguments returned by `pre_segment_func_nb` are passed right after the context.

16. Index of the current column within the current group.

Now, let's create an allocation function for an equally-weighted portfolio:

```pycon
>>> @njit
... def uniform_allocate_func_nb(c, group_memory):
...     for group_col in range(c.group_len):
...         group_memory.target_alloc[group_col] = 1 / c.group_len  # (1)!
```

1. Modify the target allocation array in place.

!!! tip
    Sometimes, you may want to rebalance dynamically using a function that relies on a third-party
    library, such as SciPy or scikit-learn, which cannot be compiled with Numba. In these cases,
    you can disable jitting of the main simulator function by passing `jitted=False`.

Now it's time to run the simulation!

```pycon
>>> def simulate_threshold_rebalancing(threshold, allocate_func_nb, *args, **kwargs):
...     return vbt.Portfolio.from_order_func(
...         data.get("Close"),
...         open=data.get("Open"),  # (1)!
...         pre_group_func_nb=pre_group_func_nb, 
...         pre_group_args=(),
...         pre_segment_func_nb=pre_segment_func_nb, 
...         pre_segment_args=(
...             0,  # (2)!
...             vbt.Rep("threshold"),  # (3)!
...             allocate_func_nb,
...             *args
...         ),
...         order_func_nb=order_func_nb, 
...         order_args=(vbt.Rep('price'), vbt.Rep('fees')),  # (4)!
...         broadcast_named_args=dict(
...             price=data.get("Close"),
...             fees=0.005,
...             threshold=threshold
...         ),
...         cash_sharing=True,
...         group_by=vbt.ExceptLevel("symbol"),  # (5)!
...         freq='1h', 
...         **kwargs
...     )

>>> pf = simulate_threshold_rebalancing(0.05, uniform_allocate_func_nb)
>>> pf.plot_allocations().show()
```

1. Provide `open` to ensure a non-NA valuation price in `pre_segment_func_nb` at the first timestamp.

2. Place an allocation at the first timestamp.

3. Threshold is an array-like object broadcast per timestamp and column,
so include price along with `close`.

4. Use templates to broadcast price along with `close`. Be sure to include such arguments in
`broadcast_named_args`.

5. Group by everything except assets so each group contains only assets.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic.dark.svg#only-dark){: .iimg loading=lazy }

We see that threshold rebalancing causes asset allocations to repeatedly jump to their target levels.

!!! info
    If your kernel dies or you want to validate your pipeline with Numba,
    it is recommended to either enable bounds checks or disable Numba entirely, then run your pipeline
    on sample data. This will help you catch any hidden indexing bugs.

    To do this, run the following in the first cell before anything else:

    ```pycon
    >>> import os

    >>> os.environ["NUMBA_BOUNDSCHECK"] = "1"
    >>> os.environ["NUMBA_DISABLE_JIT"] = "1"
    ```

We can also test multiple thresholds by simply making it an index:

```pycon
>>> pf = simulate_threshold_rebalancing(
...     vbt.Param(np.arange(1, 16) / 100, name="threshold"),  # (1)!
...     uniform_allocate_func_nb
... )

>>> pf.sharpe_ratio
threshold
0.01    1.939551
0.02    1.964451
0.03    1.985215
0.04    1.984484
0.05    1.993381
0.06    2.019594
0.07    1.983087
0.08    2.047598
0.09    2.087186
0.10    1.939077
0.11    1.962485
0.12    1.978320
0.13    1.963077
0.14    1.969721
0.15    1.983179
Name: sharpe_ratio, dtype: float64
```

1. If you used `np.arange(0.01, 0.16, 0.01)`, you would not be able to select those
values from columns since, for example, `0.6` would become `0.060000000000000005`.

## Post-analysis

But can we retrieve the rebalancing timestamps? Absolutely!

```pycon
>>> @njit
... def track_uniform_allocate_func_nb(c, group_memory, index_points, alloc_counter):
...     for group_col in range(c.group_len):
...         group_memory.target_alloc[group_col] = 1 / c.group_len
...
...     index_points[alloc_counter[0]] = c.i
...     alloc_counter[0] += 1

>>> index_points = np.empty(data.wrapper.shape[0], dtype=int_)  # (1)!
>>> alloc_counter = np.full(1, 0)  # (2)!
>>> pf = simulate_threshold_rebalancing(
...     0.05,
...     track_uniform_allocate_func_nb, 
...     index_points, 
...     alloc_counter
... )
>>> index_points = index_points[:alloc_counter[0]]  # (3)!

>>> data.wrapper.index[index_points]
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-02-02 04:00:00+00:00',
                '2020-03-07 15:00:00+00:00', '2020-05-28 18:00:00+00:00',
                '2020-06-03 16:00:00+00:00', '2020-07-07 13:00:00+00:00',
                '2020-08-14 17:00:00+00:00', '2020-09-09 01:00:00+00:00',
                '2020-11-05 13:00:00+00:00', '2020-11-21 14:00:00+00:00',
                '2020-11-24 00:00:00+00:00', '2020-12-22 17:00:00+00:00',
                '2020-12-23 11:00:00+00:00', '2020-12-28 23:00:00+00:00',
                '2020-12-29 16:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', freq=None)
```

1. Since we do not know the number of index points in advance, we should initialize an array
with the same size as the number of timestamps.

2. Allocation counter is an array with one element set to `0`. Why use an array instead of
a constant? Because we need to keep a reference to it.

3. After the simulation, part of the index point array has been filled, while the rest remains
uninitialized (it contains garbage). Use the counter to select only the filled entries.

What if we want to post-analyze both index points and target allocations?
And how should we handle situations with multiple parameter combinations?

Allocations can be saved to an array just like index points. However, when there are
multiple groups, we have two options: we can either run the whole pipeline in a loop (remember, 
VBT even encourages this since you can use chunking), or you can concatenate index points
and target allocations from all groups into a single array and track the group for each entry.
Then, you can construct an instance of [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
to conveniently post-analyze all the target allocation data!

We need to make a few adaptations, though. First, [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
requires index points to be of type [AllocPoints](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/records/#vectorbtpro.portfolio.pfopt.records.AllocPoints),
which means the underlying data should be a structured array of the complex data type
[alloc_point_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.alloc_point_dt).
Second, our counter will now track counts per group instead of globally. By summing it,
we can still get the global count. For better illustration, we will also implement a new allocation
function that generates weights randomly. Finally, if you are not afraid of complexity and want
the most flexible solution possible, see the "Flexible" tab for the same pipeline with templates
and in-place outputs :smirk:

=== "Preset"

    ```pycon
    >>> @njit
    ... def random_allocate_func_nb(
    ...     c, 
    ...     group_memory, 
    ...     alloc_points, 
    ...     alloc_weights, 
    ...     alloc_counter
    ... ):
    ...     weights = np.random.uniform(0, 1, c.group_len)
    ...     group_memory.target_alloc[:] = weights / weights.sum()
    ...
    ...     group_count = alloc_counter[c.group]
    ...     count = alloc_counter.sum()  # (1)!
    ...     alloc_points["id"][count] = group_count  # (2)!
    ...     alloc_points["col"][count] = c.group  # (3)!
    ...     alloc_points["alloc_idx"][count] = c.i  # (4)!
    ...     alloc_weights[count] = group_memory.target_alloc  # (5)!
    ...     alloc_counter[c.group] += 1
    
    >>> thresholds = pd.Index(np.arange(1, 16) / 100, name="threshold")
    >>> max_entries = data.wrapper.shape[0] * len(thresholds)  # (6)!
    >>> alloc_points = np.empty(max_entries, dtype=vbt.pf_enums.alloc_point_dt)  # (7)!
    >>> alloc_weights = np.empty((max_entries, len(data.symbols)), dtype=float_)  # (8)!
    >>> alloc_counter = np.full(len(thresholds), 0)  # (9)!
    
    >>> pf = simulate_threshold_rebalancing(
    ...     vbt.Param(thresholds),
    ...     random_allocate_func_nb, 
    ...     alloc_points, 
    ...     alloc_weights,
    ...     alloc_counter,
    ...     seed=42  # (10)!
    ... )
    >>> alloc_points = alloc_points[:alloc_counter.sum()]  # (11)!
    >>> alloc_weights = alloc_weights[:alloc_counter.sum()]
    ```

    1. The global count will be used as the index at which the current allocation data is stored.
    This allows us to incrementally write all group allocation data into a single array.

    2. Record id is always per column (in this case, per group, since columns are groups in allocation records).

    3. Store the current column (in this case, group, since columns are groups in allocation records).

    4. Store the actual index point.

    5. Store the allocation (the vector of weights, one per group member).

    6. Since we do not know the number of generated allocations in advance, we should prepare for
    the worst case, where every timestamp has an allocation. The maximum number of entries is the
    number of timestamps multiplied by the number of groups.

    7. Notice how we use [alloc_point_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.alloc_point_dt)
    as the data type for the array to make it structured.

    8. The allocation array has the same number of entries as the index point array, but with two dimensions
    to store the weight for each asset in a group (number of columns = number of assets in each group).

    9. The count is now tracked per group.

    10. Set the seed so the output is the same each time you execute this cell.

    11. To remove unused entries that have not been filled, use the global count,
    which is simply the sum of all group counts.

=== "Flexible"

    ```pycon
    >>> @njit
    ... def random_allocate_func_nb(c, group_memory):  # (1)!
    ...     weights = np.random.uniform(0, 1, c.group_len)
    ...     group_memory.target_alloc[:] = weights / weights.sum()
    ...
    ...     group_count = c.in_outputs.alloc_counter[c.group]
    ...     count = c.in_outputs.alloc_counter.sum()
    ...     c.in_outputs.alloc_points["id"][count] = group_count
    ...     c.in_outputs.alloc_points["col"][count] = c.group
    ...     c.in_outputs.alloc_points["alloc_idx"][count] = c.i
    ...     c.in_outputs.alloc_weights[count] = group_memory.target_alloc
    ...     c.in_outputs.alloc_counter[c.group] += 1
    
    >>> alloc_points = vbt.RepEval("""
    ...     max_entries = target_shape[0] * len(group_lens)
    ...     np.empty(max_entries, dtype=alloc_point_dt)
    ... """, context=dict(alloc_point_dt=vbt.pf_enums.alloc_point_dt))  # (2)!
    >>> alloc_weights = vbt.RepEval("""
    ...     max_entries = target_shape[0] * len(group_lens)
    ...     np.empty((max_entries, n_cols), dtype=float_)
    ... """, context=dict(n_cols=len(data.symbols)))
    >>> alloc_counter = vbt.RepEval("np.full(len(group_lens), 0)")
    
    >>> InOutputs = namedtuple("InOutputs", [
    ...     "alloc_points",
    ...     "alloc_weights",  # (3)!
    ...     "alloc_counter"
    ... ])
    >>> in_outputs = InOutputs(
    ...     alloc_points=alloc_points, 
    ...     alloc_weights=alloc_weights,
    ...     alloc_counter=alloc_counter,
    ... )
    
    >>> pf = simulate_threshold_rebalancing(
    ...     vbt.Param(np.arange(1, 16) / 100, name="threshold"),  # (4)!
    ...     random_allocate_func_nb, 
    ...     in_outputs=in_outputs,  # (5)!
    ...     seed=42
    ... )
    >>> alloc_points = pf.in_outputs.alloc_points[:pf.in_outputs.alloc_counter.sum()]
    >>> alloc_weights = pf.in_outputs.alloc_weights[:pf.in_outputs.alloc_counter.sum()]
    ```

    1. The function is the same as in the "Preset" example, but now the arrays are provided
    through the in-place outputs tuple instead of being passed directly as arguments.

    2. The logic is the same as in the "Preset" example, except we use templates to defer array
    creation until after all other arrays are broadcast and the final shape and number of groups
    are determined.

    3. Do not name the field `allocations`, since it is a reserved word!

    4. Notice that array creation is no longer directly tied to the threshold array.

    5. Since we now use templates, we no longer have direct references to the created arrays.
    Luckily, we can use in-place outputs, which store references to all the arrays for us.

!!! tip
    If you perform portfolio optimization across a historical data range (for example, by searching
    for the maximum Sharpe ratio), be sure to use [alloc_range_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.alloc_range_dt)
    and [AllocRanges](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/records/#vectorbtpro.portfolio.pfopt.records.AllocRanges)—
    this will add another dimension for data analysis.

All that's left is to create a [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
instance using the target allocation data we just filled:

```pycon
>>> pfo = vbt.PortfolioOptimizer(  # (1)!
...     wrapper=pf.wrapper,  # (2)!
...     alloc_records=vbt.AllocPoints(
...         pf.wrapper.resolve(),  # (3)!
...         alloc_points
...     ),
...     allocations=alloc_weights
... )
```

1. We do not call any class method here—since we already have all the data, we can instantiate the class directly!

2. The main wrapper should contain both regular columns and groups.

3. The allocation wrapper should contain only groups, as the `col` field we filled previously
points to groups instead of regular columns. By using [ArrayWrapper.resolve](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.resolve),
we create a new wrapper where columns are replaced with groups.

With this instance, we can thoroughly post-analyze the target allocation data.
Even though we used random weights during rebalancing, let's review the allocations generated
for a threshold of 10%, just as an example:

```pycon
>>> pfo[0.1].allocations.describe()
symbol   ADAUSDT   BNBUSDT   BTCUSDT   ETHUSDT   XRPUSDT
count   6.000000  6.000000  6.000000  6.000000  6.000000
mean    0.159883  0.149608  0.156493  0.292615  0.241400
std     0.092490  0.079783  0.043584  0.098891  0.083152
min     0.076678  0.056292  0.094375  0.200873  0.098709
25%     0.091023  0.082134  0.149385  0.220957  0.223424
50%     0.123982  0.157974  0.153985  0.252078  0.243109
75%     0.230589  0.204527  0.156810  0.375171  0.293097
max     0.288493  0.248507  0.231013  0.423879  0.336853

>>> pfo.plot(column=0.1).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic_01.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic_01.dark.svg#only-dark){: .iimg loading=lazy }

Here is how the target allocation visualization changes with a lower threshold:

```pycon
>>> pfo.plot(column=0.03).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic_003.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic_003.dark.svg#only-dark){: .iimg loading=lazy }

And here is what actually happened in practice:

```pycon
>>> pf[0.03].plot_allocations().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic_003_sim.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/dynamic_003_sim.dark.svg#only-dark){: .iimg loading=lazy }

Want to see something cool? If we feed our manually constructed optimizer instance to
[Portfolio.from_optimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_optimizer),
we will get exactly the same results :exploding_head:

```pycon
>>> pf.sharpe_ratio
threshold
0.01    1.098642
0.02    1.707515
0.03    1.775001
0.04    2.077479
0.05    2.082900
0.06    1.964474
0.07    2.106367
0.08    2.121511
0.09    1.838164
0.10    2.072388
0.11    2.229001
0.12    1.766305
0.13    1.859604
0.14    2.209144
0.15    2.124474
Name: sharpe_ratio, dtype: float64

>>> pf_new = vbt.Portfolio.from_optimizer(
...     data,
...     pfo, 
...     val_price=data.get("Open"), 
...     freq="1h", 
...     fees=0.005
... )

>>> pf_new.sharpe_ratio
threshold
0.01    1.098642
0.02    1.707515
0.03    1.775001
0.04    2.077479
0.05    2.082900
0.06    1.964474
0.07    2.106367
0.08    2.121511
0.09    1.838164
0.10    2.072388
0.11    2.229001
0.12    1.766305
0.13    1.859604
0.14    2.209144
0.15    2.124474
Name: sharpe_ratio, dtype: float64
```

This once again demonstrates the power of VBT: we just performed dynamic threshold
rebalancing, extracted the target allocation data from the simulation, analyzed that data,
and then fed it into another, completely different simulation method to ensure there were
no mistakes in order generation.

## Bonus 1: Own optimizer

As a bonus, let's perform periodic mean-variance optimization using our own simulator!
We will generate the rebalancing dates ahead of time and, for each date, calculate multiple
Sharpe ratios for that period and use the Efficient Frontier to select the best one.
The pipeline below is as lightweight as possible: it processes only one parameter combination
at a time, uses VBT's low-level order execution API, and only uses the information
it actually needs.

Here is our raw Numba-compiled pipeline (agnostic to the optimization function):

```pycon
>>> @njit(nogil=True)
... def optimize_portfolio_nb(
...     close, 
...     val_price,
...     range_starts,
...     range_ends,
...     optimize_func_nb,
...     optimize_args=(),
...     price=np.inf,
...     fees=0.,
...     init_cash=100.,
...     group=0
... ):
...     val_price_ = vbt.to_2d_array_nb(np.asarray(val_price))  # (1)!
...     price_ = vbt.to_2d_array_nb(np.asarray(price))
...     fees_ = vbt.to_2d_array_nb(np.asarray(fees))
...
...     order_records = np.empty(close.shape, dtype=vbt.pf_enums.order_dt)  # (2)!
...     order_counts = np.full(close.shape[1], 0, dtype=int_)
...     
...     order_value = np.empty(close.shape[1], dtype=float_)  # (3)!
...     call_seq = np.empty(close.shape[1], dtype=int_)
...     
...     last_position = np.full(close.shape[1], 0.0, dtype=float_)
...     last_debt = np.full(close.shape[1], 0.0, dtype=float_)
...     last_locked_cash = np.full(close.shape[1], 0.0, dtype=float_)
...     cash_now = float(init_cash)  # (4)!
...     free_cash_now = float(init_cash)
...     value_now = float(init_cash)
... 
...     for k in range(len(range_starts)):  # (5)!
...         i = range_ends[k]  # (6)!
...         size = optimize_func_nb(  # (7)!
...             range_starts[k], 
...             range_ends[k], 
...             *optimize_args
...         )
...         
...         # (8)!
...         value_now = cash_now
...         for col in range(close.shape[1]):
...             val_price_now = vbt.flex_select_nb(val_price_, i, col)
...             value_now += last_position[col] * val_price_now
...         
...         for col in range(close.shape[1]):
...             val_price_now = vbt.flex_select_nb(val_price_, i, col)
...             exec_state = vbt.pf_enums.ExecState(
...                 cash=cash_now,
...                 position=last_position[col],
...                 debt=last_debt[col],
...                 locked_cash=last_locked_cash[col],
...                 free_cash=free_cash_now,
...                 val_price=val_price_now,
...                 value=value_now,
...             )
...             order_value[col] = vbt.pf_nb.approx_order_value_nb(  # (9)!
...                 exec_state,
...                 size[col],
...                 vbt.pf_enums.SizeType.TargetPercent,
...                 vbt.pf_enums.Direction.Both,
...             )
...             call_seq[col] = col  # (10)!
... 
...         vbt.pf_nb.insert_argsort_nb(order_value, call_seq)  # (11)!
... 
...         for c in range(close.shape[1]):  # (12)!
...             col = call_seq[c]  # (13)!
...             
...             order = vbt.pf_nb.order_nb(  # (14)!
...                 size=size[col],
...                 price=vbt.flex_select_nb(price_, i, col),
...                 size_type=vbt.pf_enums.SizeType.TargetPercent,
...                 direction=vbt.pf_enums.Direction.Both,
...                 fees=vbt.flex_select_nb(fees_, i, col),
...             )
...
...             # (15)!
...             price_area = vbt.pf_enums.PriceArea(
...                 open=np.nan,
...                 high=np.nan,
...                 low=np.nan,
...                 close=vbt.flex_select_nb(close, i, col),
...             )
...             val_price_now = vbt.flex_select_nb(val_price_, i, col)
...             exec_state = vbt.pf_enums.ExecState(
...                 cash=cash_now,
...                 position=last_position[col],
...                 debt=last_debt[col],
...                 locked_cash=last_locked_cash[col],
...                 free_cash=free_cash_now,
...                 val_price=val_price_now,
...                 value=value_now,
...             )
...             _, new_exec_state = vbt.pf_nb.process_order_nb(
...                 group=group,
...                 col=col,
...                 i=i,
...                 exec_state=exec_state,
...                 order=order,
...                 price_area=price_area,
...                 order_records=order_records,
...                 order_counts=order_counts
...             )
... 
...             cash_now = new_exec_state.cash
...             free_cash_now = new_exec_state.free_cash
...             value_now = new_exec_state.value
...             last_position[col] = new_exec_state.position
...             last_debt[col] = new_exec_state.debt
...             last_locked_cash[col] = new_exec_state.locked_cash
... 
...     # (16)!
...     return vbt.nb.repartition_nb(order_records, order_counts)
```

1. [flex_select_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_nb) requires
flexible arrays to have exactly two dimensions, so we use the 
[to_2d_array_nb](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.to_2d_array_nb) function to convert 
scalars and one-dimensional arrays. Notice how we store the new arrays in variables with a trailing 
underscore. Numba cannot handle arrays with different shapes assigned to the same variable.

2. Since we do not know the number of orders ahead of time, create an array with the same number
of records as there are elements in `close`. Also, note that `order_records` must be two-dimensional.

3. Because we are iterating over timestamps and columns, we need to store information related to 
each asset. We do this using a one-dimensional array with elements aligned by column.

4. All information associated with cash is a constant because we have just one group with cash sharing.
We update those constants after each iteration.

5. There is no significant difference between iterating over the entire shape of `close` or just
the timestamps where optimization actually occurs. So, we iterate over ranges. If you want to
combine optimization with a stop loss or other rolling checks, you should iterate over 
`close.shape[0]` and run `optimize_func_nb` only when the current iteration matches an entry in
`range_ends`.

6. The allocation index, where optimization and order execution should occur, defaults to the range end.

7. Run the optimization function, which should return the weights. You can adapt it to return 
the size type, direction, and any other information needed.

8. Calculate the current group value using the open price.

9. Use the group value to approximate the order value for each allocation.

10. Prepare the call sequence array.

11. Sort the call sequence by order value, so assets that should be sold are processed first.

12. Iterate over each asset to place an order.

13. Get the column index of an asset based on the sorted call sequence. For example, if the first
element in `call_seq` is 2, we should process the third asset first.

14. Create an order. You can provide additional information such as fixed fees to this function.

15. Process the order and update the current balances.

16. Order records are stored per column, so flatten them.

Here is our Numba-compiled MVO function:

```pycon
>>> @njit(nogil=True)  # (1)!
... def sharpe_optimize_func_nb(
...     start_idx, 
...     end_idx, 
...     close,  # (2)!
...     num_tests, 
...     ann_factor
... ):
...     close_period = close[start_idx:end_idx]  # (3)!
...     returns = (close_period[1:] - close_period[:-1]) / close_period[:-1]
...     mean = vbt.nb.nanmean_nb(returns)
...     cov = np.cov(returns, rowvar=False)
...     best_sharpe_ratio = -np.inf
...     weights = np.full(close.shape[1], np.nan, dtype=float_)
...     
...     for i in range(num_tests):  # (4)!
...         w = np.random.random_sample(close.shape[1])
...         w = w / np.sum(w)
...         p_return = np.sum(mean * w) * ann_factor
...         p_std = np.sqrt(np.dot(w.T, np.dot(cov, w))) * np.sqrt(ann_factor)
...         sharpe_ratio = p_return / p_std
...         if sharpe_ratio > best_sharpe_ratio:
...             best_sharpe_ratio = sharpe_ratio
...             weights = w
...             
...     return weights
```

1. Remember to disable the GIL everywhere so you can use multi-threading.

2. These are additional arguments passed via `*args` in `optimize_portfolio_nb`.

3. Select the previous week in `close`.

4. Iterate over a number of tests. For each, generate a random allocation, calculate its 
Sharpe ratio, and store it if it is better than the previous best Sharpe ratios.

Now, let's run the MVO on a weekly basis:

```pycon
>>> range_starts, range_ends = data.wrapper.get_index_ranges(every="W")
>>> ann_factor = vbt.timedelta("365d") / vbt.timedelta("1h")
>>> init_cash = 100
>>> num_tests = 30
>>> fees = 0.005

>>> order_records = optimize_portfolio_nb(
...     data.get("Close").values,
...     data.get("Open").values,
...     range_starts,
...     range_ends,
...     sharpe_optimize_func_nb,
...     optimize_args=(data.get("Close").values, num_tests, ann_factor),
...     fees=fees,
...     init_cash=init_cash
... )
```

The result of our optimization is a set of order records, which can be used as input for the 
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) class:

```pycon
>>> pf = vbt.Portfolio(
...     wrapper=symbol_wrapper.regroup(True),  # (1)!
...     close=data.get("Close"), 
...     order_records=order_records, 
...     log_records=np.array([]),  # (2)!
...     cash_sharing=True, 
...     init_cash=init_cash
... )
```

1. By using [ArrayWrapper.regroup](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.regroup),
we group all assets together by setting `True`, putting them in one group.

2. We do not have any log records. If you want logs, you can generate and process them similarly 
to order records.

Now you can analyze the portfolio as usual!

```pycon
>>> pf.plot_allocations().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/mvo.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pf-opt/mvo.dark.svg#only-dark){: .iimg loading=lazy }

## Bonus 2: Parameterization

Unlike most of the examples above, our pipeline can process only one parameter combination at a time.
To test multiple parameter combinations, you must run it in a loop—either manually or with the 
special [parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) decorator, 
which magically transforms any Python function into one that can accept arbitrary parameter grids!
Behind the scenes, the decorator intercepts each argument passed to the original function and checks 
if its value, or any nested value, is wrapped with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param).
It then broadcasts and builds the Cartesian product of all parameter sequences found, prepares the arguments
and keyword arguments for each parameter combination, and forwards those argument sets to the executor
function [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute). This way, the decorator 
not only manages building the parameter grid, but also handles execution distribution, such as with Dask.

Let's test the Cartesian product of different index ranges, test counts, and fees. Since distributed 
execution returns a list of outputs, the first step is to write a merging function 
that combines all the results. In this case, each output for a parameter combination is an array
of order records, and our target metric is the Sharpe ratio. We will create a portfolio for each 
set of order records, extract the Sharpe ratios, and concatenate them:

```pycon
>>> def merge_func(order_records_list, param_index):
...     sharpe_ratios = pd.Series(index=param_index, dtype=float_)
...     for i, order_records in enumerate(order_records_list):
...         pf = vbt.Portfolio(
...             wrapper=symbol_wrapper.regroup(True), 
...             close=data.get("Close"), 
...             order_records=order_records, 
...             cash_sharing=True, 
...             init_cash=init_cash
...         )
...         sharpe_ratios.iloc[i] = pf.sharpe_ratio
...     return sharpe_ratios
```

The first argument of `merge_func` is the list of outputs from the original function 
(`optimize_portfolio_nb`). Any other argument should be specifically instructed to be passed. 
The parameter index (`param_index`) is a special argument containing the multi-index built 
internally from all parameter combinations, and it will become the index of the resulting Sharpe series.

The next step is to decorate the original function with the decorator, which is straightforward:

```pycon
>>> param_optimize_portfolio_nb = vbt.parameterized(
...     optimize_portfolio_nb, 
...     merge_func=merge_func,
...     merge_kwargs=dict(param_index=vbt.Rep("param_index")),  # (1)!
...     engine="dask",  # (2)!
...     chunk_len=4
... )
```

1. Instruct the decorator to pass `param_index` as a keyword argument to `merge_func`.

2. The parameter combinations are split into chunks of 4, and each chunk is executed with Dask. 

Next, we need to prepare our parameter grid. While passing multiple commission and `num_tests`
combinations is straightforward, setting up index ranges requires extra care. Because index ranges 
cannot be built inside Numba ([ArrayWrapper.get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges)
is not Numba-compiled), we must loop through each `every` instruction and manually extract the index
ranges. Index ranges consist of two arrays—start and end indices—which are accepted as two 
different arguments in `optimize_portfolio_nb`. However, they will appear as a single parameter in 
the final multi-index because they share the same `level` in [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param):

```pycon
>>> every_index = pd.Index(["D", "W", "M"], name="every")  # (1)!
>>> num_tests_index = pd.Index([30, 50, 100], name="num_tests")
>>> fees_index = pd.Index([0.0, 0.005, 0.01], name="fees")

>>> range_starts = []
>>> range_ends = []
>>> for every in every_index:
...     index_ranges = symbol_wrapper.get_index_ranges(every=every)
...     range_starts.append(index_ranges[0])
...     range_ends.append(index_ranges[1])
>>> num_tests = num_tests_index.tolist()

>>> range_starts = vbt.Param(range_starts, level=0, keys=every_index)  # (2)!
>>> range_ends = vbt.Param(range_ends, level=0, keys=every_index)
>>> num_tests = vbt.Param(num_tests, level=1, keys=num_tests_index)
>>> fees = vbt.Param(fees_index.values, level=2, keys=fees_index)
```

1. These indexes will become levels in the final multi-index.

2. Make each list an official parameter. The range start and end indices share the same
level so they will not be combined.

Finally, pass the prepared arguments to the parameterized function, just as you would 
to `optimize_portfolio_nb`!

```pycon
>>> sharpe_ratios = param_optimize_portfolio_nb(
...     data.get("Close").values,
...     data.get("Open").values,
...     range_starts,
...     range_ends,
...     sharpe_optimize_func_nb,
...     optimize_args=(
...         data.get("Close").values, 
...         num_tests, 
...         ann_factor
...     ),
...     fees=fees,
...     init_cash=init_cash,
...     group=vbt.Rep("config_idx")
... )
```

[=100% "Chunk 7/7"]{: .candystripe .candystripe-animate }

With Dask, each function call takes about 10 milliseconds!

Now, let's examine the generated Sharpe ratios:

```pycon
>>> sharpe_ratios
every  num_tests  fees 
D      30         0.000    2.208492
                  0.005    0.412934
                  0.010   -1.393060
        50         0.000    2.305169
                  0.005    0.396711
                  0.010   -1.444904
        100        0.000    2.202177
                  0.005    0.380674
                  0.010   -1.824848
W      30         0.000    2.507784
                  0.005    2.297914
                  0.010    1.962726
        50         0.000    2.358747
                  0.005    1.940125
                  0.010    1.736605
        100        0.000    2.563558
                  0.005    2.365637
                  0.010    1.877434
M      30         0.000    1.514242
                  0.005    1.606507
                  0.010    1.859369
        50         0.000    1.890288
                  0.005    1.555012
                  0.010    1.479636
        100        0.000    1.327935
                  0.005    1.847109
                  0.010    1.659961
dtype: float64
```

We could have also stacked all the order records and analyzed them as a single portfolio,
but this would require tiling the close price by the number of parameter combinations.
This can quickly become memory-intensive, so simple looping is preferred here.

## Bonus 3: Hyperopt

Instead of constructing and testing every combination in the parameter grid, you can use a 
statistical approach. Libraries like Hyperopt are designed to minimize objective functions.

> [Hyperopt](https://github.com/hyperopt/hyperopt) is a Python library for serial and parallel 
> optimization over awkward search spaces, which may include real-valued, discrete, and conditional dimensions.

To use Hyperopt, you first need to implement the objective function, which is straightforward in this case:

```pycon
>>> def objective(kwargs):
...     close_values = data.get("Close").values
...     open_values = data.get("Open").values
...     index_ranges = symbol_wrapper.get_index_ranges(every=kwargs["every"])
...     order_records = optimize_portfolio_nb(
...         close_values,
...         open_values,
...         index_ranges[0],  # (1)!
...         index_ranges[1],  # (2)!
...         sharpe_optimize_func_nb,
...         optimize_args=(close_values, kwargs["num_tests"], ann_factor),
...         fees=vbt.to_2d_array(kwargs["fees"]),
...         init_cash=init_cash
...     )
...     pf = vbt.Portfolio(
...         wrapper=symbol_wrapper.regroup(True), 
...         close=data.get("Close"), 
...         order_records=order_records, 
...         log_records=np.array([]), 
...         cash_sharing=True, 
...         init_cash=init_cash
...     )
...     return -pf.sharpe_ratio  # (3)!
```

1. Array with range start indices.

2. Array with range end indices.

3. Must be a float-valued function, and you want to **minimize** it.

Next, build the search space:

```pycon
>>> from hyperopt import fmin, tpe, hp

>>> space = {
...     "every": hp.choice("every", ["%dD" % n for n in range(1, 100)]),
...     "num_tests": hp.quniform("num_tests", 5, 100, 1),
...     "fees": hp.uniform('fees', 0, 0.05)
... }
```

Finally, search for the best candidate:

```pycon
>>> best = fmin(
...     fn=objective,
...     space=space,
...     algo=tpe.suggest,
...     max_evals=30
... )
100%|██████| 30/30 [00:01<00:00, 24.11trial/s, best loss: -2.4913128485273424]

>>> best
{'every': 92, 'fees': 0.018781236093979012, 'num_tests': 46.0}
```

Here is the [official tutorial](https://github.com/hyperopt/hyperopt/wiki/FMin) to help you get started.

## Bonus 4: Hybrid

We have covered generating weights strictly before and during the simulation, but what about
scenarios that fall somewhere in between? For example, how can we use external libraries (which usually 
cannot be Numba compiled) while still accessing the simulated state, such as the current
portfolio value? Disabling Numba entirely and running the external optimization function 
as part of the simulation would be too slow and would not provide any advantage over using 
backtrader or other traditional backtesting software. Here is a helpful trick: simulate the portfolio 
in chunks! For instance, allocate weights, simulate a portfolio over a set period of time, 
evaluate the portfolio, and repeat. If implemented correctly, the portfolios used for evaluation 
should closely match the performance of the post-generation portfolio.

Let's create an optimization function that allocates equal weights, but only if the current 
allocation deviates from the target allocation by a specified percentage amount:

```pycon
>>> def optimize_func(
...     data, 
...     index_slice, 
...     temp_allocations,  # (1)!
...     temp_pfs,  # (2)!
...     threshold
... ):
...     sub_data = data.iloc[index_slice]
...     if len(temp_allocations) > 0:  # (3)!
...         prev_allocation = sub_data.symbol_wrapper.wrap(  # (4)!
...             [temp_allocations[-1]], 
...             index=sub_data.wrapper.index[[0]]
...         )
...         prev_pfo = vbt.PortfolioOptimizer.from_allocations(  # (5)!
...             sub_data.symbol_wrapper,
...             prev_allocation
...         )
...         if len(temp_pfs) > 0:
...             init_cash = temp_pfs[-1].cash.iloc[-1]
...             init_position = temp_pfs[-1].assets.iloc[-1]
...             init_price = temp_pfs[-1].close.iloc[-1]
...         else:
...             init_cash = 100.
...             init_position = 0.
...             init_price = np.nan
...         prev_pf = prev_pfo.simulate(  # (6)!
...             sub_data,
...             init_cash=init_cash, 
...             init_position=init_position,
...             init_price=init_price
...         )
...         temp_pfs.append(prev_pf)
...         should_rebalance = False
...         curr_alloc = prev_pf.allocations.iloc[-1].values
...         if (np.abs(curr_alloc - temp_allocations[-1]) >= threshold).any():
...             should_rebalance = True  # (7)!
...     else:
...         should_rebalance = True
...     n_symbols = len(sub_data.symbols)
...     if should_rebalance:
...         new_alloc = np.full(n_symbols, 1 / n_symbols)  # (8)!
...     else:
...         new_alloc = np.full(n_symbols, np.nan)  # (9)!
...     temp_allocations.append(new_alloc)
...     return new_alloc

>>> pfs = []
>>> allocations = []
>>> pfopt = vbt.PortfolioOptimizer.from_optimize_func(
...     data.symbol_wrapper,
...     optimize_func,
...     data,
...     vbt.Rep("index_slice"),
...     allocations,
...     pfs,
...     0.03,  # (10)!
...     every="W"
... )
>>> pf = pfopt.simulate(data)
```

1. Temporary list with previous target allocations. An allocation is generated each week
and appended to this list, so next time we can compare it with the actual allocation.

2. Temporary list with previous portfolios. A portfolio is generated each week and added 
to this list, so the next portfolio knows where to start from.

3. If this data window had any allocation, we need to simulate it.

4. Get the allocation from the previous `optimize_func` call.

5. Create a small portfolio optimizer that allocates at the start of this data window, only once.

6. Simulate the optimizer using the last metrics of the previous portfolio as initial metrics.
This way, this portfolio will continue from the point where the previous portfolio ended.

7. Check whether any of the actual and target allocations deviate sufficiently.

8. If yes, re-allocate equally.

9. If not, do not re-allocate at all.

10. Test with a threshold of 3%.

How can we ensure that all the used sub-portfolios closely match reality? Let's compare the final 
value of each sub-portfolio to the corresponding value in the monolithic portfolio:

```pycon
>>> final_values = pd.concat(map(lambda x: x.value[[-1]], pfs))
>>> final_values
Open time
2020-01-18 23:00:00+00:00    117.448601
2020-01-25 23:00:00+00:00    109.741201
2020-02-01 23:00:00+00:00    126.428241
2020-02-08 23:00:00+00:00    143.399207
2020-02-15 23:00:00+00:00    157.673925
                                    ...
2020-11-28 23:00:00+00:00    306.180144
2020-12-05 23:00:00+00:00    312.231039
2020-12-12 23:00:00+00:00    289.465806
2020-12-19 23:00:00+00:00    338.756466
2020-12-26 23:00:00+00:00    313.229099
Name: group, dtype: float64

>>> pd.testing.assert_series_equal(  # (1)!
...     final_values,
...     pf.value.loc[final_values.index],
... )
```

1. If the two series were not equal, an error would be raised.

Perfect match! :golf:

## Summary

Regular portfolio reviews allow us to make adjustments and increase our chances of achieving
comfortable returns while maintaining our preferred level of risk. Diversification 
across asset classes is a risk-mitigation strategy, especially when spreading investments 
across a variety of asset classes. With VBT, we have powerful tools to programmatically 
select optimal portfolios. Not only are there tools that work well with third-party libraries,
but there is also a universe of options for easily implementing and testing any custom optimization 
strategy, especially when leveraging acceleration, such as compilation with Numba.

As we saw in these examples, VBT encourages us to use data science and to look at 
portfolio optimization from new angles to better understand how it influences results. For example, 
we can use the [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer) 
class to quickly tune various parameters for weight generation and rebalancing schedules. Once 
we are satisfied with the pre-analysis, we can feed the optimizer into a simulator to 
analyze the chosen strategy after execution. Or, we can choose to implement our own 
optimizer from scratch to control the entire execution process. In this case, we can 
extract target allocations and other metadata during the simulation and analyze them 
later. There are so many possibilities... :thought_balloon:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/portfolio-optimization/dynamic.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/PortfolioOptimization.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/pairs-trading/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
name: Pairs trading
type: page
icon: material-chart-multiline
---

# Pairs trading

A pairs trading strategy is a statistical arbitrage and convergence strategy based on the historical correlation 
between two instruments. It involves taking a long position in one instrument and a short position in the other. 
These offsetting positions create a hedging strategy that aims to profit from both positive and negative trends. 
A high positive correlation, typically at least 0.8, between the two instruments is the main source of the 
strategy's profits. When the correlation deviates, we look to buy the underperforming instrument and sell short 
the outperforming one. If the securities revert to their historical correlation, which is the outcome we aim for, 
a profit is generated from the convergence of their prices. As a result, pairs trading can be used to seek profits 
regardless of market conditions, whether the market is trending up, down, or moving sideways.

## Selection

When designing a pairs trading strategy, it is more important to select pairs based on 
[cointegration](https://en.wikipedia.org/wiki/Cointegration) rather than just 
[correlation](https://en.wikipedia.org/wiki/Correlation). Correlated instruments usually move in a similar 
way, but over time, the price ratio (or spread) between them may diverge significantly.
Cointegrated instruments, however, do not always move together in the same direction:
the spread between them can widen on some days, but their prices usually "pull back together" 
to the mean, providing optimal conditions for pairs arbitrage trading.

The two main methods for identifying cointegration are the Engle-Granger test and the Johansen test.
We will use the Engle-Granger test, as its augmented version is available in [`statsmodels`](https://www.statsmodels.org/).
The concept behind the Engle-Granger test is straightforward. We perform a linear regression between the two 
asset prices and check if the residual is stationary using the 
[Augmented Dickey-Fuller (ADF) test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test). 
If the residual is stationary, then the two asset prices are cointegrated.

First, let's create a universe of instruments from which to select our pairs. We will search for 
all available USDT symbols on Binance and download their daily history. Instead of downloading 
all of them at once, we will fetch each symbol individually and append it to an HDF file. 
We take this approach because most symbols have limited history, and we want to avoid using extra RAM 
by extending datasets with NaNs. We will also skip this entire process if the file already exists.

!!! note
    Make sure to delete the HDF file if you want to re-fetch.

```pycon
>>> from vectorbtpro import *

>>> SYMBOLS = vbt.BinanceData.list_symbols("*USDT")  # (1)!
>>> POOL_FILE = "temp/data_pool.h5"
>>> START = "2018"
>>> END = "2023"

>>> # vbt.remove_dir("temp", with_contents=True, missing_ok=True)
>>> vbt.make_dir("temp")  # (2)!

>>> if not vbt.file_exists(POOL_FILE):
...     with vbt.ProgressBar(total=len(SYMBOLS)) as pbar:  # (3)!
...         collected = 0
...         for symbol in SYMBOLS:
...             try:
...                 data = vbt.BinanceData.pull(
...                     symbol, 
...                     start=START,
...                     end=END,
...                     show_progress=False,
...                     silence_warnings=True
...                 )
...                 data.to_hdf(POOL_FILE)  # (4)!
...                 collected += 1
...             except Exception:
...                 pass
...             pbar.set_prefix(f"{symbol} ({collected})")  # (5)!
...             pbar.update()
```

1. Use a wildcard for the base currency to search for all pairs with USDT as the quote currency.

2. Create a temporary directory where all future artifacts will reside.

3. Display a [`tqdm`](https://github.com/tqdm/tqdm) progress bar.

4. Save each dataset to the same HDF file under `POOL_FILE`, using the symbol as a key.

5. A descriptive progress bar always helps track the process.

[=100% "Symbol 423/423"]{: .candystripe .candystripe-animate }

Although this process takes some time, we now have a file containing data for each symbol under its 
own key. One major advantage of using HDF files (and VBT in particular) is that we can load the entire file 
and join all contained keys with a single command.

We still have one more decision to make: which period should we analyze to select the optimal pair? 
It is important not to use the same date range for both pair selection and strategy backtesting, 
as this could introduce survivorship bias. Therefore, let's reserve a more recent period for backtesting.

```pycon
>>> SELECT_START = "2020"
>>> SELECT_END = "2021"

>>> data = vbt.HDFData.pull(
...     POOL_FILE, 
...     start=SELECT_START, 
...     end=SELECT_END, 
...     silence_warnings=True
... )

>>> print(len(data.symbols))
245
```

We have imported 245 datasets, but some may be incomplete.
To ensure smooth analysis, we should remove the incomplete datasets.

```pycon
>>> data = data.select([
...     k 
...     for k, v in data.data.items() 
...     if not v.isnull().any().any()
... ])

>>> print(len(data.symbols))
82
```

We have removed a large portion of the incomplete data.

The next step is to find the pairs that pass our cointegration test. There are several approaches to 
finding viable pairs. On one hand, we may have prior knowledge and specifically test a particular pair 
for cointegration. On the other hand, we can search through hundreds of instruments to find any viable 
pairs according to the test results. In this exhaustive search scenario, we might encounter a multiple comparisons 
bias, which is an increased chance of incorrectly identifying a significant p-value when performing many tests. 
For example, if we run 100 tests on random data, we would expect about 5 p-values below 0.05 just by chance.
In practice, we should include a second verification step when identifying pairs in this way, which we will do later.

The test itself is performed using [`statsmodels.tsa.stattools.coint`](https://www.statsmodels.org/stable/generated/statsmodels.tsa.stattools.coint.html).
This function returns a tuple, with the second element being the p-value of interest. Testing each pair 
would require looping through `82 * 82` or 6,724 pairs, and since the `coint` function is not especially 
fast, we will parallelize the process using [`pathos`](https://pathos.readthedocs.io/en/latest/index.html) :zap:

```pycon
>>> @vbt.parameterized(  # (1)!
...     merge_func="concat", 
...     engine="pathos",
...     distribute="chunks",  # (2)!
...     n_chunks="auto"  # (3)!
... )
... def coint_pvalue(close, s1, s2):
...     import statsmodels.tsa.stattools as ts  # (4)!
...     import numpy as np
...     return ts.coint(np.log(close[s1]), np.log(close[s2]))[1]

>>> COINT_FILE = "temp/coint_pvalues.pickle"

>>> # vbt.remove_file(COINT_FILE, missing_ok=True)
>>> if not vbt.file_exists(COINT_FILE):
...     coint_pvalues = coint_pvalue(  # (5)!
...         data.close,
...         vbt.Param(data.symbols, condition="s1 != s2"),  # (6)!
...         vbt.Param(data.symbols)
...     )
...     vbt.save(coint_pvalues, COINT_FILE)
... else:
...     coint_pvalues = vbt.load(COINT_FILE)
```

1. This decorator ([parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized)) 
takes arguments wrapped with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param),
builds their product, and passes each combination to the underlying function.

2. Build chunks of multiple parameter combinations and process these chunks in parallel, while 
combinations within each chunk are processed serially.

3. Set the number of chunks to the number of CPU cores.

4. Since we use multiprocessing, each process must import the libraries it will need.

5. The wrapped function takes the same arguments as the underlying function.

6. Do not compare symbols to themselves.

!!! tip
    Analyzing raw prices can work, but log prices are preferable.

The result is a Pandas Series where each pair of symbols in the index is mapped to its p-value.
If the p-value is small, below a critical threshold (< 0.05), we can reject the hypothesis that 
there is no cointegrating relationship. Let's now arrange the p-values in increasing order:

```pycon
>>> coint_pvalues = coint_pvalues.sort_values()

>>> print(coint_pvalues)
s1        s2      
TUSDUSDT  BUSDUSDT    6.179128e-17
USDCUSDT  BUSDUSDT    7.703666e-14
BUSDUSDT  USDCUSDT    2.687508e-13
TUSDUSDT  USDCUSDT    2.906244e-12
BUSDUSDT  TUSDUSDT    1.853641e-11
                                ...
BTCUSDT   XTZUSDT     1.000000e+00
          EOSUSDT     1.000000e+00
          ENJUSDT     1.000000e+00
          NKNUSDT     1.000000e+00
ZILUSDT   HBARUSDT    1.000000e+00
Length: 6642, dtype: float64
```

Is it a coincidence that the most cointegrated pairs are stablecoins? Probably not.

Remember the multiple comparisons bias? Let's test the top pairs by plotting their charts below and 
making sure that the difference between each pair of symbols bounces back and forth around its mean. 
For example, here is an analysis for the pair `(ALGOUSDT, QTUMUSDT)`:

```pycon
>>> S1, S2 = "ALGOUSDT", "QTUMUSDT"

>>> data.plot(column="Close", symbol=[S1, S2], base=1).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/rebased_price.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/rebased_price.dark.svg#only-dark){: .iimg loading=lazy }

These prices move together.

```pycon
>>> S1_log = np.log(data.get("Close", S1))  # (1)!
>>> S2_log = np.log(data.get("Close", S2))
>>> log_diff = (S2_log - S1_log).rename("Log diff")
>>> fig = log_diff.vbt.plot()
>>> fig.add_hline(y=log_diff.mean(), line_color="yellow", line_dash="dot")
>>> fig.show()
```

1. Avoid `data.close[S1]`, which creates the close series with all symbols first—it is slower!

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/price_log_diff.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/price_log_diff.dark.svg#only-dark){: .iimg loading=lazy }

The linear combination of the two prices oscillates around the mean.

## Testing

More data is always better. Let's re-fetch our pair's history with higher granularity.

```pycon
>>> DATA_FILE = "temp/data.pickle"

>>> # vbt.remove_file(DATA_FILE, missing_ok=True)
>>> if not vbt.file_exists(DATA_FILE):
...     data = vbt.BinanceData.pull(
...         [S1, S2], 
...         start=SELECT_END,  # (1)!
...         end=END, 
...         timeframe="hourly"
...     )
...     vbt.save(data, DATA_FILE)
... else:
...     data = vbt.load(DATA_FILE)

>>> print(len(data.index))
17507
```

1. The selection and testing periods should not overlap!

!!! note
    Make sure none of the tickers have been delisted.

We now have 17,507 data points for each symbol.

### Level: Researcher :satellite:

The spread represents the relative performance of both instruments. Whenever the two instruments 
move apart, the spread increases and may reach a certain threshold where we take a long position 
in the underperformer and a short position in the overachiever. This threshold is usually set to 
a number of standard deviations from the mean. All of this is done in a rolling manner, as the 
linear relationship between the instruments is always changing. We will use the prediction error of 
the [ordinary least squares (OLS)](https://en.wikipedia.org/wiki/Ordinary_least_squares), which is 
the difference between the true value and the predicted value. Fortunately, we have the 
[OLS](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/ols/#vectorbtpro.indicators.custom.ols.OLS) indicator, which can 
calculate both the prediction error and the z-score of that error:

```pycon
>>> import scipy.stats as st

>>> WINDOW = 24 * 30  # (1)!
>>> UPPER = st.norm.ppf(1 - 0.05 / 2)  # (2)!
>>> LOWER = -st.norm.ppf(1 - 0.05 / 2)

>>> S1_close = data.get("Close", S1)
>>> S2_close = data.get("Close", S2)
>>> ols = vbt.OLS.run(S1_close, S2_close, window=vbt.Default(WINDOW))
>>> spread = ols.error.rename("Spread")
>>> zscore = ols.zscore.rename("Z-score")
>>> print(pd.concat((spread, zscore), axis=1))
                              Spread   Z-score
Open time                                    
2021-01-01 00:00:00+00:00       NaN       NaN
2021-01-01 01:00:00+00:00       NaN       NaN
2021-01-01 02:00:00+00:00       NaN       NaN
2021-01-01 03:00:00+00:00       NaN       NaN
2021-01-01 04:00:00+00:00       NaN       NaN
...                             ...       ...
2022-12-31 19:00:00+00:00 -0.121450 -1.066809
2022-12-31 20:00:00+00:00 -0.123244 -1.078957
2022-12-31 21:00:00+00:00 -0.122595 -1.070667
2022-12-31 22:00:00+00:00 -0.125066 -1.088617
2022-12-31 23:00:00+00:00 -0.130532 -1.131498

[17507 rows x 2 columns]
```

1. One month window.

2. 95% significance level across two tails.

Let's plot the z-score, the two thresholds, and the points where the z-score crosses them:

```pycon
>>> upper_crossed = zscore.vbt.crossed_above(UPPER)
>>> lower_crossed = zscore.vbt.crossed_below(LOWER)

>>> fig = zscore.vbt.plot()
>>> fig.add_hline(y=UPPER, line_color="orangered", line_dash="dot")
>>> fig.add_hline(y=0, line_color="yellow", line_dash="dot")
>>> fig.add_hline(y=LOWER, line_color="limegreen", line_dash="dot")
>>> upper_crossed.vbt.signals.plot_as_exits(zscore, fig=fig)
>>> lower_crossed.vbt.signals.plot_as_entries(zscore, fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/zscore.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/zscore.dark.svg#only-dark){: .iimg loading=lazy }

If you look closely at the chart above, you will notice many signals of the same type appearing 
one after another. This happens because price fluctuations cause the price to cross each threshold 
repeatedly. This will not cause any issues if we use 
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
as our simulation method of choice, since it ignores duplicate signals by default.

Now, we need to construct proper signal arrays. Keep in mind that pairs trading involves two 
opposite positions that should be part of a single portfolio. We need to transform our crossover 
signals into two arrays: long entries and short entries, each with two columns. Whenever there is 
an upper-threshold crossover signal, we issue a short entry signal for the first asset and a long 
entry signal for the second asset. Conversely, when there is a lower-threshold crossover signal, 
we issue a long entry for the first asset and a short entry for the second asset.

```pycon
>>> long_entries = data.symbol_wrapper.fill(False)
>>> short_entries = data.symbol_wrapper.fill(False)

>>> short_entries.loc[upper_crossed, S1] = True
>>> long_entries.loc[upper_crossed, S2] = True
>>> long_entries.loc[lower_crossed, S1] = True
>>> short_entries.loc[lower_crossed, S2] = True

>>> print(long_entries.sum())
symbol
ALGOUSDT    52
QTUMUSDT    73
dtype: int64
>>> print(short_entries.sum())
symbol
ALGOUSDT    73
QTUMUSDT    52
dtype: int64
```

Now it is time to simulate our configuration! The position size of each pair should be matched by 
dollar value rather than number of shares; this way, a 5% move in one matches a 5% move in the other. 
To avoid running out of cash, we will make each order's position size dependent on the current equity. 
You might wonder why we can use [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
even though it does not support target size types. In pairs trading, a position is either opened or 
reversed (closed out and opened again with the opposite sign), so we do not need target size types 
at all—regular size types are sufficient.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     data,
...     entries=long_entries,
...     short_entries=short_entries,
...     size=10,  # (1)!
...     size_type="valuepercent100",  # (2)!
...     group_by=True,  # (3)!
...     cash_sharing=True,
...     call_seq="auto"
... )
```

1. Open a position whose value matches 10% of the current equity.

2. Size is provided as a percentage of the equity, where 1 = 1%.

3. Pack both positions into the same group, share the same cash balance, and reorder orders so sell 
orders come before buy orders to release cash early.

The simulation is complete. When working with grouped portfolios that involve rebalancing, 
we should always check the allocations first to validate them:

```pycon
>>> fig = pf.plot_allocations()
>>> rebalancing_dates = data.index[np.unique(pf.orders.idx.values)]
>>> for date in rebalancing_dates:
...     fig.add_vline(x=date, line_color="teal", line_dash="dot")
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/from_signals_allocations.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/from_signals_allocations.dark.svg#only-dark){: .iimg loading=lazy }

The result makes sense: positions in both symbols are constantly switching, almost like watching 
a chessboard. Also, the long and short allocations rarely go above the specified 10% position size. 
Now, we should calculate the portfolio statistics to evaluate the profitability of our strategy:

```pycon
>>> pf.stats()
Start                          2021-01-01 00:00:00+00:00
End                            2022-12-31 23:00:00+00:00
Period                                 729 days 11:00:00
Start Value                                        100.0
Min Value                                      96.401924
Max Value                                     127.670782
End Value                                     119.930329
Total Return [%]                               19.930329
Benchmark Return [%]                          -34.051206
Total Time Exposure [%]                        89.946878
Max Gross Exposure [%]                          12.17734
Max Drawdown [%]                                8.592299
Max Drawdown Duration                  318 days 00:00:00
Total Orders                                          34
Total Fees Paid                                      0.0
Total Trades                                          34
Win Rate [%]                                       43.75
Best Trade [%]                                160.511614
Worst Trade [%]                               -54.796964
Avg Winning Trade [%]                          41.851493
Avg Losing Trade [%]                          -20.499826
Avg Winning Trade Duration    42 days 22:04:17.142857143
Avg Losing Trade Duration               33 days 14:43:20
Profit Factor                                   1.553538
Expectancy                                      0.713595
Sharpe Ratio                                    0.782316
Calmar Ratio                                     1.10712
Omega Ratio                                     1.034258
Sortino Ratio                                   1.221721
Name: group, dtype: object
```

We gained almost 20% from the initial portfolio value—a comfortable win, especially considering 
that the benchmark is down nearly 35%. As expected, the portfolio was in a position 90% of the time; 
the only period without a position was the initial phase before the first signal. 
Even with a win rate below 50%, we were profitable because the average winning trade returned 
50% more profit than the average loss, resulting in a long-term profit of $0.70 per trade.

But how can we verify that the simulation closely mirrors the signals it is based on? 
To do this, we can reframe our problem as a portfolio optimization task: 
we will mark the points where the z-score crosses any of the thresholds as allocation points 
and assign the corresponding weights at those times. This is very straightforward using 
[PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer):

```pycon
>>> allocations = data.symbol_wrapper.fill()  # (1)!
>>> allocations.loc[upper_crossed, S1] = -0.1
>>> allocations.loc[upper_crossed, S2] = 0.1
>>> allocations.loc[lower_crossed, S1] = 0.1
>>> allocations.loc[lower_crossed, S2] = -0.1
>>> pfo = vbt.PortfolioOptimizer.from_filled_allocations(allocations)

>>> print(pfo.allocations)  # (2)!
symbol                     ALGOUSDT  QTUMUSDT
Open time                                    
2021-03-15 10:00:00+00:00       0.1      -0.1
2021-03-23 03:00:00+00:00      -0.1       0.1
2021-04-17 14:00:00+00:00       0.1      -0.1
2021-04-19 00:00:00+00:00      -0.1       0.1
2021-06-03 16:00:00+00:00       0.1      -0.1
2021-06-30 22:00:00+00:00      -0.1       0.1
2021-08-19 06:00:00+00:00       0.1      -0.1
2021-10-02 21:00:00+00:00      -0.1       0.1
2021-11-12 03:00:00+00:00       0.1      -0.1
2022-02-02 09:00:00+00:00      -0.1       0.1
2022-04-28 21:00:00+00:00       0.1      -0.1
2022-07-22 02:00:00+00:00      -0.1       0.1
2022-09-04 01:00:00+00:00       0.1      -0.1
2022-09-27 03:00:00+00:00      -0.1       0.1
2022-10-13 10:00:00+00:00       0.1      -0.1
2022-10-23 19:00:00+00:00      -0.1       0.1
2022-11-08 20:00:00+00:00       0.1      -0.1

>>> pfo.plot().show()
```

1. Build an array of weights.

2. Get a compressed allocations array.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/optimizer.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/optimizer.dark.svg#only-dark){: .iimg loading=lazy }

There is also a convenient method to simulate the optimizer:

```pycon
>>> pf = pfo.simulate(data, pf_method="from_signals")
>>> pf.total_return
0.19930328736504038
```

!!! info
    This method is based on a dynamic signal function that converts target percentages into signals, 
    so the first compilation may take up to a minute (after that, it will be extremely fast). 
    You can also omit the `pf_method` argument to use the cacheable 
    [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
    with similar results.

What about parameter optimization? There are several parts of the code that are open for parameterization.
The signal generation is affected by the `WINDOW`, `UPPER`, and `LOWER` parameters. 
The simulation part offers more flexibility; for example, you can add stops to limit your exposure 
to adverse price movements. Let's start with the signal generation.

A key challenge is how to combine the `UPPER` and `LOWER` parameters. We cannot simply pass them 
to their respective crossover functions and expect the results to align; we need to unify the resulting 
columns before combining them later. One useful approach is to build a meta-indicator that encapsulates 
other indicators and handles multiple parameter combinations automatically:

```pycon
>>> PTS_expr = """
...     PTS:
...     x = @in_close.iloc[:, 0]
...     y = @in_close.iloc[:, 1]
...     ols = vbt.OLS.run(x, y, window=@p_window, hide_params=True)
...     upper = st.norm.ppf(1 - @p_upper_alpha / 2)
...     lower = -st.norm.ppf(1 - @p_lower_alpha / 2)
...     upper_crossed = ols.zscore.vbt.crossed_above(upper)
...     lower_crossed = ols.zscore.vbt.crossed_below(lower)
...     long_entries = wrapper.fill(False)
...     short_entries = wrapper.fill(False)
...     short_entries.loc[upper_crossed, x.name] = True
...     long_entries.loc[upper_crossed, y.name] = True
...     long_entries.loc[lower_crossed, x.name] = True
...     short_entries.loc[lower_crossed, y.name] = True
...     long_entries, short_entries
... """

>>> PTS = vbt.IF.from_expr(PTS_expr, keep_pd=True, st=st)  # (1)!
>>> vbt.phelp(PTS.run)  # (2)!
PTS.run(
    close,
    window,
    upper_alpha,
    lower_alpha,
    short_name='pts',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `PTS` indicator.
    

    * Inputs: `close`

    * Parameters: `window`, `upper_alpha`, `lower_alpha`

    * Outputs: `long_entries`, `short_entries`
```

1. By default, the factory passes two-dimensional NumPy arrays. Use `keep_pd` to pass Pandas objects 
instead. Also, since expressions are processed in isolation from global variables, specify any variables 
that cannot be parsed.

2. Verify the parsed input, parameter, and output names.

Our goal was to create an indicator that accepts an input array with two columns (representing assets), 
runs our pipeline, and returns two signal arrays, each with two columns. To accomplish this, we wrote 
an expression that is standard Python code but includes some useful enhancements. For example, 
we defined the variable `close` as an input array by using the prefix `@in_`. Additionally, we 
marked the three parameters—`window`, `upper_alpha`, and `lower_alpha`—by adding the prefix `@p_`. 
When we run the expression, the indicator factory replaces `@in_close` with our close prices and 
`@p_window`, `@p_upper_alpha`, and `@p_lower_alpha` with the specified parameter values. 
The factory also automatically recognizes well-known variables such as `vbt` and `wrapper`, 
substituting them with the VBT module and the current Pandas wrapper. The last line lists the 
variables we want to return, and their names will automatically be used as output names.

Now, let's run this indicator across a grid of parameter combinations we want to test. 
Keep in mind that wide parameter grids can lead to out-of-memory errors: 
each parameter combination generates multiple arrays of the same shape as the data. 
To address this, you can use random search to efficiently reduce the number of parameter combinations.

```pycon
>>> WINDOW_SPACE = np.arange(5, 50).tolist()  # (1)!
>>> ALPHA_SPACE = (np.arange(1, 100) / 1000).tolist()  # (2)!

>>> long_entries, short_entries = data.run(  # (3)!
...     PTS, 
...     window=WINDOW_SPACE,
...     upper_alpha=ALPHA_SPACE,
...     lower_alpha=ALPHA_SPACE,
...     param_product=True,
...     random_subset=1000,  # (4)!
...     seed=42,  # (5)!
...     unpack=True  # (6)!
... )
>>> print(long_entries.columns)
MultiIndex([( 5, 0.007,  0.09, 'ALGOUSDT'),
            ( 5, 0.007,  0.09, 'QTUMUSDT'),
            ( 5, 0.009, 0.086, 'ALGOUSDT'),
            ( 5, 0.009, 0.086, 'QTUMUSDT'),
            ( 5, 0.015, 0.082, 'ALGOUSDT'),
            ...
            (49, 0.091, 0.094, 'QTUMUSDT'),
            (49, 0.094, 0.054, 'ALGOUSDT'),
            (49, 0.094, 0.054, 'QTUMUSDT'),
            (49, 0.095, 0.074, 'ALGOUSDT'),
            (49, 0.095, 0.074, 'QTUMUSDT')],
            names=['pts_window', 'pts_upper_alpha', 'pts_lower_alpha', 'symbol'], length=2000)
```

1. Test window values from 5 to 49.

2. Test alphas for the upper threshold from 0.1% to 10%.

3. Use [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) to execute an indicator.

4. Randomly select 1,000 parameter combinations.

5. Make the results deterministic by setting a seed.

6. Return the output arrays instead of an indicator instance.

We used a data instance and instructed it to inspect the `PTS` indicator, find its input names 
among the arrays stored in the data instance, and pass these arrays along with the parameters 
to the indicator. This approach allows us to avoid dealing directly with input and output names :tada:

So, which parameter combinations are the most profitable?

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     data,
...     entries=long_entries,
...     short_entries=short_entries,
...     size=10,
...     size_type="valuepercent100",
...     group_by=vbt.ExceptLevel("symbol"),  # (1)!
...     cash_sharing=True,
...     call_seq="auto"
... )

>>> opt_results = pd.concat((
...     pf.total_return,
...     pf.trades.expectancy,
... ), axis=1)
>>> print(opt_results.sort_values(by="total_return", ascending=False))
                                            total_return  expectancy
pts_window pts_upper_alpha pts_lower_alpha                          
41         0.076           0.001                0.503014    0.399218
15         0.079           0.001                0.489249    2.718049
16         0.023           0.016                0.474538    0.104986
6          0.078           0.048                0.445623    0.057574
41         0.028           0.001                0.441388    0.387182
...                                                  ...         ...
43         0.003           0.004               -0.263967   -0.131984
15         0.002           0.049               -0.273170   -0.182113
42         0.002           0.036               -0.316947   -0.110821
35         0.001           0.008               -0.330056   -0.196462
41         0.001           0.015               -0.363547   -0.191341

[1000 rows x 2 columns]
```

1. Do not group symbols; in other words, group by parameter combination.

Now, let's proceed to the second optimization step. Pick one parameter combination from above 
and test various stop configurations using [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param):

```pycon
>>> best_index = opt_results.idxmax()["expectancy"]  # (1)!
>>> best_long_entries = long_entries[best_index]
>>> best_short_entries = short_entries[best_index]
>>> STOP_SPACE = [np.nan] + np.arange(1, 100).tolist()  # (2)!

>>> pf = vbt.Portfolio.from_signals(
...     data,
...     entries=best_long_entries,
...     short_entries=best_short_entries,
...     size=10,
...     size_type="valuepercent100",
...     group_by=vbt.ExceptLevel("symbol"),
...     cash_sharing=True,
...     call_seq="auto",
...     sl_stop=vbt.Param(STOP_SPACE),  # (3)!
...     tsl_stop=vbt.Param(STOP_SPACE),
...     tp_stop=vbt.Param(STOP_SPACE),
...     delta_format="percent100",  # (4)!
...     stop_exit_price="close",  # (5)!
...     broadcast_kwargs=dict(random_subset=1000, seed=42)  # (6)!
... )

>>> opt_results = pd.concat((
...     pf.total_return,
...     pf.trades.expectancy,
... ), axis=1)
>>> print(opt_results.sort_values(by="total_return", ascending=False))
                          total_return  expectancy
sl_stop tsl_stop tp_stop                          
86.0    98.0     NaN          0.602834    2.740152
47.0    62.0     NaN          0.587525    1.632014
43.0    90.0     NaN          0.579859    1.757150
16.0    62.0     54.0         0.412477    0.448345
2.0     95.0     71.0         0.406624    0.125115
...                                ...         ...
27.0    41.0     20.0        -0.063945   -0.046337
52.0    46.0     20.0        -0.065675   -0.067706
23.0    61.0     22.0        -0.071294   -0.057495
6.0     57.0     31.0        -0.080679   -0.029232
23.0    45.0     22.0        -0.090643   -0.073099

[1000 rows x 2 columns]
```

1. Select the parameter combination with the highest expectancy.

2. Use NaN or a range from 1% to 99%.

3. The method will create the Cartesian product of all parameter arrays 
(same as using `param_product=True` earlier).

4. All stops are provided as a percentage from the entry price, where 1 = 1%.

5. All orders will be executed at the end of the bar; otherwise, we cannot use `call_seq="auto"`.

6. Randomly select 1,000 parameter combinations and make the results deterministic by setting a seed.

We can see how performance declines with lower SL and TSL values, and how the optimizer
tends to discourage the use of TP stops. Let's examine how a metric of interest depends 
on each type of stop value:

```pycon
>>> def plot_metric_by_stop(stop_name, metric_name, stat_name, smooth):
...     from scipy.signal import savgol_filter
...
...     values = pf.deep_getattr(metric_name)  # (1)!
...     values = values.vbt.select_levels(stop_name)  # (2)!
...     values = getattr(values.groupby(values.index), stat_name)()  # (3)!
...     smooth_values = savgol_filter(values, smooth, 1)  # (4)!
...     smooth_values = values.vbt.wrapper.wrap(smooth_values)  # (5)!
...     fig = values.rename(metric_name).vbt.plot()
...     smooth_values.rename(f"{metric_name} (smoothed)").vbt.plot(
...         trace_kwargs=dict(line=dict(dash="dot", color="yellow")),
...         fig=fig, 
...     )
...     return fig
```

1. This method allows us to access deeply chained attributes in the portfolio object,
such as `trades.winning.pnl.mean` for the average P&L of winning trades.

2. The index of any metric is a `pd.MultiIndex` with three levels, one for each parameter.
Let's select just one parameter (stop type) for analysis.

3. Since we tested a product of multiple parameters, each stop value might have multiple observations.
Aggregate these observations using a statistic of interest.

4. Apply the [Savitzky–Golay filter](https://en.wikipedia.org/wiki/Savitzky%E2%80%93Golay_filter)
to smooth the values.

5. Because the filter returns a NumPy array, wrap it as a Pandas Series with the same index as our values.

=== "SL"

    ```pycon
    >>> plot_metric_by_stop(
    ...     "sl_stop", 
    ...     "trades.expectancy", 
    ...     "median",
    ...     10
    ... ).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/SL.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/SL.dark.svg#only-dark){: .iimg loading=lazy }

=== "TSL"

    ```pycon
    >>> plot_metric_by_stop(
    ...     "tsl_stop", 
    ...     "trades.expectancy", 
    ...     "median",
    ...     10
    ... ).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/TSL.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/TSL.dark.svg#only-dark){: .iimg loading=lazy }

=== "TP"

    ```pycon
    >>> plot_metric_by_stop(
    ...     "tp_stop", 
    ...     "trades.expectancy", 
    ...     "median",
    ...     10
    ... ).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/TP.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/TP.dark.svg#only-dark){: .iimg loading=lazy }

Now we have a general overview of how stop orders impact strategy performance.

### Level: Engineer :satellite_orbital:

The approach above works well if you are in the strategy discovery phase.
Pandas and VBT's high-level API enable rapid and easy experimentation.
However, once you finish building a general framework for your strategy, you should 
focus on optimizing for the best CPU and memory performance. This will allow you 
to test the strategy on more assets, longer periods, and more parameter combinations.
Previously, we could test multiple combinations by loading them all into memory.
The only reason we did not encounter issues was thanks to using random search.
To make parameter search more practical, we should both speed up our backtests 
and reduce memory usage.

Let's begin by rewriting our indicator strictly with Numba:

```pycon
>>> @njit(nogil=True)  # (1)!
... def pt_signals_nb(close, window=WINDOW, upper=UPPER, lower=LOWER):
...     x = np.expand_dims(close[:, 0], 1)  # (2)!
...     y = np.expand_dims(close[:, 1], 1)
...     _, _, zscore = vbt.ind_nb.ols_nb(x, y, window)  # (3)!
...     zscore_1d = zscore[:, 0]  # (4)!
...     upper_ts = np.full_like(zscore_1d, upper, dtype=float_)  # (5)!
...     lower_ts = np.full_like(zscore_1d, lower, dtype=float_)
...     upper_crossed = vbt.nb.crossed_above_1d_nb(zscore_1d, upper_ts)  # (6)!
...     lower_crossed = vbt.nb.crossed_above_1d_nb(lower_ts, zscore_1d)  # (7)!
...     long_entries = np.full_like(close, False, dtype=np.bool_)  # (8)!
...     short_entries = np.full_like(close, False, dtype=np.bool_)
...     short_entries[upper_crossed, 0] = True  # (9)!
...     long_entries[upper_crossed, 1] = True
...     long_entries[lower_crossed, 0] = True
...     short_entries[lower_crossed, 1] = True
...     return long_entries, short_entries
```

1. Releases the GIL, enabling multithreading in the future.

2. Select the specific column and expand to two dimensions as required for the next function.

3. This is a Numba-compiled function to run OLS. Every VBT indicator function without the `_1d` suffix 
expects two-dimensional input.

4. Two dimensions in, two dimensions out. Since no more functions below require two-dimensional arrays, 
reduce it to one dimension.

5. Crossover functions need arrays of the same shape, but we want to compare against a single value,
so create an array of the same shape as the single value.

6. Notice the `_1d` suffix? The function expects a one-dimensional input.

7. A crosses below B is equivalent to B crossing above A.

8. The output arrays have the same shape as the input array.

9. Numba does not support labels, so set the target column using its integer index.

Next, let's verify that our indicator produces the same number of signals as before:

```pycon
>>> long_entries, short_entries = pt_signals_nb(data.close.values)  # (1)!
>>> long_entries = data.symbol_wrapper.wrap(long_entries)  # (2)!
>>> short_entries = data.symbol_wrapper.wrap(short_entries)

>>> print(long_entries.sum())
symbol
ALGOUSDT    52
QTUMUSDT    73
dtype: int64
>>> print(short_entries.sum())
symbol
ALGOUSDT    73
QTUMUSDT    52
dtype: int64
```

1. Numba does not accept Pandas objects, so extract the NumPy array.

2. The function returns NumPy arrays, so wrap them back into the Pandas format.

The signals align perfectly.

Although this function is faster than the expression-based version, it does not solve the memory 
issue because output arrays must still be stored in memory to be passed to the simulator—especially 
when running multiple parameter combinations. To address this, we can wrap both the signal 
generation and simulation steps into a single pipeline and have it return lightweight arrays, such 
as just the total return. By leveraging an effective chunking approach, 
we could run nearly unlimited parameter combinations! :kite:

The next step is to rewrite the simulation part using Numba:

```pycon
>>> @njit(nogil=True)
... def pt_portfolio_nb(
...     open, 
...     high, 
...     low, 
...     close,
...     long_entries,
...     short_entries,
...     sl_stop=np.nan,
...     tsl_stop=np.nan,
...     tp_stop=np.nan,
... ):
...     target_shape = close.shape  # (1)!
...     group_lens = np.array([2])  # (2)!
...     sim_out = vbt.pf_nb.from_signals_nb(  # (3)!
...         target_shape=target_shape,
...         group_lens=group_lens,
...         auto_call_seq=True,  # (4)!
...         open=open,
...         high=high,
...         low=low,
...         close=close,
...         long_entries=long_entries,
...         short_entries=short_entries,
...         size=10,
...         size_type=vbt.pf_enums.SizeType.ValuePercent100,  # (5)!
...         sl_stop=sl_stop,
...         tsl_stop=tsl_stop,
...         tp_stop=tp_stop,
...         delta_format=vbt.pf_enums.DeltaFormat.Percent100,
...         stop_exit_price=vbt.pf_enums.StopExitPrice.Close
...     )
...     return sim_out
```

1. Target shape refers to the shape over which the simulator "walks," matching the shape of
our OHLC arrays.

2. One group consists of two columns. When any group has more than one column,
cash sharing is automatically enabled.

3. There are two available functions: [from_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signals_nb)
and [from_signal_func_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signal_func_nb).
Since we already have the signal arrays, we should use the first function.

4. This is equivalent to using `call_seq="auto"` as before.

5. Numba functions do not accept enumerated types as strings, only as integers.

Let's run it:

```pycon
>>> sim_out = pt_portfolio_nb(
...     data.open.values,
...     data.high.values,
...     data.low.values,
...     data.close.values,
...     long_entries.values,
...     short_entries.values
... )
```

The output of this function is an instance of the 
[SimulationOutput](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput) type,
which you can use to construct a new [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
instance for analysis:

```pycon
>>> pf = vbt.Portfolio(
...     data.symbol_wrapper.regroup(group_by=True),  # (1)!
...     sim_out,  # (2)!
...     open=data.open,  # (3)!
...     high=data.high,
...     low=data.low,
...     close=data.close,
...     cash_sharing=True,
...     init_cash=100  # (4)!
... )

>>> print(pf.total_return)
0.19930328736504038
```

1. The first argument should be the wrapper. Be sure to enable grouping using the same `group_by` setting as before.

2. The second argument should be either the order records or the simulation output.

3. We must provide the arguments used during simulation, since this information
is not stored in the simulation output.

4. The default initial cash for the simulator is $100. If we do not provide it here, the portfolio
will set it based on the order records, which is a more expensive operation.

Now we need a Numba-compiled version of the analysis part:

```pycon
>>> @njit(nogil=True)
... def pt_metrics_nb(close, sim_out):
...     target_shape = close.shape
...     group_lens = np.array([2])
...     filled_close = vbt.nb.fbfill_nb(close)  # (1)!
...     col_map = vbt.rec_nb.col_map_nb(  # (2)!
...         col_arr=sim_out.order_records["col"], 
...         n_cols=target_shape[1]
...     )
...     total_profit = vbt.pf_nb.total_profit_nb(  # (3)!
...         target_shape=target_shape,
...         close=filled_close,
...         order_records=sim_out.order_records,
...         col_map=col_map
...     )
...     total_profit_grouped = vbt.pf_nb.total_profit_grouped_nb(  # (4)!
...         total_profit=total_profit,
...         group_lens=group_lens,
...     )[0]  # (5)!
...     total_return = total_profit_grouped / 100  # (6)!
...     trade_records = vbt.pf_nb.get_exit_trades_nb(  # (7)!
...         order_records=sim_out.order_records, 
...         close=filled_close, 
...         col_map=col_map
...     )
...     trade_records = trade_records[  # (8)!
...         trade_records["status"] == vbt.pf_enums.TradeStatus.Closed
...     ]
...     expectancy = vbt.pf_nb.expectancy_reduce_nb(  # (9)!
...         pnl_arr=trade_records["pnl"]
...     )
...     return total_return, expectancy
```

1. Forward-fill and backward-fill the close price to avoid NaNs in the equity.

2. The column mapper aggregates order records per column for easier mapping.

3. Total profit is calculated directly from the order records. This is one of the few metrics
that does not require any intermediate arrays, making it lightweight and fast.

4. The total profit calculated above is per column. We need to aggregate it by group.

5. Since this function returns an array and we have only one group, extract the sole value.

6. Total return is calculated from total profit using the initial value, which is $100.

7. Aggregate order records into trade records to extract P&L information.

8. Ignore any open trades.

9. Calculate expectancy. Normally this function runs per column. Since
we have only two columns that belong to the same group, we can run it directly.

Do not let this function intimidate you! We have calculated our metrics using only
the close price and order records—a process called "reconstruction."
The principle is simple: start with the metric you want, identify the information it requires,
find the function that provides that information, and repeat as needed.

Let's validate it:

```pycon
>>> pt_metrics_nb(data.close.values, sim_out)
(0.19930328736504038, 0.7135952049405152)
```

100% accuracy.

Finally, let's combine all parts into a single pipeline and benchmark it:

```pycon
>>> @njit(nogil=True)
... def pt_pipeline_nb(
...     open, 
...     high, 
...     low, 
...     close,
...     window=WINDOW,  # (1)!
...     upper=UPPER,
...     lower=LOWER,
...     sl_stop=np.nan,
...     tsl_stop=np.nan,
...     tp_stop=np.nan,
... ):
...     long_entries, short_entries = pt_signals_nb(
...         close, 
...         window=window, 
...         upper=upper, 
...         lower=lower
...     )
...     sim_out = pt_portfolio_nb(
...         open,
...         high,
...         low,
...         close,
...         long_entries,
...         short_entries,
...         sl_stop=sl_stop,
...         tsl_stop=tsl_stop,
...         tp_stop=tp_stop
...     )
...     return pt_metrics_nb(close, sim_out)

>>> pt_pipeline_nb(
...     data.open.values,
...     data.high.values,
...     data.low.values,
...     data.close.values
... )
(0.19930328736504038, 0.7135952049405152)

>>> %%timeit
... pt_pipeline_nb(
...     data.open.values,
...     data.high.values,
...     data.low.values,
...     data.close.values
... )
5.4 ms ± 13.1 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

1. Use our global defaults as default arguments.

Only 5 milliseconds per complete backtest :fire:

This performance encourages us to try intensive parameter optimization. Fortunately,
there are several ways to do this. The most convenient is to wrap the pipeline with the
[@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized)
decorator, which builds the parameter grid and calls the pipeline function on every parameter
combination. Since all our Numba functions release the GIL, we can use multithreading!
Let's test all parameter combinations for signal generation by breaking them into chunks
and distributing combinations within each chunk. This process took about 5 minutes on Apple Silicon.

!!! important
    Keep in mind that `@parameterized` builds the entire parameter grid even when a random subset
    is specified, which can take a significant amount of time. For example, 6 parameters with 100
    values each will create a grid with `100 ** 6`, or one trillion combinations, which is far
    too many to combine.

```pycon
>>> param_pt_pipeline = vbt.parameterized(  # (1)!
...     pt_pipeline_nb, 
...     merge_func="concat",  # (2)!
...     seed=42,
...     engine="threadpool",  # (3)!
...     chunk_len="auto"
... )

>>> UPPER_SPACE = [st.norm.ppf(1 - x / 2) for x in ALPHA_SPACE]  # (4)!
>>> LOWER_SPACE = [-st.norm.ppf(1 - x / 2) for x in ALPHA_SPACE]
>>> POPT_FILE = "temp/param_opt.pickle"

>>> # vbt.remove_file(POPT_FILE, missing_ok=True)
>>> if not vbt.file_exists(POPT_FILE):
...     param_opt = param_pt_pipeline(
...         data.open.values,
...         data.high.values,
...         data.low.values,
...         data.close.values,
...         window=vbt.Param(WINDOW_SPACE),
...         upper=vbt.Param(UPPER_SPACE),
...         lower=vbt.Param(LOWER_SPACE)
...     )
...     vbt.save(param_opt, POPT_FILE)
... else:
...     param_opt = vbt.load(POPT_FILE)

>>> total_return, expectancy = param_opt  # (5)!
```

1. You can either decorate the function directly with `@`, or call the decorator with the function as
the first argument.

2. Concatenate each output across all parameter combinations.

3. Create a pool with the same number of threads as there are CPU cores.

4. Convert the alphas to z-score thresholds.

5. The format is the same as that returned by our pipeline.

[=100% "Chunk 55131/55131"]{: .candystripe .candystripe-animate }

Now, let's analyze the total return:

```pycon
>>> print(total_return)
window  upper     lower    
5       3.290527  -3.290527    0.000000
                  -3.090232    0.000000
                  -2.967738    0.000000
                  -2.878162    0.000000
                  -2.807034    0.000000
                                    ...
49      1.649721  -1.669593    0.196197
                  -1.664563    0.192152
                  -1.659575    0.190713
                  -1.654628    0.201239
                  -1.649721    0.204764
Length: 441045, dtype: float64

>>> grouped_metric = total_return.groupby(level=["upper", "lower"]).mean()
>>> grouped_metric.vbt.heatmap(
...     trace_kwargs=dict(colorscale="RdBu", zmid=0),
...     yaxis=dict(autorange="reversed")
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/popt_total_return.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/popt_total_return.dark.svg#only-dark){: .iimg loading=lazy }

The effect of the thresholds appears to be asymmetric: the highest total return is achieved for the lowest
upper threshold and the lowest lower threshold. This means the neutral range between the two thresholds
is shifted downward as much as possible.

However, the `@parameterized` decorator has two key limitations: it runs only one parameter combination
at a time, and it must build the entire parameter grid, even when evaluating a random subset.
Why is running one combination at a time a limitation? Because only a few pipeline calls can be
parallelized at once. Even with the special argument `distribute="chunks"` that allows you
to parallelize entire chunks, calls within each chunk occur serially in a regular Python loop.
This is not ideal for multithreading, which requires the GIL to be released for the entire
procedure. If you try multiprocessing, the method must serialize all arguments (including the data)
on each pipeline call, which adds significant overhead. This may still be faster than the above approach
in some scenarios.

To process multiple parameter combinations within each thread, we need to split them into chunks and
write a parent Numba function that loops over the combinations in each chunk. Then, we can
parallelize this parent function using multithreading. The steps are: 1) manually construct
the parameter grid, 2) split it into chunks, and 3) iterate over the chunks and pass each to the parent
function for execution. The first step can be accomplished with
[combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params), which is the :anatomical_heart:
of the `@parameterized` decorator. The second and third steps can be done using another decorator,
[@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked), which specializes in argument
chunking. Let's do that! :muscle:

```pycon
>>> @njit(nogil=True)
... def pt_pipeline_mult_nb(
...     n_params: int,  # (1)!
...     open:     tp.Array2d,  # (2)!
...     high:     tp.Array2d, 
...     low:      tp.Array2d, 
...     close:    tp.Array2d,
...     window:   tp.FlexArray1dLike = WINDOW,  # (3)!
...     upper:    tp.FlexArray1dLike = UPPER,
...     lower:    tp.FlexArray1dLike = LOWER,
...     sl_stop:  tp.FlexArray1dLike = np.nan,
...     tsl_stop: tp.FlexArray1dLike = np.nan,
...     tp_stop:  tp.FlexArray1dLike = np.nan,
... ):
...     window_ = vbt.to_1d_array_nb(np.asarray(window))  # (4)!
...     upper_ = vbt.to_1d_array_nb(np.asarray(upper))
...     lower_ = vbt.to_1d_array_nb(np.asarray(lower))
...     sl_stop_ = vbt.to_1d_array_nb(np.asarray(sl_stop))
...     tsl_stop_ = vbt.to_1d_array_nb(np.asarray(tsl_stop))
...     tp_stop_ = vbt.to_1d_array_nb(np.asarray(tp_stop))
...
...     total_return = np.empty(n_params, dtype=float_)  # (5)!
...     expectancy = np.empty(n_params, dtype=float_)
...
...     for i in range(n_params):  # (6)!
...         total_return[i], expectancy[i] = pt_pipeline_nb(
...             open,
...             high,
...             low,
...             close,
...             window=vbt.flex_select_1d_nb(window_, i),  # (7)!
...             upper=vbt.flex_select_1d_nb(upper_, i),
...             lower=vbt.flex_select_1d_nb(lower_, i),
...             sl_stop=vbt.flex_select_1d_nb(sl_stop_, i),
...             tsl_stop=vbt.flex_select_1d_nb(tsl_stop_, i),
...             tp_stop=vbt.flex_select_1d_nb(tp_stop_, i),
...         )
...     return total_return, expectancy
```

1. Number of parameter combinations in the chunk. This should be equal to the length of each parameter array.

2. These are called [type hints](https://realpython.com/lessons/type-hinting/):
they are not used by VBT (yet), but they help you quickly understand the expected argument format.
Type hint `Array2d` refers to a two-dimensional NumPy array.

3. The type hint `FlexArray1dLike` means either a single value or any NumPy array
that broadcasts against `n_params`.

4. The required function below expects each parameter to be a one-dimensional NumPy array,
so we convert each to 1D in case it is not already. The result is saved to a new variable.

5. Create empty NumPy arrays to return. We will populate them gradually in the loop below.

6. Iterate over all parameter combinations in the chunk.

7. Select the parameter value for the current combination.

The magic of this function is that you do not need to make all parameters into arrays.
Thanks to flexible indexing, you can pass some parameters as arrays and keep others
at their default values. For example, you can test three window combinations like this:

```pycon
>>> pt_pipeline_mult_nb(
...     3,
...     data.open.values,
...     data.high.values,
...     data.low.values,
...     data.close.values,
...     window=np.array([10, 20, 30])
... )
(array([ 0.11131525, -0.04819178,  0.13124959]),
  array([ 0.01039436, -0.00483853,  0.01756337]))
```

Next, we will wrap the function with the `@chunked` decorator. Unlike `@parameterized`, we must specify
not only the chunks but also where to get the total number of parameter combinations and how to split
each individual argument. To help with this, VBT provides a collection of annotation classes.
For example, we can instruct `@chunked` to obtain the total number of combinations from the argument
`n_params` by annotating this argument with the class [ArgSizer](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ArgSizer).
Then, we annotate each parameter as a flexible one-dimensional array using the class
[FlexArraySlicer](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.FlexArraySlicer). When `@chunked`
builds a new chunk, it will "slice" the corresponding subset of values from each parameter array.

```pycon
>>> chunked_pt_pipeline = vbt.chunked(
...     pt_pipeline_mult_nb,
...     size=vbt.ArgSizer(arg_query="n_params"),
...     arg_take_spec=dict(
...         n_params=vbt.CountAdapter(),
...         open=None,  # (1)!
...         high=None,
...         low=None,
...         close=None,
...         window=vbt.FlexArraySlicer(),
...         upper=vbt.FlexArraySlicer(),
...         lower=vbt.FlexArraySlicer(),
...         sl_stop=vbt.FlexArraySlicer(),
...         tsl_stop=vbt.FlexArraySlicer(),
...         tp_stop=vbt.FlexArraySlicer()
...     ),
...     chunk_len=1000,  # (2)!
...     merge_func="concat",  # (3)!
...     execute_kwargs=dict(  # (4)!
...         chunk_len="auto",
...         engine="threadpool"
...     )
... )
```

1. Set arguments that should be passed without chunking to None.

2. At most 1,000 parameter combinations are passed to `pt_pipeline_mult_nb` at once.

3. Concatenate the resulting arrays.

4. In addition to chunking the parameter arrays, we can also bundle chunks into "super chunks".
Each super chunk consists of as many chunks as there are CPU cores, with one chunk per thread.

Here is what happens: whenever you call `chunked_pt_pipeline`, it retrieves the total number
of parameter combinations from the argument `n_params`. Chunks of length 1,000 are created,
and each chunk is sliced from the parameter arguments; one chunk corresponds to a single call to
`pt_pipeline_mult_nb`. To parallelize chunk execution, we group them into super chunks.
Chunks inside each super chunk are processed in parallel, while super chunks themselves run
serially. The progress bar reflects the progress of super chunks.

Now, let's build the complete parameter grid and run our function:

```pycon
>>> param_product, param_index = vbt.combine_params(  # (1)!
...     dict(
...         window=vbt.Param(WINDOW_SPACE),  # (2)!
...         upper=vbt.Param(UPPER_SPACE),
...         lower=vbt.Param(LOWER_SPACE)
...     )
... )

>>> COPT_FILE = "temp/chunked_opt.pickle"

>>> # vbt.remove_file(COPT_FILE, missing_ok=True)
>>> if not vbt.file_exists(COPT_FILE):
...     chunked_opt = chunked_pt_pipeline(
...         len(param_index),  # (3)!
...         data.open.values,
...         data.high.values,
...         data.low.values,
...         data.close.values,
...         window=param_product["window"],
...         upper=param_product["upper"],
...         lower=param_product["lower"]
...     )
...     vbt.save(chunked_opt, COPT_FILE)
... else:
...     chunked_opt = vbt.load(COPT_FILE)
```

1. Returns a dictionary of new parameter arrays and the associated index.

2. Uses the same format as for `@parameterized`.

3. Total number of parameter combinations.

[=100% "Super chunk 56/56"]{: .candystripe .candystripe-animate }

This approach runs about 40% faster than the previous method because the overhead of spawning a thread
is proportionally much lower compared to the larger workload within each thread. Additionally, memory
usage does not increase, since `pt_pipeline_mult_nb` still processes only one parameter combination at a time.

```pycon
>>> total_return, expectancy = chunked_opt

>>> total_return = pd.Series(total_return, index=param_index)  # (1)!
>>> expectancy = pd.Series(expectancy, index=param_index)
```

1. The parameter index becomes the index for each array.

There are only two limitations left: generating parameter combinations takes a significant amount of time,
and if execution is interrupted, all optimization results are lost. These issues can be addressed by creating
a while-loop that, at each step, generates a subset of parameter combinations, executes them,
and saves the results to disk. This continues until all combinations have been processed successfully.
Another advantage is that you can resume from where execution last stopped, or even run the
optimization until you find a satisfactory set of parameters.

Suppose you want to include the stop parameters as well, but only execute a random subset of parameter
combinations. Generating all of them with [combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params)
would not be feasible:

```pycon
>>> GRID_LEN = len(WINDOW_SPACE) * \
...     len(UPPER_SPACE) * \
...     len(LOWER_SPACE) * \
...     len(STOP_SPACE) ** 3
>>> print(GRID_LEN)
441045000000
```

There are nearly half a trillion parameter combinations :face_exhaling:

Instead, we can select parameter combinations more efficiently using the
[pick_from_param_grid](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.pick_from_param_grid) function.
This function takes a dictionary of parameter spaces (order matters), as well as the position of the
combination of interest, and returns the specific parameter combination at that position.
For example, to pick the combination at index 123,456,789:

```pycon
>>> GRID = dict(
...     window=WINDOW_SPACE,
...     upper=UPPER_SPACE,
...     lower=LOWER_SPACE,
...     sl_stop=STOP_SPACE,
...     tsl_stop=STOP_SPACE,
...     tp_stop=STOP_SPACE,
... )
>>> vbt.pprint(vbt.pick_from_param_grid(GRID, 123_456_789))
dict(
    window=5,
    upper=3.090232306167813,
    lower=-2.241402727604947,
    sl_stop=45.0,
    tsl_stop=67.0,
    tp_stop=89.0
)
```

This gives you the exact parameter combination you would get by combining all parameters and running
`param_index[123_456_789]`, but with almost no impact on performance or memory!

Now, we can build our while-loop. Let's perform a random parameter search until we get at least
100 values with expectancy of 1 or more!

```pycon
>>> FOUND_FILE = "temp/found.pickle"
>>> BEST_N = 100  # (1)!
>>> BEST_TH = 1.0  # (2)!
>>> CHUNK_LEN = 10_000  # (3)!

>>> # vbt.remove_file(FOUND_FILE, missing_ok=True)
>>> if vbt.file_exists(FOUND_FILE):
...     found = vbt.load(FOUND_FILE)  # (4)!
>>> else:
...     found = None
>>> with (  # (5)!
...     vbt.ProgressBar(
...         desc="Found", 
...         initial=0 if found is None else len(found),
...         total=BEST_N
...     ) as pbar1,
...     vbt.ProgressBar(
...         desc="Processed"
...     ) as pbar2
... ):
...     while found is None or len(found) < BEST_N:  # (6)!
...         param_df = pd.DataFrame([
...             vbt.pick_from_param_grid(GRID)  # (7)!
...             for _ in range(CHUNK_LEN)
...         ])
...         param_index = pd.MultiIndex.from_frame(param_df)
...         _, expectancy = chunked_pt_pipeline(  # (8)!
...             CHUNK_LEN,
...             data.open.values,
...             data.high.values,
...             data.low.values,
...             data.close.values,
...             window=param_df["window"],
...             upper=param_df["upper"],
...             lower=param_df["lower"],
...             sl_stop=param_df["sl_stop"],
...             tsl_stop=param_df["tsl_stop"],
...             tp_stop=param_df["tp_stop"],
...             _chunk_len=None,
...             _execute_kwargs=dict(
...                 chunk_len=None
...             )
...         )
...         expectancy = pd.Series(expectancy, index=param_index)
...         best_mask = expectancy >= BEST_TH
...         if best_mask.any():  # (9)!
...             best = expectancy[best_mask]
...             if found is None:
...                 found = best
...             else:
...                 found = pd.concat((found, best))
...                 found = found[~found.index.duplicated(keep="first")]
...             vbt.save(found, FOUND_FILE)
...             pbar1.update_to(len(found))
...             pbar1.refresh()
...         pbar2.update(len(expectancy))
```

1. Number of best parameter combinations to search for.

2. Expectancy threshold after which a parameter combination is considered the best.

3. Number of parameter combinations to process during each iteration of the while-loop.

4. Load cached results from previous execution(s).

5. Create two progress bars: one shows how many best parameter combinations have been found, 
and the other shows the total number of parameter combinations processed.

6. Run the loop until the requested number of best parameter combinations have been found.

7. Generate a random parameter combination `CHUNK_LEN` times.

8. Backtest the generated parameter combinations using chunking.

9. If any parameter combinations that meet the condition are found, concatenate them with others, 
cache to disk, and update the first progress bar.

[=100% "Found 100/100"]{: .candystripe .candystripe-animate }

You can now run the cell above, stop execution at any time, and resume later :date:

By grouping similar parameter combinations into the same bucket, you can also aggregate them
to derive a single optimal parameter combination:

```pycon
>>> def get_param_median(param):  # (1)!
...     return found.index.get_level_values(param).to_series().median()

>>> pt_pipeline_nb(
...     data.open.values, 
...     data.high.values, 
...     data.low.values, 
...     data.close.values,
...     window=int(get_param_median("window")),
...     upper=get_param_median("upper"),
...     lower=get_param_median("lower"),
...     sl_stop=get_param_median("sl_stop"),
...     tsl_stop=get_param_median("tsl_stop"),
...     tp_stop=get_param_median("tp_stop")
... )
(0.24251123364060986, 1.7316489316495804)
```

1. Get the median of the parameter values.

You can see that the median parameter combination also meets our expectancy condition.

However, sometimes you do not need to test so many parameter combinations. You can simply use
an established parameter optimization framework like [Optuna](https://optuna.org/).
This approach offers several advantages: you can use the original `pt_pipeline_nb` function
without decorators, you do not need to handle large parameter grids, and you can apply
various statistical strategies to improve search effectiveness and reduce the number
of parameter combinations you need to test.

!!! info
    Make sure to install Optuna before running the following cell.

```pycon
>>> import optuna

>>> optuna.logging.disable_default_handler()
>>> optuna.logging.set_verbosity(optuna.logging.WARNING)

>>> def objective(trial):
...     window = trial.suggest_categorical("window", WINDOW_SPACE)  # (1)!
...     upper = trial.suggest_categorical("upper", UPPER_SPACE)  # (2)!
...     lower = trial.suggest_categorical("lower", LOWER_SPACE)
...     sl_stop = trial.suggest_categorical("sl_stop", STOP_SPACE)
...     tsl_stop = trial.suggest_categorical("tsl_stop", STOP_SPACE)
...     tp_stop = trial.suggest_categorical("tp_stop", STOP_SPACE)
...     total_return, expectancy = pt_pipeline_nb(
...         data.open.values,
...         data.high.values,
...         data.low.values,
...         data.close.values,
...         window=window,
...         upper=upper,
...         lower=lower,
...         sl_stop=sl_stop,
...         tsl_stop=tsl_stop,
...         tp_stop=tp_stop
...     )
...     if np.isnan(total_return):
...         raise optuna.TrialPruned()  # (3)!
...     if np.isnan(expectancy):
...         raise optuna.TrialPruned()
...     return total_return, expectancy

>>> study = optuna.create_study(directions=["maximize", "maximize"])  # (4)!
>>> study.optimize(objective, n_trials=1000)  # (5)!

>>> trials_df = study.trials_dataframe(attrs=["params", "values"])
>>> trials_df.set_index([
...     "params_window", 
...     "params_upper", 
...     "params_lower",
...     "params_sl_stop",
...     "params_tsl_stop",
...     "params_tp_stop"
... ], inplace=True)
>>> trials_df.index.rename([
...     "window", 
...     "upper", 
...     "lower",
...     "sl_stop",
...     "tsl_stop",
...     "tp_stop"
... ], inplace=True)
>>> trials_df.columns = ["total_return", "expectancy"]
>>> trials_df = trials_df[~trials_df.index.duplicated(keep="first")]
>>> print(trials_df.sort_values(by="total_return", ascending=False))
                                                    total_return  expectancy
window upper    lower     sl_stop tsl_stop tp_stop                          
44     1.746485 -3.072346 9.0     67.0     55.0         0.558865    0.184924
42     1.871392 -3.286737 53.0    98.0     55.0         0.500062    0.330489
                                            77.0         0.496029    0.334432
5      1.746485 -1.759648 76.0    94.0     45.0         0.492721    0.043832
43     1.807618 -3.192280 87.0    36.0     60.0         0.475732    0.229682
...                                                          ...         ...
7      2.639845 -3.072346 80.0    95.0     55.0              NaN         NaN
5      3.117886 -3.072346 78.0    90.0     47.0              NaN         NaN
        2.769169 -3.072346 78.0    90.0     55.0              NaN         NaN
7      3.098951 -3.072346 78.0    95.0     55.0              NaN         NaN
        2.607536 -3.072346 78.0    95.0     77.0              NaN         NaN

[892 rows x 2 columns]
```

1. You can also use `suggest_int` here (pass two bounds instead of a list).

2. You can also use `suggest_float` here (pass two bounds instead of a list).

3. Prune the trial because Optuna does not tolerate NaNs.

4. Maximize both metrics.

5. Run 1,000 non-pruned trials.

The only downside of this approach is that you are limited by the chosen results and cannot 
explore the entire parameter landscape, which is something VBT is fully designed for.

### Level: Architect :flying_saucer:

Suppose you want full control over execution, such as allowing at most
one rebalancing operation within N days. Additionally, you want to avoid pre-calculating
arrays and do everything in an event-driven manner. For simplicity, let's switch our 
signaling algorithm from cointegration with OLS to a basic distance measure: log prices.

We will implement the strategy as a custom signal function for [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
This is the most challenging approach because the signal function is called for each column,
while our decisions need to be made per segment (i.e., once for both columns). A good strategy
is to perform calculations for the first column processed at each bar, store results in some
temporary arrays, and then access those arrays from each column to return the signals.
An ideal place to store these arrays is the built-in named tuple `in_outputs`, accessible
both during simulation and analysis.

```pycon
>>> InOutputs = namedtuple("InOutputs", ["spread", "zscore"])  # (1)!

>>> @njit(nogil=True, boundscheck=True)  # (2)!
... def can_execute_nb(c, wait_days):  # (3)!
...     if c.order_counts[c.col] == 0:  # (4)!
...         return True
...     last_order = c.order_records[c.order_counts[c.col] - 1, c.col]  # (5)!
...     ns_delta = c.index[c.i] - c.index[last_order.idx]  # (6)!
...     if ns_delta >= wait_days * vbt.dt_nb.d_ns:  # (7)!
...         return True
...     return False

>>> @njit(nogil=True, boundscheck=True)
... def create_signals_nb(c, upper, lower, wait_days):  # (8)!
...     _upper = vbt.pf_nb.select_nb(c, upper)  # (9)!
...     _lower = vbt.pf_nb.select_nb(c, lower)
...     _wait_days = vbt.pf_nb.select_nb(c, wait_days)
...
...     if c.i > 0:  # (10)!
...         prev_zscore = c.in_outputs.zscore[c.i - 1, c.group]
...         zscore = c.in_outputs.zscore[c.i, c.group]
...         if prev_zscore < _upper and zscore > _upper:  # (11)!
...             if can_execute_nb(c, _wait_days):
...                 if c.col % 2 == 0:
...                     return False, False, True, False  # (12)!
...                 return True, False, False, False  # (13)!
...         if prev_zscore > _lower and zscore < _lower:
...             if can_execute_nb(c, _wait_days):
...                 if c.col % 2 == 0:
...                     return True, False, False, False
...                 return False, False, True, False
...     return False, False, False, False  # (14)!

>>> @njit(nogil=True, boundscheck=True)
... def signal_func_nb(c, window, upper, lower, wait_days):  # (15)!
...     _window = vbt.pf_nb.select_nb(c, window)
...         
...     if c.col % 2 == 0:  # (16)!
...         x = vbt.pf_nb.select_nb(c, c.close, col=c.col)  # (17)!
...         y = vbt.pf_nb.select_nb(c, c.close, col=c.col + 1)
...         c.in_outputs.spread[c.i, c.group] = np.log(y) - np.log(x)  # (18)!
...         
...         window_start = c.i - _window + 1  # (19)!
...         window_end = c.i + 1  # (20)!
...         if window_start >= 0:  # (21)!
...             s = c.in_outputs.spread[window_start : window_end, c.group]
...             s_mean = np.nanmean(s)
...             s_std = np.nanstd(s)
...             c.in_outputs.zscore[c.i, c.group] = (s[-1] - s_mean) / s_std
...     return create_signals_nb(c, upper, lower, wait_days)
```

1. Defines a named tuple containing two arrays: spread and its z-score.

2. It is always recommended to enable bound checking while developing a signal function 
to avoid out-of-bounds errors; however, be aware this may slightly reduce performance.

3. Helper Numba function to check whether orders can be placed at the current bar.

4. Checks if this is the asset's first order in the simulation.

5. Retrieves the last order if this is not the asset's first order in the simulation.

6. Gets the elapsed time in nanoseconds between the current bar and the bar of the last order.

7. Checks if the elapsed time is at least `wait_days`. Since we compare nanoseconds,
we must multiply `wait_days` by one day's nanoseconds to get the value in nanoseconds.

8. Another helper Numba function to create signals from the available spread and z-score.

9. Our parameters are flexible arrays rather than single values, so we need to select 
the element corresponding to the current row and column.

10. Crossovers are based on the current and previous value, so ensure the previous value is available.

11. Very basic crossover comparison.

12. Short entry.

13. Long entry.

14. Do not forget this line; otherwise, the function will return None and cause an ugly error.

15. The main signal function, acting as a callback and being called by the simulator. 
The first argument in each signal function is the context of type
[SignalContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext). 
Other arguments are user-defined, such as the parameters.

16. If this is the first column in the bar, calculate the spread and its z-score,
since both are group metrics and must be available to both columns.

17. The close price is also a flexible array.

18. Both spread and z-score are two-dimensional arrays with columns representing groups.

19. The z-score is calculated using the mean and standard deviation of a data window.

20. In integer indexing, the stop is exclusive, so add 1 to include the current bar.

21. Ensure the window start is greater than or equal to zero; if it is negative, the last values 
of the array will be used instead!

Now, create a pipeline that runs simulation using the signal function above:

```pycon
>>> WAIT_DAYS = 30

>>> def iter_pt_portfolio(
...     window=WINDOW, 
...     upper=UPPER, 
...     lower=LOWER, 
...     wait_days=WAIT_DAYS,
...     signal_func_nb=signal_func_nb,  # (1)!
...     more_signal_args=(),
...     **kwargs
... ):
...     return vbt.Portfolio.from_signals(
...         data,
...         broadcast_named_args=dict(  # (2)!
...             window=window,
...             upper=upper,
...             lower=lower,
...             wait_days=wait_days
...         ),
...         in_outputs=vbt.RepEval("""
...             InOutputs(
...                 np.full((target_shape[0], target_shape[1] // 2), np.nan), 
...                 np.full((target_shape[0], target_shape[1] // 2), np.nan)
...             )
...         """, context=dict(InOutputs=InOutputs)),  # (3)!
...         signal_func_nb=signal_func_nb,  # (4)!
...         signal_args=(  # (5)!
...             vbt.Rep("window"),  # (6)!
...             vbt.Rep("upper"),
...             vbt.Rep("lower"),
...             vbt.Rep("wait_days"),
...             *more_signal_args
...         ),
...         size=10,
...         size_type="valuepercent100",
...         group_by=vbt.ExceptLevel("symbol"),
...         cash_sharing=True,
...         call_seq="auto",
...         delta_format="percent100",
...         stop_exit_price="close",
...         **kwargs
...     )

>>> pf = iter_pt_portfolio()
```

1. This lets you later replace the signal function with another one—stay tuned!

2. By defining parameters here, you make them flexible; you can
provide single values, or arrays per row, column, or even element.

3. Since we do not know the target shape for the temporary arrays yet, create a Python 
expression that generates the named tuple with arrays and delay evaluation until the target 
shape is known.

4. Instead of passing signal arrays, we now pass the signal function.

5. These arguments appear after the context argument `c` in `signal_func_nb`.

6. Use templates to substitute array names from `broadcast_named_args` with the actual arrays 
once they are prepared.

Let's visually validate our implementation:

```pycon
>>> fig = vbt.make_subplots(  # (1)!
...     rows=2, 
...     cols=1, 
...     vertical_spacing=0,
...     shared_xaxes=True
... )
>>> zscore = pf.get_in_output("zscore").rename("Z-score")  # (2)!
>>> zscore.vbt.plot(  # (3)!
...     add_trace_kwargs=dict(row=1, col=1),
...     fig=fig
... )
>>> fig.add_hline(row=1, y=UPPER, line_color="orangered", line_dash="dot")
>>> fig.add_hline(row=1, y=0, line_color="yellow", line_dash="dot")
>>> fig.add_hline(row=1, y=LOWER, line_color="limegreen", line_dash="dot")
>>> orders = pf.orders.regroup(group_by=False).iloc[:, 0]  # (4)!
>>> exit_mask = orders.side_sell.get_pd_mask(idx_arr="signal_idx")  # (5)!
>>> entry_mask = orders.side_buy.get_pd_mask(idx_arr="signal_idx")  # (6)!
>>> upper_crossed = zscore.vbt.crossed_above(UPPER)
>>> lower_crossed = zscore.vbt.crossed_below(LOWER)
>>> (upper_crossed & ~exit_mask).vbt.signals.plot_as_exits(  # (7)!
...     pf.get_in_output("zscore"),
...     trace_kwargs=dict(
...         name="Exits (ignored)", 
...         marker=dict(color="lightgray"), 
...         opacity=0.5
...     ),
...     add_trace_kwargs=dict(row=1, col=1),
...     fig=fig
... )
>>> (lower_crossed & ~entry_mask).vbt.signals.plot_as_entries(
...     pf.get_in_output("zscore"),
...     trace_kwargs=dict(
...         name="Entries (ignored)", 
...         marker=dict(color="lightgray"), 
...         opacity=0.5
...     ),
...     add_trace_kwargs=dict(row=1, col=1),
...     fig=fig
... )
>>> exit_mask.vbt.signals.plot_as_exits(  # (8)!
...     pf.get_in_output("zscore"),
...     add_trace_kwargs=dict(row=1, col=1),
...     fig=fig
... )
>>> entry_mask.vbt.signals.plot_as_entries(
...     pf.get_in_output("zscore"),
...     add_trace_kwargs=dict(row=1, col=1),
...     fig=fig
... )
>>> pf.plot_allocations(  # (9)!
...     add_trace_kwargs=dict(row=2, col=1),
...     fig=fig
... )
>>> rebalancing_dates = data.index[np.unique(orders.idx.values)]
>>> for date in rebalancing_dates:
...     fig.add_vline(row=2, x=date, line_color="teal", line_dash="dot")
>>> fig.update_layout(height=600)
>>> fig.show()
```

1. Create two subplots.

2. The temporary spread array filled in the signal function can be accessed with
[Portfolio.get_in_output](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_in_output).
It will be automatically converted into a Pandas format for you.

3. Plot the spread on the first subplot.

4. The portfolio object and the orders object are grouped. Since we only need the first asset 
to visualize rebalancing decisions, we ungroup the object and select the first asset.

5. Get the mask for when the first asset was sold, indicating the upper threshold was crossed.

6. Get the mask for when the first asset was bought, indicating the lower threshold was crossed.

7. Plot ignored signals, which are crossover signals that were not executed.

8. Plot executed signals.

9. Plot the allocations on the second subplot.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/iter_pt_portfolio.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/iter_pt_portfolio.dark.svg#only-dark){: .iimg loading=lazy }

Great! But what about parameter optimization? Since our parameters are defined as flexible arrays,
we can pass them in many formats, including [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param).
Now, let's see how varying the waiting time affects the number of orders:

```pycon
>>> WAIT_SPACE = np.arange(30, 370, 5).tolist()

>>> pf = iter_pt_portfolio(wait_days=vbt.Param(WAIT_SPACE))
>>> pf.orders.count().vbt.scatterplot(
...     xaxis_title="Wait days",
...     yaxis_title="Order count"
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/wait_days.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/pairs-trading/wait_days.dark.svg#only-dark){: .iimg loading=lazy }

Keep in mind that the optimization approach above requires a lot of RAM because the OHLC data 
must be tiled as many times as there are parameter combinations. This is the same consideration
as when optimizing `sl_stop` and other built-in parameters.
You may also notice that both compilation and execution take much longer than before.
Signal functions cannot be cached, so the entire simulator must be compiled from scratch
with each runtime. Additionally, we must use a different simulation path than the faster
path based on signal arrays that we used earlier. Also, our z-score implementation is quite slow 
since the mean and standard deviation need to be recomputed for every single bar 
(unlike the previous OLS indicator, which used one of the fastest algorithms available).

```pycon
>>> with (vbt.Timer() as timer, vbt.MemTracer() as tracer):
...     iter_pt_portfolio(wait_days=vbt.Param(WAIT_SPACE))

>>> print(timer.elapsed())
8.62 seconds

>>> print(tracer.peak_usage())
306.2 MB
```

While compilation time is difficult to reduce, execution time can be significantly decreased
by replacing both the mean and standard deviation operations with a z-score accumulator.
A z-score accumulator computes z-scores incrementally, avoiding expensive aggregations.
Fortunately, VBT already includes such an accumulator:
[rolling_zscore_acc_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_zscore_acc_nb).
The process works like this: you provide an input state containing the information 
needed to process the value for this bar, pass it to the accumulator, and receive an output state 
that contains both the calculated z-score and the updated information for the next bar.

The first question is: what information should we store? In general, you need to keep 
the data that the accumulator updates and will require as input for the next bar.
The next question is: how do we store this information? A state usually consists 
of several individual values, but named tuples are immutable. The trick is to use 
a [structured NumPy array](https://numpy.org/doc/stable/user/basics.rec.html), which you can think 
of as a regular NumPy array that holds mutable named tuples. We will create a 
one-dimensional array with one tuple per group.

```pycon
>>> zscore_state_dt = np.dtype(  # (1)!
...     [
...         ("cumsum", float_),
...         ("cumsum_sq", float_),
...         ("nancnt", int_)
...     ],
...     align=True,
... )

>>> @njit(nogil=True, boundscheck=True)
... def stream_signal_func_nb(
...     c, 
...     window, 
...     upper, 
...     lower, 
...     wait_days, 
...     zscore_state  # (2)!
... ):
...     _window = vbt.pf_nb.select_nb(c, window)
...         
...     if c.col % 2 == 0:
...         x = vbt.pf_nb.select_nb(c, c.close, col=c.col)
...         y = vbt.pf_nb.select_nb(c, c.close, col=c.col + 1)
...         c.in_outputs.spread[c.i, c.group] = np.log(y) - np.log(x)
...         
...         value = c.in_outputs.spread[c.i, c.group]  # (3)!
...         pre_i = c.i - _window
...         if pre_i >= 0:
...             pre_window_value = c.in_outputs.spread[pre_i, c.group]  # (4)!
...         else:
...             pre_window_value = np.nan
...         zscore_in_state = vbt.enums.RollZScoreAIS(  # (5)!
...             i=c.i,
...             value=value,
...             pre_window_value=pre_window_value,
...             cumsum=zscore_state["cumsum"][c.group],
...             cumsum_sq=zscore_state["cumsum_sq"][c.group],
...             nancnt=zscore_state["nancnt"][c.group],
...             window=_window,
...             minp=_window,
...             ddof=0
...         )
...         zscore_out_state = vbt.nb.rolling_zscore_acc_nb(zscore_in_state)  # (6)!
...         c.in_outputs.zscore[c.i, c.group] = zscore_out_state.value  # (7)!
...         zscore_state["cumsum"][c.group] = zscore_out_state.cumsum  # (8)!
...         zscore_state["cumsum_sq"][c.group] = zscore_out_state.cumsum_sq
...         zscore_state["nancnt"][c.group] = zscore_out_state.nancnt
...         
...     return create_signals_nb(c, upper, lower, wait_days)
```

1. Specify the fields of a structured array that will hold the state.

2. The structured array will be created later using a template.

3. The z-score is based on the spread.

4. The calculation function also needs the spread value from `window` bars ago.

5. Create an input state using all available information.

6. Run the calculation function.

7. Store the returned z-score.

8. Remember to also store the new state.

Next, we will adapt the portfolio function to use the new signal function:

```pycon
>>> stream_pt_portfolio = partial(  # (1)!
...     iter_pt_portfolio,
...     signal_func_nb=stream_signal_func_nb,
...     more_signal_args=(  # (2)!
...         vbt.RepEval(  # (3)!
...             """
...             n_groups = target_shape[1] // 2
...             zscore_state = np.empty(n_groups, dtype=zscore_state_dt)
...             zscore_state["cumsum"] = 0.0
...             zscore_state["cumsum_sq"] = 0.0
...             zscore_state["nancnt"] = 0
...             zscore_state
...             """, 
...             context=dict(zscore_state_dt=zscore_state_dt)
...         ),
...     )
... )
```

1. Copy the `iter_pt_portfolio` function and replace its default signal function.

2. The new signal function expects a structured array as its last argument. This is why
we made it possible to provide additional arguments earlier.

3. Since the number of groups is not known yet, create an expression template that will be
substituted once this information becomes available. The template should create, initialize, and return a new
structured array with the data type defined above.

All set! Let's build the portfolio and validate it by comparing with the previous one:

```pycon
>>> stream_pf = stream_pt_portfolio()
>>> print(stream_pf.total_return)
0.15210165047643728

>>> pf = iter_pt_portfolio()
>>> print(pf.total_return)
0.15210165047643728
```

Finally, let's benchmark the new portfolio using `WAIT_SPACE`:

```pycon
>>> with (vbt.Timer() as timer, vbt.MemTracer() as tracer):
...     stream_pt_portfolio(wait_days=vbt.Param(WAIT_SPACE))

>>> print(timer.elapsed())
1.52 seconds

>>> print(tracer.peak_usage())
306.2 MB
```

!!! info
    Run this cell at least twice, because the simulation may need to compile during the first run.
    This is necessary because compilation is triggered whenever a new set of argument __types__ is detected.
    A single parameter combination and multiple parameter combinations produce two distinct sets of argument types.

The final optimization to further speed up the simulation for multiple parameter combinations is to enable
in-house chunking. There is one important detail: [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
cannot chunk user-defined arrays automatically, so we must provide the chunking
specifications for all arguments in both `signal_args` and `in_outputs`:

```pycon
>>> chunked_stream_pt_portfolio = partial(
...     stream_pt_portfolio,
...     chunked=dict(  # (1)!
...         engine="threadpool",
...         arg_take_spec=dict(
...             signal_args=vbt.ArgsTaker(
...                 vbt.flex_array_gl_slicer,  # (2)!
...                 vbt.flex_array_gl_slicer,
...                 vbt.flex_array_gl_slicer,
...                 vbt.flex_array_gl_slicer,
...                 vbt.ArraySlicer(axis=0)  # (3)!
...             ),
...             in_outputs=vbt.SequenceTaker([
...                 vbt.ArraySlicer(axis=1),  # (4)!
...                 vbt.ArraySlicer(axis=1)
...             ])
...         )
...     )
... )

>>> with (vbt.Timer() as timer, vbt.MemTracer() as tracer):
...     chunked_stream_pt_portfolio(wait_days=vbt.Param(WAIT_SPACE))

>>> print(timer.elapsed())
520.08 milliseconds

>>> print(tracer.peak_usage())
306.4 MB
```

1. Accepts the same arguments as `@chunked`, provided as a dictionary.

2. This special type, widely used by simulators, first chunks the groups, then maps each group to its
respective columns, and finally slices the columns out of a flexible array.

3. An array slicer without a mapper will slice arrays by groups. This is exactly what we
want here, since `zscore_state` has its length equal to the number of groups.

4. Temporary arrays `spread` and `zscore` are also group metrics, but they are two-dimensional arrays,
so we must use the second axis (columns).

With this optimization, simulation is now an order of magnitude faster than before :dash:

!!! tip
    To reduce memory usage during execution, use the "serial" engine or build super chunks.

## Summary

Pairs trading uses two columns with opposite position signs, making it an almost perfect use case
for VBT's integrated grouping mechanics. It gets even better: most pairs trading strategies
can be implemented using semi-vectorized, iterative, and streaming approaches alike. This
tutorial demonstrates how to incrementally develop a strategy and then optimize it for high
performance and low resource consumption.

We started with the discovery phase, designing and implementing the strategy from
the ground up using various high-level tools without focusing on performance. Once the
strategy's framework was established, we focused on speeding up execution to discover
more profitable configurations in less time. Finally, we moved to an iterative approach to
gain complete control over execution. But this does not have to be the end of the story:
if you are curious, you can build your own simulator and unlock power that others can only
dream of :superhero:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/pairs-trading.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/PairsTrading.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections/patterns/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections/
name: Patterns
type: page
icon: material-star-shooting-outline
---

# Patterns

Patterns help bring order to what may seem like chaos. They appear throughout nature—such as circles 
in road signs or rectangles in windows and doors. Just as children gradually learn to navigate an 
entirely unknown environment by recognizing regularities, participants in financial markets must also 
learn to operate within their complex, ever-changing surroundings. Much like children who rely on 
parents, teachers, or even Google for guidance, we have a valuable tool to aid our understanding: 
quantitative analysis.

In finance, patterns are distinctive formations formed by price movements on a chart and form 
the foundation of technical analysis. They can provide clues about what prices might do next, based 
on past behavior. For example, we can search for all previous occurrences of the same pattern we see 
today to analyze how events unfolded each time, enabling more nuanced trading decisions that consider 
multiple possibilities. Patterns are particularly valuable for identifying transition points between 
rising and falling trends, which is key to successful trade entries and exits. However, patterns do 
not guarantee future results, nor do they last forever. Unlike the real world, where structure and 
behavior tend to be consistent, financial markets are dominated by noise and 
[false positives](https://en.wikipedia.org/wiki/False_positives_and_false_negatives) that arise from 
intense interactions among people, systems, and other entities. Discovering something that performs 
just slightly better than random in a specific market regime is already an accomplishment—an 
exciting game with probabilities.

Let's walk through a simple use case where we want to identify the 
[Double Top](https://www.investopedia.com/terms/d/doubletop.asp) pattern.
We will start by pulling two years of daily `BTCUSDT` history as our baseline data:

```pycon
>>> from vectorbtpro import *

>>> data = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="2020-06-01 UTC", 
...     end="2022-06-01 UTC"
... )
>>> data.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/data.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/data.dark.svg#only-dark){: .iimg loading=lazy }

A quick visual inspection suggests the most apparent occurrence of this pattern was between October
and December 2021:

```pycon
>>> data_window = data.loc["2021-09-25":"2021-11-25"]
>>> data_window.plot(plot_volume=False).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/data_window.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/data_window.dark.svg#only-dark){: .iimg loading=lazy }

As humans, we can easily detect patterns visually in data. But how can we accomplish this
programmatically, where everything is represented by numbers? It would be costly, unnecessary, and
likely ineffective to train a [DNN](https://en.wikipedia.org/wiki/Deep_learning) for such a simple
task. Following the principle "the simpler the algorithm, the better" for noisy data, we should
design an algorithm that tackles the problem using simple loops and basic math.

Since each pattern is matched against a single feature of the data, we will use the
[typical price](https://en.wikipedia.org/wiki/Typical_price):

```pycon
>>> price_window = data_window.hlc3
>>> price_window.vbt.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/hlc3.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/hlc3.dark.svg#only-dark){: .iimg loading=lazy }

Next, let's design the pattern. Price patterns are identified using a series of lines or curves.
For example, the "Double Top" pattern can be represented by the following array:

```pycon
>>> pattern = np.array([1, 2, 3, 2, 3, 2])
>>> pd.Series(pattern).vbt.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/double_top_pattern.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/double_top_pattern.dark.svg#only-dark){: .iimg loading=lazy }

It is important to note that the length and absolute values in the pattern definition do not matter.
Whether the pattern scales from 1 to 3 or 1 to 60, it will be stretched horizontally and vertically to
align with the corresponding price data. What matters for computation is the relative positioning of
each point. For instance, the first point `2` comes exactly in the middle between `1` and `3`,
meaning if the asset rises in price to match the second point, it should ideally make the same move
to reach the first peak. Furthermore, since some values like `2` and `3` repeat, we do not expect the
price at those points to deviate significantly, which helps define support and resistance levels.

Another key rule relates to the horizontal structure of the pattern: regardless of the value at any
given point, the location (timing) of a point is always relative to its neighboring points. For
example, if the first point was matched on `2020-01-01` and the second on `2020-01-03`, the third
point should be matched on `2020-01-06`. If any part of the pattern takes longer to form, changing
the horizontal structure, a match becomes less likely.

## Interpolation

Once we have defined our pattern, we need to ensure that both the pattern and the price array are the
same length. In image processing, increasing the size of an image involves reconstructing the image
by [interpolating](https://en.wikipedia.org/wiki/Interpolation) new pixels, while reducing the size
involves [downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)) the existing
pixels. In pattern processing, the approach is similar, but we work with one-dimensional arrays
instead of two-dimensional ones. We also prefer interpolation (stretching) over downsampling
(shrinking) to avoid losing information. This means that if the price array is shorter than the
pattern, it should be stretched to match the pattern's length, rather than compressing the pattern to
fit the price.

There are four main interpolation modes in pattern processing: linear, nearest neighbor, discrete,
and mixed. All of these are implemented using the Numba-compiled function
[interp_resize_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.interp_resize_1d_nb),
which takes an array, a target size, and an interpolation mode of type
[InterpMode](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.InterpMode). The implementation is very
efficient: it iterates through the array only once and does not require creating extra arrays except
for the final output.

### Linear

The linear interpolation algorithm estimates new values by connecting two adjacent known values with a
straight line (see [here](https://scientific-python.readthedocs.io/en/latest/notebooks_rst/1_Interpolation/1D_interpolation.html#linear-interpolation)
for an illustration).

Let's stretch our pattern array to 10 points:

```pycon
>>> resized_pattern = vbt.nb.interp_resize_1d_nb(
...     pattern, 10, vbt.enums.InterpMode.Linear
... )
>>> resized_pattern
array([1.        , 1.55555556, 2.11111111, 2.66666667, 2.77777778,
        2.22222222, 2.33333333, 2.88888889, 2.55555556, 2.        ])
```

This mode works best when the target size is close to `2 * n - 1`, where `n` is the original array 
size. In this case, the characteristics of the resized array closely match those of the original. 
Otherwise, the relationship between points may be distorted, unless the target size is significantly 
larger than the original. This is best illustrated below, where we resize the array to lengths of 7, 11, 
and 30 points:

```pycon
>>> def plot_linear(n):
...     resized_pattern = vbt.nb.interp_resize_1d_nb(
...         pattern, n, vbt.enums.InterpMode.Linear
...     )
...     return pd.Series(resized_pattern).vbt.plot()
```

=== "7 points"

    ```pycon
    >>> plot_linear(7).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_7.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_7.dark.svg#only-dark){: .iimg loading=lazy }

=== "11 points"

    ```pycon
    >>> plot_linear(11).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_11.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_11.dark.svg#only-dark){: .iimg loading=lazy }

=== "30 points"

    ```pycon
    >>> plot_linear(30).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_30.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_30.dark.svg#only-dark){: .iimg loading=lazy }

Why does the first graph look so "ugly"? This happens because the algorithm maintains the same
distance between each pair of points in the resized array. To stretch a 6-point array to 7 points,
the algorithm splits the graph of the 6-point array into 7 parts and selects the value at the midpoint
of each part, like this:

```pycon
>>> resized_pattern = vbt.nb.interp_resize_1d_nb(
...     pattern, 7, vbt.enums.InterpMode.Linear
... )
>>> ratio = (len(pattern) - 1) / (len(resized_pattern) - 1)
>>> new_points = np.arange(len(resized_pattern)) * ratio
>>> fig = pd.Series(pattern).vbt.plot()
>>> pd.Series(resized_pattern, index=new_points).vbt.scatterplot(fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_7_scatter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/linear_7_scatter.dark.svg#only-dark){: .iimg loading=lazy }

As you can see, linear interpolation is about selecting a specific number of equidistant values from
the original array. The more points there are, the better the result. The main issue is that when the
target size is suboptimal, the scale of the resized array changes. In the example above, the pattern
is distorted and the original link between points with a value of `2` is lost. However, other
interpolation algorithms may perform better in these cases.

### Nearest

The nearest neighbor algorithm selects the value of the closest point and does not consider other
values at all (see [here](https://scientific-python.readthedocs.io/en/latest/notebooks_rst/1_Interpolation/1D_interpolation.html#nearest-aka-piecewise-interpolation)
for an illustration). With this approach, the resized array consists only of values from the original
array, so there are no intermediate floating-point values:

```pycon
>>> resized_pattern = vbt.nb.interp_resize_1d_nb(
...     pattern, 10, vbt.enums.InterpMode.Nearest
... )
>>> resized_pattern
array([1., 2., 2., 3., 3., 2., 2., 3., 3., 2.])
```

!!! info
    For consistency, the resized array is always floating-point.

Below are resized arrays for different target sizes:

```pycon
>>> def plot_nearest(n):
...     resized_pattern = vbt.nb.interp_resize_1d_nb(
...         pattern, n, vbt.enums.InterpMode.Nearest
...     )
...     return pd.Series(resized_pattern).vbt.plot()
```

=== "7 points"

    ```pycon
    >>> plot_nearest(7).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/nearest_7.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/nearest_7.dark.svg#only-dark){: .iimg loading=lazy }

=== "11 points"

    ```pycon
    >>> plot_nearest(11).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/nearest_11.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/nearest_11.dark.svg#only-dark){: .iimg loading=lazy }

=== "30 points"

    ```pycon
    >>> plot_nearest(30).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/nearest_30.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/nearest_30.dark.svg#only-dark){: .iimg loading=lazy }

As you can see, each graph is a step curve with horizontal and vertical lines. Because of this, it can
be challenging to use when we expect the price to change smoothly rather than jump abruptly. This
interpolation mode should only be used if the original array is granular enough to smooth out the
transitions between local extrema.

!!! tip
    The `2 * n - 1` guideline does not apply to this mode.

### Discrete

The discrete interpolation algorithm selects a value if it is closer to the target position than the
other values; if not, it assigns a value of NaN. This mode ensures that each value from the original
array appears only once in the output, but it may still change their temporal distribution. It is best
used in scenarios where transitions between points are not important.

```pycon
>>> resized_pattern = vbt.nb.interp_resize_1d_nb(
...     pattern, 10, vbt.enums.InterpMode.Discrete
... )
>>> resized_pattern
array([ 1., nan,  2., nan,  3.,  2., nan,  3., nan,  2.])
```

Here is a comparison of arrays resized to different target sizes:

```pycon
>>> def plot_discrete(n):
...     resized_pattern = vbt.nb.interp_resize_1d_nb(
...         pattern, n, vbt.enums.InterpMode.Discrete
...     )
...     return pd.Series(resized_pattern).vbt.plot(
...         trace_kwargs=dict(
...             line=dict(dash="dot"),
...             connectgaps=True
...         )
...     )
```

=== "7 points"

    ```pycon
    >>> plot_discrete(7).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/discrete_7.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/discrete_7.dark.svg#only-dark){: .iimg loading=lazy }

=== "11 points"

    ```pycon
    >>> plot_discrete(11).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/discrete_11.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/discrete_11.dark.svg#only-dark){: .iimg loading=lazy }

=== "30 points"

    ```pycon
    >>> plot_discrete(30).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/discrete_30.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/discrete_30.dark.svg#only-dark){: .iimg loading=lazy }

Each graph contains exactly 6 points that have been mapped to a new interval by aiming to keep the
distance between them as equal as possible. Similar to linear interpolation, this mode also gives the
best results when the target size is `2 * n - 1`, while other sizes distort the temporal distribution
of the points. However, unlike linear interpolation, this mode preserves the absolute values of the
original array, so the point `2` is always guaranteed to be midway between points `1` and `3`.

### Mixed

The mixed interpolation algorithm combines the linear and discrete interpolation algorithms. First, it
applies the discrete interpolator, and if the value is NaN, it then applies the linear interpolator.
This approach ensures that each value from the original array is included at least once to preserve
the original scale, while intermediate values are filled using linear interpolation—giving us the best
of both worlds.

```pycon
>>> resized_pattern = vbt.nb.interp_resize_1d_nb(
...     pattern, 10, vbt.enums.InterpMode.Mixed
... )
>>> resized_pattern
array([1.        , 1.55555556, 2.        , 2.66666667, 3.        ,

        2.        , 2.33333333, 3.        , 2.55555556, 2.        ])
```

Let's show how the mixed approach "fixes" the scale issue of the linear approach:

```pycon
>>> def plot_mixed(n):
...     lin_resized_pattern = vbt.nb.interp_resize_1d_nb(
...         pattern, n, vbt.enums.InterpMode.Linear
...     )
...     mix_resized_pattern = vbt.nb.interp_resize_1d_nb(
...         pattern, n, vbt.enums.InterpMode.Mixed
...     )
...     fig = pd.Series(lin_resized_pattern, name="Linear").vbt.plot()
...     pd.Series(mix_resized_pattern, name="Mixed").vbt.plot(fig=fig)
...     return fig
```

=== "7 points"

    ```pycon
    >>> plot_mixed(7).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/mixed_7.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/mixed_7.dark.svg#only-dark){: .iimg loading=lazy }

=== "11 points"

    ```pycon
    >>> plot_mixed(11).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/mixed_11.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/mixed_11.dark.svg#only-dark){: .iimg loading=lazy }

=== "30 points"

    ```pycon
    >>> plot_mixed(30).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/mixed_30.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/mixed_30.dark.svg#only-dark){: .iimg loading=lazy }

!!! info
    As you may have noticed, the last pattern line on the graph is not completely straight.
    This is because mixed interpolation uses both linear and discrete approaches to retain the original
    scale. Producing a perfectly straight line would require passing through the data multiple times, and
    for performance reasons, we chose efficiency over complete visual smoothness.

We have restored the original connections between the various points, so this algorithm should be 
(and is) the default choice for interpolation without gaps.

```pycon
>>> resized_pattern = vbt.nb.interp_resize_1d_nb(
...     pattern, len(price_window), vbt.enums.InterpMode.Mixed
... )
>>> resized_pattern.shape
(62,)
```

## Rescaling

After bringing the pattern and the price to the same length, we also need to match their scales to make 
them comparable. To do this, we compute the minimum and maximum of both the pattern and the price, then 
rescale the pattern to match the scale of the price. We can use the Numba-compiled function 
[rescale_nb](https://vectorbt.pro/pvt_41531c19/api/utils/array_/#vectorbtpro.utils.array_.rescale_nb), which takes an array, 
the scale of the array, and the target scale:

```pycon
>>> pattern_scale = (resized_pattern.min(), resized_pattern.max())
>>> price_window_scale = (price_window.min(), price_window.max())
>>> rescaled_pattern = vbt.utils.array_.rescale_nb(
...     resized_pattern, pattern_scale, price_window_scale
... )
>>> rescaled_pattern = pd.Series(rescaled_pattern, index=price_window.index)
```

Now we can finally overlay the pattern on top of the price:

```pycon
>>> fig = price_window.vbt.plot()
>>> rescaled_pattern.vbt.plot(
...     trace_kwargs=dict(
...         fill="tonexty", 
...         fillcolor="rgba(255, 0, 0, 0.25)"
...     ), 
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rescaled_pattern.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rescaled_pattern.dark.svg#only-dark){: .iimg loading=lazy }

### Rebasing

Another way to match the scales of both arrays is rebasing. When rebasing, the first value of both arrays
is made equal, and all other values are rescaled based on their relative distance from the starting point.
This is useful if your pattern should enforce a specific percentage change from the starting value.
For example, let's ensure a relative distance of 60% between the peak and the starting point:

```pycon
>>> pct_pattern = np.array([1, 1.3, 1.6, 1.3, 1.6, 1.3])
>>> resized_pct_pattern = vbt.nb.interp_resize_1d_nb(
...     pct_pattern, len(price_window), vbt.enums.InterpMode.Mixed
... )
>>> rebased_pattern = resized_pct_pattern / resized_pct_pattern[0]
>>> rebased_pattern *= price_window.values[0]
>>> rebased_pattern = pd.Series(rebased_pattern, index=price_window.index)
>>> fig = price_window.vbt.plot()
>>> rebased_pattern.vbt.plot(
...     trace_kwargs=dict(
...         fill="tonexty", 
...         fillcolor="rgba(255, 0, 0, 0.25)"
...     ), 
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rebased_pattern.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rebased_pattern.dark.svg#only-dark){: .iimg loading=lazy }

## Fitting

Interpolation and rescaling are required to bring both the pattern and the price to the same
length and scale so they can be compared and combined as regular NumPy arrays. Rather than performing
the above steps manually, let's use a special function that does the entire job for us:
[fit_pattern_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.fit_pattern_nb).

```pycon
>>> new_pattern, _ = vbt.nb.fit_pattern_nb(
...     price_window.values,  # (1)!
...     pct_pattern,  # (2)!
...     interp_mode=vbt.enums.InterpMode.Mixed,
...     rescale_mode=vbt.enums.RescaleMode.Rebase  # (3)!
... )
```

1. The first argument is a price array. Since Numba does not recognize Pandas objects,
extract the NumPy array from your Series.

2. The second argument is a pattern array. You can pass this as-is, since it is already in NumPy format.

3. See [RescaleMode](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.RescaleMode).

What does `_` mean? The function actually returns two arrays: one for the pattern and one for the maximum
error (see [Max error](#max-error)). Since we do not need the maximum error array, we ignore it by using
`_` instead of a variable name. Let's check that the automatically generated array matches the manually
generated one:

```pycon
>>> np.testing.assert_array_equal(new_pattern, rebased_pattern)
```

No errors are raised—both arrays are identical!

## Similarity

Now that our arrays are perfectly comparable, how do we calculate their similarity? The algorithm is
simple: compute the absolute, element-wise differences between the two arrays and add them up (also known
as [Mean Absolute Error](https://en.wikipedia.org/wiki/Mean_absolute_error) or MAE). At the same time,
compute the maximum possible absolute, element-wise differences and add those up. The maximum distance is
calculated relative to the global minimum and maximum value. Finally, divide both totals and subtract from
1 to get the similarity score, which ranges between 0 and 1:

```pycon
>>> abs_distances = np.abs(rescaled_pattern - price_window.values)
>>> mae = abs_distances.sum()
>>> max_abs_distances = np.column_stack((
...     (price_window.max() - rescaled_pattern), 
...     (rescaled_pattern - price_window.min())
... )).max(axis=1)
>>> max_mae = max_abs_distances.sum()
>>> similarity = 1 - mae / max_mae
>>> similarity
0.8726845123416802
```

To penalize large distances and make pattern detection more "strict," you can switch the distance measure
to the root of the sum of squared differences (also called
[Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) or RMSE):

```pycon
>>> quad_distances = (rescaled_pattern - price_window.values) ** 2
>>> rmse = np.sqrt(quad_distances.sum())
>>> max_quad_distances = np.column_stack((
...     (price_window.max() - rescaled_pattern), 
...     (rescaled_pattern - price_window.min())
... )).max(axis=1) ** 2
>>> max_rmse = np.sqrt(max_quad_distances.sum())
>>> similarity = 1 - rmse / max_rmse
>>> similarity
0.8484851233108504
```

As another option, you could also remove the root from the equation above and calculate just the sum of
squared differences (also known as [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) or MSE):

```pycon
>>> quad_distances = (rescaled_pattern - price_window.values) ** 2
>>> mse = quad_distances.sum()
>>> max_quad_distances = np.column_stack((
...     (price_window.max() - rescaled_pattern), 
...     (rescaled_pattern - price_window.min())
... )).max(axis=1) ** 2
>>> max_mse = max_quad_distances.sum()
>>> similarity = 1 - mse / max_mse
>>> similarity
0.9770432421418718
```

!!! note
    Since the maximum distance is now the absolute maximum distance squared, the similarity
    will often exceed 90%. Do not forget to adjust your thresholds accordingly.

If you are not excited about writing the code above each time you need to measure similarity 
between two arrays, do not worry! As with everything, there is a convenient Numba-compiled function 
[pattern_similarity_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.pattern_similarity_nb), 
which combines all the steps above to produce a single score. This function accepts many options
for interpolation, rescaling, and distance measurement, and runs in 
[O(n)](https://en.wikipedia.org/wiki/Time_complexity) time without creating any new arrays. 
Due to its exceptional efficiency and Numba compilation, you can run this function millions of times in 
a fraction of a second :fire: The only difference from our approach above is that it rescales the price array 
to the pattern scale, not the other way around (which we did for plotting purposes).

Let's see the power of this function by replicating our pipeline above:

```pycon
>>> vbt.nb.pattern_similarity_nb(price_window.values, pattern)
0.8726845123416802
```

This produces the same score as our manual calculation :eyes:

Now, let's calculate the similarity score for `pct_pattern` with rebasing:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pct_pattern, 
...     rescale_mode=vbt.enums.RescaleMode.Rebase  # (1)!
... )
0.8647140967291362
```

1. See [RescaleMode](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.RescaleMode)

Let's get the similarity score by interpolating the pattern using nearest-neighbor interpolation,
rebasing, and RMSE:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pct_pattern, 
...     interp_mode=vbt.enums.InterpMode.Nearest,  # (1)!
...     rescale_mode=vbt.enums.RescaleMode.Rebase,
...     distance_measure=vbt.enums.DistanceMeasure.RMSE  # (2)!
... )
0.76151009787845
```

1. See [InterpMode](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.InterpMode).

2. See [DistanceMeasure](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.DistanceMeasure).

Often, we are not only interested in getting the similarity measure but also in visualizing and
debugging the pattern. In this case, we can call the accessor method
[GenericSRAccessor.plot_pattern](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericSRAccessor.plot_pattern),
which reconstructs the similarity calculation and visually displays various artifacts. To produce an
accurate plot, provide the same arguments as you did to the similarity calculation function. Our last
example produced a similarity of 75%. Let's visualize the fit:

```pycon
>>> price_window.vbt.plot_pattern(
...     pct_pattern, 
...     interp_mode="nearest",  # (1)!
...     rescale_mode="rebase",
...     fill_distance=True
... ).show()
```

1. Here, you can provide each option as a string.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_pattern.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_pattern.dark.svg#only-dark){: .iimg loading=lazy }

We can see that the largest discrepancy appears at the valley in the middle. The interpolated pattern
expects the price to dip deeper than it actually does. Let's add 15% to that point to increase the
similarity:

```pycon
>>> adj_pct_pattern = np.array([1, 1.3, 1.6, 1.45, 1.6, 1.3])
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     adj_pct_pattern, 
...     interp_mode=vbt.enums.InterpMode.Nearest,
...     rescale_mode=vbt.enums.RescaleMode.Rebase,
...     distance_measure=vbt.enums.DistanceMeasure.RMSE
... )
0.8086016654243109
```

Here is how the discrete interpolation applied to the new pattern looks:

```pycon
>>> price_window.vbt.plot_pattern(
...     adj_pct_pattern, 
...     interp_mode="discrete",
...     rescale_mode="rebase",
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_pattern_discrete.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_pattern_discrete.dark.svg#only-dark){: .iimg loading=lazy }

The pattern trace appears as a scatter plot rather than a line plot because the similarity score is
calculated only at those points, and the grayed out points are ignored. If we calculate the similarity
score again, we see a higher value than before because the pattern at those points matches the price
more closely:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     adj_pct_pattern, 
...     interp_mode=vbt.enums.InterpMode.Discrete,
...     rescale_mode=vbt.enums.RescaleMode.Rebase,
...     distance_measure=vbt.enums.DistanceMeasure.RMSE
... )
0.8719692914480557
```

### Relative

Since the price is not static and may change significantly during the period of comparison, it is better to
calculate the relative distance (error) instead of the absolute distance. For example, if the first price
point is `10` and the last price point is `1000`, the distance to the latter would have a much greater
impact on the similarity score than the distance to the former. Let's re-calculate the score manually and
automatically using relative distances:

```pycon
>>> abs_pct_distances = abs_distances / rescaled_pattern
>>> pct_mae = abs_pct_distances.sum()
>>> max_abs_pct_distances = max_abs_distances / rescaled_pattern
>>> max_pct_mae = max_abs_pct_distances.sum()
>>> similarity = 1 - pct_mae / max_pct_mae
>>> similarity
0.8732697724295595

>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pct_pattern, 
...     error_type=vbt.enums.ErrorType.Relative  # (1)!
... )
0.8732697724295594
```

1. See [ErrorType](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.ErrorType).

The difference is not very large in this scenario, but here is what happens when the price moves sharply:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     np.array([10, 30, 100]),
...     np.array([1, 2, 3]),
...     error_type=vbt.enums.ErrorType.Absolute
... )
0.8888888888888888

>>> vbt.nb.pattern_similarity_nb(
...     np.array([10, 30, 100]),
...     np.array([1, 2, 3]),
...     error_type=vbt.enums.ErrorType.Relative
... )
0.9575911789652247
```

In both examples, the pattern has been rescaled to `[10, 55, 100]` using the min-max rescaler (which is the
default). In the first example, the normalized error is `abs(30 - 55) / (100 - 30) = 0.36`, while in the
second example, the normalized error is `(abs(30 - 55) / 55) / ((100 - 30) / 30) = 0.19`, which also takes
into account the volatility of the price.

### Inverse

You can also invert the pattern internally:

```pycon
>>> vbt.nb.pattern_similarity_nb(price_window.values, pattern, invert=True)
0.32064009029620244

>>> price_window.vbt.plot_pattern(pattern, invert=True).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/inverted_pattern.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/inverted_pattern.dark.svg#only-dark){: .iimg loading=lazy }

!!! note
    This is not the same as simply inverting the score.

To produce the inverted pattern manually:

```pycon
>>> pattern.max() + pattern.min() - pattern
array([3, 2, 1, 2, 1, 2])
```

### Max error

Sometimes, you may want to define patterns as "corridors" within which the price should move.
If any of the corridor points are violated, you can either set the distance at that point to the
maximum distance (`max_error_strict=False`), or set the entire similarity to NaN
(`max_error_strict=True`). Such a corridor is referred to as "maximum error". This error can be
provided through the array-like argument `max_error`, which should be defined in the same way as the
pattern. It mostly needs to have the same length and scale as the pattern.

For example, if you choose min-max rescaling and the pattern is defined from `1` to `6`, a maximum
error of `0.5` would be `0.5 / (6 - 1) = 0.1`, that is, 10% of the pattern's scale. Let's check the
similarity of our original pattern without and with a corridor of `0.5`:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pattern,
... )
0.8726845123416802

>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pattern, 
...     max_error=np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),
... )
0.8611332262389184
```

Since `max_error` is a flexible argument, you can also provide it as a zero-dimensional or
one-dimensional array with one value, which will be applied to each point in the pattern:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pattern, 
...     max_error=np.array([0.5]),  # (1)!
... )
0.8611332262389184
```

1. The argument cannot be provided as a constant (`0.5`). It must always be an array.

The similarity score has decreased, meaning some corridor points were violated.
Let's visualize the whole thing to see the exact points:

```pycon
>>> price_window.vbt.plot_pattern(
...     pattern, 
...     max_error=0.5  # (1)!
... ).show()
```

1. You can provide constants here.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error.dark.svg#only-dark){: .iimg loading=lazy }

We see two points that were violated, so their distance to the price was set to the maximum
possible distance, which lowered the similarity. If we enable strict mode, the similarity becomes NaN
to let the user know that it did not pass the test:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pattern, 
...     max_error=np.array([0.5]),
...     max_error_strict=True
... )
nan
```

If you are interested in relative distances using `ErrorType.Relative`, the maximum error should
be provided as a percentage change:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pattern, 
...     max_error=np.array([0.1]),  # (1)!
...     error_type=vbt.enums.ErrorType.Relative
... )
0.8548520433078988

>>> price_window.vbt.plot_pattern(
...     pattern, 
...     max_error=0.1,
...     error_type="relative"
... ).show()
```

1. Corridor of 10% at any point.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error_relative.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error_relative.dark.svg#only-dark){: .iimg loading=lazy }

The same applies for rescaling using rebasing mode, where each error, regardless of error type, should
be given as a percentage change. For example, if the current pattern value has been mapped to a price of
`12000` and the maximum error is `0.1`, the corridor will cover all values from `12000 * 0.9 = 10800`
to `12000 * 1.1 = 13200`. Let's allow deviations for the adjusted percentage pattern of no more than
20% at the first level, 10% at the second level, and 5% at the third level:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     adj_pct_pattern, 
...     rescale_mode=vbt.enums.RescaleMode.Rebase,
...     max_error=np.array([0.2, 0.1, 0.05, 0.1, 0.05, 0.1]),
...     max_error_strict=True
... )
nan

>>> price_window.vbt.plot_pattern(
...     adj_pct_pattern, 
...     rescale_mode="rebase",
...     max_error=np.array([0.2, 0.1, 0.05, 0.1, 0.05, 0.1])
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error_rebase.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error_rebase.dark.svg#only-dark){: .iimg loading=lazy }

As shown, some points fall outside the corridor. If we add an additional 5% to all points,
the pattern passes the test easily:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     adj_pct_pattern, 
...     rescale_mode=vbt.enums.RescaleMode.Rebase,
...     max_error=np.array([0.2, 0.1, 0.05, 0.1, 0.05, 0.1]) + 0.05,
...     max_error_strict=True
... )
0.8789689066239321
```

#### Interpolation

Is there a way to provide the maximum error discretely? In other words, can we force the function
to adhere to the corridor only at certain points, rather than gradually at all points? By default,
the maximum error is interpolated in the same way as the pattern (linearly in our case).
To make the maximum error array interpolate differently, provide a different mode as
`max_error_interp_mode`. For example, let's require only the peak points to be within the corridor of 10%.
For this, use the discrete interpolation mode and set all intermediate points to NaN:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     adj_pct_pattern, 
...     rescale_mode=vbt.enums.RescaleMode.Rebase,
...     max_error=np.array([np.nan, np.nan, 0.1, np.nan, 0.1, np.nan]),
...     max_error_interp_mode=vbt.enums.InterpMode.Discrete,
...     max_error_strict=True
... )
0.8789689066239321

>>> price_window.vbt.plot_pattern(
...     adj_pct_pattern, 
...     rescale_mode="rebase",
...     max_error=np.array([np.nan, np.nan, 0.1, np.nan, 0.1, np.nan]),
...     max_error_interp_mode="discrete"
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error_discrete.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_error_discrete.dark.svg#only-dark){: .iimg loading=lazy }

Even though the scatter points of the maximum error are connected by a greyed-out line, the price is only
required to be between each pair of purple points, not between those lines.

#### Max distance

Final question: is there a way to adjust the maximum distance? Yes! You can use the maximum error as the
maximum distance by enabling `max_error_as_maxdist`. This has the following effect:
the smaller the maximum distance at any point, the more the price volatility at that point
affects the similarity. Let's compare our original pattern without and with a maximum distance cap
of `0.5` (10% of the scale):

```pycon
>>> vbt.nb.pattern_similarity_nb(price_window.values, pattern)
0.8726845123416802

>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, 
...     pattern, 
...     max_error=np.array([0.5]),
...     max_error_as_maxdist=True
... )
0.6193594883412921
```

This method introduces a penalty for increased price volatility.

!!! note
    You can also set a different maximum distance at different points in the pattern.
    Keep in mind that points with a larger maximum distance will have more weight in the
    similarity calculation than points with a smaller maximum distance. For example, if there are two points
    with maximum distances of `100` and `1` respectively, even with a perfect match at the second point,
    the similarity would be determined mostly by the distance at the first point.

### Further filters

When matching a large number of price windows against a pattern, we may want to skip windows
where the volatility is either too low or too high. You can do this by setting the arguments
`min_pct_change` and `max_pct_change`, respectively:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, pattern, max_pct_change=0.3
... )
nan
```

An added benefit is improved performance: if the test fails, the price window is only traversed once
to get the minimum and maximum value.

We can also filter out similarity scores that fall below a predefined threshold.
For example, to set the minimum similarity to 90%:

```pycon
>>> vbt.nb.pattern_similarity_nb(
...     price_window.values, pattern, min_similarity=0.9
... )
nan
```

!!! tip
    Do not worry about NaNs, they simply mean "didn't pass some tests, should be ignored during analysis".

Setting a similarity threshold also improves performance: if the algorithm detects that the threshold can
no longer be reached, even if the remaining points matched perfectly, it will stop and set the final
score to NaN. Depending on the threshold, this can make the computation about 30% faster on average.

## Rolling similarity

Now that we have learned the theory behind pattern recognition, it is time to put it into practice.
To search for a pattern within a price space, we need to roll a window across that space.
This can be done using the accessor method
[GenericAccessor.rolling_pattern_similarity](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_pattern_similarity),
which accepts the same arguments as before, plus the window length to roll.
If you set the window length to `None`, it will default to the length of the pattern array.

Let's roll a window of 30 data points over the entire typical price, and match it against a pattern
that uses a discrete soft corridor of 5%:

```pycon
>>> price = data.hlc3

>>> similarity = price.vbt.rolling_pattern_similarity(
...     pattern, 
...     window=30,
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete"
... )
>>> similarity.describe()
count    701.000000
mean       0.499321
std        0.144088
min        0.148387
25%        0.394584
50%        0.502231
75%        0.607962
max        0.838393
dtype: float64
```

We can see that among 701 comparisons, roughly half have a score below 50%.
The highest score is around 84%. Let's visualize the best match:

```pycon
>>> end_row = similarity.argmax() + 1  # (1)!
>>> start_row = end_row - 30
>>> fig = data.iloc[start_row:end_row].plot(plot_volume=False)
>>> price.iloc[start_row:end_row].vbt.plot_pattern(
...     pattern, 
...     error_type="relative",  # (2)!
...     max_error=0.05,
...     max_error_interp_mode="discrete",
...     plot_obj=False, 
...     fig=fig
... )
>>> fig.show()
```

1. Get the start (inclusive) and end (exclusive) row of the window with the highest similarity.

2. Be sure to use the same parameters that were used to calculate the similarity.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_rolling_similarity.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/max_rolling_similarity.dark.svg#only-dark){: .iimg loading=lazy }

Pretty accurate, right? And this window matches even better than the one we examined previously.
But what about the lowest similarity? Is it the same as inverting the pattern? No!

```pycon
>>> end_row = similarity.argmin() + 1
>>> start_row = end_row - 30
>>> fig = data.iloc[start_row:end_row].plot(plot_volume=False)
>>> price.iloc[start_row:end_row].vbt.plot_pattern(
...     pattern, 
...     invert=True,
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete",
...     plot_obj=False, 
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/min_rolling_similarity.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/min_rolling_similarity.dark.svg#only-dark){: .iimg loading=lazy }

Inverting the score will invalidate all the requirements set initially, so you should
always start a new pattern search with the `invert` flag enabled:

```pycon
>>> inv_similarity = price.vbt.rolling_pattern_similarity(
...     pattern, 
...     window=30,
...     invert=True,  # (1)!
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete"
... )
>>> end_row = inv_similarity.argmax() + 1  # (2)!
>>> start_row = end_row - 30
>>> fig = data.iloc[start_row:end_row].plot(plot_volume=False)
>>> price.iloc[start_row:end_row].vbt.plot_pattern(
...     pattern, 
...     invert=True,
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete",
...     plot_obj=False, 
...     fig=fig
... )
>>> fig.show()
```

1. Here.

2. We are looking for the highest similarity.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/inv_rolling_similarity.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/inv_rolling_similarity.dark.svg#only-dark){: .iimg loading=lazy }

The best match is not a perfect fit, but it is still much better than the previous one.

### Indicator

Once you have finalized the optimal pattern parameters through exploration and debugging, you should
consider integrating the pattern detection component into your backtesting stack. This can be achieved
by running the process inside an indicator. Since indicators must return output arrays of the same shape
as their input arrays, you can safely use the rolling pattern similarity as the output.
For this, you can use the [PATSIM](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM)
indicator, which takes a price array as its only input and accepts all arguments related to
calculating pattern similarity as parameters, including the pattern array itself. Another benefit
of this indicator is its ability to automatically convert arguments provided as a string (such as `interp_mode`)
into a Numba-compatible format. Finally, indicators are excellent for testing multiple window sizes,
since the accuracy of pattern detection heavily depends on the choice of window length.

Let's test several window combinations using the same setup as above:

```pycon
>>> patsim = vbt.PATSIM.run(
...     price, 
...     vbt.Default(pattern),  # (1)!
...     error_type=vbt.Default("relative"),
...     max_error=vbt.Default(0.05),
...     max_error_interp_mode=vbt.Default("discrete"),
...     window=[30, 45, 60, 75, 90]
... )
```

1. Wrap with [Default](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Default)
to hide the parameter from columns.

You can plot how the similarity develops using [PATSIM.plot](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM.plot).
Like any other plotting method, it allows only one column to be plotted, so you need
to specify the column name beforehand using the `column` argument:

```pycon
>>> patsim.wrapper.columns  # (1)!
Int64Index([30, 45, 60, 75, 90], dtype='int64', name='patsim_window')

>>> patsim.plot(column=60).show()
```

1. Review the column names generated by this indicator. If there are multiple column
levels, you need to provide the column name as a tuple.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/patsim.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/patsim.dark.svg#only-dark){: .iimg loading=lazy }

The generated similarity series is as fascinating as the price series itself,
and can be used for all kinds of technical analysis on its own :triangular_ruler:

An even more informative plot can be produced by
[PATSIM.overlay_with_heatmap](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM.overlay_with_heatmap),
which overlays a price line with a similarity heatmap:

```pycon
>>> patsim.overlay_with_heatmap(column=60).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/patsim_heatmap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/patsim_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

!!! info
    Bright vertical lines on the graph are located at the very end of their windows, marking
    where the pattern is considered completed. This makes the similarity score safe to use in backtesting.

So, how do we use this indicator to generate signals? We can compare the resulting similarity score
to a threshold to generate signals. In the example above, let's set a threshold of 80%
to build the exit signals:

```pycon
>>> exits = patsim.similarity >= 0.8
>>> exits.sum()
patsim_window
30     6
45     8
60    14
75     0
90     5
dtype: int64
```

If you want to test multiple thresholds, you can use the parameter `min_similarity`,
which sets all scores below it to NaN and also makes pattern recognition faster on average.
Deriving the signals is then as simple as checking whether each element is NaN. Let's also
test the inverted pattern:

```pycon
>>> patsim = vbt.PATSIM.run(
...     price, 
...     vbt.Default(pattern),
...     error_type=vbt.Default("relative"),
...     max_error=vbt.Default(0.05),
...     max_error_interp_mode=vbt.Default("discrete"),
...     window=[30, 45, 60, 75, 90],
...     invert=[False, True],
...     min_similarity=[0.7, 0.8],
...     param_product=True  # (1)!
... )
>>> exits = ~patsim.similarity.isnull()  # (2)!
>>> exits.sum()
patsim_window  patsim_invert  patsim_min_similarity
30             False          0.7                      68
                              0.8                       6
                True           0.7                      64
                              0.8                       2
...            ...            ...                     ...
90             False          0.7                      61
                              0.8                       5
                True           0.7                      70
                              0.8                       8
dtype: int64
```

1. Build the Cartesian product of all parameters with more than one element.
Keep in mind that the number of parameter combinations can grow quickly: testing three parameters
with 10 elements each results in 1,000 combinations.

2. Any element that is not NaN is a potential signal.

If you are not interested in having the window as a backtestable parameter, but instead want to
create a signal as soon as any of the windows at that row crosses the similarity threshold, you can
react immediately once a pattern of any length is detected. This is easily achieved
using Pandas:

```pycon
>>> groupby = [  # (1)!
...     name for name in patsim.wrapper.columns.names 
...     if name != "patsim_window"
... ]
>>> max_sim = patsim.similarity.groupby(groupby, axis=1).max()  # (2)!
>>> entries = ~max_sim.xs(True, level="patsim_invert", axis=1).isnull()  # (3)!
>>> exits = ~max_sim.xs(False, level="patsim_invert", axis=1).isnull()  # (4)!
```

1. Group by each column level except `patsim_window`.

2. Select the highest score among all windows.

3. Select non-NaN scores among columns where `patsim_invert` is `True` to build entries.

4. Select non-NaN scores among columns where `patsim_invert` is `False` to build exits.

Now, let's plot the entry and exit signals corresponding to the threshold of 80%:

```pycon
>>> fig = data.plot(ohlc_trace_kwargs=dict(opacity=0.5))
>>> entries[0.8].vbt.signals.plot_as_entries(price, fig=fig)
>>> exits[0.8].vbt.signals.plot_as_exits(price, fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/patsim_signals.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/patsim_signals.dark.svg#only-dark){: .iimg loading=lazy }

Apart from a few missed regular and inverted double top patterns, the indicator works well.
By further tweaking the pattern similarity parameters and choosing a stricter pattern
configuration, you could easily filter out most failed patterns.

## Search

Searching for patterns of variable length using indicators with a parameterizable window is expensive.
Each window requires allocating an array at least the same size as the entire price array.
We need a more compact representation of pattern search results.
Fortunately, VBT's native support for record arrays makes this possible!

The procedural logic is implemented by the Numba-compiled function
[find_pattern_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.find_pattern_1d_nb)
and its two-dimensional version [find_pattern_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.find_pattern_nb).
Here is the idea: iterate over the rows of a price array, and at each row,
iterate over a range of windows going forward or backward. For each window, run the
[pattern_similarity_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.pattern_similarity_nb)
function to get the similarity score. If the score meets all requirements and is not NaN,
create a NumPy record of the type [pattern_range_dt](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.pattern_range_dt),
which stores the start index (inclusive), the end index (exclusive, or inclusive if it's the last index),
and the similarity score. This record is appended to a record array and returned to the user.
The function supports selecting rows and windows randomly with a specified probability to reduce
the number of candidates and can handle overlapping pattern ranges. It is also extremely
time and memory efficient :zap:

Let's search for the pattern we defined above by rolling a window with sizes ranging from 30 to 90
and require the similarity score to be at least 85%:

```pycon
>>> pattern_range_records = vbt.nb.find_pattern_1d_nb(
...     price.values,  # (1)!
...     pattern,
...     window=30,  # (2)!
...     max_window=90,
...     error_type=vbt.enums.ErrorType.Relative,
...     max_error=np.array([0.05]),  # (3)!
...     max_error_interp_mode=vbt.enums.InterpMode.Discrete,
...     min_similarity=0.85
... )
>>> pattern_range_records
array([(0, 0, 270, 314, 1, 0.86226468), (1, 0, 484, 540, 1, 0.89078042)])
```

1. Our price contains only one asset, so we use the one-dimensional version of this function.

2. If `max_window` is provided, the `window` argument becomes the minimum window.

3. Always wrap any array-like argument with NumPy when working with Numba-compiled functions.

The call returned two records, with 86% and 89% (!) similarity, respectively.
The first window is `314 - 270 = 44` data points long, and the second is `540 - 484 = 56`.
Let's plot the second fit, including the 30 bars that follow the pattern:

```pycon
>>> start_row = pattern_range_records[1]["start_idx"]
>>> end_row = pattern_range_records[1]["end_idx"]
>>> fig = data.iloc[start_row:end_row + 30].plot(plot_volume=False)
>>> price.iloc[start_row:end_row].vbt.plot_pattern(
...     pattern, 
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete",
...     plot_obj=False, 
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/find_pattern.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/find_pattern.dark.svg#only-dark){: .iimg loading=lazy }

Voilà, this is the same pattern we examined at the beginning of this tutorial! This example shows
how important it is to test a dense grid of windows to find optimal matches. Unlike the
[PATSIM](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM) indicator, this approach
uses almost no memory and includes several tricks to speed up calculation,
such as pre-calculating the price's expanding minimum and maximum values.

As is often the case, using the raw Numba-compiled function is all fun and games until you come across a
more convenient method that wraps it: [PatternRanges.from_pattern_search](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.from_pattern_search).
This class method takes all parameters accepted by [find_pattern_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.find_pattern_1d_nb),
builds a grid of parameter combinations, splits the price array into one-dimensional column arrays,
and executes each parameter combination on each column array using the
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function, which supports both
sequential and parallel processing. After processing all input combinations, the method concatenates
the resulting record arrays and wraps them with the class
[PatternRanges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges).
This class extends the base class [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges)
by adding the similarity field and various pattern analysis and plotting methods. Enough theory;
let's do the same as above using this method:

```pycon
>>> pattern_ranges = vbt.PatternRanges.from_pattern_search(
...     price,  # (1)!
...     pattern,
...     window=30,
...     max_window=90,
...     error_type="relative",  # (2)!
...     max_error=0.05,  # (3)!
...     max_error_interp_mode="discrete",
...     min_similarity=0.85
... )
>>> pattern_ranges
<vectorbtpro.generic.ranges.PatternRanges at 0x7f8bdab039d0>
```

1. Accepts Pandas objects.

2. Accepts strings.

3. Accepts constants.

If the price is a Pandas object, there is also an accessor method
[GenericAccessor.find_pattern](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.find_pattern)
that calls the class method above and can save you a few lines:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     pattern,
...     window=30,
...     max_window=90,
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete",
...     min_similarity=0.85
... )
```

Let's view the records in a human-readable format:

```pycon
>>> pattern_ranges.records_readable
    Pattern Range Id  Column               Start Index  \
0                 0       0 2021-02-26 00:00:00+00:00   
1                 1       0 2021-09-28 00:00:00+00:00   

                  End Index  Status  Similarity  
0 2021-04-11 00:00:00+00:00  Closed    0.862265  
1 2021-11-23 00:00:00+00:00  Closed    0.890780 
```

Additionally, the returned [PatternRanges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges) instance
stores the search configuration used to generate those records for each column.
The property [PatternRanges.search_configs](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.search_configs)
returns a list of these configurations, with each one being an instance of the data class
[PSC](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PSC) (short for "Pattern Search Config").
There is one search configuration for each column.

!!! tip
    Please refer to the [documentation of PSC](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PSC)
    for detailed descriptions of each parameter used in pattern search.

```pycon
>>> pattern_ranges.wrapper.columns
Int64Index([0], dtype='int64')

>>> pattern_ranges.search_configs
[PSC(
    pattern=array([1, 2, 3, 2, 3, 2]), 
    window=30, 
    max_window=120, 
    row_select_prob=1.0, 
    window_select_prob=1.0, 
    roll_forward=False,
    interp_mode=3, 
    rescale_mode=0, 
    vmin=nan, 
    vmax=nan, 
    pmin=nan, 
    pmax=nan, 
    invert=False, 
    error_type=1, 
    distance_measure=0, 
    max_error=array([0.05]), 
    max_error_interp_mode=2, 
    max_error_as_maxdist=False, 
    max_error_strict=False, 
    min_pct_change=nan, 
    max_pct_change=nan, 
    min_similarity=0.85, 
    max_one_per_row=True,
    max_overlap=0, 
    max_records=None, 
    name=None
)]
```

This configuration instance contains all the argument names and values passed to
[find_pattern_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.find_pattern_1d_nb).
Why do we need to keep it? For plotting! Remember how inconvenient it was to provide the
exact same arguments to a plotting method that were used in the similarity calculation.
To make things more streamlined, each pattern range instance keeps track of the search configuration
for each column to be plotted. Plotting itself is handled by the method
[PatternRanges.plot](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.plot),
which uses the [Ranges.plot](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.plot)
method to plot the data and ranges, and the
[GenericSRAccessor.plot_pattern](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericSRAccessor.plot_pattern)
method to plot the patterns:

```pycon
>>> pattern_ranges.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_ranges.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_ranges.dark.svg#only-dark){: .iimg loading=lazy }

By default, it fills the distance between the price and the pattern (set `fill_distance=False` to hide this),
and does not display the corridor (set `plot_max_error=True` to show it).
As with any other subclass of [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),
an instance of [PatternRanges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges)
behaves in many ways like a regular Pandas object. For example, you can filter a date range using the regular
`loc` and `iloc` operations to zoom in on any pattern programmatically.

!!! note
    When selecting a date range, the indexing operation will filter out all range records
    that do not *completely* fall within the new date range. That is, if a pattern range
    starts on `2020-01-01` and lasts until `2021-01-01`, it will be included in the new
    pattern range instance if the new date range fully contains that period, for example
    `2019-01-01` to `2021-01-01`, but not `2019-01-01` to `2020-12-31`.

```pycon
>>> pattern_ranges.loc["2021-09-01":"2022-01-01"].plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_ranges_zoomed.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_ranges_zoomed.dark.svg#only-dark){: .iimg loading=lazy }

!!! info
    You might have noticed that the bright area is larger than the window by one bar, and the green
    marker is located a bit farther from the last window point. This is because the field `end_idx`
    represents the exclusive end of the range: the first point after the last window point.
    This approach is needed to properly calculate the duration of any range. The only exception is when the
    last window point is the last point in the entire price array; in such cases, the marker will be
    placed at that point and the range will be marked as open. Open ranges do not mean that the pattern
    is not completed, though.

Since all information is now represented using records, we can query various useful
metrics to describe the results:

```pycon
>>> pattern_ranges.stats()
Start                 2020-06-01 00:00:00+00:00
End                   2022-05-31 00:00:00+00:00
Period                        730 days 00:00:00
Total Records                                 2
Coverage                               0.136986
Overlap Coverage                            0.0
Duration: Min                  44 days 00:00:00
Duration: Median               50 days 00:00:00
Duration: Max                  56 days 00:00:00
Similarity: Min                        0.862265
Similarity: Median                     0.876523
Similarity: Max                         0.89078
dtype: object
```

Here, for example, we see that there are two non-overlapping patterns covering 13.7% of the
entire period. We also see various duration and similarity quantiles.

### Overlapping

In the example above, we searched a total of `90 - 30 + 1 = 61` windows at each row.
Why did we not get any overlapping ranges? By default, overlapping is not allowed.
There are two optional mechanisms in place. First, when multiple windows start at the
same row, the algorithm selects the one with the highest similarity. This ensures that the
number of filled records is always equal to or less than the number of rows in the price array.
Second, when there are multiple consecutive records that overlap in ranges (regardless of whether
they start at the same row), the algorithm also selects the one with the highest similarity.

However, sometimes you may want to allow overlapping ranges. For example, one pattern
might start right before another ends, or a large pattern might cover a range that includes
smaller patterns. These cases can be addressed by adjusting the `overlap_mode` argument, which is of
type [OverlapMode](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.OverlapMode). Setting it to `AllowAll`
disables both mechanisms and appends every single record. Setting it to `Allow` disables
the second mechanism and only filters out ranges starting at the same row. Setting it to `Disallow`
enables both mechanisms (default), while setting it to any other positive integer specifies
the maximum number of rows that any two neighboring ranges are allowed to overlap.

!!! important
    Setting the argument to `AllowAll` may produce a record array larger than the price array.
    In this case, you need to manually increase the number of records allocated by setting `max_records`,
    for example, `max_records=len(price) * 2`.

Let's allow overlapping ranges as long as they do not start at the same row:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     pattern,
...     window=30,
...     max_window=120,
...     error_type="relative",
...     max_error=0.05,
...     max_error_interp_mode="discrete",
...     min_similarity=0.85,
...     overlap_mode="allow"  # (1)!
... )

>>> pattern_ranges.count()
16

>>> pattern_ranges.overlap_coverage
0.9642857142857143
```

1. Here

Now we see that instead of 2, there are 16 detected ranges.
Also, ranges overlap by 96%, which means that almost all ranges share rows with other ranges.
Let's visualize the result:

```pycon
>>> pattern_ranges.plot(plot_zones=False, plot_patterns=False).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_ranges_overlap.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_ranges_overlap.dark.svg#only-dark){: .iimg loading=lazy }

As shown in the graph, there are only two global matches, each confirmed by windows of
different lengths. If we set `overlap_mode="disallow"`, only the most similar windows in each
region would remain.

!!! info
    There is an argument controlling the direction in which windows are rolled: `roll_forward`.
    If this argument is `False` (default), ranges are sorted by the end index and may have multiple
    records pointing to the same start index. Otherwise, ranges are sorted by the start index and
    may have multiple records pointing to the same end index.

### Random selection

Sometimes, not every row and window combination is worth searching. If the input data is too large,
or if there are too many parameter combinations, the search could take a long time
in VBT terms (although it would still be incredibly fast!). To make
searchable regions more sparse, we can introduce a probability of picking a specific row or window.
For example, if the probability is `0.5`, the algorithm would search every second row or window on average.
Let's set a probability of 50% for rows and 25% for windows, and benchmark the execution to see
if the process would be about 8 times faster on average:

```pycon
>>> def run_prob_search(row_select_prob, window_select_prob):
...     return price.vbt.find_pattern(
...         pattern,
...         window=30,
...         max_window=120,
...         row_select_prob=row_select_prob,  # (1)!
...         window_select_prob=window_select_prob,
...         error_type="relative",
...         max_error=0.05,
...         max_error_interp_mode="discrete",
...         min_similarity=0.8,  # (2)!
...     )

>>> %timeit run_prob_search(1.0, 1.0)
111 ms ± 247 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %timeit run_prob_search(0.5, 0.25)
15 ms ± 183 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

1. Both arguments must be between 0 and 1.

2. Lower the similarity threshold to capture more ranges.

Just note that unless you set a random seed (`seed` argument), detected pattern ranges
may vary greatly with each method call. Let's run `run_prob_search` 100 times and
plot the number of filled records:

```pycon
>>> run_prob_search(1.0, 1.0).count()  # (1)!
6

>>> pd.Series([
...     run_prob_search(0.5, 0.25).count() 
...     for i in range(100)
... ]).vbt.plot().show()
```

1. Get the maximum possible number of detected patterns.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/random_selection.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/random_selection.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    The lower the selection probabilities, the less likely you are to detect all patterns
    in a single call. Always make sure to run the same search multiple times to assess
    the stability of the detection accuracy.

### Params

The advantage of the class method [PatternRanges.from_pattern_search](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.from_pattern_search)
is its ability to act like a full-blown indicator. It uses the same broadcasting and
parameter combining mechanism as VBT's broadcaster; both rely on the function
[combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params). To mark any argument
as a parameter, wrap it with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param).
This enables the parameter to be broadcast and combined with others,
and it will appear as its own level in the final column hierarchy.

Let's test four patterns: "V-Top", "V-Bottom", a rising market, and a falling market pattern.

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     vbt.Param([
...         [1, 2, 1],
...         [2, 1, 2],
...         [1, 2, 3],
...         [3, 2, 1]
...     ]),
...     window=30,
...     max_window=120,
... )
```

[=100% "Param 4/4"]{: .candystripe .candystripe-animate }

Since we provided more than one parameter combination, the executor displayed a progress bar.
Let's count the number of found patterns:

```pycon
>>> pattern_ranges.count()
pattern
list_0    3
list_1    0
list_2    7
list_3    3
Name: count, dtype: int64
```

We see that the `pattern` argument received four lists. Let's make it more readable
by providing an index that assigns each list a name:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     vbt.Param([
...         [1, 2, 1],
...         [2, 1, 2],
...         [1, 2, 3],
...         [3, 2, 1]
...     ], keys=["v-top", "v-bottom", "rising", "falling"]),
...     window=30,
...     max_window=120,
... )
>>> pattern_ranges.count()
pattern
v-top       3
v-bottom    0
rising      7
falling     3
Name: count, dtype: int64
```

Let's display the three detected falling patterns:

```pycon
>>> pattern_ranges.plot(column="falling").show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/params_falling.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/params_falling.dark.svg#only-dark){: .iimg loading=lazy }

But there are many more falling patterns on the chart. Why were they not recognized?
There are three reasons: 1) the algorithm searched only regions that are at least 30 bars long, 2) the default
minimum similarity threshold is 85%, so the algorithm chose only regions most similar to a straight line, and
3) the algorithm removed any overlapping regions.

Now, let's pass multiple parameters. In this case, their values will be combined to form a
Cartesian product of all parameter combinations. Let's also test multiple similarity thresholds:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     vbt.Param([
...         [1, 2, 1],
...         [2, 1, 2],
...         [1, 2, 3],
...         [3, 2, 1]
...     ], keys=["v-top", "v-bottom", "rising", "falling"]),
...     window=30,
...     max_window=120,
...     min_similarity=vbt.Param([0.8, 0.85])
... )
>>> pattern_ranges.count()
pattern   min_similarity
v-top     0.80              6
          0.85              3
v-bottom  0.80              3
          0.85              0
rising    0.80              8
          0.85              7
falling   0.80              6
          0.85              3
Name: count, dtype: int64
```

We can now see some detected "V-Bottom" ranges:

```pycon
>>> pattern_ranges.plot(column=("v-bottom", 0.8)).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/params_v_bottom.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/params_v_bottom.dark.svg#only-dark){: .iimg loading=lazy }

What if we do not want to build a product of some parameters? For example, if we want
to use different window lengths for different patterns, this can be achieved by providing a level.
Parameters set to the same level are not combined, but instead are simply broadcast together.

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     vbt.Param([
...         [1, 2, 1],
...         [2, 1, 2],
...         [1, 2, 3],
...         [3, 2, 1]
...     ], keys=["v-top", "v-bottom", "rising", "falling"], level=0),
...     window=vbt.Param([30, 30, 7, 7], level=0),  # (1)!
...     max_window=vbt.Param([120, 120, 30, 30], level=0),
...     min_similarity=vbt.Param([0.8, 0.85], level=1)  # (2)!
... )
>>> pattern_ranges.count()
pattern   window  max_window  min_similarity
v-top     30      120         0.80               6
                              0.85               3
v-bottom  30      120         0.80               3
                              0.85               0
rising    7       30          0.80              27
                              0.85              23
falling   7       30          0.80              25
                              0.85              15
Name: count, dtype: int64
```

1. Must have the same length as other parameters on the same level.

2. Will be combined with blocks of parameters that share the same level.

!!! note
    If you use levels, you must provide a level for all parameters. You can also reorder
    some parameters in the column hierarchy by assigning them a lower or higher level.

### Configs

It is important to note that when using multiple assets, each parameter will be applied over
the entire price array. But what if you want to search for different patterns in the prices
of different assets? Remember how each instance of [PatternRanges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges)
keeps track of the search configuration for each column in
[PatternRanges.search_configs](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.search_configs)?
Similarly, you can manually provide search configurations using the `search_configs` argument,
which must be a list of [PSC](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PSC) instances
(for the entire price array) or a list of lists of [PSC](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PSC)
instances (for each column). This way, you can define custom parameter combinations.

To demonstrate this, let's fetch the prices for the `BTCUSDT` and `ETHUSDT` symbols,
and search for the "Double Top" pattern in both assets, as well as for any occurrence of the latest
30 bars in each asset individually:

```pycon
>>> mult_data = vbt.BinanceData.pull(
...     ["BTCUSDT", "ETHUSDT"], 
...     start="2020-06-01 UTC", 
...     end="2022-06-01 UTC"
... )
>>> mult_price = mult_data.hlc3

>>> pattern_ranges = mult_price.vbt.find_pattern(
...     search_configs=[
...         vbt.PSC(pattern=[1, 2, 3, 2, 3, 2], window=30),  # (1)!
...         [
...             vbt.PSC(pattern=mult_price.iloc[-30:, 0]),  # (2)!
...             vbt.PSC(pattern=mult_price.iloc[-30:, 1]),
...         ]
...     ],
...     min_similarity=0.8  # (3)!
... )
>>> pattern_ranges.count()
search_config  symbol 
0              BTCUSDT    6
1              ETHUSDT    4
2              BTCUSDT    5
3              ETHUSDT    8
Name: count, dtype: int64
```

1. Each `PSC` instance in the outer list is applied to the entire array.

2. Each `PSC` instance in the inner list is applied per column.

3. If an argument should apply to all columns, provide it as a regular argument.

!!! tip
    You can provide arguments to [PSC](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PSC)
    in a human-readable format. Each config will be automatically prepared for use in Numba.

We see the column hierarchy now contains two levels: the search config identifier
and the column name. Let's make it more descriptive by choosing a name for each config:

```pycon
>>> pattern_ranges = mult_price.vbt.find_pattern(
...     search_configs=[
...         vbt.PSC(pattern=[1, 2, 3, 2, 3, 2], window=30, name="double_top"),
...         [
...             vbt.PSC(pattern=mult_price.iloc[-30:, 0], name="last"),
...             vbt.PSC(pattern=mult_price.iloc[-30:, 1], name="last"),
...         ]
...     ],
...     min_similarity=0.8
... )
>>> pattern_ranges.count()
search_config  symbol 
double_top     BTCUSDT    6
                ETHUSDT    4
last           BTCUSDT    5
                ETHUSDT    8
Name: count, dtype: int64
```

You can also combine search configurations and parameters. In this case, the method will clone
the provided search configurations for each parameter combination, and override the parameters
of each search configuration with the current parameter combination. Let's test various rescaling modes:

```pycon
>>> pattern_ranges = mult_price.vbt.find_pattern(
...     search_configs=[
...         vbt.PSC(pattern=[1, 2, 3, 2, 3, 2], window=30, name="double_top"),
...         [
...             vbt.PSC(pattern=mult_price.iloc[-30:, 0], name="last"),
...             vbt.PSC(pattern=mult_price.iloc[-30:, 1], name="last"),
...         ]
...     ],
...     rescale_mode=vbt.Param(["minmax", "rebase"]),
...     min_similarity=0.8,
...     open=mult_data.open,  # (1)!
...     high=mult_data.high,
...     low=mult_data.low,
...     close=mult_data.close,
... )
>>> pattern_ranges.count()
rescale_mode  search_config  symbol 
minmax        double_top     BTCUSDT    6
                              ETHUSDT    4
              last           BTCUSDT    5
                              ETHUSDT    8
rebase        double_top     BTCUSDT    0
                              ETHUSDT    0
              last           BTCUSDT    2
                              ETHUSDT    2
Name: count, dtype: int64
```

1. Provide OHLC for plotting.

For example, our search for a pattern based on the last 30 bars in `ETHUSDT` found 8 occurrences
that are similar by shape (using min-max rescaling) and only 2 occurrences that are similar by both
shape and percentage change (using rebasing):

```pycon
>>> pattern_ranges.plot(column=("rebase", "last", "ETHUSDT")).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/configs_and_params.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/configs_and_params.dark.svg#only-dark){: .iimg loading=lazy }

The first range has a similarity of 85%, while the second range is still open and has a similarity
of 100%. This makes sense because it was used as the pattern.

!!! note
    Again, an open range does not mean that it has not finished developing. It only means that the last
    point in the range is also the last point in the price array, so the duration can be calculated correctly.

### Mask

How do we use a pattern range instance to generate signals? Since such an instance usually
stores only ranges that meet a certain similarity threshold, we only need to determine whether
there is any range that closes at a specific row and column. This mask can be generated by calling
the property [PatternRanges.last_pd_mask](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.last_pd_mask):

```pycon
>>> mask = pattern_ranges.last_pd_mask
>>> mask.sum()  # (1)!
rescale_mode  search_config  symbol 
minmax        touble_top     BTCUSDT    6
                              ETHUSDT    4
              last           BTCUSDT    5
                              ETHUSDT    8
rebase        touble_top     BTCUSDT    0
                              ETHUSDT    0
              last           BTCUSDT    2
                              ETHUSDT    2
dtype: int64
```

1. Count of `True` values in the mask.

We can then use this mask, for example, in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

### Indicator

If you do not need to plot or analyze pattern ranges, you can use the same
[PATSIM](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM) indicator used
previously to generate a Series or DataFrame of similarity scores. What was not discussed earlier is
that this indicator also accepts the arguments `max_window`, `row_select_prob`, and `window_select_prob`.
Let's confirm that the indicator produces the same similarity scores as pattern ranges:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     pattern,
...     window=30,
...     max_window=120,
...     row_select_prob=0.5,
...     window_select_prob=0.5,
...     overlap_mode="allow",  # (1)!
...     seed=42
... )
>>> pr_mask = pattern_ranges.map_field(
...     "similarity", 
...     idx_arr=pattern_ranges.last_idx.values
... ).to_pd()
>>> pr_mask[~pr_mask.isnull()]
Open time
2021-03-23 00:00:00+00:00    0.854189
2021-03-26 00:00:00+00:00    0.853817
2021-04-10 00:00:00+00:00    0.866913
2021-04-11 00:00:00+00:00    0.866106
2021-11-17 00:00:00+00:00    0.868276
2021-11-18 00:00:00+00:00    0.873757
2021-11-21 00:00:00+00:00    0.890225
2021-11-23 00:00:00+00:00    0.892541
2021-11-24 00:00:00+00:00    0.879475
2021-11-26 00:00:00+00:00    0.877245
2021-11-27 00:00:00+00:00    0.872172
dtype: float64

>>> patsim = vbt.PATSIM.run(
...     price,
...     vbt.Default(pattern),  # (2)!
...     window=vbt.Default(30),
...     max_window=vbt.Default(120),
...     row_select_prob=vbt.Default(0.5),
...     window_select_prob=vbt.Default(0.5),
...     min_similarity=vbt.Default(0.85),  # (3)!
...     seed=42
... )
>>> ind_mask = patsim.similarity
>>> ind_mask[~ind_mask.isnull()]
Open time
2021-03-23 00:00:00+00:00    0.854189
2021-03-26 00:00:00+00:00    0.853817
2021-04-10 00:00:00+00:00    0.866913
2021-04-11 00:00:00+00:00    0.866106
2021-11-17 00:00:00+00:00    0.868276
2021-11-18 00:00:00+00:00    0.873757
2021-11-21 00:00:00+00:00    0.890225
2021-11-23 00:00:00+00:00    0.892541
2021-11-24 00:00:00+00:00    0.879475
2021-11-26 00:00:00+00:00    0.877245
2021-11-27 00:00:00+00:00    0.872172
dtype: float64
```

1. This is the only mode supported in [PATSIM](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM).

2. Use `Default` for each argument to hide it from columns.

3. This is the default threshold in [PatternRanges.from_pattern_search](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.from_pattern_search).

## Combination

We have learned how to generate signals from a single pattern found in one array, but what about scenarios
where signals should only be triggered when a combination of patterns is found across different arrays?
For example, how can we quantify convergence and divergence? To combine multiple patterns conditionally,
we need to combine their similarity scores. In the example below, we search for a bearish divergence
between the high price and MACD:

```pycon
>>> price_highs = vbt.PATSIM.run(
...     data.high, 
...     pattern=np.array([1, 3, 2, 4]),  # (1)!
...     window=40,
...     max_window=50
... )
>>> macd = data.run("talib_macd").macd  # (2)!
>>> macd_lows = vbt.PATSIM.run(
...     macd, 
...     pattern=np.array([4, 2, 3, 1]),  # (3)!
...     window=40,
...     max_window=50
... )

>>> fig = vbt.make_subplots(
...     rows=3, cols=1, shared_xaxes=True, vertical_spacing=0.02
... )
>>> fig.update_layout(height=500)
>>> data.high.rename("Price").vbt.plot(
...     add_trace_kwargs=dict(row=1, col=1), fig=fig
... )
>>> macd.rename("MACD").vbt.plot(
...     add_trace_kwargs=dict(row=2, col=1), fig=fig
... )
>>> price_highs.similarity.rename("Price Sim").vbt.plot(
...     add_trace_kwargs=dict(row=3, col=1), fig=fig
... )
>>> macd_lows.similarity.rename("MACD Sim").vbt.plot(
...     add_trace_kwargs=dict(row=3, col=1), fig=fig
... )
>>> fig.show()
```

1. Two rising highs.

2. Use [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) to quickly run indicators.

3. Two falling highs.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/macd_divergence.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/macd_divergence.dark.svg#only-dark){: .iimg loading=lazy }

By reviewing the chart, we can see that the chosen parameters accurately represent the events we
are seeking. The regions where the price and MACD are rising and falling, respectively, also lead
to rising similarity scores. We can also find the optimal similarity threshold that yields a reasonable
number of crossovers, such as 80%. Additionally, the points where the price similarity line crosses
the threshold often occur slightly before those of the MACD, so it is not sufficient to simply test
whether both crossovers happen at the same time. We need to introduce a waiting period using the rolling
"any" operation. For example, below we generate an exit signal if both similarities have crossed
the threshold during the last 10 bars, not necessarily at the same time:

```pycon
>>> cond1 = (price_highs.similarity >= 0.8).vbt.rolling_any(10)
>>> cond2 = (macd_lows.similarity >= 0.8).vbt.rolling_any(10)
>>> exits = cond1 & cond2
>>> fig = data.plot(ohlc_trace_kwargs=dict(opacity=0.5))
>>> exits.vbt.signals.plot_as_exits(data.close, fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/macd_divergence_exits.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/macd_divergence_exits.dark.svg#only-dark){: .iimg loading=lazy }

For more ideas, see [Signal Development](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development).

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/patterns-and-projections/patterns.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/PatternsProjections.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections/projections/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections/
name: Projections
type: page
icon: material-star-shooting-outline
---

# Projections

Given a set of detected patterns, how can we determine if they give us an edge? One approach is to 
backtest them using signals and analyze the resulting trades, but the performance of each trade 
will also depend on the chosen backtesting parameters and the outcomes of previous trades. However, 
what we really need are clear statistics that rely solely on the price movement following each 
pattern. Here is a simplified workflow: capture the price movement (also called a projection) for 
a set number of bars after each pattern, store them all in a single array (called projections), 
and run various statistics on that array. The main idea is that if the pattern is effective, the 
price will generally rise or fall depending on whether the pattern is bullish or bearish. In this 
case, "generally" refers to the median return, which is just one of many statistics you can analyze.

Projections represent the price movement over a specified period of time. Since time periods are 
best handled using range records of the type [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges), 
we can easily map each range to a projection. The Numba-compiled function that performs this task is 
[map_ranges_to_projections_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.map_ranges_to_projections_nb). 
It takes the close price (or any other price), information on each range, and returns a 
two-dimensional NumPy array where each *row* is a single projection. Before using this function, 
let's find all the ranges in which the price increased by 20% within 7 days. We will allow this 
requirement to deviate by up to 1%:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     [1, 1.2], 
...     window=7, 
...     rescale_mode="rebase",  # (1)!
...     max_error=0.01,
...     max_error_interp_mode="discrete",  # (2)!
...     max_error_strict=True  # (3)!
... )
>>> pattern_ranges.count()
3
```

1. Use this mode to search for a specific percentage change.

2. Apply the 1% requirement only to the last point in the pattern array.

3. Any violation of the 1% requirement will cause the range to be skipped.

## Pattern projections

Let's call the Numba-compiled function on the pattern ranges we generated above. This operation 
builds projections from the price **inside** each range, meaning each projection should be highly 
similar to our pattern:

```pycon
>>> range_idxs, raw_projections = vbt.nb.map_ranges_to_projections_nb(
...     vbt.to_2d_array(price),  # (1)!
...     pattern_ranges.get_field_arr("col"),  # (2)!
...     pattern_ranges.get_field_arr("start_idx"),
...     pattern_ranges.get_field_arr("end_idx"),
...     pattern_ranges.get_field_arr("status")
... )
>>> range_idxs
array([0, 1, 2])

>>> raw_projections
array([[1.        , 1.00936136, 1.0528197 , 1.11080047, 1.12247632,
        1.13616486, 1.18824968, 1.16303367],
        [1.        , 1.03041164, 1.07854881, 1.08766541, 1.14380648,
        1.17483528, 1.20173755, 1.11737666],
        [1.        , 1.02418635, 1.10605106, 1.12944688, 1.17161711,
        1.17323145, 1.20239992, 1.22269832]])
```

1. A two-dimensional NumPy array is expected.

2. Use [Records.get_field_arr](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.get_field_arr)
to extract any field as a one-dimensional NumPy array. It will have the same number of values as 
there are ranges in the record array, one per range.

Here is what happened: the function iterated through each range, selected the corresponding price 
subset within that range, and stored that price subset in the output array. By default, it also 
rebased all projections to `1` to make them comparable. Additionally, it returned an array of 
indices that link the passed ranges to the returned projections.

To avoid manually preparing input and output data, an extra-convenient method 
[Ranges.get_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.get_projections)
returns a Series or DataFrame with projections as *columns*, labeled accordingly:

```pycon
>>> projections = pattern_ranges.get_projections()
>>> projections
range_id                          0         1         2
2022-05-31 00:00:00+00:00  1.000000  1.000000  1.000000
2022-06-01 00:00:00+00:00  1.009361  1.030412  1.024186
2022-06-02 00:00:00+00:00  1.052820  1.078549  1.106051
2022-06-03 00:00:00+00:00  1.110800  1.087665  1.129447
2022-06-04 00:00:00+00:00  1.122476  1.143806  1.171617
2022-06-05 00:00:00+00:00  1.136165  1.174835  1.173231
2022-06-06 00:00:00+00:00  1.188250  1.201738  1.202400
2022-06-07 00:00:00+00:00  1.163034  1.117377  1.222698
```

If you look at the DataFrame's index above, you will see that it starts with the last index 
in the price. This is because projections are meant to be *projected* into the future.
You will also notice there are 8 elements, even though our patterns are set to be 7 bars long:

```pycon
>>> pattern_ranges.duration.values
array([7, 7, 7])
```

This happens because the first element (`1`) is treated as the base of each projection and is not 
counted as part of the number of projected entries, which defaults to the duration of the range.
If you want to analyze only the duration of the range, you can disable this behavior by setting 
`incl_end_idx` to `False`:

```pycon
>>> projections = pattern_ranges.get_projections(incl_end_idx=False)
>>> projections
range_id                          0         1         2
2022-05-31 00:00:00+00:00  1.000000  1.000000  1.000000
2022-06-01 00:00:00+00:00  1.009361  1.030412  1.024186
2022-06-02 00:00:00+00:00  1.052820  1.078549  1.106051
2022-06-03 00:00:00+00:00  1.110800  1.087665  1.129447
2022-06-04 00:00:00+00:00  1.122476  1.143806  1.171617
2022-06-05 00:00:00+00:00  1.136165  1.174835  1.173231
2022-06-06 00:00:00+00:00  1.188250  1.201738  1.202400
```

Now you can observe the cumulative returns of each price range that fits our pattern.
This data format allows you to apply many data analysis techniques to assess the quality of the pattern.
For example, let's get the total return of each range:

```pycon
>>> projections.iloc[-1] / projections.iloc[0] - 1
range_id
0    0.188250
1    0.201738
2    0.202400
dtype: float64
```

!!! info
    You might wonder why the first return is 18.8% even though the maximum error requires it to
    be within 19% and 21%. The 1% requirement is not defined relative to the total return, but relative
    to the percentage change between the price and the pattern at the respective point.

And here is the visualization:

```pycon
>>> projections.vbt.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/pattern_projections.dark.svg#only-dark){: .iimg loading=lazy }

As you can see, each price range fits the pattern quite well.

## Delta projections

The main use case for projections is not debugging patterns, but analyzing their effect on price 
to see whether they provide alpha and outperform randomness consistently, even if only slightly.
Since projections are taken from between the start and end row of each range, we need to shift 
the range of each detected pattern so it starts at the last point, and has a specific duration 
(= delta) that we want to analyze. This is handled by the class method 
[Ranges.from_delta](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.from_delta) and 
the instance method [Ranges.with_delta](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.with_delta), 
which is based on the former. Let's create new ranges that start at the last point in each pattern 
range and end after a duration of 4 bars, meaning we want to analyze the impact of our pattern 
4 bars into the future:

```pycon
>>> delta_ranges = pattern_ranges.with_delta(4)  # (1)!
```

1. Delta can also be specified as a timedelta, for example, "4d".

To better understand how the newly created ranges relate to the pattern ranges they were 
generated from, let's visualize both range types between January and March 2021:

```pycon
>>> fig = pattern_ranges.loc["2021-01":"2021-03"].plot()
>>> delta_ranges.loc["2021-01":"2021-03"].plot(
...     plot_ohlc=False,
...     plot_close=False,
...     plot_markers=False,
...     closed_shape_kwargs=dict(fillcolor="DeepSkyBlue"),
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/delta_ranges.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/delta_ranges.dark.svg#only-dark){: .iimg loading=lazy }

We see that the "DeepSkyBlue" colored ranges start from the last point in the pattern range and last for 4 bars.
Why not have them start after the last point in the range to avoid overlaps?
Remember that the first point in each projection is the base point (`1`) from which
the actual projections propagate, so this base point should be the last pattern point.
Let's generate projections once again:

```pycon
>>> projections = delta_ranges.get_projections()
>>> projections
range_id                          0         1         2
2022-05-31 00:00:00+00:00  1.000000  1.000000  1.000000
2022-06-01 00:00:00+00:00  0.978779  0.929801  1.016882
2022-06-02 00:00:00+00:00  1.108145  0.864097  0.992104
2022-06-03 00:00:00+00:00  1.178824  0.864522  0.963215
2022-06-04 00:00:00+00:00  1.147739  0.851382  0.941327
```

These three columns contain the price development for 4 bars after their respective pattern
and are now perfectly analyzable. For example, we can calculate the mean total return of that development:

```pycon
>>> np.mean(projections.iloc[-1] / projections.iloc[0] - 1)
-0.01985064245106242
```

We see that whenever the price rises by 20% in 7 bars, after 4 bars it tends to fall by
2% on average. If you have some statistics experience, you will quickly point out that there are too few
observations to draw meaningful conclusions.
So instead, let's search for a 20% price increase in windows ranging from 7 to 30 bars, allowing overlaps,
and perform our analysis again on both assets:

```pycon
>>> pattern_ranges = mult_price.vbt.find_pattern(
...     [1, 1.2], 
...     window=7, 
...     max_window=30,
...     rescale_mode="rebase",
...     max_error=0.01,
...     max_error_interp_mode="discrete",
...     max_error_strict=True,
...     overlap_mode="allow"
... )
>>> pattern_ranges.count()
symbol
BTCUSDT    48
ETHUSDT    64
Name: count, dtype: int64

>>> delta_ranges = pattern_ranges.with_delta(4)
>>> projections = delta_ranges.get_projections()
>>> (projections.iloc[-1] / projections.iloc[0] - 1).describe()
count    112.000000
mean       0.010467
std        0.079566
min       -0.203760
25%       -0.044565
50%        0.014051
75%        0.055390
max        0.270107
dtype: float64
```

Half the time, the total return after 4 bars is at least 1.4%.

Since projection columns are sorted by time (the pattern ranges are sorted by the end index, and the delta ranges
are sorted by the start index), we can also visualize this metric over time to observe how it
changes. We just need to split this operation by symbol, because projections from multiple symbols are stacked
block-wise as columns. We also need to generate projections again, but this time with the argument `id_level`
set to "end_idx" to replace the column level "range_id" with the actual time of the total return:

```pycon
>>> projections = delta_ranges.get_projections(id_level="end_idx")
>>> projections.columns
MultiIndex([('BTCUSDT', '2020-08-03 00:00:00+00:00'),
            ('BTCUSDT', '2020-08-04 00:00:00+00:00'),
            ('BTCUSDT', '2020-08-05 00:00:00+00:00'),
            ...
            ('ETHUSDT', '2022-04-07 00:00:00+00:00'),
            ('ETHUSDT', '2022-04-08 00:00:00+00:00'),
            ('ETHUSDT', '2022-04-09 00:00:00+00:00')],
            names=['symbol', 'end_idx'], length=112)

>>> btc_projections = projections.xs("BTCUSDT", level="symbol", axis=1)
>>> total_proj_return = btc_projections.iloc[-1] / btc_projections.iloc[0] - 1
>>> total_proj_return.vbt.scatterplot(
...     trace_kwargs=dict(
...         marker=dict(
...             color=total_proj_return.values,
...             colorscale="Temps_r",
...             cmid=0
...         )
...     )
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/total_proj_return.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/total_proj_return.dark.svg#only-dark){: .iimg loading=lazy }

We can confirm that after a 20% increase, the price mostly continues to grow over the next 4 bars.

## Plotting

But what about plotting the projections themselves? Since all projections in our array share the same
index, we can plot them as regular lines. Instead of doing this manually and wasting your valuable time,
there is an accessor method [GenericDFAccessor.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections)
specialized for projection visualization:

```pycon
>>> btc_projections.vbt.plot_projections(plot_bands=False).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_projections.dark.svg#only-dark){: .iimg loading=lazy }

You can see the price development after each detected pattern plotted as a line.

### Colorization

Each line is colorized based on the projection's performance. By default, the color reflects
the median value of the projection relative to other projections. Here is the first
projection in the array as an example:

```pycon
>>> btc_projections["2020-08-03"]
2022-05-31 00:00:00+00:00    1.000000
2022-06-01 00:00:00+00:00    1.019300
2022-06-02 00:00:00+00:00    1.053823
2022-06-03 00:00:00+00:00    1.018510
2022-06-04 00:00:00+00:00    1.015957
Freq: D, Name: 2020-08-03 00:00:00+00:00, dtype: float64

>>> btc_projections["2020-08-03"].median()
1.018510232892989
```

Why use the median instead of the final value? Imagine the projection is above the baseline (`1`)
90% of the time but then falls below the baseline abruptly at the end. Should this be classified as a positive or
negative performing projection? There are two factors that influence an event's impact on price:
the delay before impact and the duration of the impact. Some events cause an immediate reaction in price,
while for others the reaction unfolds over time. In most cases, the effect of an event
lasts only for a limited period and gradually fades. Valuing projections by their last value is risky
and highly dependent on the chosen projection duration, while using the median is more robust
and not affected by duration.

If you are instead interested in the final value or any other statistic, you can use the
`colorize` argument. This argument accepts several modes ("median", "mean", and "last") as well as a custom
function that takes a Series or DataFrame and returns a number per column. For example, let's colorize the
projections by their volatility:

```pycon
>>> btc_projections.vbt.plot_projections(
...     plot_bands=False, colorize=np.std
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/colorize_std.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/colorize_std.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    If you prefer shades of gray, set `colorize=False`.

### Bands

Seeing so many projections at once does not provide much insight into the overall performance
of the pattern. What we need is a way to distill the most important price changes
and present them in a clear and understandable format. If we look at the array
with the projections again, we quickly notice that we can reduce that array
across both rows and columns. This would collapse all the projections into a single
projection, where each value is a statistic computed from all projections at each bar.
For example, let's calculate the median projection for one symbol and for multiple symbols:

```pycon
>>> projections.xs("ETHUSDT", level="symbol", axis=1).median(axis=1)  # (1)!
2022-05-31 00:00:00+00:00    1.000000
2022-06-01 00:00:00+00:00    1.005677
2022-06-02 00:00:00+00:00    1.014560
2022-06-03 00:00:00+00:00    1.017341
2022-06-04 00:00:00+00:00    1.019444
Freq: D, dtype: float64

>>> projections.groupby("symbol", axis=1).median()  # (2)!
symbol                      BTCUSDT   ETHUSDT
2022-05-31 00:00:00+00:00  1.000000  1.000000
2022-06-01 00:00:00+00:00  1.003365  1.005677
2022-06-02 00:00:00+00:00  1.001214  1.014560
2022-06-03 00:00:00+00:00  1.010576  1.017341
2022-06-04 00:00:00+00:00  1.009568  1.019444

>>> projections.median(axis=1)  # (3)!
2022-05-31 00:00:00+00:00    1.000000
2022-06-01 00:00:00+00:00    1.004447
2022-06-02 00:00:00+00:00    1.006168
2022-06-03 00:00:00+00:00    1.014375
2022-06-04 00:00:00+00:00    1.014051
Freq: D, dtype: float64
```

1. One symbol, one projection.

2. Multiple symbols, multiple projections.

3. Multiple symbols, one projection.

If we can build one projection from many, we can also construct so-called confidence bands,
within which a certain percentage of projections fall. Such bands are automatically displayed
when calling [GenericDFAccessor.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections):

```pycon
>>> btc_projections.vbt.plot_projections().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/bands.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/bands.dark.svg#only-dark){: .iimg loading=lazy }

The method generates four bands: the 20% quantile, 50% quantile (median), 80% quantile, and mean for the projections.
Whenever quantiles are plotted, their trace name is prefixed with `Q=`. So, what do these
quantiles mean? Quantiles divide sorted data into equal parts. In this context,
quantiles are calculated across all projections at each time step (`axis=1`). For example,
the 20% quantile means that 20% of the values are below that threshold. The 50% quantile
is typically the most important because it tells us that 50% of values are below and 50% are above that value.
If the median is zero, predicting the market's movement is no better than flipping a coin.
If the median is not zero, there are more projections with a positive or negative return than the other way around,
which is already worth investigating.

Quantiles are useful to explore because they not only tell us how many values fall below a
certain band, but also let's determine how many values are between bands.
For example, in the chart above, `80% - 20% = 60%` of values are between the upper and lower bands.
This also means that only `100% - 80% = 20%` of values are above the upper band, so the probability
of exceeding a return of 7% (see below) is at most one in five:

```pycon
>>> btc_projections.iloc[-1].quantile(0.8)
1.0702371602730685
```

Now that we can approximate the probability of specific market reactions to our events, such an analysis can
be helpful when deciding on optimal stop loss and take profit levels. For example, depending on
your risk preference, you can set the stop loss at the lowest point of the 20% quantile band.
If the 30% quantile is a better match for your trading style, you can plot it as the lower band
by setting the `plot_lower` argument accordingly. You can also plot it as the middle band while hiding other bands:

```pycon
>>> btc_projections.vbt.plot_projections(
...     plot_lower=False,
...     plot_middle="30%",  # (1)!
...     plot_upper=False, 
...     plot_aux_middle=False, 
... ).show()
```

1. Supported quantile formats include `Q=30%` and `Q=0.3`.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/bands_30.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/bands_30.dark.svg#only-dark){: .iimg loading=lazy }

This means that 30% of projections are below (or 70% are above) that band.

The middle band, like any other band argument, supports a wide range of formats.
For example, instead of quantiles, you can use significance levels and 
[z-scores](https://en.wikipedia.org/wiki/Standard_score). Standard scores assume
that the distribution of projection values at any point is approximately normal,
which may not always be true, but can be the case, especially for shorter timeframes. To check whether
any set of values is normally distributed, you can display a QQ plot:

```pycon
>>> btc_projections.iloc[-1].vbt.qqplot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/qqplot.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/qqplot.dark.svg#only-dark){: .iimg loading=lazy }

Aside from a few outliers, most points lie on the red line, indicating that the distribution of all
projection values at the last bar is roughly normal. Given this, you can instruct
the projection plotting method to display bands based on a certain significance level.
For example, similar to the 10% quantile, specifying a (one-tailed) significance level of 10%
marks where 10% of the lowest data points are contained. You can provide this percentage
using the format `P=X%` or as a float `P=0.X`. Internally, this is translated into a
z-score using [scipy.stats.norm.ppf](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html).
The z-score, or standard score, indicates the number of standard deviations from the mean.
If the significance level is 95%, the z-score is `1.96`, or almost twice the volatility.
To specify a z-score directly, use the format `Z=X`.

Let's plot significance levels of 20% and 80%:

```pycon
>>> btc_projections.vbt.plot_projections(
...     plot_lower="P=20%",
...     plot_middle="P=50%",  # (1)!
...     plot_upper="P=80%", 
...     plot_aux_middle=False, 
... ).show()
```

1. You could also provide `mean`.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/significance_levels.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/significance_levels.dark.svg#only-dark){: .iimg loading=lazy }

If the actual distribution of projection values at each bar is normal, the probability
of a new projection falling between the lower and upper bands would be 60%. But what about
custom bands? Just like with the `colorize` argument, you can provide a UDF here. Let's
highlight two real projections: the one that finishes better than 20% of projections, and the one
that finishes better than 80% of projections.

```pycon
>>> def finishes_at_quantile(df, q):
...     nth_element = int(np.ceil(q * (df.shape[1] - 1)))  # (1)!
...     nth_index = np.argsort(df.iloc[-1])[nth_element]  # (2)!
...     return df.iloc[:, nth_index]  # (3)!

>>> btc_projections.vbt.plot_projections(
...     plot_lower=partial(finishes_at_quantile, q=0.2),  # (4)!
...     plot_middle=False, 
...     plot_upper=partial(finishes_at_quantile, q=0.8), 
... ).show()
```

1. Get the position among projections that corresponds to a certain quantile.

2. Get the new indices of projections sorted by their last value, and select the quantile position.

3. Return the selected projection.

4. Use [functools.partial](https://docs.python.org/3/library/functools.html#functools.partial)
to return `finishes_at_quantile` with `q` set to a predefined value.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/custom_bands.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/custom_bands.dark.svg#only-dark){: .iimg loading=lazy }

This shows why the final (or any other specific) performance is not always representative of the overall
performance: even though the upper band finished in the top 20% of projections by their final value,
it spent most of the time below the baseline.

## Filtering

We do not always want to analyze the entire projection array. What if we are only interested
in projections that cross a certain threshold within a specific period of time?
Let's filter and display both `BTCUSDT` and `ETHUSDT` projections that crossed a 5% return threshold
at any point during the first two bars:

```pycon
>>> crossed_mask = projections.expanding().max().iloc[1] >= 1.05  # (1)!
>>> filt_projections = projections.loc[:, crossed_mask]
>>> filt_projections.iloc[-1].describe()  # (2)!
count    8.000000
mean     1.095870
std      0.084343
min      0.955078
25%      1.059109
50%      1.092623
75%      1.139190
max      1.214039
Name: 2022-06-04 00:00:00+00:00, dtype: float64

>>> filt_projections.loc[:, crossed_mask].vbt.plot_projections().show()
```

1. Get the mask for projections whose highest high at the second position is above the value `1.05`.

2. Assess the final performance of the filtered projections.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/filtering.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/filtering.dark.svg#only-dark){: .iimg loading=lazy }

If the price increased by 5% during the first two bars after our pattern, 80% of the found occurrences
remained above that 5% mark, 50% crossed the 9% mark, and every fourth occurrence
even passed the 14% mark after four bars.

## Latest projections

So far, we have discussed projection analysis for patterns and periods that follow them,
but there is a use case that requires special handling: patterns based on the latest price.
The idea is to find similar occurrences of the most recent price movements so we can project
them into the future and make predictions. Let's select the most recent 7-bar date range from our `BTCUSDT`
data for prediction. We will manually set search configs and use rescaling through
rebasing to search for occurrences that match our pattern in both shape and magnitude:

```pycon
>>> pattern_ranges = price.vbt.find_pattern(
...     pattern=data.close.iloc[-7:],
...     rescale_mode="rebase",
...     overlap_mode="allow"
... )
>>> pattern_ranges.count()
5
```

The algorithm found a total of 5 occurrences with a similarity score above 85%.
One of these occurrences is marked as an open range and has a score of 100% (see
`pattern_ranges.records_readable`) because it exactly matches the latest price as the prototype
of our pattern. Let's exclude it and plot the pattern projections:

```pycon
>>> pattern_ranges = pattern_ranges.status_closed
>>> pattern_ranges.count()
4

>>> projections = pattern_ranges.get_projections()
>>> projections.vbt.plot_projections(plot_bands=False).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/latest_pattern_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/latest_pattern_projections.dark.svg#only-dark){: .iimg loading=lazy }

Although we have identified precise pattern ranges, what we actually need are delta ranges that occur
after the pattern ranges. Let's project 7 bars into the future and visualize these projections as possible
continuations of the latest price:

```pycon
>>> delta_ranges = pattern_ranges.with_delta(7)
>>> projections = delta_ranges.get_projections(start_value=-1)  # (1)!
>>> fig = data.iloc[-7:].plot(plot_volume=False)
>>> projections.vbt.plot_projections(fig=fig)
>>> fig.show()
```

1. The default baseline is `1`. To override it, you can specify the value or array manually,
or set it to `-1` to use the latest price. The latter starts the projections from the most recent price.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/latest_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/latest_projections.dark.svg#only-dark){: .iimg loading=lazy }

As you can see, both the median and the mean band are rising, which suggests that the price
may increase in the future. Let's quantify this observation:

```pycon
>>> projections.mean(axis=1)
2022-05-31 00:00:00+00:00    31801.040000
2022-06-01 00:00:00+00:00    31955.412346
2022-06-02 00:00:00+00:00    32950.202299
2022-06-03 00:00:00+00:00    33227.236565
2022-06-04 00:00:00+00:00    35188.975154
2022-06-05 00:00:00+00:00    35928.715257
2022-06-06 00:00:00+00:00    35916.453055
2022-06-07 00:00:00+00:00    36003.074673
Freq: D, dtype: float64
```

Our data is defined between `2020-06-01` and `2022-06-01`. Let's fetch the next 7 bars
to see how that prediction turned out:

```pycon
>>> next_data = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="2022-05-31", 
...     end="2022-06-08"
... )
>>> next_data.close
Open time
2022-05-31 00:00:00+00:00    31801.04
2022-06-01 00:00:00+00:00    29805.83
2022-06-02 00:00:00+00:00    30452.62
2022-06-03 00:00:00+00:00    29700.21
2022-06-04 00:00:00+00:00    29864.04
2022-06-05 00:00:00+00:00    29919.21
2022-06-06 00:00:00+00:00    31373.10
2022-06-07 00:00:00+00:00    31125.33
Freq: D, Name: Close, dtype: float64
```

Even though three out of four projections predicted the price would be well above the baseline,
the actual price development was less optimistic. This is a very important concept to understand:
there is no guarantee that the future will unfold as projected. What truly matters is having enough
projections to provide statistical significance (a sample of four projections is much too small),
and consistency in predictions (a single comparison above is not enough to represent the pattern's
predictive power). To achieve both of these in the hundreds, you need more data (higher frequency
and/or more assets) and cross-validation.

### Quick plotting

Manually building and visualizing projections can be useful when the main
objective is statistical analysis, but this process can be cumbersome for quickly plotting results.
There is a method, [Ranges.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.plot_projections),
that combines both tasks: generating and visualizing projections. The first part
is handled by calling [Ranges.get_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.get_projections).
The second part is handled by calling [GenericDFAccessor.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections).
This is why it accepts all arguments supported by both of these methods, and more. It also
lets you plot projection lines as continuations of the price line, which is especially useful
when making predictions:

```pycon
>>> delta_ranges.plot_projections().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/ranges_plot_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/ranges_plot_projections.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    To plot OHLC, pass `open`, `high`, `low`, and `close` to any method that creates
    ranges, such as [Ranges.with_delta](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.with_delta).

## Non-uniform projections

Projecting the impact of patterns on price is straightforward when projections have the same duration
(delta). However, sometimes projections need to be generated from ranges that are not uniform,
such as ranges spanning between entry and exit signals with varying durations. Let's explore
a slightly more advanced example: moving average crossovers!

We will generate several window combinations and check whether price behavior after
entry signals is different from price behavior after exit signals. In the example below,
entry and exit signals are created from crossovers using 136 different window combinations. 
We require a minimum difference of 5 between fast and slow windows to reduce noisy crossovers.

```pycon
>>> windows = np.arange(10, 31)
>>> window_tuples = combinations(windows, 2)
>>> window_tuples = filter(lambda x: abs(x[0] - x[1]) >= 5, window_tuples)
>>> fast_windows, slow_windows = zip(*window_tuples)
>>> fast_sma = data.run("sma", fast_windows, short_name="fast_sma")
>>> slow_sma = data.run("sma", slow_windows, short_name="slow_sma")
>>> entries = fast_sma.real_crossed_above(slow_sma.real)
>>> exits = fast_sma.real_crossed_below(slow_sma.real)

>>> entries.shape
(730, 136)
```

First, let's examine whether the generated entry and exit signals produce different delta projections.
To do this, we use the accessor method [SignalsAccessor.delta_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.delta_ranges),
which builds delta ranges directly from the signals. The only parameter we need
to specify is the delta, which will be 30 bars. To avoid incomplete data, we will only use closed ranges:

```pycon
>>> entry_ranges = entries.vbt.signals.delta_ranges(30, close=data.close)
>>> entry_ranges = entry_ranges.status_closed
>>> entry_ranges.count().sum()
2233

>>> exit_ranges = exits.vbt.signals.delta_ranges(30, close=data.close)
>>> exit_ranges = exit_ranges.status_closed
>>> exit_ranges.count().sum()
2233
```

Next, we will extract the price projections within those ranges:

```pycon
>>> entry_projections = entry_ranges.get_projections()
>>> entry_projections.shape
(31, 2233)

>>> exit_projections = exit_ranges.get_projections()
>>> exit_projections.shape
(31, 2233)
```

Remember when we worked with just 4 projections? Now we have 4.5k!

Finally, we will plot the bands of both datasets on a single chart:

```pycon
>>> fig = entry_projections.vbt.plot_projections(
...     plot_projections=False,
...     plot_aux_middle=False,
...     plot_fill=False,
...     lower_trace_kwargs=dict(name="Lower (entry)", line_color="green"),
...     middle_trace_kwargs=dict(name="Middle (entry)", line_color="green"),
...     upper_trace_kwargs=dict(name="Upper (entry)", line_color="green"),
... )
>>> fig = exit_projections.vbt.plot_projections(
...     plot_projections=False,
...     plot_aux_middle=False,
...     plot_fill=False,
...     lower_trace_kwargs=dict(name="Lower (exit)", line_color="orangered"),
...     middle_trace_kwargs=dict(name="Middle (exit)", line_color="orangered"),
...     upper_trace_kwargs=dict(name="Upper (exit)", line_color="orangered"),
...     fig=fig
... )
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/entry_exit_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/entry_exit_projections.dark.svg#only-dark){: .iimg loading=lazy }

Surprised? Exits seem to consistently outperform entries right after the event but
ultimately lose to them in the long term. Of course, you can further analyze this by window
length or difference, but that is left as an exercise for the reader.

The projections we generated above are still uniform because they all have the same length,
which makes them easy to manage. But what if our projections had varying lengths? Let's create a
projection from the price between each consecutive entry and exit pair for long trades,
and the reverse for short trades. This approach closely matches the performance of trades that
would be generated by passing the signal arrays to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
To represent each pair of signals as a range, we can use the accessor method
[SignalsAccessor.between_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.between_ranges):

```pycon
>>> entry_ranges = entries.vbt.signals.between_ranges(exits, close=data.close)
>>> entry_ranges = entry_ranges.status_closed
>>> entry_ranges.count().sum()
2240

>>> exit_ranges = exits.vbt.signals.between_ranges(entries, close=data.close)
>>> exit_ranges = exit_ranges.status_closed
>>> exit_ranges.count().sum()
2118

>>> entry_projections = entry_ranges.get_projections()
>>> entry_projections.shape
(124, 2240)

>>> exit_projections = exit_ranges.get_projections()
>>> exit_projections.shape
(83, 2118)
```

You will notice that both arrays now have a different number of rows. Furthermore,
the projections themselves have different lengths and are padded at the end with NaNs.
If you tried to plot any of these arrays directly, it would be extremely slow due to the creation
of thousands of scatter traces. When you cannot plot something because of its size,
it is best to select a random subset of elements (columns in this case):

```pycon
>>> rand_cols = np.random.choice(entry_projections.shape[1], 100)
>>> entry_projections.iloc[:, rand_cols].vbt.plot_projections(plot_bands=False).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rand_entry_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rand_entry_projections.dark.svg#only-dark){: .iimg loading=lazy }

```pycon
>>> rand_cols = np.random.choice(exit_projections.shape[1], 100)
>>> exit_projections.iloc[:, rand_cols].vbt.plot_projections(plot_bands=False).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rand_exit_projections.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/rand_exit_projections.dark.svg#only-dark){: .iimg loading=lazy }

There are many shorter projections and a few longer ones. Interestingly, on a larger scale,
entries are followed exclusively by bull runs and exits by bear runs.

### Shrinking

One challenge is building confidence bands, since the first points will have many more observations 
(non-NaN values) than the last ones. If we want to "shrink" the projection period,
we can keep the larger projections and trim them later in Pandas, or simply use the `proj_period` 
argument in [Ranges.get_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.get_projections),
which accepts either an integer (number of rows) or a timedelta. Let's
reduce our projections to 30 days:

```pycon
>>> entry_projections = entry_ranges.get_projections(proj_period="30d")
>>> entry_projections.shape
(31, 2240)

>>> exit_projections = exit_ranges.get_projections(proj_period="30d")
>>> exit_projections.shape
(31, 2118)
```

Now each array has 31 rows: one baseline row and 30 projection rows.
Let's plot the generated projections:

```pycon
>>> rand_cols = np.random.choice(entry_projections.shape[1], 100)
>>> entry_projections.iloc[:, rand_cols].vbt.plot_projections().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/entry_proj_period.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/entry_proj_period.dark.svg#only-dark){: .iimg loading=lazy }

```pycon
>>> rand_cols = np.random.choice(exit_projections.shape[1], 100)
>>> exit_projections.iloc[:, rand_cols].vbt.plot_projections().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/exit_proj_period.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/exit_proj_period.dark.svg#only-dark){: .iimg loading=lazy }

We can now be more confident that using above-crossovers for entries and below-crossovers for exits 
mostly produced positive returns, especially over the long term. These two graphs are more informative 
than many crossover heatmaps because they let's analyze the impact of any events
across the time dimension. Randomly selecting projections, as we did above, is also a form of validation:
by plotting random projections multiple times, we can test the consistency of our findings.

### Stretching

There is also a way to convert non-uniform projections into a uniform format: using the `extend` argument, 
we can "extend" shorter projections to match the period of the largest projection (or `proj_period`) 
by including the price that follows the end of the projection. This is essentially the same as generating 
delta ranges. Let's shrink our signal projections to 30 days and then extend
the shorter projections so that all projections have the same length:

```pycon
>>> entry_projections = entry_ranges.get_projections(
...     proj_period="30d", extend=True
... )
>>> entry_projections.shape
(31, 2240)

>>> exit_projections = exit_ranges.get_projections(
...     proj_period="30d", extend=True
... )
>>> exit_projections.shape
(31, 2118)

>>> rand_cols = np.random.choice(entry_projections.shape[1], 100)
>>> entry_projections.iloc[:, rand_cols].vbt.plot_projections().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/entry_extend.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/entry_extend.dark.svg#only-dark){: .iimg loading=lazy }

```pycon
>>> rand_cols = np.random.choice(exit_projections.shape[1], 100)
>>> exit_projections.iloc[:, rand_cols].vbt.plot_projections().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/exit_extend.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/exit_extend.dark.svg#only-dark){: .iimg loading=lazy }

Stretching has extended each projection beyond its opposite signal, making it redundant. Looking 
at the chart, we can clearly see that without the opposite signal, the advantage of using crossovers 
disappears. This analysis once again underlines the importance of properly timing your exits.

### Quick plotting

Just as we combined projection generation and visualization before,
we can combine all the steps above using [Ranges.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.plot_projections).
The main difference is that this method only allows plotting one column at a time. We can also use
various arguments to filter the ranges. For example, we can use `min_duration` and `max_duration` to
filter out shorter or longer ranges before generating projections, or select a number of the
most recent ranges using the `last_n` argument. Let's select the 10 most recent ranges:

```pycon
>>> entry_ranges.wrapper.columns
MultiIndex([(10, 15),
            (10, 16),
            (10, 17),
            ...
            (24, 29),
            (24, 30),
            (25, 30)],
            names=['fast_sma_timeperiod', 'slow_sma_timeperiod'], length=136)

>>> entry_ranges.plot_projections(
...     column=(25, 30),
...     last_n=10,
...     proj_period="30d",
...     extend=True,
...     plot_lower=False,
...     plot_upper=False,
...     plot_aux_middle=False,
...     projection_trace_kwargs=dict(opacity=0.3)
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_projections_25_30.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/plot_projections_25_30.dark.svg#only-dark){: .iimg loading=lazy }

This graph answers the question: "How would the price most likely react if we saw an upward
crossover of the 25 and 30 SMA today?"

!!! tip
    You can disable plotting of the price by setting `plot_close` to `False`.

## Open projections

Another use case arises when the latest range (or trade) is not closed. In the crossover scenario 
above, this means the latest entry does not have a corresponding exit (or vice versa). In this case, 
we are mainly interested in projections that do not start from the signal, but instead continue from the current 
point. Let's regenerate our ranges, allowing open ranges. Since the most recent crossovers were exits, we will 
focus our analysis on exit signals. Suppose we have opened a short trade at the latest exit signal and want 
to examine future price development to decide whether to close the trade before the next entry:

```pycon
>>> exit_ranges = exits.vbt.signals.between_ranges(
...     entries, 
...     incl_open=True, 
...     close=data.close
... )
>>> exit_ranges.count().sum()
2240
```

Which columns have a last exit with no entry?

```pycon
>>> exit_ranges.wrapper.columns[exit_ranges.status_open.col_arr]
MultiIndex([(10, 17),
            (10, 18),
            (10, 19),
            (24, 29),
            (24, 30),
            (25, 30)],
            names=['fast_sma_timeperiod', 'slow_sma_timeperiod'], length=122)
```

Now, let's call the [Ranges.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.plot_projections)
method on the **closed** ranges of the column `(20, 30)`:

```pycon
>>> exit_ranges.status_closed.plot_projections(
...     column=(20, 30), plot_bands=False
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/open_projections_closed.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/open_projections_closed.dark.svg#only-dark){: .iimg loading=lazy }

Now, try including the open range:

```pycon
>>> exit_ranges.plot_projections(
...     column=(20, 30), plot_bands=False
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/open_projections_all.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/open_projections_all.dark.svg#only-dark){: .iimg loading=lazy }

What happened? The method detected that the last range is open (meaning the last signal does not 
have an opposite signal) and instructed [Ranges.get_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.get_projections)
to generate projections starting from the signal point. This behavior is controlled by the 
`proj_start` argument, which defaults to `current_or_0`. When there is an open range, it uses the 
signal point as the start; otherwise, it uses the first point. Another argument,
`plot_past_period`, controls the plotted period of the price and defaults to `current_or_proj_period`. 
If there is an open range, it plots the close from the signal point; otherwise, it uses the 
projection period.

To illustrate, let's plot all projections from the signal point:

```pycon
>>> column = (20, 30)
>>> signal_index = data.wrapper.index[np.flatnonzero(exits[column])[-1]]
>>> plot_start_index = signal_index - pd.Timedelta(days=10)
>>> sub_close = data.close[plot_start_index:]
>>> sub_exits = exits.loc[plot_start_index:, column]

>>> fig = sub_close.vbt.plot()
>>> sub_exits.vbt.signals.plot_as_exits(sub_close, fig=fig)
>>> projections = exit_ranges[column].status_closed.get_projections(
...     start_value=sub_close.loc[signal_index],
...     start_index=signal_index
... )
>>> projections.vbt.plot_projections(plot_bands=False, fig=fig)
>>> fig.show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/open_projections_sub.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/patterns/open_projections_sub.dark.svg#only-dark){: .iimg loading=lazy }

As shown above, by the end of the price data there are only two projections left, which is why
the other projections were omitted in the previous plot.

## Summary

We have learned how to recognize patterns in any time series and evaluate their impact on 
price development using projections, without any backtesting. This approach allows us to 
analyze arbitrary events without dealing with the many challenges of backtesting, such as 
configuring initial capital, position sizing, and performance accumulation. By considering
only the price itself, we set up a fair experiment that can offer insights into how our trading 
strategy would have performed if we entered at one point and exited at another. However, 
having only a few projections is not enough. As with all statistics, we need a large enough sample 
of events to estimate their impact with confidence. Otherwise, we could easily be misled 
by impressive results.

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/patterns-and-projections/projections.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/PatternsProjections.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
children:
  - https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/splitter/
  - https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/applications/
name: Cross-validation
type: page
icon: material-check-all
---

# Cross-validation

??? youtube "Cross-validation on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/_BSSPZplLHs?si=s-lqiyASBqeGigW9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

After developing a rule-based or machine learning-based strategy, it is time to backtest it. If our first
backtest yields a low Sharpe ratio, we might tweak the strategy to try to improve it. After several rounds
of parameter adjustments, we may arrive at a "flawless" set of parameters and a strategy showing an
exceptional Sharpe ratio. However, in live trading, the strategy may perform poorly, resulting in losses.
What went wrong?

Markets naturally contain noise—small and frequent inconsistencies in price data. When designing a
strategy, we should avoid optimizing for a single period, because the model may fit the historical data so
closely that it fails to predict the future effectively. This is similar to tuning a car for one specific
racetrack and expecting it to perform equally well everywhere. Especially with VBT, which allows us to
search large databases of historical market data for patterns, it can be easy to create complex rules that
appear highly accurate at predicting price changes (see [_p_-hacking](https://en.wikipedia.org/wiki/Data_dredging))
but make random guesses when applied to data outside the sample used to build the model.

Overfitting (also known as [curve fitting](https://en.wikipedia.org/wiki/Curve_fitting)) typically occurs
for one or more of these reasons: mistaking noise for signal, or excessively tweaking too many parameters.
To avoid overfitting, we should use [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))
(CV), which involves splitting a data sample into complementary subsets, analyzing one subset called the
training or _in-sample_ (IS) set, and validating the analysis on the other subset called the validation or
_out-of-sample_ (OOS) set. This procedure is repeated until we have multiple OOS periods and can calculate
statistics from all these results combined. The key questions we need to ask are: are our parameter choices
robust in the IS periods? Is our performance robust in the OOS periods? If not, we are essentially guessing,
and as quant investors we should not leave room for second-guessing when real money is at stake.

Let's consider a simple strategy based on a moving average crossover.

First, we will pull some data:

```pycon
>>> from vectorbtpro import *

>>> data = vbt.BinanceData.pull("BTCUSDT", end="2022-11-01 UTC")
>>> data.index
DatetimeIndex(['2017-08-17 00:00:00+00:00', '2017-08-18 00:00:00+00:00',
                '2017-08-19 00:00:00+00:00', '2017-08-20 00:00:00+00:00',
                ...
                '2022-10-28 00:00:00+00:00', '2022-10-29 00:00:00+00:00',
                '2022-10-30 00:00:00+00:00', '2022-10-31 00:00:00+00:00'],
    dtype='datetime64[ns, UTC]', name='Open time', length=1902, freq='D')
```

Next, let's create a parameterized mini-pipeline that takes data and parameters and returns the Sharpe
ratio, which should reflect our strategy's performance on that test period:

```pycon
>>> @vbt.parameterized(merge_func="concat")  # (1)!
... def sma_crossover_perf(data, fast_window, slow_window):
...     fast_sma = data.run("sma", fast_window, short_name="fast_sma")  # (2)!
...     slow_sma = data.run("sma", slow_window, short_name="slow_sma")
...     entries = fast_sma.real_crossed_above(slow_sma)
...     exits = fast_sma.real_crossed_below(slow_sma)
...     pf = vbt.Portfolio.from_signals(
...         data, entries, exits, direction="both")  # (3)!
...     return pf.sharpe_ratio  # (4)!
```

1. The [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) decorator allows
`sma_crossover_perf` to accept arguments wrapped with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param),
build the grid of parameter combinations, run `sma_crossover_perf` for each, and combine the results using
concatenation.

2. Use [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) to execute an indicator on a data instance.

3. Enable shorting by marking exit signals as short entry signals.

4. This function returns a single value, which becomes a Series after processing all parameter combinations.

Let's test a grid of `fast_window` and `slow_window` combinations over one year of that data:

```pycon
>>> perf = sma_crossover_perf(  # (1)!
...     data["2020":"2020"],  # (2)!
...     vbt.Param(np.arange(5, 50), condition="x < slow_window"),  # (3)!
...     vbt.Param(np.arange(5, 50)),  # (4)!
...     _execute_kwargs=dict(  # (5)!
...         clear_cache=50,  # (6)!
...         collect_garbage=50
...     )
... )
>>> perf
fast_window  slow_window
5            6              0.625318
              7              0.333243
              8              1.171861
              9              1.062940
              10             0.635302
                                  ...   
46           48             0.534582
              49             0.573196
47           48             0.445239
              49             0.357548
48           49            -0.826995
Length: 990, dtype: float64
```

1. Even with the `parameterized` decorator, the function arguments remain the same as
`sma_crossover_perf`.

2. [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instances can be sliced like standard Pandas objects.
Remember, the right bound is inclusive.

3. Using a fast SMA window longer than the slow SMA window does not make sense, so we set a condition to
reduce parameter combinations.

4. No condition is needed for the second parameter, since the first already enforces this relationship.

5. Any arguments intended for the [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized)
decorator must be prefixed with `_`.

6. By default, the `parameterized` decorator uses the [SerialEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.SerialEngine)
to execute each parameter combination. This engine accepts arguments to clear the cache and
collect memory garbage. Here, we do this every 50 iterations to manage memory usage.

[=100% "Combination 990/990"]{: .candystripe .candystripe-animate }

It took 30 seconds to test 990 parameter combinations, or about 30 milliseconds per run. Below, we sort
the Sharpe ratios in descending order to find the best parameter combinations:

```pycon
>>> perf.sort_values(ascending=False)
fast_window  slow_window
15           20             3.669815
14           19             3.484855
15           18             3.480444
14           21             3.467951
13           19             3.457093
                                  ...   
36           41             0.116606
              37             0.075805
42           43             0.004402
10           12            -0.465247
48           49            -0.826995
Length: 990, dtype: float64
```

It appears that `fast_window=15` and `slow_window=20` could make us very wealthy! But before putting all
our savings on that configuration, let's test it on the following year:

```pycon
>>> best_fast_window, best_slow_window = perf.idxmax()  # (1)!
>>> sma_crossover_perf(
...     data["2021":"2021"],
...     best_fast_window,  # (2)!
...     best_slow_window
... )
-1.1940481501019478
```

1. Retrieve the parameter values with the highest Sharpe ratio.

2. There is no need to wrap the parameters with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
if they are single values or if we do not want to build a parameter grid. They are simply forwarded to
`sma_crossover_perf`.

The result is disappointing, but did we at least outperform a baseline? Let's calculate the Sharpe ratio
for the buy-and-hold strategy during that year:

```pycon
>>> data["2021":"2021"].run("from_holding").sharpe_ratio  # (1)!
0.9641311236043749
```

1. [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) accepts not only indicator names, but also
portfolio method names. The first run may take some time because it needs to be compiled.

It seems that our strategy performed very poorly :speak_no_evil:

But this was only one optimization test. What if that period was an outlier and our strategy actually
performs well _on average_? Let's try to answer this by repeating the test above on each consecutive
180-day period in the data:

```pycon
>>> start_index = data.index[0]  # (1)!
>>> period = pd.Timedelta(days=180)  # (2)!
>>> all_is_bounds = {}  # (3)!
>>> all_is_bl_perf = {}
>>> all_is_perf = {}
>>> all_oos_bounds = {}  # (4)!
>>> all_oos_bl_perf = {}
>>> all_oos_perf = {}
>>> split_idx = 0
>>> period_idx = 0

>>> with vbt.ProgressBar() as pbar:  # (5)!
...     while start_index + 2 * period <= data.index[-1]:  # (6)!
...         pbar.set_prefix(str(start_index))
...
...         is_start_index = start_index
...         is_end_index = start_index + period - pd.Timedelta(nanoseconds=1)  # (7)!
...         is_data = data[is_start_index : is_end_index]
...         is_bl_perf = is_data.run("from_holding").sharpe_ratio
...         is_perf = sma_crossover_perf(
...             is_data,
...             vbt.Param(np.arange(5, 50), condition="x < slow_window"),
...             vbt.Param(np.arange(5, 50)),
...             _execute_kwargs=dict(
...                 clear_cache=50,
...                 collect_garbage=50
...             )
...         )
...
...         oos_start_index = start_index + period  # (8)!
...         oos_end_index = start_index + 2 * period - pd.Timedelta(nanoseconds=1)
...         oos_data = data[oos_start_index : oos_end_index]
...         oos_bl_perf = oos_data.run("from_holding").sharpe_ratio
...         best_fw, best_sw = is_perf.idxmax()
...         oos_perf = sma_crossover_perf(oos_data, best_fw, best_sw)
...         oos_perf_index = is_perf.index[is_perf.index == (best_fw, best_sw)]
...         oos_perf = pd.Series([oos_perf], index=oos_perf_index)  # (9)!
...
...         all_is_bounds[period_idx] = (is_start_index, is_end_index)
...         all_oos_bounds[period_idx + 1] = (oos_start_index, oos_end_index)
...         all_is_bl_perf[(split_idx, period_idx)] = is_bl_perf
...         all_oos_bl_perf[(split_idx, period_idx + 1)] = oos_bl_perf
...         all_is_perf[(split_idx, period_idx)] = is_perf
...         all_oos_perf[(split_idx, period_idx + 1)] = oos_perf
...         start_index = start_index + period  # (10)!
...         split_idx += 1
...         period_idx += 1
...         pbar.update()  # (11)!
```

1. Get the first date in the index. This has the type [pandas.Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html).

2. Define the period using [pandas.Timedelta](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html)
so it can be added to [pandas.Timestamp](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html).

3. Store range start (inclusive) and end (exclusive) indexes, baseline (i.e., buy-and-hold) performance,
and optimization performance for each IS period.

4. Store range start and end indexes, baseline performance, and validation performance for each OOS period.

5. Be sure to install [tqdm](https://github.com/tqdm/tqdm) to display the progress bar.

6. Why use a while-loop instead of a for-loop? Because we do not know the number of splits in advance. We
process one split, increment the starting date, and repeat. This works naturally as a while-loop, and we
continue as long as both periods are fully covered by our index.

7. The right bound of the IS period should be **exclusive** because we do not want it to overlap with the
left bound of the OOS period. Since [pandas.DataFrame.loc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html)
includes the right bound, we simply subtract one nanosecond from it (keep this trick in mind!).

8. The IS period covers the days from [0, 180), while the OOS period covers the days from [180, 360).

9. The function call above returns a single value, but we convert it to a Series with the proper
index so we know which parameter combination it corresponds to.

10. The next IS period starts where the current IS period ends.

11. Update the progress bar to indicate this split has been processed.

[=100% "Period 9/9"]{: .candystripe .candystripe-animate }

```pycon
>>> is_period_ranges = pd.DataFrame.from_dict(  # (1)!
...     all_is_bounds, 
...     orient="index",
...     columns=["start", "end"]
... )
>>> is_period_ranges.index.name = "period"
>>> oos_period_ranges = pd.DataFrame.from_dict(
...     all_oos_bounds, 
...     orient="index",
...     columns=["start", "end"]
... )
>>> oos_period_ranges.index.name = "period"
>>> period_ranges = pd.concat((is_period_ranges, oos_period_ranges))  # (2)!
>>> period_ranges = period_ranges.drop_duplicates()
>>> period_ranges
                            start                                 end
period                                                              
0      2017-08-17 00:00:00+00:00 2018-02-12 23:59:59.999999999+00:00
1      2018-02-13 00:00:00+00:00 2018-08-11 23:59:59.999999999+00:00
2      2018-08-12 00:00:00+00:00 2019-02-07 23:59:59.999999999+00:00
3      2019-02-08 00:00:00+00:00 2019-08-06 23:59:59.999999999+00:00
4      2019-08-07 00:00:00+00:00 2020-02-02 23:59:59.999999999+00:00
5      2020-02-03 00:00:00+00:00 2020-07-31 23:59:59.999999999+00:00
6      2020-08-01 00:00:00+00:00 2021-01-27 23:59:59.999999999+00:00
7      2021-01-28 00:00:00+00:00 2021-07-26 23:59:59.999999999+00:00
8      2021-07-27 00:00:00+00:00 2022-01-22 23:59:59.999999999+00:00
9      2022-01-23 00:00:00+00:00 2022-07-21 23:59:59.999999999+00:00

>>> is_bl_perf = pd.Series(all_is_bl_perf)  # (3)!
>>> is_bl_perf.index.names = ["split", "period"]
>>> oos_bl_perf = pd.Series(all_oos_bl_perf)
>>> oos_bl_perf.index.names = ["split", "period"]
>>> bl_perf = pd.concat((  # (4)!
...     is_bl_perf.vbt.select_levels("period"), 
...     oos_bl_perf.vbt.select_levels("period")
... ))
>>> bl_perf = bl_perf.drop_duplicates()
>>> bl_perf
period
0    1.846205
1   -0.430642
2   -1.741407
3    3.408079
4   -0.556471
5    0.954291
6    3.241618
7    0.686198
8   -0.038013
9   -0.917722
dtype: float64

>>> is_perf = pd.concat(all_is_perf, names=["split", "period"])  # (5)!
>>> is_perf
split  period  fast_window  slow_window
0      0       5            6              1.766853
                            7              2.200321
                            8              2.698365
                            9              1.426788
                            10             0.849323
                                                ...   
8      8       46           48             0.043127
                            49             0.358875
                47           48             1.093769
                            49             1.105751
                48           49             0.159483
Length: 8910, dtype: float64

>>> oos_perf = pd.concat(all_oos_perf, names=["split", "period"])
>>> oos_perf
split  period  fast_window  slow_window
0      1       19           34             0.534007
1      2       6            7             -1.098628
2      3       18           20             1.687363
3      4       14           18             0.035346
4      5       18           21             1.877054
5      6       20           27             2.567751
6      7       11           18            -2.061754
7      8       29           30             0.965434
8      9       25           28             1.253361
dtype: float64

>>> is_best_mask = is_perf.index.vbt.drop_levels("period").isin(  # (6)!
...     oos_perf.index.vbt.drop_levels("period"))
>>> is_best_perf = is_perf[is_best_mask]
>>> is_best_perf
split  period  fast_window  slow_window
0      0       19           34             4.380746
1      1       6            7              2.538909
2      2       18           20             4.351354
3      3       14           18             3.605775
4      4       18           21             3.227437
5      5       20           27             3.362096
6      6       11           18             4.644594
7      7       29           30             3.379370
8      8       25           28             2.143645
dtype: float64
```

1. If each value in a dict has multiple elements, use
[pandas.DataFrame.from_dict](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.from_dict.html#pandas.DataFrame.from_dict)
to combine them into a single DataFrame.

2. We know the bounds of each IS and OOS set, but since OOS periods essentially become the next IS period,
let's create just one Series keyed by period.

3. If each value in a dict is a single element, you can wrap the dict directly with a Series.

4. The approach is the same as for bounds, but we need to remove the `split` level first.

5. If each value in a dict is a Series, use [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)
to concatenate them into a single Series.

6. Get the performance for the best parameter combination in each IS set using the index
from the corresponding OOS set. Since both Series are linked by `split`, `fast_window`, and
`slow_window`, remove `period` before doing this operation.

We have collected information for 9 splits and 10 periods. Now it is time to evaluate the results!
The index of each Series makes it easy to connect information and analyze everything together: the
`split` level connects elements within the same split, the `period` level links elements in the
same time period, and `fast_window` and `slow_window` relate elements by parameter combination.
To begin, let's compare their distributions:

```pycon
>>> pd.concat((
...     is_perf.describe(),
...     is_best_perf.describe(),
...     is_bl_perf.describe(),
...     oos_perf.describe(),
...     oos_bl_perf.describe()
... ), axis=1, keys=[
...     "IS", 
...     "IS (Best)", 
...     "IS (Baseline)", 
...     "OOS (Test)", 
...     "OOS (Baseline)"
... ])
                IS  IS (Best)  IS (Baseline)  OOS (Test)  OOS (Baseline)
count  8882.000000   9.000000       9.000000    9.000000        9.000000
mean      0.994383   3.514881       0.818873    0.639993        0.511770
std       1.746003   0.843435       1.746682    1.480066        1.786012
min      -3.600854   2.143645      -1.741407   -2.061754       -1.741407
25%      -0.272061   3.227437      -0.430642    0.035346       -0.556471
50%       1.173828   3.379370       0.686198    0.965434       -0.038013
75%       2.112042   4.351354       1.846205    1.687363        0.954291
max       4.644594   4.644594       3.408079    2.567751        3.408079
```

Although the OOS results are much lower than the best IS results, our strategy still outperforms the
baseline on average! Over 50% of periods have a Sharpe ratio of 0.96 or higher, while the baseline's
median sits at only -0.03. Another way to analyze this data is by plotting it. Since all these Series
can be linked by period, we will use the `period` level as the X-axis and the performance (Sharpe in
this case) as the Y-axis. Most Series can be shown as lines, but because IS sets include multiple
parameter combinations, we should show their distributions as boxes instead:

```pycon
>>> fig = is_perf.vbt.boxplot(  # (1)!
...     by_level="period",  # (2)!
...     trace_kwargs=dict(  # (3)!
...         line=dict(color="lightskyblue"), 
...         opacity=0.4,
...         showlegend=False
...     ),
...     xaxis_title="Period",  # (4)!
...     yaxis_title="Sharpe",
... )
>>> is_best_perf.vbt.select_levels("period").vbt.plot(  # (5)!
...     trace_kwargs=dict(
...         name="Best", 
...         line=dict(color="limegreen", dash="dash")
...     ), 
...     fig=fig  # (6)!
... )
>>> bl_perf.vbt.plot(
...     trace_kwargs=dict(
...         name="Baseline", 
...         line=dict(color="orange", dash="dash")
...     ), 
...     fig=fig
... )
>>> oos_perf.vbt.select_levels("period").vbt.plot(
...     trace_kwargs=dict(
...         name="Test", 
...         line=dict(color="orangered")
...     ), 
...     fig=fig
... )
>>> fig.show()
```

1. Use [GenericAccessor.boxplot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.boxplot).

2. This Series has many levels; we need to choose which level(s) to aggregate by.

3. This argument controls the appearance of the trace (see [plotly.graph_objects.Box](https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.Box.html)).

4. The first plotting method will create a Plotly figure and set it up, so pass layout settings such as axis labels here.

5. Use [GenericAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.plot).

6. Make sure to pass the figure to plot the new trace on, otherwise it will create a new figure.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/example.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/example.dark.svg#only-dark){: .iimg loading=lazy }

Here is how to interpret the plot above.

The green line tracks the performance of the best parameter combination in each IS set. The fact that it
touches the highest point in each box shows that our best-parameter selection algorithm works correctly.
The dashed orange line represents the performance of the "buy-and-hold" strategy during each period as
the baseline. The red line shows the test performance; it starts at the second range and corresponds to
the parameter combination that delivered the best result in the previous period (that is, the previous
green dot).

The semi-transparent blue boxes show the distribution of Sharpe ratios during the IS (training) periods,
meaning each box summarizes 990 parameter combinations tested in each optimization period. There is no
box on the far right because the last period is an OOS (test) period. For example, period `6` (which is
the seventh period, since counting starts at 0) includes all Sharpe ratios from `1.07` to `4.64`.
That likely means there was an upward trend during that period. Here is the proof:

```pycon
>>> is_perf_split6 = is_perf.xs(6, level="split")  # (1)!
>>> is_perf_split6.describe()
count    990.000000
mean       3.638821
std        0.441206
min        1.073553
25%        3.615566
50%        3.696611
75%        3.844124
max        4.644594
dtype: float64

>>> first_left_bound = period_ranges.loc[6, "start"]
>>> first_right_bound = period_ranges.loc[6, "end"]
>>> data[first_left_bound : first_right_bound].plot().show()  # (2)!
```

1. Select the values for the `split` level equal to `6` and collect statistics for them.

2. Plot the candlestick chart for that period to confirm visually.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/example_candlestick.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/example_candlestick.dark.svg#only-dark){: .iimg loading=lazy }

No matter which parameter combination we select for that time, the Sharpe ratio remains relatively high
and could give us a false sense of strategy performance. To make sure this is not the case, we need to
compare the test performance to other points. That's the main reason we drew lines over the
[box plot](https://en.wikipedia.org/wiki/Box_plot). For example, we can see that for period `6`, both
the baseline and test performances are below the first quartile (or 25th percentile). They are worse
than at least 75% of the parameter combinations tested during that time range:

```pycon
>>> oos_perf.xs(6, level="period")
split  fast_window  slow_window
5      20           27             2.567751
dtype: float64

>>> is_perf_split6.quantile(0.25)  # (1)!
3.615566166097048
```

1. 25% of parameter combinations have a Sharpe ratio below that value, including our
parameter combination used for testing.

The chart gives us mixed feelings: on the one hand, the chosen parameter combination outperforms most of
the combinations tested during 5 different time periods. On the other hand, it fails to beat even the
lowest-performing 25% of parameter combinations in 3 other periods. In defense of our strategy, the
number of splits is relatively low. Most statisticians agree that at least 100 samples are needed for
meaningful results, so this analysis offers only a small glimpse into the true performance of the
SMA crossover.

So, how can we make all of this simpler?

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/cross-validation/index.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/CrossValidation.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/splitter/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/
name: Splitter
type: page
icon: material-check-all
---

# Splitter

The manual approach we used earlier can be divided into three distinct steps: splitting the entire
period into sub-periods, applying a UDF to each sub-period and merging its outputs, and analyzing
the merged outputs with data science tools. The first two steps are easy to automate. For example,
[scikit-learn](https://scikit-learn.org/stable/) provides several
[classes for cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html),
each designed to take an array and operate on chunks of it. However, that otherwise excellent
Python package lacks robust cross-validation schemes for time series data, tools for charting and
analyzing split distributions, and an easy-to-extend interface for custom use cases. It is also
focused on machine learning (ML) models trained on one dataset and validated on another by making
predictions (as the name scikit-learn suggests), while rule-based algorithms that do not predict,
but instead produce a set of scores (one per test rather than per data point), receive less
attention.

That's why VBT takes a different approach and offers functionality designed for the needs of
quantitative analysts rather than ML practitioners. The core of this functionality is the class
[Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter), whose main
purpose is to create arbitrary splits and perform operations on them. The workings of this class
are straightforward: you call one of the class methods with the `from_` prefix (which should feel
familiar) to generate splits. In response, a splitter instance is returned, with splits and their
labels stored in a memory-efficient array format. This instance can be used to analyze split
distribution, chunk array-like objects, and run UDFs. Get ready—this class alone contains twice as
many lines of code as the entire [backtesting.py](https://github.com/kernc/backtesting.py) library
:smiling_imp:

Let's create a splitter for the schema from our first example:

```pycon
>>> splitter = vbt.Splitter.from_rolling(
...     data.index, 
...     length=360, 
...     split=0.5,
...     set_labels=["IS", "OOS"]
... )
>>> splitter.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/splitter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/splitter.dark.svg#only-dark){: .iimg loading=lazy }

That's it! We now have a splitter that can manipulate the periods (blue and orange boxes on the
plot) and the data within them any way we like, with no more while-loops necessary. Before we dive
into the many built-in generation and analysis techniques, let's look under the hood and get
familiar with some basic concepts first.

## Schema

The smallest unit of a splitter is a _range_, which is a period of time that can be mapped onto
the data. In the plot above, you can count a total of 18 ranges: 9 blue and 9 orange. Multiple
ranges placed side by side and representing a single test are called a _split_. There are 9 splits
shown in the chart, so we expect one pipeline to be tested on 9 different data ranges. Different
range types within each split are called _sets_. Usually, there is either one set; two sets—such
as "training" and "test" (commonly used in backtesting); or three sets—"training," "validation,"
and "test" (commonly used in ML). The number of sets stays the same across all splits.

This schema fits perfectly into the philosophy of VBT, as we can represent everything in an
array format where rows are splits, columns are sets, and the elements are ranges:

```pycon
>>> splitter.splits
set                         IS                      OOS
split                                                  
0          slice(0, 180, None)    slice(180, 360, None)
1        slice(180, 360, None)    slice(360, 540, None)
2        slice(360, 540, None)    slice(540, 720, None)
3        slice(540, 720, None)    slice(720, 900, None)
4        slice(720, 900, None)   slice(900, 1080, None)
5       slice(900, 1080, None)  slice(1080, 1260, None)
6      slice(1080, 1260, None)  slice(1260, 1440, None)
7      slice(1260, 1440, None)  slice(1440, 1620, None)
8      slice(1440, 1620, None)  slice(1620, 1800, None)
```

Notice that the index contains split labels and the columns contain set labels. Unlike other
classes in VBT, the wrapper for this class does not represent time and assets, but splits and
sets. Time is tracked separately as [Splitter.index](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.index),
while assets are not tracked at all, since they have no effect on splitting.

```pycon
>>> splitter.index
DatetimeIndex(['2017-08-17 00:00:00+00:00', '2017-08-18 00:00:00+00:00',
                '2017-08-19 00:00:00+00:00', '2017-08-20 00:00:00+00:00',
                ...
                '2022-10-28 00:00:00+00:00', '2022-10-29 00:00:00+00:00',
                '2022-10-30 00:00:00+00:00', '2022-10-31 00:00:00+00:00'],
    dtype='datetime64[ns, UTC]', name='Open time', length=1902, freq='D')
              
>>> splitter.wrapper.index
RangeIndex(start=0, stop=9, step=1, name='split')

>>> splitter.wrapper.columns
Index(['IS', 'OOS'], dtype='object', name='set')
```

This design has a useful property: you can apply indexing directly to a splitter instance to
select specific splits and sets. Let's select the OOS set:

```pycon
>>> oos_splitter = splitter["OOS"]
>>> oos_splitter.splits
split
0      slice(180, 360, None)
1      slice(360, 540, None)
2      slice(540, 720, None)
3      slice(720, 900, None)
4     slice(900, 1080, None)
5    slice(1080, 1260, None)
6    slice(1260, 1440, None)
7    slice(1440, 1620, None)
8    slice(1620, 1800, None)
Name: OOS, dtype: object
```

This operation creates a completely new splitter for OOS ranges :open_mouth:

Why is this useful? Because you can select one set and apply a UDF to it, then select another
set and apply a completely different UDF to that one. Sounds like a prerequisite for CV, right?

### Range format

So, what do ranges look like? In the first example of this tutorial, we used a start and end date
to slice the data with `loc`. However, as we learned, the end date in a `loc` operation should be
inclusive, which makes it a bit tricky to ensure that neighboring ranges do not overlap. Also,
dates cannot be used to slice NumPy arrays unless you first convert them into positions. That's
why the splitter uses integer-location based indexing and accepts the following range formats.
These can be used to slice both Pandas objects (using [pandas.DataFrame.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html))
and NumPy arrays:

* A list or array of integers, for example `[4, 3, 0]`.

* A slice object with ints, for example `slice(1, 7)`.

* A boolean array.

* A callable function that takes one argument (the calling Series or DataFrame) and returns a valid
output for indexing (one of the above formats).

For example, the slice `slice(1, 7)` covers the indices `[0, 1, 2, 3, 4, 5, 6]`:

```pycon
>>> index = vbt.date_range("2020", periods=14)
>>> index[slice(1, 7)]  # (1)!
DatetimeIndex(['2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05',
                '2020-01-06', '2020-01-07'],
              dtype='datetime64[ns]', freq='D')

>>> index[1], index[6]
(Timestamp('2020-01-02 00:00:00', freq='D'),
  Timestamp('2020-01-07 00:00:00', freq='D'))
```

1. This is the same as `index[1:7]`.

Separating the index and integer-location based ranges makes it much easier to design non-overlapping,
bug-free ranges.

#### Relative

The range format introduced above is called "fixed" because ranges do not depend on each other.
However, there is another range format called "relative," which makes one range depend on the
previous range. For example, instead of defining a range between fixed index positions `110` and
`150`, you can define a range that starts `10` points after the end of the previous range and has a
length of `40`. Such an instruction can be created using [RelRange](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.RelRange):

```pycon
>>> rel_range = vbt.RelRange(offset=10, length=40)
>>> rel_range
RelRange(
    offset=10, 
    offset_anchor='prev_end', 
    offset_space='free', 
    length=40, 
    length_space='free', 
    out_of_bounds='warn',
    is_gap=False
)
```

This instruction will be evaluated later by calling [RelRange.to_slice](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.RelRange.to_slice):

```pycon
>>> rel_range.to_slice(total_len=len(splitter.index), prev_end=100)  # (1)!
slice(110, 150, None)
```

1. `prev_end` is the end of the previous range.

Relative ranges are typically converted into fixed ones before building a splitter instance,
but splitter instances can also hold relative ranges if the user wants this behavior.

### Array format

How do we efficiently store such range formats? The most flexible formats are index arrays and
masks because they allow gaps and enable classical
[k-fold cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html), but they
may use a lot of memory even for simple cases. For example, consider a range covering one year of
1-minute data. This requires about 4MB of RAM as an integer array, or 0.5MB as a mask:

```pycon
>>> index = vbt.date_range("2020", "2021", freq="1min")
>>> range_ = np.arange(len(index))
>>> range_.nbytes / 1024 / 1024
4.02099609375

>>> range_ = np.full(len(index), True)
>>> range_.nbytes / 1024 / 1024
0.50262451171875
```

This means that just 100 splits and 2 sets would use 800MB and 100MB of RAM, respectively. This is
just for holding the splitter metadata in memory. Most "ranges" do not need to be that complex:
they usually only need predefined start and end points (which use at most 18 bytes of memory),
while still being able to pull the exact period of data as their integer or boolean array
equivalents. That's why the [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter)
class tries to convert any array into a slice whenever possible.

To make sure users can work with lightweight ranges, complex arrays, and relative ranges using the
same API, the array that stores ranges has an object data type:

```pycon
>>> splitter.splits_arr.dtype  # (1)!
dtype('O')
```

1. [Splitter.splits_arr](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.splits_arr)
is the NumPy (raw) version of [Splitter.splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.splits).

!!! info
    If an element of an array is a complex object, the array does not store the whole object—
    it only stores a [reference](https://realpython.com/lessons/object-value-vs-object-identity/)
    to the object.

    ```pycon
    >>> id(slice(0, 180, None))
    140627839366784
    ```

The object data type is totally valid. It only becomes an issue if you try to pass it to Numba,
but the splitting functionality is entirely in Python because the number of ranges (splits x sets)
is usually quite low. The main bottleneck is in running UDFs, not iterating over ranges 
(and don't worry, UDFs can still be run in Numba :relieved:). Another drawback is that the 
array can no longer be processed numerically with NumPy or Pandas. That's why we use the 
[Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter) class,
which can extract the meaning out of such an array.

This array format provides even more flexibility: you can use different range formats across 
different splits, store index arrays of different lengths, and since the `splits` array only 
stores references, you do not need to duplicate an array if two range values point to the same 
object. For example, let's build a splitter where ranges of different sizes are stored as 
integer-location based arrays:

```pycon
>>> range_00 = np.arange(0, 5)
>>> range_01 = np.arange(5, 15)
>>> range_10 = np.arange(15, 30)
>>> range_11 = np.arange(30, 50)

>>> ind_splitter = vbt.Splitter.from_splits(
...     data.index,
...     [[range_00, range_01], [range_10, range_11]],
...     fix_ranges=False
... )
>>> ind_splitter.splits
set                                                set_0  \
split                                                      
0                FixRange(range_=array([0, 1, 2, 3, 4]))   
1      FixRange(range_=array([15, 16, 17, 18, 19, 20,...   

set                                                set_1  
split                                                     
0      FixRange(range_=array([ 5,  6,  7,  8,  9, 10,...  
1      FixRange(range_=array([30, 31, 32, 33, 34, 35,... 

>>> ind_splitter.splits.loc[0, "set_1"]  # (1)!
FixRange(range_=array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]))

>>> ind_splitter.splits.loc[0, "set_1"].range_  # (2)!
array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29])
```

1. Get the fixed range for `range_01`.

2. Get the actual array.

!!! tip
    Why is the value of [FixRange](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.FixRange)
    called `range_` and not `range`? Because `range` is a reserved keyword in Python.

As we can see, each element of the splits array is a NumPy array wrapped with
[FixRange](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.FixRange), which works
perfectly fine. Why are arrays wrapped while slices are not? If sub-arrays were not wrapped, the
entire array would expand to three dimensions, which is not well-supported by Pandas.

## Preparation

In short, a splitter instance manages three objects:

1. `wrapper` - The wrapper that includes split and set labels ([ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)).

2. `index` - The data index, usually a [pandas.DatetimeIndex](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DatetimeIndex.html).

3. `splits` - A two-dimensional NumPy array containing range objects.

While you can prepare these objects manually, convenient methods are available to automate this
process. The main class method, which serves as the foundation for most other class methods, is
[Splitter.from_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_splits).
This method accepts a sequence of splits, optionally performs some pre-processing on each split,
and converts the sequence into an appropriate array format. It also generates the labels and the
wrapper. However, let's first focus on preparing the splits.

### Splits

Splitting involves dividing a larger range into smaller subranges. This is accomplished using the
[Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range)
method. This method requires a fixed range `range_` and a split specification `new_split`, and returns a
tuple of new fixed ranges. The returned ranges are always fixed, meaning this method is also used to
convert relative ranges into fixed ones. Its main function is to ensure that the provided specification
is valid and does not exceed any bounds. For example, you can generate two ranges: one occupying 75% of
the space and the other taking up the remaining 25%.

```pycon
>>> vbt.Splitter.split_range(
...     slice(None),  # (1)!
...     (vbt.RelRange(length=0.75), vbt.RelRange()),  # (2)!
...     index=data.index
... )
(slice(0, 1426, None), slice(1426, 1902, None))
```

1. Use the entire range for splitting. This is equivalent to `index[None:None]` or simply `index[:]`.
See [this explanation](https://stackoverflow.com/a/509295).

2. If the length is not specified, the range uses 100% of the remaining space.

!!! tip
    This method is a hybrid: it can be called as either a class method or an instance method.
    If you call it on an instance, you do not need to provide the index, since it is already stored:

    ```pycon
    >>> splitter.split_range(
    ...     slice(None),
    ...     (vbt.RelRange(length=0.75), vbt.RelRange())
    ... )
    (slice(0, 1426, None), slice(1426, 1902, None))
    ```

These slices can then be used to slice your data:

```pycon
>>> data[slice(0, 1426, None)]  # (1)!
<vectorbtpro.data.custom.binance.BinanceData at 0x7fe12a7df310>
```

1. This is the same as `data.iloc[:1426]`.

The two relative ranges above can also be replaced with a single number, which represents the 
length reserved for the first range, with the remaining space assigned to the second range:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     0.75, 
...     index=data.index
... )
(slice(0, 1426, None), slice(1426, 1902, None))
```

In cross-validation, you often want to set a fixed length for the OOS period and assign the rest to
the IS period. You can do this with a negative number, which reverses the processing order. For
example, to make the OOS period 25% of the total:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     -0.25,
...     index=data.index
... )
(slice(0, 1427, None), slice(1427, 1902, None))
```

!!! tip
    Why do the results differ? This is due to rounding:

    ```pycon
    >>> int(0.75 * len(data.index))
    1426

    >>> len(data.index) - int(0.25 * len(data.index))
    1427
    ```

You can also do this manually:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (vbt.RelRange(), vbt.RelRange(length=0.25)),
...     backwards=True,
...     index=data.index
... )
(slice(0, 1427, None), slice(1427, 1902, None))
```

Relative ranges defined only by length can be replaced with numbers for convenience.
For instance, to make the OOS period 30 data points long:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (1.0, 30), 
...     backwards=True,
...     index=data.index
... )
(slice(0, 1872, None), slice(1872, 1902, None))
```

!!! tip
    How does the method decide if the length is relative or absolute? If the number is between 0 and 1,
    it is treated as a relative length; otherwise, it is treated as the number of data points.

When using relative lengths, you can specify the reference space for the length using
[RelRange.length_space](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.RelRange.length_space).
By default, the length is relative to the remaining space (from the right-most bound of the previous
range to the right-most bound of the whole period), but you can set it to be relative to the entire
space instead. For example, here we define three ranges: 40%, 40%, and 20%:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (
...         vbt.RelRange(length=0.4, length_space="all"), 
...         vbt.RelRange(length=0.4, length_space="all"),
...         vbt.RelRange()
...     ),
...     index=data.index
... )
(slice(0, 760, None), slice(760, 1520, None), slice(1520, 1902, None))
```

To create a gap between two ranges, use an offset. Offsets, like lengths, can be relative or
absolute. Each offset also has an anchor, which defaults to the right-most bound of the previous
range (if one exists, otherwise 0). For example, to require a gap of 1 point between the two ranges:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None),
...     (vbt.RelRange(length=0.75), vbt.RelRange(offset=1)),
...     index=data.index
... )
(slice(0, 1426, None), slice(1427, 1902, None))
```

You can achieve the same result by placing a relative range of 1 data point between the ranges and
enabling [RelRange.is_gap](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.RelRange.is_gap):

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (
...         vbt.RelRange(length=0.75), 
...         vbt.RelRange(length=1, is_gap=True),
...         vbt.RelRange()
...     ),
...     index=data.index
... )
(slice(0, 1426, None), slice(1427, 1902, None))
```

This method is not only useful for converting relative ranges into fixed ones, but also for
optimizing target ranges for better memory efficiency. For example, if you make the first range an
array without gaps and the second range an array with gaps, you will see:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (np.array([3, 4, 5]), np.array([6, 8, 10])),
...     index=data.index
... )
(slice(3, 6, None), array([ 6,  8, 10]))
```

Here, the method optimized the first array into a slice, but not the second. If you do not want this
conversion, you can disable it using the `range_format` argument:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (np.array([3, 4, 5]), np.array([6, 8, 10])),
...     range_format="indices",
...     index=data.index
... )
(array([3, 4, 5]), array([ 6,  8, 10]))
```

Since [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter) uses
integer-location and mask-based indexing internally, you cannot use dates and times directly to slice
arrays. Fortunately, [Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range)
accepts slices and arrays as `pd.Timestamp`, `np.datetime64`, `datetime.datetime`, or even
datetime-like strings, and automatically converts them to integers for you. It also manages timezones.

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (slice("2020", "2021"), slice("2021", "2022")),  # (1)!
...     index=data.index
... )
(slice(867, 1233, None), slice(1233, 1598, None))

>>> data.index[867:1233]
DatetimeIndex(['2020-01-01 00:00:00+00:00', ..., '2020-12-31 00:00:00+00:00'],
    dtype='datetime64[ns, UTC]', name='Open time', length=366, freq='D')

>>> data.index[1233:1598]
DatetimeIndex(['2021-01-01 00:00:00+00:00', ..., '2021-12-31 00:00:00+00:00'],
    dtype='datetime64[ns, UTC]', name='Open time', length=365, freq='D')
```

1. The stop date must be exclusive.

The same applies to relative ranges, where the `offset` and `length` arguments can be provided as
`pd.Timedelta`, `np.timedelta64`, `datetime.timedelta`, or timedelta-like strings:

```pycon
>>> vbt.Splitter.split_range(
...     slice(None), 
...     (
...         vbt.RelRange(length="180 days"), 
...         vbt.RelRange(offset="1 day", length="90 days")
...     ),
...     index=data.index
... )
(slice(0, 180, None), slice(181, 271, None))
```

### Method

Returning to [Splitter.from_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_splits).
As shown earlier, each split is created by converting a split specification into a sequence of ranges,
one for each set. By passing multiple such specifications, you get a two-dimensional array, which is
available under [Splitter.splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.splits).
Let's manually generate expanding splits, with the OOS set having a fixed length of 25%:

```pycon
>>> manual_splitter = vbt.Splitter.from_splits(
...     data.index,
...     [
...         (vbt.RelRange(), vbt.RelRange(offset=0.5, length=0.25, length_space="all")),  # (1)!
...         (vbt.RelRange(), vbt.RelRange(offset=0.25, length=0.25, length_space="all")),
...         (vbt.RelRange(), vbt.RelRange(offset=0, length=0.25, length_space="all")),
...     ],
...     split_range_kwargs=dict(backwards=True),  # (2)!
...     set_labels=["IS", "OOS"]
... )
>>> manual_splitter.splits
set                      IS                      OOS
split                                               
0       slice(0, 476, None)    slice(476, 951, None)
1       slice(0, 952, None)   slice(952, 1427, None)
2      slice(0, 1427, None)  slice(1427, 1902, None)

>>> manual_splitter.plot().show()
```

1. Offset the range by 25% and reserve 25% of the entire period for it. Since we are moving
backwards (see below), the operation starts at the end of the period and moves to the left.

2. Reverses the range processing order and inverts the period anchors.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/manual_splitter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/manual_splitter.dark.svg#only-dark){: .iimg loading=lazy }

## Generation

We know how to build a splitter manually, but most CV schemes involve generating splits through
iteration, similar to what we did with the while-loop in our first example. Additionally, the
starting point of a split often depends on the previous split, which would require you to
explicitly call [Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range)
on each split to determine its boundaries. To reduce the amount of boilerplate code needed for
this workflow, [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter)
includes a collection of class methods, such as [Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling),
that can generate a logically coherent schema from a simple user query.

Most of these methods first divide the entire period into windows (either in advance or
iteratively), and then split each sub-period using the `split` argument, which is passed directly
to [Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range)
as `new_split`. In this way, the split specification becomes relative to the sub-period and not to
the entire period as we did earlier.

!!! info
    Internally, `slice(None)` (which we have used every time so far) is replaced by the window
    slice, so that `0.5` would split only the window in half, not the entire period.

### Rolling

The most important method for CV is [Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling),
which uses a simple while-loop to append splits until any split exceeds the right boundary of the
index. If the last split is shorter than the requested length, it is discarded, so there is usually
some unused space at the end of the backtesting period.

But the most interesting question is: where should we place the next split? By default, if there is
only one set, the next split is placed immediately after the previous one. If there are multiple
sets, the next split is placed right after the first (IS) range in the previous split, so that IS
ranges never overlap between splits. Of course, you can control the offset behavior using
`offset_anchor_set` (which range in the previous split acts as an anchor?), `offset_anchor` (whether
the left or right bound of that range acts as the anchor?), `offset` (the positive or negative
distance from the anchor), and `offset_space` (see [RelRange](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.RelRange)).

=== "One set"

    ```pycon
    >>> vbt.Splitter.from_rolling(
    ...     data.index,
    ...     length=360,  # (1)!
    ... ).plot().show()
    ```

    1. Length can be a number of bars (int), a percentage (float, such as `0.1` for 10% of the entire
    period), or a duration (string such as `"360 days"` or any timedelta-like object).

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling1.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling1.dark.svg#only-dark){: .iimg loading=lazy }

=== "One set with a gap"

    ```pycon
    >>> vbt.Splitter.from_rolling(
    ...     data.index,
    ...     length=360,
    ...     offset=90  # (1)!
    ... ).plot().show()
    ```

    1. Like length, an offset can also be a percentage (but relative to `offset_space`) or a
    duration.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling2.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling2.dark.svg#only-dark){: .iimg loading=lazy }

=== "One set with overlaps"

    ```pycon
    >>> vbt.Splitter.from_rolling(
    ...     data.index,
    ...     length=360,
    ...     offset=-0.5  # (1)!
    ... ).plot().show()
    ```

    1. Unlike length, the offset can also be negative (for example, as a duration: `"-180 days"`).
    Since `offset_space` defaults to the length of the previous split or anchor range, this moves the
    next window to the left by 50% of the previous window's length.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling3.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling3.dark.svg#only-dark){: .iimg loading=lazy }

=== "Two sets"

    ```pycon
    >>> vbt.Splitter.from_rolling(
    ...     data.index,
    ...     length=360,
    ...     split=0.5  # (1)!
    ... ).plot().show()
    ```

    1. Split instruction passed to [Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range)
    to split each window. Here, 50% is allocated to the first range and the remaining space to the
    second.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling4.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling4.dark.svg#only-dark){: .iimg loading=lazy }

=== "Two sets without overlaps"

    ```pycon
    >>> vbt.Splitter.from_rolling(
    ...     data.index,
    ...     length=360,
    ...     split=0.5,
    ...     offset_anchor_set=None  # (1)!
    ... ).plot().show()
    ```

    1. Use the entire previous split (`set_0 + set_1`) as the anchor. Otherwise, it would use only
    the first range in that split (`set_0`).

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling5.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_rolling5.dark.svg#only-dark){: .iimg loading=lazy }

Another common approach is to divide the entire period into `n` equally spaced, potentially
overlapping windows, as implemented by [Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling).
If the window length is `None` (that is, not provided), it simply calls
[Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling)
with the length set to `len(index) // n`. Note that unlike the previous method, this one does not
allow you to control the offset.

=== "Without length"

    ```pycon
    >>> vbt.Splitter.from_n_rolling(
    ...     data.index,
    ...     n=5,
    ...     split=0.5
    ... ).plot().show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_rolling1.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_rolling1.dark.svg#only-dark){: .iimg loading=lazy }

=== "With length and without overlaps"

    ```pycon
    >>> vbt.Splitter.from_n_rolling(
    ...     data.index,
    ...     n=3,  # (1)!
    ...     length=360,
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Splits are not overlapping when `n` is small enough.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_rolling2.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_rolling2.dark.svg#only-dark){: .iimg loading=lazy }

=== "With length and with overlaps"

    ```pycon
    >>> vbt.Splitter.from_n_rolling(
    ...     data.index,
    ...     n=7,  # (1)!
    ...     length=360,
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Splits are overlapping when `n` is large enough.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_rolling3.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_rolling3.dark.svg#only-dark){: .iimg loading=lazy }

The windows we generated above all have the same length, which makes it easier to conduct fair
experiments in backtesting. However, sometimes, especially when training ML models, we need each
training period to include all previous history. Such windows are called expanding and can be
generated automatically with [Splitter.from_expanding](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_expanding),
which works similarly to its rolling counterpart, except that the offset controls the number of
windows, the offset anchor is always the end of the previous split (window), and the `min_length`
argument specifies a minimum window length. There is also a method
[Splitter.from_n_expanding](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_expanding)
that lets you generate a predefined number of expanding windows.

=== "Using offset"

    ```pycon
    >>> vbt.Splitter.from_expanding(
    ...     data.index,
    ...     min_length=360,  # (1)!
    ...     offset=180,  # (2)!
    ...     split=-180  # (3)!
    ... ).plot().show()
    ```

    1. Same format as `length` in the rolling version.

    2. Same format as `offset` in the rolling version, but must always be positive.

    3. A minus sign means to set the resolution in reversed order: 180 bars for the second (OOS) range and
    everything else for the first (IS) set.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_expanding.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_expanding.dark.svg#only-dark){: .iimg loading=lazy }

=== "Using number of windows"

    ```pycon
    >>> vbt.Splitter.from_n_expanding(
    ...     data.index, 
    ...     n=5,
    ...     min_length=360,
    ...     split=-180
    ... ).plot().show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_expanding.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_expanding.dark.svg#only-dark){: .iimg loading=lazy }

### Anchored

Consider a scenario where you want to generate a set of splits, each lasting one year. Using any
of the approaches above, you would get splits that last for one year but would most likely start
somewhere in the middle of the year. But what if you want each split to start exactly at the
beginning of the year? Such time anchors are only possible by grouping or resampling. There are two
class methods in [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter)
that enable this behavior: [Splitter.from_ranges](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_ranges)
and [Splitter.from_grouper](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_grouper).

The first method uses [get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.get_index_ranges)
to translate a user query into a set of start and end indices. It allows you to provide custom start
and end dates, resample using a lookback period, select a time range within each day, and more—just
like resampling but with even more flexibility :pill:

=== "Yearly"

    ```pycon
    >>> vbt.Splitter.from_ranges(
    ...     data.index,
    ...     every="Y",  # (1)!
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Every year start. See [DateOffset objects](https://pandas.pydata.org/docs/user_guide/timeseries.html#dateoffset-objects).

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges1.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges1.dark.svg#only-dark){: .iimg loading=lazy }

=== "Quarterly from year start"

    ```pycon
    >>> vbt.Splitter.from_ranges(
    ...     data.index,
    ...     every="Q",  # (1)!
    ...     lookback_period="Y",  # (2)!
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Every quarter start.

    2. Look back all the way to the year start.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges2.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges2.dark.svg#only-dark){: .iimg loading=lazy }

=== "Last month for OOS"

    ```pycon
    >>> vbt.Splitter.from_ranges(
    ...     data.index,
    ...     every="Q",
    ...     lookback_period="Y",
    ...     split=(  # (1)!
    ...         vbt.RepEval("index.month != index.month[-1]"),  # (2)!
    ...         vbt.RepEval("index.month == index.month[-1]")
    ...     )
    ... ).plot().show()
    ```

    1. Same as above, but reserve the last calendar month for the OOS set in each split.

    2. Template should return a range (here, a mask). The variable `index` in each template will be
    substituted by the index of the respective split.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges3.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges3.dark.svg#only-dark){: .iimg loading=lazy }

=== "Expanding with last quarter for OOS"

    ```pycon
    >>> def qyear(index):  # (1)!
    ...     return index.to_period("Q")
    
    >>> vbt.Splitter.from_ranges(
    ...     data.index,
    ...     start=0,
    ...     fixed_start=True,  # (2)!
    ...     every="Q",
    ...     split=(
    ...         lambda index: qyear(index) != qyear(index)[-1],  # (3)!
    ...         lambda index: qyear(index) == qyear(index)[-1]  # (4)!
    ...     )
    ... ).plot().show()
    ```

    1. Function to classify each date by quarter.

    2. Make each range start from the very first bar.

    3. Assign each quarter except the last one to the IS set.

    4. Assign the last quarter to the OOS set.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges4.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_ranges4.dark.svg#only-dark){: .iimg loading=lazy }

The second method takes a grouping or resampling instruction and converts each group into a split.
It is based on the method [BaseIDXAccessor.get_grouper](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper)
and accepts a variety of formats from both VBT and Pandas, including
[pandas.Grouper](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Grouper.html)
and [pandas.Resampler](https://pandas.pydata.org/docs/reference/resampling.html). The only issue
you may encounter is incomplete splits, which can be filtered out using a template provided as
`split_check_template` and forwarded down to [Splitter.from_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_splits).

=== "Resampling annually"

    ```pycon
    >>> vbt.Splitter.from_grouper(
    ...     data.index,
    ...     by="Y",  # (1)!
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Recognized as a resampling instruction.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper1.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper1.dark.svg#only-dark){: .iimg loading=lazy }

=== "Removing incomplete years"

    ```pycon
    >>> def is_split_complete(index, split):  # (1)!
    ...     first_range = split[0]
    ...     first_index = index[first_range][0]
    ...     last_range = split[-1]
    ...     last_index = index[last_range][-1]
    ...     return first_index.is_year_start and last_index.is_year_end
    
    >>> vbt.Splitter.from_grouper(
    ...     data.index,
    ...     by="Y",
    ...     split=0.5,
    ...     split_check_template=vbt.RepFunc(is_split_complete)  # (2)!
    ... ).plot().show()
    ```

    1. Check whether the first day is the year's start and the last day is the year's end.
    This function will become a template, so VBT will substitute `index` for the full index
    and `split` for the current split. The function should return False if the split should be discarded.

    2. Wrap with [RepFunc](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.RepFunc) to make a template
    from `is_split_complete`.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper2.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper2.dark.svg#only-dark){: .iimg loading=lazy }

=== "Formatting labels"

    ```pycon
    >>> def format_split_labels(index, splits_arr):  # (1)!
    ...     years = map(lambda x: index[x[0]][0].year, splits_arr)  # (2)!
    ...     return pd.Index(years, name="split_year")
    
    >>> vbt.Splitter.from_grouper(
    ...     data.index,
    ...     by="Y",
    ...     split=0.5,
    ...     split_check_template=vbt.RepFunc(is_split_complete),
    ...     split_labels=vbt.RepFunc(format_split_labels)  # (3)!
    ... ).plot().show()
    ```

    1. Extract the year from each split. This function becomes a template, so VBT
    will substitute `index` for the full index and `splits_arr` for the splits array.
    It should return an index-like object.

    2. For each split, get the year of the first timestamp in the first range.

    3. Split labels can be an index-like object or a template.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper3.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper3.dark.svg#only-dark){: .iimg loading=lazy }

=== "Using grouping"

    ```pycon
    >>> vbt.Splitter.from_grouper(
    ...     data.index,
    ...     by=data.index.year,  # (1)!
    ...     split=0.5,
    ...     split_check_template=vbt.RepFunc(is_split_complete)  # (2)!
    ... ).plot().show()
    ```

    1. Regular Pandas Index with years as values. Each unique value becomes a split label.

    2. You still need to check for split completeness, but at least labels are already formatted.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper4.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_grouper4.dark.svg#only-dark){: .iimg loading=lazy }

### Random

So far, we have generated windows based on a predefined schema. However, randomness also plays
an important role in CV, especially when it comes to [bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))
and [block bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Block_bootstrap) in particular.
The method [Splitter.from_n_random](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_random)
draws a given number of windows of (optionally) variable length. At the core of this method are two
callbacks: `length_choice_func` and `start_choice_func`, which select the next window's length
and start point, respectively. By default, they use [numpy.random.Generator.choice](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html),
which generates a random sample with replacement (meaning the same window can occur more than once).
Two additional callbacks, `length_p_func` and `start_p_func`, control the probabilities of picking
each entry (for example, to select more windows toward the end of the period).

=== "Fixed length"

    ```pycon
    >>> vbt.Splitter.from_n_random(
    ...     data.index,
    ...     n=50,
    ...     min_length=360,  # (1)!
    ...     seed=42,  # (2)!
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Without `max_length`, this becomes the fixed length.

    2. Set a seed to ensure the results are the same each run.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_random1.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_random1.dark.svg#only-dark){: .iimg loading=lazy }

=== "Variable length"

    ```pycon
    >>> vbt.Splitter.from_n_random(
    ...     data.index,
    ...     n=50,
    ...     min_length=30,
    ...     max_length=300,
    ...     seed=42,
    ...     split=0.5
    ... ).plot().show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_random2.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_random2.dark.svg#only-dark){: .iimg loading=lazy }

=== "With probabilities"

    ```pycon
    >>> def start_p_func(i, indices):  # (1)!
    ...     return indices / indices.sum()
    
    >>> vbt.Splitter.from_n_random(
    ...     data.index,
    ...     n=50,
    ...     min_length=30,
    ...     max_length=300,
    ...     seed=42,
    ...     start_p_func=start_p_func,
    ...     split=0.5
    ... ).plot().show()
    ```

    1. Accepts the iteration number and a set of potential start points.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_random3.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_n_random3.dark.svg#only-dark){: .iimg loading=lazy }

### Scikit-learn

For k-fold and many other standard CV schemes where scikit-learn excels, there is
a method [Splitter.from_sklearn](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_sklearn)
that can parse almost any cross-validator that subclasses scikit-learn's `BaseCrossValidator` class.

```pycon
>>> from sklearn.model_selection import KFold

>>> vbt.Splitter.from_sklearn(
...     data.index, 
...     KFold(n_splits=5)
... ).plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_sklearn.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_sklearn.dark.svg#only-dark){: .iimg loading=lazy }

!!! warning
    There is a temporal dependency between observations: it makes no sense to use values
    from the future to forecast values in the past, so ensure that the test period always
    follows the training period.

### Dynamic

The final and most flexible generation method involves calling a UDF that takes a context,
including all previously generated splits, and returns a new split to append.
This all happens in an infinite while loop; to break out of the loop, the UDF must return `None`.
As with many other methods in VBT that take functions as arguments, this method also uses
templates to substitute information from the context. The context itself
includes the appended and resolved splits (`splits`) and the bounds of each range in each split
(but only when `fix_ranges=True`), making the generation process a breeze.

=== "Roll one-year window each month"

    ```pycon
    >>> def split_func(index, prev_start):
    ...     if prev_start is None:  # (1)!
    ...         prev_start = index[0]
    ...     new_start = prev_start + pd.offsets.MonthBegin(1)  # (2)!
    ...     new_end = new_start + pd.DateOffset(years=1)  # (3)!
    ...     if new_end > index[-1] + index.freq:  # (4)!
    ...         return None
    ...     return [
    ...         slice(new_start, new_start + pd.offsets.MonthBegin(9)),  # (5)!
    ...         slice(new_start + pd.offsets.MonthBegin(9), new_end)
    ...     ]
    
    >>> vbt.Splitter.from_split_func(
    ...     data.index,
    ...     split_func=split_func,
    ...     split_args=(vbt.Rep("index"), vbt.Rep("prev_start")),  # (6)!
    ...     range_bounds_kwargs=dict(index_bounds=True)  # (7)!
    ... ).plot().show()
    ```

    1. If this is the first split, `prev_start` (the start index of the previous split) will be `None`.

    2. The start date of this window is the first day of the next month (inclusive).

    3. The end date of this window is the same date in the following year (exclusive).

    4. If the split is incomplete (the end date comes after the next allowable end date), abort.

    5. Allocate 9 months for the IS period and 3 months for the OOS period.

    6. Tell VBT to replace these strings with the appropriate context.

    7. Tell VBT to return the bounds of each range as timestamps, not as the number of bars.
    Otherwise, `prev_start` will be an integer.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_split_func1.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_split_func1.dark.svg#only-dark){: .iimg loading=lazy }

=== "Train one business week, test next"

    ```pycon
    >>> def get_next_monday(from_date):
    ...     if from_date.weekday == 0 and from_date.ceil("H").hour <= 9:  # (1)!
    ...         return from_date.floor("D")
    ...     return from_date.floor("D") + pd.offsets.Week(n=0, weekday=0)  # (2)!
    
    >>> def get_next_business_range(from_date):
    ...     monday_0000 = get_next_monday(from_date)
    ...     monday_0900 = monday_0000 + pd.DateOffset(hours=9)  # (3)!
    ...     friday_1700 = monday_0900 + pd.DateOffset(days=4, hours=8)  # (4)!
    ...     return slice(monday_0900, friday_1700)
    
    >>> def split_func(index, bounds):
    ...     if len(bounds) == 0:
    ...         from_date = index[0]  # (5)!
    ...     else:
    ...         from_date = bounds[-1][1][0]  # (6)!
    ...     train_range = get_next_business_range(from_date)
    ...     test_range = get_next_business_range(train_range.stop)
    ...     if test_range.stop > index[-1] + index.freq:
    ...         return None
    ...     return train_range, test_range

    >>> vbt.Splitter.from_split_func(
    ...     vbt.date_range("2020-01", "2020-03", freq="15min"),  # (7)!
    ...     split_func=split_func,
    ...     split_args=(vbt.Rep("index"), vbt.Rep("bounds")),
    ...     range_bounds_kwargs=dict(index_bounds=True)
    ... ).plot().show()
    ```

    1. If `from_date` is a Monday at 09:00 or earlier, normalize it to get Monday 00:00.

    2. Otherwise, normalize and find the next Monday 00:00.

    3. Add 9 hours to get Monday 09:00.

    4. Add 4 days and 9 hours to reach Friday 17:00.

    5. Start from the first date if this is the first split.

    6. Use the start date of the previous test set as the start date of the new train set.

    7. Generate a 15-minute index for this example.

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_split_func2.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/from_split_func2.dark.svg#only-dark){: .iimg loading=lazy }

## Validation

Consider the following splitter that divides the entire period into years, creating one split per year:

```pycon
>>> splitter = vbt.Splitter.from_ranges(
...     data.index,
...     every="Y",
...     split=0.5,
...     set_labels=["IS", "OOS"]
... )
>>> splitter.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/flawed_splitter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/flawed_splitter.dark.svg#only-dark){: .iimg loading=lazy }

By default, `closed_end` is set to `False` so that neighboring ranges do not overlap.
In this example, however, we intentionally made a mistake by setting `closed_end` to `True`.
This causes the splits to overlap by exactly one bar. How can we detect such a mistake after the fact?
The [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter) class 
provides several tools to help with this.

### Bounds

The first tool is calculating the bounds of each range. Bounds are represented by two numbers
(`index_bounds=False`) or dates (`index_bounds=True`): the start (always inclusive) and the end
(exclusive, unless you include it by using `right_inclusive=True`). Depending on your analysis needs,
bounds can be returned in two different formats. The first format is a three-dimensional NumPy array,
returned by [Splitter.get_bounds_arr](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_bounds_arr).
In this array, the first axis corresponds to splits, the second to sets, and the third to bounds:

```pycon
>>> bounds_arr = splitter.get_bounds_arr()  # (1)!
>>> bounds_arr.shape
(4, 2, 2)

>>> bounds_arr
[[[ 137  320]
  [ 320  503]]
  [[ 502  685]
  [ 685  868]]
  [[ 867 1050]
  [1050 1234]]
  [[1233 1416]
  [1416 1599]]]
```

1. Also available via the property [Splitter.bounds_arr](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.bounds_arr).

Another, perhaps more user-friendly, format is a DataFrame returned by
[Splitter.get_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_bounds).
In this DataFrame, each row represents a range and the columns represent the bounds:

```pycon
>>> bounds = splitter.get_bounds(index_bounds=True)  # (1)!
>>> bounds.shape
(8, 2)

>>> bounds
bound                         start                       end
split set                                                    
0     IS  2018-01-01 00:00:00+00:00 2018-07-03 00:00:00+00:00
      OOS 2018-07-03 00:00:00+00:00 2019-01-02 00:00:00+00:00
1     IS  2019-01-01 00:00:00+00:00 2019-07-03 00:00:00+00:00
      OOS 2019-07-03 00:00:00+00:00 2020-01-02 00:00:00+00:00
2     IS  2020-01-01 00:00:00+00:00 2020-07-02 00:00:00+00:00
      OOS 2020-07-02 00:00:00+00:00 2021-01-02 00:00:00+00:00
3     IS  2021-01-01 00:00:00+00:00 2021-07-03 00:00:00+00:00
      OOS 2021-07-03 00:00:00+00:00 2022-01-02 00:00:00+00:00
```

1. Also available as the properties [Splitter.bounds](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.bounds)
and [Splitter.index_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.index_bounds).

With this, you can see that some IS ranges start before the preceding OOS range ends:

```pycon
>>> bounds.loc[(0, "OOS"), "end"]
Timestamp('2019-01-02 00:00:00+0000', tz='UTC')

>>> bounds.loc[(1, "IS"), "start"]
Timestamp('2019-01-01 00:00:00+0000', tz='UTC')
```

### Masks

Another tool works with range masks. Since the index is shared by every range, we can convert a range
into a mask with the same length as the index, and then stack all masks into a single array. Another
benefit of masks is that you can combine them, use logical operators, reduce them, and check for `True`
values. The main drawback is memory usage: as shown earlier, 100 splits and 2 sets of 1 year of 1-minute
data would use up 100MB of RAM. Just like bounds, there are two methods available: one returns a
three-dimensional NumPy array and the other a DataFrame:
[Splitter.get_mask_arr](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_mask_arr)
and [Splitter.get_mask](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_mask).
Let's retrieve the mask in DataFrame format:

```pycon
>>> mask = splitter.get_mask()  # (1)!
>>> mask.shape
(1902, 8)

>>> mask
split                          0             1             2             3  \
set                           IS    OOS     IS    OOS     IS    OOS     IS   
Open time                                                                    
2017-08-17 00:00:00+00:00  False  False  False  False  False  False  False   
2017-08-18 00:00:00+00:00  False  False  False  False  False  False  False   
2017-08-19 00:00:00+00:00  False  False  False  False  False  False  False   
...                          ...    ...    ...    ...    ...    ...    ...   
2022-10-29 00:00:00+00:00  False  False  False  False  False  False  False   
2022-10-30 00:00:00+00:00  False  False  False  False  False  False  False   
2022-10-31 00:00:00+00:00  False  False  False  False  False  False  False   

split                             
set                          OOS  
Open time                         
2017-08-17 00:00:00+00:00  False  
2017-08-18 00:00:00+00:00  False  
2017-08-19 00:00:00+00:00  False  
...                          ...  
2022-10-29 00:00:00+00:00  False  
2022-10-30 00:00:00+00:00  False  
2022-10-31 00:00:00+00:00  False  

[1902 rows x 8 columns]
```

1. Also available via the property [Splitter.mask](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.mask).

In contrast to bounds, the DataFrame lists split and set labels (range labels) in columns
instead of rows. To illustrate how masks can be useful, let's answer this question:
which ranges cover the year 2021?

```pycon
>>> mask["2021":"2021"].any()
split  set
0      IS     False
        OOS    False
1      IS     False
        OOS    False
2      IS     False
        OOS     True
3      IS      True
        OOS     True
dtype: bool
```

You will notice the mistake again: there is an OOS range that clearly extends into the next year.
Here is another question: how many dates are covered by each set in each year?

```pycon
>>> mask.resample(vbt.offset("Y")).sum()  # (1)!
split                        0         1         2         3     
set                         IS  OOS   IS  OOS   IS  OOS   IS  OOS
Open time                                                        
2017-01-01 00:00:00+00:00    0    0    0    0    0    0    0    0
2018-01-01 00:00:00+00:00  183  182    0    0    0    0    0    0
2019-01-01 00:00:00+00:00    0    1  183  182    0    0    0    0
2020-01-01 00:00:00+00:00    0    0    0    1  183  183    0    0
2021-01-01 00:00:00+00:00    0    0    0    0    0    1  183  182
2022-01-01 00:00:00+00:00    0    0    0    0    0    0    0    1
```

To address potential memory issues, there are special approaches that convert only a subset of ranges
into a mask at a time. These approaches use two iteration schemes: by split and by set, implemented by
[Splitter.get_iter_split_masks](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_iter_split_masks)
and [Splitter.get_iter_set_masks](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_iter_set_masks),
respectively. Each method returns a [Python generator](https://realpython.com/introduction-to-python-generators/)
that you can use in a loop. Here is how you can answer the question above in a memory-efficient way
(if needed):

```pycon
>>> results = []
>>> for mask in splitter.get_iter_split_masks():
...     results.append(mask.resample(vbt.offset("Y")).sum())
>>> pd.concat(results, axis=1, keys=splitter.split_labels)
split                        0         1         2         3     
set                         IS  OOS   IS  OOS   IS  OOS   IS  OOS
Open time                                                        
2017-01-01 00:00:00+00:00    0    0    0    0    0    0    0    0
2018-01-01 00:00:00+00:00  183  182    0    0    0    0    0    0
2019-01-01 00:00:00+00:00    0    1  183  182    0    0    0    0
2020-01-01 00:00:00+00:00    0    0    0    1  183  183    0    0
2021-01-01 00:00:00+00:00    0    0    0    0    0    1  183  182
2022-01-01 00:00:00+00:00    0    0    0    0    0    0    0    1
```

### Coverage

Bounds and masks are convenient range formats that let's analyze ranges from different perspectives.
To save users extra work, there are additional methods that automate this analysis. Since we are mainly
interested in whether and how much splits, sets, and ranges overlap, there are four methods that quickly
provide these insights:
[Splitter.get_split_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_split_coverage)
for split coverage, [Splitter.get_set_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_set_coverage)
for set coverage, [Splitter.get_range_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_range_coverage)
for range coverage, and [Splitter.get_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_coverage)
for any of the above. For example, split-relative coverage will return the percentage of bars in the
index covered by each split, while total coverage returns the percentage of bars in the index covered by
any range.

!!! warning
    Most of the methods below, except for the plotting method, require the entire mask array
    to be loaded in memory.

```pycon
>>> splitter.get_split_coverage()  # (1)!
split
0    0.192429
1    0.192429
2    0.192955
3    0.192429
Name: split_coverage, dtype: float64

>>> splitter.get_set_coverage()  # (2)!
set
IS     0.384858
OOS    0.385384
Name: set_coverage, dtype: float64

>>> splitter.get_range_coverage()  # (3)!
split  set
0      IS     0.096215
        OOS    0.096215
1      IS     0.096215
        OOS    0.096215
2      IS     0.096215
        OOS    0.096740
3      IS     0.096215
        OOS    0.096215
Name: range_coverage, dtype: float64

>>> splitter.get_coverage()  # (4)!
0.768664563617245
```

1. Also available via the property [Splitter.split_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_coverage).

2. Also available via the property [Splitter.set_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.set_coverage).

3. Also available via the property [Splitter.range_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.range_coverage).

4. Also available via the property [Splitter.coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.coverage).

!!! note
    The default arguments always return a metric relative to the length of the index.

As we saw above, the first split covers 19.24% of the entire period, while both ranges in that split
occupy 9.62%, which is exactly 50% of the split. Both sets cover about the same period of time:
38.53% of the entire period. The last metric tells us that 23.14% of the period is not covered by the
splitter, which makes sense since the years 2017 and 2022 are incomplete, so no split was made for
those years. Finally, why do all ranges cover the same time except for the OOS set in split `2`? It is
because that year was a leap year, so the final months had one more day than those months in other
years:

```pycon
>>> splitter.index_bounds.loc[(2, "OOS"), "start"].is_leap_year
True
```

So far, we have analyzed coverage relative to the full index. However, there is a special argument,
`relative`, that lets us analyze splits and sets relative to total coverage, and ranges relative to split
coverage. For example, here is how to get the fraction of IS and OOS sets in their respective splits:

```pycon
>>> splitter.get_range_coverage(relative=True)
split  set
0      IS     0.500000
        OOS    0.500000
1      IS     0.500000
        OOS    0.500000
2      IS     0.498638
        OOS    0.501362
3      IS     0.500000
        OOS    0.500000
Name: range_coverage, dtype: float64
```

Most periods, except for the leap year, have a near-perfect 50/50 split as expected. We can expand our
analysis to check whether the sets overall also follow this 50/50 split:

```pycon
>>> splitter.get_set_coverage(relative=True)
set
IS     0.500684
OOS    0.501368
Name: set_coverage, dtype: float64
```

These two numbers do not sum to one, which (again) points to overlapping ranges. Using the `overlapping`
argument, we can check for any overlap of sets within each split, overlap of splits within each set, and
any overlaps of ranges globally:

```pycon
>>> splitter.get_split_coverage(overlapping=True)
split
0    0.0
1    0.0
2    0.0
3    0.0
Name: split_coverage, dtype: float64

>>> splitter.get_set_coverage(overlapping=True)
set
IS     0.0
OOS    0.0
Name: set_coverage, dtype: float64

>>> splitter.get_coverage(overlapping=True)
0.002051983584131327
```

Ranges do not overlap within each split or within each set. However, some overlapping ranges are
detected globally, which means they belong to different splits and sets. To get a better view, we can
visualize the coverage using [Splitter.plot_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.plot_coverage):

```pycon
>>> splitter.plot_coverage().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plot_coverage.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plot_coverage.dark.svg#only-dark){: .iimg loading=lazy }

The Y-axis shows the total number of ranges that cover a particular date. You can see that there are
three dates covered by two ranges at the same time.

The last and most powerful tool for detecting overlap is the overlap matrix,
which calculates overlaps between splits (`by="split"`), sets (`by="set"`), or ranges (`by="range"`).
If `normalize` is True, which is the default, the intersection of two range masks will be
normalized by their union. Although this operation is Numba-compiled, it can still be quite expensive:
100 splits with 2 sets would require comparing all `200 * 200 = 40000` range pairs to build
the range overlap matrix. Let's finally clarify which ranges are overlapping:

```pycon
>>> splitter.get_overlap_matrix(by="range", normalize=False)  # (1)!
split        0         1         2         3     
set         IS  OOS   IS  OOS   IS  OOS   IS  OOS
split set                                        
0     IS   183    0    0    0    0    0    0    0
      OOS    0  183    1    0    0    0    0    0
1     IS     0    1  183    0    0    0    0    0
      OOS    0    0    0  183    1    0    0    0
2     IS     0    0    0    1  183    0    0    0
      OOS    0    0    0    0    0  184    1    0
3     IS     0    0    0    0    0    1  183    0
      OOS    0    0    0    0    0    0    0  183
```

1. The default version is also available as the property
[Splitter.range_overlap_matrix](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.range_overlap_matrix).

### Grouping

Each method above (and many others, including [Splitter.plot](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.plot))
accepts the arguments `split_group_by` and `set_group_by`, which let you group splits and sets, respectively.
Their format matches the `group_by` argument, which appears throughout the VBT codebase.
For example, if you pass `True`, you can put all ranges in the same bucket and merge them.
You can also pass a list with the same length as the number of splits or sets, so that
splits or sets with the same unique value will be merged. The actual merging is done by
the method [Splitter.merge_split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.merge_split).

For example, let's retrieve the bounds of each entire split:

```pycon
>>> splitter.get_bounds(index_bounds=True, set_group_by=True)
bound                               start                       end
split set_group                                                    
0     group     2018-01-01 00:00:00+00:00 2019-01-02 00:00:00+00:00
1     group     2019-01-01 00:00:00+00:00 2020-01-02 00:00:00+00:00
2     group     2020-01-01 00:00:00+00:00 2021-01-02 00:00:00+00:00
3     group     2021-01-01 00:00:00+00:00 2022-01-02 00:00:00+00:00
```

This makes certain types of analysis much easier :magic_wand:

## Manipulation

We will end this page with an overview of the methods that can be used to modify a splitter.
Let's build a splitter that includes just one set representing the current year:

```pycon
>>> splitter = vbt.Splitter.from_grouper(
...     data.index, 
...     by=data.index.year.rename("split_year")  # (1)!
... )
```

1. Without renaming, the name will be derived from the index and will become "Open time".

Since the class [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter)
is a subclass of [Analyzable](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#analyzing), we can get
a quick and insightful overview of key metrics and plots:

```pycon
>>> splitter.stats()
Index Start             2017-08-17 00:00:00+00:00
Index End               2022-10-31 00:00:00+00:00
Index Length                                 1902
Splits                                          6
Sets                                            1
Coverage [%]                                100.0
Overlap Coverage [%]                          0.0
Name: agg_stats, dtype: object

>>> splitter.plots().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plots1.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plots1.dark.svg#only-dark){: .iimg loading=lazy }

!!! info
    Since there is only one split, most metrics are hidden from the statistics.

As we have seen, you can select specific splits and sets using standard Pandas indexing
(which is another great feature of [Analyzable](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#analyzing)).
Since we are not interested in incomplete years, let's remove the first and last splits:

```pycon
>>> splitter = splitter.iloc[1:-1]
>>> splitter.stats()
Index Start             2017-08-17 00:00:00+00:00
Index End               2022-10-31 00:00:00+00:00
Index Length                                 1902
Splits                                          4  << changed
Sets                                            1
Coverage [%]                             76.81388  << changed
Overlap Coverage [%]                          0.0
Name: agg_stats, dtype: object
```

Now, let's split the only set into three: a train set covering the first two quarters, a validation set
covering the third quarter, and a test set covering the last quarter. This is possible using the method
[Splitter.split_set](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_set),
which accepts the split specification and the labels for the new sets as `new_split` and `new_set_labels`,
respectively. We will use a function template to divide the set:

```pycon
>>> def new_split(index):  # (1)!
...     return [
...         np.isin(index.quarter, [1, 2]),  # (2)!
...         index.quarter == 3, 
...         index.quarter == 4
...     ]

>>> splitter = splitter.split_set(
...     vbt.RepFunc(new_split),  # (3)!
...     new_set_labels=["train", "valid", "test"]  # (4)!
... )
```

1. `index` will be recognized as an argument and replaced with the index
covered by the original set. Why not the entire index? Because we are only splitting a subset of it.

2. `index.quarter` returns a NumPy array with the quarter number for each date.

3. We can use a template here because it is accepted by
[Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range),
which is used to split each range of the original set.

4. These labels should include only the labels for the new sets.

!!! info
    Each operation on a splitter returns a new splitter: no information is changed in place,
    to avoid interfering with caching and to keep the splitter (like any other VBT object) side effect free.

Take a look at the new splitter:

```pycon
>>> splitter.stats()
Index Start                     2017-08-17 00:00:00+00:00
Index End                       2022-10-31 00:00:00+00:00
Index Length                                         1902
Splits                                                  4
Sets                                                    3
Coverage [%]                                     76.81388
Coverage [%]: train                             38.117771
Coverage [%]: valid                             19.348055
Coverage [%]: test                              19.348055
Mean Rel Coverage [%]: train                    49.623475
Mean Rel Coverage [%]: valid                    25.188263
Mean Rel Coverage [%]: test                     25.188263
Overlap Coverage [%]                                  0.0
Overlap Coverage [%]: train                           0.0
Overlap Coverage [%]: valid                           0.0
Overlap Coverage [%]: test                            0.0
Name: agg_stats, dtype: object

>>> splitter.plots().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plots2.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plots2.dark.svg#only-dark){: .iimg loading=lazy }

As you might have guessed, there is also a method that can merge multiple sets:
[Splitter.merge_sets](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.merge_sets).
Your homework is to merge the "valid" and "test" sets into "test" :wink:

We have done our job perfectly, so let's move on to applications! :airplane:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/cross-validation/splitter.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/CrossValidation.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/applications/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/
name: Applications
type: page
icon: material-check-all
---

# Applications

Now that we have a splitter instance, what should we do next?

Remember, cross-validation (CV) involves running a backtesting job on each range. Thanks to VBT's
ability to process two-dimensional data, there are two main approaches:

1. Split all arrays into chunks, combine them into two-dimensional arrays, and run a single, monolithic
backtesting job.

2. Run a backtesting job on each chunk separately.

There is also a hybrid approach. For instance, you can build a two-dimensional array for each set and
backtest it independently from the other sets. The best method depends on your RAM requirements and
performance needs. Two-dimensional arrays are processed faster, but adding too many columns can reduce
performance because your system may start using swap memory.

## Taking

Taking refers to extracting one or more "slices" from an array-like object. This is handled by the
[Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take) method.
This method receives an object, iterates over each specified range, and extracts the range from the object
using [Splitter.take_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take_range).
This straightforward method performs `arr.iloc[range_]` on Pandas-like arrays and `arr[range_]` on NumPy arrays.

Since many VBT classes inherit the indexing schema of Pandas, this method can also extract slices from
VBT objects such as [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

After collecting all slices, you can stack them by rows (`stack_axis=0`), by columns (`stack_axis=1`, which
is the default), stack only splits and sets, or choose not to stack them at all. If some slices should not
be stacked, they will be returned as a single Pandas Series containing the slices as values. This may seem
unusual—having an array as a value within another array—but it is more convenient for indexing than using
a list.

### Without stacking

Let's split the close price using the default arguments:

```pycon
>>> close_slices = splitter.take(data.close)
>>> close_slices
split_year  set  
2018        train    Open time
2018-01-01 00:00:00+00:00    13380.0...
            test     Open time
2018-07-01 00:00:00+00:00    6356.81...
2019        train    Open time
2019-01-01 00:00:00+00:00     3797.1...
            test     Open time
2019-07-01 00:00:00+00:00    10624.9...
2020        train    Open time
2020-01-01 00:00:00+00:00    7200.85...
            test     Open time
2020-07-01 00:00:00+00:00     9232.0...
2021        train    Open time
2021-01-01 00:00:00+00:00    29331.6...
            test     Open time
2021-07-01 00:00:00+00:00    33504.6...
dtype: object
```

If you are wondering about this format: it is a regular Pandas Series with the split and set labels
as the index and the close price slices as values—in other words, a `pd.Series` inside a `pd.Series`.
:stuck_out_tongue: Keep in mind that array values can be any complex Python objects. For example,
let's get the close price for the test set in 2020:

```pycon
>>> close_slices[2020, "test"]
Open time
2020-07-01 00:00:00+00:00     9232.00
2020-07-02 00:00:00+00:00     9086.54
2020-07-03 00:00:00+00:00     9058.26
                                  ...
2020-12-29 00:00:00+00:00    27385.00
2020-12-30 00:00:00+00:00    28875.54
2020-12-31 00:00:00+00:00    28923.63
Freq: D, Name: Close, Length: 184, dtype: float64
```

Bingo!

And here is how easy it is to apply a UDF to each range:

```pycon
>>> def get_total_return(sr):
...     return sr.vbt.to_returns().vbt.returns.total()

>>> close_slices.apply(get_total_return)  # (1)!
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64
```

1. Using [pandas.Series.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html).

#### Complex objects

One of the unique features of VBT is the standardized behavior of its classes. Since this package 
is highly specialized in processing Pandas and NumPy arrays, most classes act as a proxy between the user 
and a set of such arrays. Each class essentially extends the features of Pandas and NumPy and allows you 
to build connections between multiple arrays based on shared metadata stored in a 
[wrapper](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#wrapping). Because this wrapper can be sliced just like a 
regular Pandas array, you can also slice most VBT objects that contain this wrapper, including 
[Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter). You can slice these 
objects using the same indexing API provided by Pandas arrays (thanks to 
[indexing](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#indexing)), which means you can pass most VBT objects 
directly to [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)!

For example, let's analyze the performance of a portfolio during different market regimes.
First, we will use the forward-looking label generator
[TRENDLB](https://vectorbt.pro/pvt_41531c19/api/labels/generators/trendlb/#vectorbtpro.labels.generators.trendlb.TRENDLB)
to label each data point with either 1 (uptrend), 0 (downtrend), or NaN (unclassified).
Given the volatility of our data, we will mark an uptrend when the price rises by 100% from
its previous low, and a downtrend when the price falls by 50% from its previous high:

```pycon
>>> trendlb = data.run("trendlb", 1.0, 0.5)
>>> trendlb.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trendlb.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trendlb.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    If you are unsure which pair of thresholds to use, take a look at the plot produced by the labeler
    and choose the thresholds that work best for your needs.

Next, we will build a splitter that converts the labels into splits:

```pycon
>>> grouper = pd.Index(trendlb.labels.map({1: "U", 0: "D"}), name="trend")
>>> trend_splitter = vbt.Splitter.from_grouper(data.index, grouper)
>>> trend_splitter.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trend_splitter1.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trend_splitter1.dark.svg#only-dark){: .iimg loading=lazy }

In the next step, we will run the full grid of parameter combinations on the entire
period using column stacking, and extract the returns accessor of the type
[ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor),
which will let's analyze the returns. For comparison, we will also do the same with our baseline model.

```pycon
>>> hold_pf = vbt.Portfolio.from_holding(data)
>>> hold_returns_acc = hold_pf.returns_acc

>>> fast_sma, slow_sma = vbt.talib("SMA").run_combs(
...     data.close, np.arange(5, 50), short_names=["fast_sma", "slow_sma"])
>>> entries = fast_sma.real_crossed_above(slow_sma)
>>> exits = fast_sma.real_crossed_below(slow_sma)
>>> strat_pf = vbt.Portfolio.from_signals(
...     data, entries, exits, direction="both")
>>> strat_returns_acc = strat_pf.returns_acc
```

Now, take both slices from the accessor (remember that most VBT objects are indexable, including
accessors) and plot the Sharpe heatmap for each market regime:

```pycon
>>> hold_returns_acc_slices = trend_splitter.take(hold_returns_acc)
>>> strat_returns_acc_slices = trend_splitter.take(strat_returns_acc)
```

=== "Uptrends"

    ```pycon
    >>> hold_returns_acc_slices["U"].sharpe_ratio()
    3.4490778178230763

    >>> strat_returns_acc_slices["U"].sharpe_ratio().vbt.heatmap(
    ...     x_level="fast_sma_timeperiod",
    ...     y_level="slow_sma_timeperiod",
    ...     symmetric=True
    ... ).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/uptrend_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/uptrend_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

=== "Downtrends"

    ```pycon
    >>> hold_returns_acc_slices["D"].sharpe_ratio()
    -1.329832516209626

    >>> strat_returns_acc_slices["D"].sharpe_ratio().vbt.heatmap(
    ...     x_level="fast_sma_timeperiod",
    ...     y_level="slow_sma_timeperiod",
    ...     symmetric=True
    ... ).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/downtrend_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/downtrend_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

We can see that it takes a lot of effort (and some may even say luck) to pick the right parameter 
combination and consistently outperform the baseline. Since both pictures are completely different, we 
cannot rely on a single parameter combination. Instead, we have to recognize the current market regime 
and adjust our actions accordingly, which is a major challenge itself. This analysis should be used 
with caution, though: there may be position overflow from one market regime to another, which can 
skew the results, since both periods are part of a single backtest. Still, we have gained another 
piece of valuable information :detective:

But what if we try to take slices from the portfolio itself? This operation would fail because each 
split contains gaps, and a portfolio can only be indexed using non-interrupting ranges, not splits 
with gaps. Therefore, we need to break up each split into multiple smaller splits, also called 
"split parts". Fortunately, there are two features that can help with this: 
[Splitter.split_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.split_range) 
accepts an option "by_gap" as `new_split` to split a range by gaps, while 
[Splitter.break_up_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.break_up_splits) 
can apply this operation to each split. This way, we can flatten the splits so that only one trend 
period is processed at a time. We will also sort the splits by their starting index so they appear 
in the same temporal order as the labels:

```pycon
>>> trend_splitter = trend_splitter.break_up_splits("by_gap", sort=True)
>>> trend_splitter.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trend_splitter2.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trend_splitter2.dark.svg#only-dark){: .iimg loading=lazy }

One more trick: instead of calling [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take),
you can call `split` on the object directly!

```pycon
>>> strat_pf_slices = pf.split(trend_splitter)
>>> strat_pf_slices
trend  split_part
U      0             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
D      0             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
U      1             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
D      1             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
U      2             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
D      2             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
U      3             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
D      3             Portfolio(\n    wrapper=ArrayWrapper(\n       ...
dtype: object
```

Let's analyze the median. This means there are 50 percent of the parameter combinations with the same
value or better, and, conversely, 50 percent with the same value or worse.

```pycon
>>> trend_range_perf = strat_pf_slices.apply(lambda pf: pf.sharpe_ratio)
>>> median_trend_perf = trend_range_perf.median(axis=1)
>>> median_trend_perf
trend  split_part
U      0             4.829429
D      0            -0.058159
U      1             2.874709
D      1            -0.510703
U      2             2.220633
D      2             0.917946
U      3            -0.736989
D      3             0.225841
dtype: float64
```

Now, let's visualize it. Replace the labels in `trendlb` with the newly computed values.

```pycon
>>> trend_perf_ts = data.symbol_wrapper.fill().rename("trend_perf")
>>> for label, sr in trend_splitter.bounds.iterrows():
...     trend_perf_ts.iloc[sr["start"]:sr["end"]] = median_trend_perf[label]
>>> data.close.vbt.overlay_with_heatmap(trend_perf_ts).show()  # (1)!
```

1. Using [GenericAccessor.overlay_with_heatmap](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.overlay_with_heatmap)

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trend_perf.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/trend_perf.dark.svg#only-dark){: .iimg loading=lazy }

Do you notice anything unusual? At least 50 percent of the parameter combinations during the last
uptrend have a negative Sharpe :thinking: My explanation is that moving averages are lagging indicators,
and by the time they reacted to the previous sharp decline, the market had already reached the next top.
In other words, the rebound happened so quickly that any short positions from the earlier decline were
not able to close out on time.

### Column stacking

This section explores the second approach mentioned at the start of the page, where you apply a UDF to 
each range separately. But what if you stack all the slices into a single array and apply 
`get_total_return` just once for a performance improvement? As it turns out, you can manually stack 
the slices using [pandas.concat](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html):

```pycon
>>> close_stacked = pd.concat(
...     close_slices.values.tolist(), 
...     axis=1,  # (1)!
...     keys=close_slices.index  # (2)!
... )
>>> close_stacked
split_year                     2018       2019       2020       2021          
set                           train test train test train test train      test
Open time                                                                     
2018-01-01 00:00:00+00:00  13380.00  NaN   NaN  NaN   NaN  NaN   NaN       NaN
2018-01-02 00:00:00+00:00  14675.11  NaN   NaN  NaN   NaN  NaN   NaN       NaN
2018-01-03 00:00:00+00:00  14919.51  NaN   NaN  NaN   NaN  NaN   NaN       NaN
...                             ...  ...   ...  ...   ...  ...   ...       ...
2021-12-29 00:00:00+00:00       NaN  NaN   NaN  NaN   NaN  NaN   NaN  46464.66
2021-12-30 00:00:00+00:00       NaN  NaN   NaN  NaN   NaN  NaN   NaN  47120.87
2021-12-31 00:00:00+00:00       NaN  NaN   NaN  NaN   NaN  NaN   NaN  46216.93

[1461 rows x 8 columns]
```

1. Along columns.

2. Slice labels become column labels.

As you can see, even though this operation produces a lot of NaNs, the resulting format is 
perfectly acceptable to VBT. Let's apply our UDF to this array:

```pycon
>>> get_total_return(close_stacked)
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
Name: total_return, dtype: float64
```

Pure magic :sparkles:

However, the stacking approach above works only when splitting Pandas objects. What about NumPy arrays 
and more complex VBT objects? You can instruct [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
to handle the stacking for you. This method uses [column_stack_merge](https://vectorbt.pro/pvt_41531c19/api/base/merging/#vectorbtpro.base.merging.column_stack_merge)
to efficiently stack any array-like objects. For example, you can replicate the format above with a 
single line of code:

```pycon
>>> close_stacked = splitter.take(data.close, into="stacked")
>>> close_stacked.shape
(1461, 8)
```

To reduce the number of NaNs in the stacked data, you can reset the index of each slice before stacking 
by adding the prefix "reset_" to any "stacked" option provided as `into`:

```pycon
>>> close_stacked = splitter.take(data.close, into="reset_stacked")
>>> close_stacked
split_year      2018               2019               2020            \
set            train     test     train      test    train      test   
0           13380.00  6356.81   3797.14  10624.93  7200.85   9232.00   
1           14675.11  6615.29   3858.56  10842.85  6965.71   9086.54   
2           14919.51  6513.86   3766.78  11940.00  7344.96   9058.26   
..               ...      ...       ...       ...      ...       ...   
181              NaN  3695.32       NaN   7388.24  9138.55  27385.00   
182              NaN  3801.91       NaN   7246.00      NaN  28875.54   
183              NaN  3702.90       NaN   7195.23      NaN  28923.63   

split_year      2021            
set            train      test  
0           29331.69  33504.69  
1           32178.33  33786.55  
2           33000.05  34669.13  
..               ...       ...  
181              NaN  46464.66  
182              NaN  47120.87  
183              NaN  46216.93  

[184 rows x 8 columns]
```

You can also tell the method to align each slice by its end point instead of its start point, 
which will move NaNs to the beginning of the array:

```pycon
>>> close_stacked = splitter.take(data.close, into="from_end_stacked")
>>> close_stacked
split_year      2018               2019               2020            \
set            train     test     train      test    train      test   
0                NaN  6356.81       NaN  10624.93      NaN   9232.00   
1                NaN  6615.29       NaN  10842.85      NaN   9086.54   
2                NaN  6513.86       NaN  11940.00  7200.85   9058.26   
..               ...      ...       ...       ...      ...       ...   
181          5853.98  3695.32  12400.63   7388.24  9116.35  27385.00   
182          6197.92  3801.91  11903.13   7246.00  9192.56  28875.54   
183          6390.07  3702.90  10854.10   7195.23  9138.55  28923.63   

split_year      2021            
set            train      test  
0                NaN  33504.69  
1                NaN  33786.55  
2                NaN  34669.13  
..               ...       ...  
181         34494.89  46464.66  
182         35911.73  47120.87  
183         35045.00  46216.93  

[184 rows x 8 columns]
```

!!! tip
    If all slices have the same length, both alignments will produce the same array.

As you can see, there are two potential issues with this operation: the final array does not have 
a datetime index, and the slices can have different lengths, which means there may still be some NaNs 
in the array. Additionally, stacking training and test sets together is not always what you want 
since they belong to different pipelines. Instead, stack only the splits that belong to the same set, 
which will also make the slices roughly the same length, by using "reset_stacked_splits" as `into`:

```pycon
>>> close_stacked = splitter.take(data.close, into="reset_stacked_by_set")
>>> close_stacked
set
train    split_year      2018      2019     2020      2...
test     split_year     2018      2019      2020      2...
dtype: object
```

This format might look a bit unusual, but it is simply a Series with the set labels as the index 
and the stacked close price slices as the values:

```pycon
>>> close_stacked["train"]
split_year      2018      2019     2020      2021
0           13380.00   3797.14  7200.85  29331.69
1           14675.11   3858.56  6965.71  32178.33
2           14919.51   3766.78  7344.96  33000.05
..               ...       ...      ...       ...
179          6197.92  11903.13  9116.35  35911.73
180          6390.07  10854.10  9192.56  35045.00
181              NaN       NaN  9138.55       NaN

[182 rows x 4 columns]
```

By resetting the index, we save a significant amount of RAM: our arrays now store only 
`182 * 8 = 1456` values instead of `1461 * 8 = 11688`, which is an 88% reduction in memory usage.
But how do we access the index associated with each column? We can slice it just like a regular array!

```pycon
>>> index_slices = splitter.take(data.index)
>>> index_slices
split_year  set  
2018        train    DatetimeIndex(['2018-01-01 00:00:00+00:00', '2...
            test     DatetimeIndex(['2018-07-01 00:00:00+00:00', '2...
2019        train    DatetimeIndex(['2019-01-01 00:00:00+00:00', '2...
            test     DatetimeIndex(['2019-07-01 00:00:00+00:00', '2...
2020        train    DatetimeIndex(['2020-01-01 00:00:00+00:00', '2...
            test     DatetimeIndex(['2020-07-01 00:00:00+00:00', '2...
2021        train    DatetimeIndex(['2021-01-01 00:00:00+00:00', '2...
            test     DatetimeIndex(['2021-07-01 00:00:00+00:00', '2...
dtype: object
```

Another way to obtain index information is by attaching the bounds to the range labels 
using `attach_bounds`. This argument can be set to `True` or "index" to attach the bounds in either 
integer or datetime format. Let's use the latter and also enable `right_inclusive` to make the right 
bound inclusive, which is useful for indexing with `loc`:

```pycon
>>> close_stacked_wb = splitter.take(
...     data.close, 
...     into="reset_stacked_by_set",
...     attach_bounds="index",
...     right_inclusive=True
... )
>>> close_stacked_wb["train"]
split_year                      2018                      2019  \
start      2018-01-01 00:00:00+00:00 2019-01-01 00:00:00+00:00   
end        2018-06-30 00:00:00+00:00 2019-06-30 00:00:00+00:00   
0                           13380.00                   3797.14   
1                           14675.11                   3858.56   
2                           14919.51                   3766.78   
..                               ...                       ...   
179                          6197.92                  11903.13   
180                          6390.07                  10854.10   
181                              NaN                       NaN   

split_year                      2020                      2021  
start      2020-01-01 00:00:00+00:00 2021-01-01 00:00:00+00:00  
end        2020-06-30 00:00:00+00:00 2021-06-30 00:00:00+00:00  
0                            7200.85                  29331.69  
1                            6965.71                  32178.33  
2                            7344.96                  33000.05  
..                               ...                       ...  
179                          9116.35                  35911.73  
180                          9192.56                  35045.00  
181                          9138.55                       NaN  

[182 rows x 4 columns]
```

However, to keep the index clean, we will start with the arrays without bounds. We now have two arrays: 
one for training and another for testing. Next, let's modify our `sma_crossover_perf` pipeline from 
the first page to run on a single set. Replace the `data` argument with `close`, and add an argument 
for the frequency required by Sharpe since our index is no longer datetime-like.

```pycon
>>> @vbt.parameterized(merge_func="concat")
... def set_sma_crossover_perf(close, fast_window, slow_window, freq):
...     fast_sma = vbt.talib("sma").run(
...         close, fast_window, short_name="fast_sma", hide_params=True)  # (1)!
...     slow_sma = vbt.talib("sma").run(
...         close, slow_window, short_name="slow_sma", hide_params=True) 
...     entries = fast_sma.real_crossed_above(slow_sma)
...     exits = fast_sma.real_crossed_below(slow_sma)
...     pf = vbt.Portfolio.from_signals(
...         close, entries, exits, freq=freq, direction="both")
...     return pf.sharpe_ratio
```

1. Do not add a parameter to the index, as it will be added by the `parameterized` decorator.

Apply this function on the training set to get performance for each parameter combination and split:

```pycon
>>> train_perf = set_sma_crossover_perf(
...     close_stacked["train"],
...     vbt.Param(np.arange(5, 50), condition="x < slow_window"),
...     vbt.Param(np.arange(5, 50)),
...     data.index.freq,
...     _execute_kwargs=dict(
...         clear_cache=50,
...         collect_garbage=50
...     )
... )
```

[=100% "Combination 990/990"]{: .candystripe .candystripe-animate }

```pycon
>>> train_perf
fast_window  slow_window  split_year
5            6            2018          1.158471
                          2019          1.901410
                          2020         -0.426441
                          2021          0.052654
              7            2018          2.231909
                                              ...
47           49           2021         -0.446099
48           49           2018         -1.169584
                          2019          3.727154
                          2020         -1.913321
                          2021         -0.400270
Name: sharpe_ratio, Length: 3960, dtype: float64
```

A total of 990 parameter combinations ran in 20 seconds, or 20ms per run :fire:

Now, let's generate a performance heatmap for each split:

```pycon
>>> train_perf.vbt.heatmap(
...     x_level="fast_window",
...     y_level="slow_window",
...     slider_level="split_year",
...     symmetric=True
... ).show()
```

<div class="grid cards width-eighty" markdown>

- :material-monitor-dashboard:{ .lg .middle } __Dashboard__

    ---

    Run the notebook to view the dashboard!
    
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/split_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/split_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

</div>

Looking at the plot above, you can spot several yellow points that may be strong candidates for the strategy.
However, rather than picking the best parameter combination as we did on the first page, let's take a more 
robust approach by performing a neighborhood analysis. We will look for Sharpe ratios that are surrounded 
by other high Sharpe ratios, which helps to reduce the influence of outliers and to build a more robust optimizer.

This approach is well quantified by first converting the `train_perf` Series into a DataFrame using 
the [BaseAccessor.unstack_to_df](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.unstack_to_df)
accessor method, then applying [GenericAccessor.proximity_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.proximity_apply),
which rolls a two-dimensional window over the matrix and applies a UDF for reduction. This mini-pipeline 
should be applied separately to each split. We will use a window size of 2, meaning two rows and columns 
around the central point, for a total of `(n * 2 + 1) ** 2 = 25` values. As a UDF, we will use the median, 
so each value will indicate that at least 50% of its neighbors are better than it. The UDF will also 
filter out windows that have fewer than 20 valid values.

```pycon
>>> @njit
... def prox_median_nb(arr):  # (1)!
...     if (~np.isnan(arr)).sum() < 20:
...         return np.nan
...     return np.nanmedian(arr)  # (2)!

>>> prox_perf_list = []
>>> for split_label, perf_sr in train_perf.groupby("split_year"):  # (3)!
...     perf_df = perf_sr.vbt.unstack_to_df(0, [1, 2])  # (4)!
...     prox_perf_df = perf_df.vbt.proximity_apply(2, prox_median_nb)  # (5)!
...     prox_perf_sr = prox_perf_df.stack([0, 1])  # (6)!
...     prox_perf_list.append(prox_perf_sr.reindex(perf_sr.index))  # (7)!

>>> train_prox_perf = pd.concat(prox_perf_list)  # (8)!
>>> train_prox_perf
fast_window  slow_window  split_year
5            6            2018         NaN
                          2019         NaN
                          2020         NaN
                          2021         NaN
              7            2018         NaN
                                        ...
47           49           2021         NaN
48           49           2018         NaN
                          2019         NaN
                          2020         NaN
                          2021         NaN
Length: 3960, dtype: float64
```

1. This function computes the median of the proximity windows. The argument `arr` is a two-dimensional
window flattened to one dimension.

2. Since the window may still contain NaN values, use the NaN-resistant median.

3. Run the following for each split.

4. Create a DataFrame with one level as rows and the other two as columns.
Level names could also be used here.

5. Run the smoothing window over the entire DataFrame.

6. Convert the DataFrame into a Series by stacking all columns. Level names could be used as well.

7. Reindex the Series to match the original index and append it to the list.

8. Concatenate all Series in the list into one big Series.

At first glance, it might seem that all values are NaN, but let's check this:

```pycon
>>> train_prox_perf.vbt.heatmap(
...     x_level="fast_window",
...     y_level="slow_window",
...     slider_level="split_year",
...     symmetric=True
... ).show()
```

<div class="grid cards width-eighty" markdown>

- :material-monitor-dashboard:{ .lg .middle } __Dashboard__

    ---

    Run the notebook to view the dashboard!
    
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/split_prox_heatmap.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/split_prox_heatmap.dark.svg#only-dark){: .iimg loading=lazy }

</div>

Fans of [computer vision](https://en.wikipedia.org/wiki/Computer_vision) might recognize what we did above:
we used a 5x5 neighboring window as a [filter](https://ai.stanford.edu/~syyeung/cvweb/tutorial1.html)
that replaces each pixel with the median pixel value from itself and its neighboring pixels.
The result is a smoother image with sharp features removed (in our case, Sharpe outliers). For example,
in the first split, the highest Sharpe decreased from `2.5` to `1.3`, which is still impressive,
because it means there are points with at least 50% of neighboring points having a Sharpe of `1.3`
or higher! We can now search in each split for the parameter combination with the highest proximity
performance:

```pycon
>>> best_params = train_prox_perf.groupby("split_year").idxmax()
>>> best_params = train_prox_perf[best_params].index
>>> train_prox_perf[best_params]
fast_window  slow_window  split_year
10           24           2018          1.311910
9            39           2019          3.801643
14           19           2020          2.077684
31           41           2021          2.142695
dtype: float64
```

What are we waiting for? Let's test these combinations on our test set! But how do we
apply each parameter combination to each column in the test array? Normally, each parameter 
combination would be applied to the entire input, but here's the trick: use templates to instruct
the `parameterized` decorator to provide only one column at a time, matching the parameter combination
being processed.

```pycon
>>> test_perf = set_sma_crossover_perf(
...     vbt.RepEval(
...         "test_close.iloc[:, [config_idx]]",  # (1)!
...         context=dict(test_close=close_stacked["test"])
...     ),
...     vbt.Param(best_params.get_level_values("fast_window"), level=0),  # (2)!
...     vbt.Param(best_params.get_level_values("slow_window"), level=0),
...     data.index.freq
... )
>>> test_perf
fast_window  slow_window  split_year
10           24           2018         -0.616204
9            39           2019          0.017269
14           19           2020          4.768589
31           41           2021         -0.363900
Name: sharpe_ratio, dtype: float64
```

1. This template is replaced during processing of each parameter combination (also called a parameter
config). Here, we select the column from our test data with the same index as the parameter config.
The index is wrapped in a list to run the pipeline on a DataFrame with meaningful columns.

2. Extract the parameter values and assign both `fast_window` and `slow_window` the same
level to avoid computing a product between them.

Let's compare these values to the baseline:

```pycon
>>> def get_index_sharpe(index):
...     return data.loc[index].run("from_holding").sharpe_ratio

>>> index_slices.xs("test", level="set").apply(get_index_sharpe)  # (1)!
split_year
2018   -1.327655
2019   -0.788038
2020    4.425057
2021    1.304871
dtype: float64
```

1. Each value in `index_slices` is an index, so we simply apply `get_index_sharpe` to it!

Not bad! Our model outperforms the baseline for three years in a row.

### Row stacking

Just as we stacked ranges along columns, we can also stack ranges along rows. The major
difference is that column stacking is intended for independent tests, while row stacking
combines ranges into the same test, introducing temporal dependency between them.

Row stacking is ideal for block resampling, which is essential for time-series bootstrapping.
The bootstrap is a flexible and powerful statistical tool used to quantify uncertainty.
Instead of obtaining independent datasets (which are scarce in finance), we repeatedly
generate new datasets by sampling observations from the original dataset.
Each of these "bootstrap datasets" is created by sampling _with replacement_, resulting in a
dataset of the same size as the original. This means that some observations may appear
multiple times, while others may not appear at all (about two-thirds of the original data
points are present in each bootstrap sample). Since we are working with time series, we
should not simply sample observations with replacement; instead, we create blocks of consecutive
observations and sample those. Afterward, we paste the sampled blocks together to create a
bootstrap dataset (learn more [here](https://asbates.rbind.io/2019/03/30/time-series-bootstrap-methods/)).

For our example, we will use the [moving block bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Time_series:_Moving_block_bootstrap),
which involves rolling a fixed-size window with an offset of one bar:

```pycon
>>> block_size = int(3.15 * len(data.index) ** (1 / 3))  # (1)!
>>> block_size
39

>>> block_splitter = vbt.Splitter.from_rolling(
...     data.index, 
...     length=block_size, 
...     offset=1,
...     offset_anchor="prev_start"  # (2)!
... )
>>> block_splitter.n_splits
1864
```

1. Default block length used in the R package.

2. By default, the anchor is the right bound of the previous window.

We have generated 1864 blocks. Next, we need to sample. To create one sample, shuffle the blocks 
(i.e., splits) with replacement. You can do this easily using [Splitter.shuffle_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.shuffle_splits).
We also need to limit the number of blocks so that the total number of data points 
roughly matches the original dataset:

```pycon
>>> size = int(block_splitter.n_splits / block_size)
>>> sample_splitter = block_splitter.shuffle_splits(size=size, replace=True)
>>> sample_splitter.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/sample_splitter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/sample_splitter.dark.svg#only-dark){: .iimg loading=lazy }

Let's calculate the returns, extract the slices that correspond to the blocks in the splitter, and
stack them along the rows:

```pycon
>>> returns = data.returns
>>> sample_rets = sample_splitter.take(
...     returns, 
...     into="stacked",  # (1)!
...     stack_axis=0  # (2)!
... )
>>> sample_rets
split  Open time                
920    2020-02-23 00:00:00+00:00    0.029587
        2020-02-24 00:00:00+00:00   -0.028206
        2020-02-25 00:00:00+00:00   -0.035241
        2020-02-26 00:00:00+00:00   -0.056956
        2020-02-27 00:00:00+00:00    0.004321
                                          ...
1617   2022-02-23 00:00:00+00:00   -0.025642
        2022-02-24 00:00:00+00:00    0.028918
        2022-02-25 00:00:00+00:00    0.023272
        2022-02-26 00:00:00+00:00   -0.002612
        2022-02-27 00:00:00+00:00   -0.036242
Name: Close, Length: 1833, dtype: float64
```

1. Stack all slices into one Series.

2. Stack along the rows.

We have created a "frankenstein" price series!

```pycon
>>> sample_rets.index = data.index[:len(sample_rets)]  # (1)!
>>> sample_cumrets = data.close[0] * (sample_rets + 1).cumprod()  # (2)!
>>> sample_cumrets.vbt.plot().show()
```

1. For plotting, we need to reset the index properly.

2. Calculate the cumulative returns and project them from the beginning of our data.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/frankenstein.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/frankenstein.dark.svg#only-dark){: .iimg loading=lazy }

However, one sample is not enough. To improve the accuracy of our estimates, we should generate 100,
1000, or even 10000 samples.

```pycon
>>> samples_rets_list = []
>>> for i in vbt.ProgressBar(range(1000)):
...     sample_spl = block_splitter.shuffle_splits(size=size, replace=True)
...     sample_rets = sample_spl.take(returns, into="stacked", stack_axis=0)
...     sample_rets.index = returns.index[:len(sample_rets)]
...     sample_rets.name = i
...     samples_rets_list.append(sample_rets)
>>> sample_rets_stacked = pd.concat(samples_rets_list, axis=1)
```

[=100% "Sample 1000/1000"]{: .candystripe .candystripe-animate }

We can now analyze the distribution of any statistic of interest:

```pycon
>>> sample_sharpe = sample_rets_stacked.vbt.returns.sharpe_ratio()
>>> sample_sharpe.vbt.boxplot(horizontal=True).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/sharpe_boxplot.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/sharpe_boxplot.dark.svg#only-dark){: .iimg loading=lazy }

This histogram provides an estimate of the distribution shape of the sample Sharpe, allowing us to
answer questions about how much the Sharpe varies across samples. Because this bootstrap
distribution is symmetric, we can use percentile-based confidence intervals. For example, a 95%
confidence interval is given by the 2.5th and 97.5th percentiles:

```pycon
>>> sample_sharpe.quantile(0.025), sample_sharpe.quantile(0.975)
(-0.13636235254958026, 1.726050620753774)
```

This approach can be applied to almost any statistic or estimator, including more
complex backtesting metrics.

## Applying

While the "taking" approach lets us work with object slices directly, the "applying" approach uses
[Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply)
to run a UDF on each range. This not only takes care of the "taking" for us, but also makes it easy
to combine the outputs from the UDF. First, this method determines the ranges to iterate over. Like
other methods, it accepts `split_group_by` and `set_group_by` to merge ranges, as well as the
arguments `split` and `set_` to select specific (merged) ranges. Next, while iterating over each
range, it substitutes any templates and other instructions in the positional and keyword arguments
that are meant for the UDF. For example, by wrapping an argument with the class
[Takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Takeable), the method
will select a slice from it and replace the instruction with that slice. The arguments prepared for
each iteration are collected into a list and passed to the executor,
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute), which you may already be
familiar with. The executor performs all iterations lazily and can also combine the outputs.
_Lazy_ here means that none of the arrays are sliced until the range is executed—which is great for
memory usage :heartbeat:

Let's look at a simple example that calculates the total return for each close slice:

```pycon
>>> splitter.apply(
...     get_total_return,
...     vbt.Takeable(data.close),  # (1)!
...     merge_func="concat"  # (2)!
... )
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64
```

1. All positional arguments after the function are passed to it.
You could also pass this as a keyword argument, for example, `sr=vbt.Takeable(data.close)`.

2. See [resolve_merge_func](https://vectorbt.pro/pvt_41531c19/api/base/merging/#vectorbtpro.base.merging.resolve_merge_func).

It's really that easy!

Templates can be used as well:

```pycon
>>> splitter.apply(
...     get_total_return,
...     vbt.RepFunc(lambda range_: data.close[range_]),  # (1)!
...     merge_func="concat"
... )
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64
```

1. `range_` will be replaced with the current range.

Or, you can manually select the range inside your function:

```pycon
>>> def get_total_return(range_, data):
...     return data.returns[range_].vbt.returns.total()

>>> splitter.apply(
...     get_total_return,
...     vbt.Rep("range_"),
...     data,
...     merge_func="concat"
... )
split_year  set  
2018        train   -0.534128
            test    -0.420523
2019        train    1.931243
            test    -0.337096
2020        train    0.270084
            test     2.165013
2021        train    0.211639
            test     0.318788
dtype: float64
```

!!! tip
    Results are slightly different because slicing returns is more accurate than slicing the price.

You can also take slices from more complex VBT objects:

```pycon
>>> def get_total_return(data):
...     return data.returns.vbt.returns.total()

>>> splitter.apply(
...     get_total_return,
...     vbt.Takeable(data),  # (1)!
...     merge_func="concat"
... )
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64
```

1. Splits the [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instance.

Let's run the function above on the whole split:

```pycon
>>> splitter.apply(
...     get_total_return,
...     vbt.Takeable(data),
...     set_group_by=True,
...     merge_func="concat"
... )
split_year
2018   -0.723251
2019    0.894908
2020    3.016697
2021    0.575665
dtype: float64
```

If you want to select specific ranges to run the pipeline on, just use `split` and `set_`, which can
be an integer, a label, or a sequence of such values:

```pycon
>>> splitter.apply(
...     get_total_return,
...     vbt.Takeable(data),
...     split=[2020, 2021],
...     set_="train",
...     merge_func="concat"
... )
split_year
2020    0.270084
2021    0.211639
dtype: float64
```

Now, let's apply this approach to cross-validate our SMA crossover strategy:

```pycon
>>> train_perf = splitter.apply(
...     sma_crossover_perf,
...     vbt.Takeable(data),
...     vbt.Param(np.arange(5, 50), condition="x < slow_window"),  # (1)!
...     vbt.Param(np.arange(5, 50)),
...     _execute_kwargs=dict(  # (2)!
...         clear_cache=50,
...         collect_garbage=50
...     ),
...     set_="train",  # (3)!
...     merge_func="concat"
... )
```

1. Parameters are processed inside `sma_crossover_perf`, not by `splitter.apply`.

2. This dictionary is also passed to `sma_crossover_perf`. We will hide the progress bar for
parameter combinations and show only the one for splits below.

3. You could also select this set using `splitter.loc["train"]`.

[=100% "Split 4/4"]{: .candystripe .candystripe-animate }

```pycon
>>> train_perf
split_year  fast_window  slow_window
2018        5            6              1.161661
                          7              2.238117
                          8              1.583246
                          9              1.369400
                          10             0.563251
                                              ...
2021        46           48            -0.238667
                          49            -0.437822
            47           48            -0.799317
                          49            -0.447324
            48           49            -0.401369
Length: 3960, dtype: float64
```

Let's find the best parameter combinations and pass them to the test set using templates:

```pycon
>>> best_params = train_perf.groupby("split_year").idxmax()
>>> best_params = train_perf[best_params].index
>>> train_perf[best_params]
split_year  fast_window  slow_window
2018        6            7              2.612006
2019        35           36             4.246530
2020        16           17             2.605763
2021        32           45             3.073246
dtype: float64

>>> best_fast_windows = best_params.get_level_values("fast_window")
>>> best_slow_windows = best_params.get_level_values("slow_window")

>>> test_perf = splitter.apply(
...     sma_crossover_perf,
...     vbt.Takeable(data),
...     vbt.RepFunc(lambda split_idx: best_fast_windows[split_idx]),  # (1)!
...     vbt.RepFunc(lambda split_idx: best_slow_windows[split_idx]),
...     set_="test",
...     merge_func="concat"
... )
>>> test_perf
split_year
2018    1.501380
2019    0.152723
2020    4.473387
2021   -1.147178
dtype: float64
```

1. `split_idx` will be replaced by the split number. Make sure the values in `best_fast_windows`
and `best_slow_windows` appear in the same order as the splits in the splitter. If the order is
different, use `split_label` instead.

We have now effectively outsourced the steps of range iteration, taking, and execution.

### Iteration schemes

The method [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply)
supports multiple iteration schemes. For example, it can iterate over the selected ranges in either
split-major or set-major order as a single sequence. It can also group the splits/sets of the same
set/split into a single bucket and run them as one iteration—split-wise and set-wise, respectively.
Why is this important? Consider a scenario where your UDF needs to write results to an object and then
access those results in a specific order. For instance, you might want an OOS range to read the
results from the previous IS range, which is common in parameterized CV. In that case, the iteration
can be in any order if execution is sequential, in any major order if (carefully) running across
multiple threads, and in split-wise order if execution uses multiple processes, since sets need to
share memory.

=== "Split-major"

    ```mermaid
    flowchart LR;
        subgraph 2019 [2019]
        id1["train"]
        id2["test"]
        
        id1 -->|"parallel"| id2;
        end
        subgraph 2020 [2020]
        id3["train"]
        id4["test"]
        
        id2 -->|"parallel"| id3;
        id3 -->|"parallel"| id4;
        end
    ```

=== "Set-major"

    ```mermaid
    flowchart LR;
        subgraph train [train]
        id1["2019"]
        id2["2020"]
        
        id1 -->|"parallel"| id2;
        end
        subgraph test [test]
        id3["2019"]
        id4["2020"]
        
        id2 -->|"parallel"| id3;
        id3 -->|"parallel"| id4;
        end
    ```

=== "Split-wise"

    ```mermaid
    flowchart LR;
        subgraph 2019 [2019]
        id1["train"]
        id2["test"]
        
        id1 -->|"sequence"| id2;
        end
        subgraph 2020 [2020]
        id3["train"]
        id4["test"]
        
        id3 -->|"sequence"| id4;
        end
        2019 -->|"parallel"| 2020;
    ```

=== "Set-wise"

    ```mermaid
    flowchart LR;
        subgraph train [train]
        id1["2019"]
        id2["2020"]
        
        id1 -->|"sequence"| id2;
        end
        subgraph test [test]
        id3["2019"]
        id4["2020"]
        
        id3 -->|"sequence"| id4;
        end
        train -->|"parallel"| test;
    ```

Let's cross-validate our SMA crossover strategy in a single call! When an IS range is processed,
find the parameter combination with the highest Sharpe ratio and return it. When an OOS range is
processed, access the parameter combination from the previous IS range and use it for testing.
If you look at the context available in each iteration, there is no variable that holds previous
results: you need to manually store, write, and access them. We will use set-major iteration so that
the splits in the training set are processed first.

```pycon
>>> def cv_sma_crossover(
...     data, 
...     fast_windows, 
...     slow_windows, 
...     split_idx,  # (1)!
...     set_idx,  # (2)!
...     train_perf_list  # (3)!
... ):
...     if set_idx == 0:  # (4)!
...         train_perf = sma_crossover_perf(
...             data,
...             vbt.Param(fast_windows, condition="x < slow_window"),
...             vbt.Param(slow_windows),
...             _execute_kwargs=dict(
...                 clear_cache=50,
...                 collect_garbage=50
...             )
...         )
...         train_perf_list.append(train_perf)  # (5)!
...         best_params = train_perf.idxmax()
...         return train_perf[[best_params]]  # (6)!
...     else:
...         train_perf = train_perf_list[split_idx]  # (7)!
...         best_params = train_perf.idxmax()
...         test_perf = sma_crossover_perf(
...             data,
...             vbt.Param([best_params[0]]),  # (8)!
...             vbt.Param([best_params[1]]),
...         )
...         return test_perf  # (9)!
    
>>> train_perf_list = []
>>> cv_perf = splitter.apply(
...     cv_sma_crossover,
...     vbt.Takeable(data),
...     np.arange(5, 50),
...     np.arange(5, 50),
...     vbt.Rep("split_idx"),
...     vbt.Rep("set_idx"),
...     train_perf_list,
...     iteration="set_major",
...     merge_func="concat"
... )
```

1. Replaced by the current split number.

2. Replaced by the current set number.

3. List containing the results from the training set.

4. If we are currently in the training set, run the optimization.

5. Append the results to the list.

6. Return the best result, which will be merged with the test results.
Wrap it with a list to return a Series, rather than a single value. This gives a nicer index.

7. Access the training results of the current split and test the best combination.

8. Use [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) for `sma_crossover_perf`
to return a Series instead of a single value for a better index.

9. Return the test result.

[=100% "Iteration 8/8"]{: .candystripe .candystripe-animate }

We end up with 8 iterations, one per range: the first 4 iterations are for the training ranges,
which took the most time to run, and the last 4 iterations are for the test ranges, which ran
almost instantly due to having just one parameter combination.
Let's concatenate the training results from `train_perf_dict` and take a look at what is inside `cv_perf`:

```pycon
>>> train_perf = pd.concat(train_perf_list, keys=splitter.split_labels)
>>> train_perf
split_year  fast_window  slow_window
2018        5            6              1.161661
                          7              2.238117
                          8              1.583246
                          9              1.369400
                          10             0.563251
                                              ...   
2021        46           48            -0.238667
                          49            -0.437822
            47           48            -0.799317
                          49            -0.447324
            48           49            -0.401369
Length: 3960, dtype: float64

>>> cv_perf
set    split_year  fast_window  slow_window
train  2018        6            7              2.612006
        2019        35           36             4.246530
        2020        16           17             2.605763
        2021        32           45             3.073246
test   2018        6            7              1.501380
        2019        35           36             0.152723
        2020        16           17             4.473387
        2021        32           45            -1.147178
dtype: float64
```

Same results, awesome!

### Merging

Another powerful feature of this method is that it can merge a wide range of outputs: from single
values and Series, to DataFrames, and even tuples of them. There are two main merging options:
merging all outputs into a single object (`merge_all=True`), and merging by the main unit of
iteration (`merge_all=False`). The first option works as follows: it flattens all outputs into a
single sequence, resolves the merging function using [resolve_merge_func](https://vectorbt.pro/pvt_41531c19/api/base/merging/#vectorbtpro.base.merging.resolve_merge_func),
and applies the merging function to that sequence. If no merging function is specified,
it wraps the sequence into a Pandas Series (even if each output is a complex object). If each
output is a tuple, it returns multiple such Series. Let's see this in action by returning
both entries and exits, and stacking them along the columns:

```pycon
>>> def get_entries_and_exits(data, fast_window, slow_window):
...     fast_sma = data.run("sma", fast_window, short_name="fast_sma")
...     slow_sma = data.run("sma", slow_window, short_name="slow_sma")
...     entries = fast_sma.real_crossed_above(slow_sma)
...     exits = fast_sma.real_crossed_below(slow_sma)
...     return entries, exits

>>> entries, exits = splitter.apply(
...     get_entries_and_exits,
...     vbt.Takeable(data),
...     20,
...     30,
...     merge_func="column_stack"
... )

>>> entries
split_year                  2018       2019       2020       2021       
set                        train test train test train test train   test
Open time                                                               
2018-01-01 00:00:00+00:00  False  NaN   NaN  NaN   NaN  NaN   NaN    NaN
2018-01-02 00:00:00+00:00  False  NaN   NaN  NaN   NaN  NaN   NaN    NaN
2018-01-03 00:00:00+00:00  False  NaN   NaN  NaN   NaN  NaN   NaN    NaN
...                          ...  ...   ...  ...   ...  ...   ...    ...
2021-12-29 00:00:00+00:00    NaN  NaN   NaN  NaN   NaN  NaN   NaN  False
2021-12-30 00:00:00+00:00    NaN  NaN   NaN  NaN   NaN  NaN   NaN  False
2021-12-31 00:00:00+00:00    NaN  NaN   NaN  NaN   NaN  NaN   NaN  False

[1461 rows x 8 columns]
```

You can then replace NaN values with `False` and backtest the results. If you prefer not to have as
many NaN values, use "reset_column_stack" as the `merge_func`. You can also provide multiple merging
functions (as a tuple) if your UDF returns outputs in different formats.

!!! note
    Even though you can return multiple different formats, the formats must remain
    the same across all ranges.

As mentioned earlier, you can also merge outputs by the main unit of iteration. Let's run the
same UDF but only stack the masks that belong to the same split:

```pycon
>>> entries, exits = splitter.apply(
...     get_entries_and_exits,
...     vbt.Takeable(data),
...     50,
...     200,
...     merge_all=False,
...     merge_func="row_stack"
... )

>>> entries.loc[2018]
set    Open time                
train  2018-01-01 00:00:00+00:00    False
        2018-01-02 00:00:00+00:00    False
        2018-01-03 00:00:00+00:00    False
        2018-01-04 00:00:00+00:00    False
        2018-01-05 00:00:00+00:00    False
                                      ...
test   2018-12-27 00:00:00+00:00    False
        2018-12-28 00:00:00+00:00    False
        2018-12-29 00:00:00+00:00    False
        2018-12-30 00:00:00+00:00    False
        2018-12-31 00:00:00+00:00     True
Length: 365, dtype: bool
```

With this approach, each mask covers a full year and can be backtested as a whole.
The additional `set` level provides information about which set each timestamp belongs to.

If the start and end dates for a range cannot be determined from the merged data alone,
you can instruct the method to attach this information. Let's get the total number of signals:

```pycon
>>> def get_signal_count(*args, **kwargs):
...     entries, exits = get_entries_and_exits(*args, **kwargs)
...     return entries.vbt.signals.total(), exits.vbt.signals.total()

>>> entry_count, exit_count = splitter.apply(
...     get_signal_count,
...     vbt.Takeable(data),
...     20,
...     30,
...     merge_func="concat",
...     attach_bounds="index"  # (1)!
... )
>>> entry_count
split_year  set    start                      end                      
2018        train  2018-01-01 00:00:00+00:00  2018-07-01 00:00:00+00:00    2
            test   2018-07-01 00:00:00+00:00  2019-01-01 00:00:00+00:00    4
2019        train  2019-01-01 00:00:00+00:00  2019-07-01 00:00:00+00:00    2
            test   2019-07-01 00:00:00+00:00  2020-01-01 00:00:00+00:00    3
2020        train  2020-01-01 00:00:00+00:00  2020-07-01 00:00:00+00:00    1
            test   2020-07-01 00:00:00+00:00  2021-01-01 00:00:00+00:00    2
2021        train  2021-01-01 00:00:00+00:00  2021-07-01 00:00:00+00:00    4
            test   2021-07-01 00:00:00+00:00  2022-01-01 00:00:00+00:00    1
dtype: int64
```

1. Calculate the integer bounds, map them to `data.index`, and attach them to the output's index.

Finally, to show just how powerful merging functions can be, let's create a custom merging function
that plots the returned signals based on the set!

```pycon
>>> def plot_entries_and_exits(results, data, keys):
...     set_labels = keys.get_level_values("set")  # (1)!
...     fig = data.plot(plot_volume=False)  # (2)!
...     train_seen = False
...     test_seen = False
...
...     for i in range(len(results)):  # (3)!
...         entries, exits = results[i]  # (4)!
...         set_label = set_labels[i]
...         if set_label == "train":
...             entries.vbt.signals.plot_as_entries(  # (5)!
...                 data.close,
...                 trace_kwargs=dict(  # (6)!
...                     marker=dict(color="limegreen"), 
...                     name=f"Entries ({set_label})",
...                     legendgroup=f"Entries ({set_label})",  # (7)!
...                     showlegend=not train_seen  # (8)!
...                 ),
...                 fig=fig
...             ),
...             exits.vbt.signals.plot_as_exits(
...                 data.close,
...                 trace_kwargs=dict(
...                     marker=dict(color="orange"), 
...                     name=f"Exits ({set_label})",
...                     legendgroup=f"Exits ({set_label})",
...                     showlegend=not train_seen
...                 ),
...                 fig=fig
...             )
...             train_seen = True
...         else:
...             entries.vbt.signals.plot_as_entries(
...                 data.close,
...                 trace_kwargs=dict(
...                     marker=dict(color="skyblue"), 
...                     name=f"Entries ({set_label})",
...                     legendgroup=f"Entries ({set_label})",
...                     showlegend=not test_seen
...                 ),
...                 fig=fig
...             ),
...             exits.vbt.signals.plot_as_exits(
...                 data.close,
...                 trace_kwargs=dict(
...                     marker=dict(color="magenta"), 
...                     name=f"Exits ({set_label})",
...                     legendgroup=f"Entries ({set_label})",
...                     showlegend=not test_seen
...                 ),
...                 fig=fig
...             )
...             test_seen = True
...     return fig  # (9)!

>>> splitter.apply(
...     get_entries_and_exits,
...     vbt.Takeable(data),
...     20,
...     30,
...     merge_func=plot_entries_and_exits,
...     merge_kwargs=dict(data=data, keys=vbt.Rep("keys")),
... ).show()
```

1. `keys` is a multi-index for `results` that includes split labels, set labels, and (optionally) bounds.

2. Plots OHLC.

3. Whenever `merge_all` is `True`, the results are flattened into a single list.
To determine which results belong to which set, we can use `keys`.

4. Each result is a tuple of entries and exits, just as returned by `get_entries_and_exits`.

5. Use [SignalsAccessor.plot_as_entries](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.plot_as_entries)
and [SignalsAccessor.plot_as_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.plot_as_exits)
to plot the signals as entry and exit markers, respectively.

6. Adjust the marker appearance based on the signal type and set.

7. By assigning the same legend group to multiple traces, we can ensure they react to user
interactions in the same way.

8. Show the legend only if no other trace with the same name has already been plotted;
otherwise, duplicates may appear.

9. Our merging function returns a Plotly figure.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plot_entries_and_exits.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/plot_entries_and_exits.dark.svg#only-dark){: .iimg loading=lazy }

As you can see, [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply)
is a flexible method that can execute any UDF on each range in the splitter. It can not only
return arrays in an analysis-friendly format, but also post-process and merge the outputs
using another UDF, making it ideal for quick CV.

### Decorators

Even the method above is not the limit of automation that VBT provides. Just like the
decorator [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized), which
can enhance any function with parameter processing logic, there is also a decorator
[@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
that can enrich almost any function with split processing logic. This decorator works simply:
it wraps a function, resolves a splitting specification into a splitter, and forwards all
arguments to [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply).
This makes the CV process pipeline-centric instead of splitter-centric.

There are several ways to make a function splittable:

```pycon
>>> @vbt.split(splitter=splitter)  # (1)!
... def get_split_total_return(data):
...     return data.returns.vbt.returns.total()

>>> get_split_total_return(vbt.Takeable(data))
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64

>>> def get_total_return(data):
...     return data.returns.vbt.returns.total()

>>> get_split_total_return = vbt.split(  # (2)!
...     get_total_return,
...     splitter=splitter
... )
>>> get_split_total_return(vbt.Takeable(data))
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64
```

1. Through decoration.

2. By passing the function as the first argument.

If you do not pass any arguments to the decorator, or if you want to override an argument,
you can add a prefix `_` to an argument name to direct it to
[@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
rather than to the function itself:

```pycon
>>> @vbt.split
... def get_split_total_return(data):
...     return data.returns.vbt.returns.total()

>>> get_split_total_return(vbt.Takeable(data), _splitter=splitter)
split_year  set  
2018        train   -0.522416
            test    -0.417491
2019        train    1.858493
            test    -0.322797
2020        train    0.269093
            test     2.132976
2021        train    0.194783
            test     0.379417
dtype: float64
```

One potential inconvenience with this approach is that for each new data input, you need to
manually construct a splitter using the index from the same data. To solve this, the decorator
can also accept instructions on how to create a splitter from the provided data. The instruction
consists of a method name (or an actual callable) provided as `splitter`, such as "from_rolling",
along with keyword arguments to this method as `splitter_kwargs`. For example, let's roll a window
of 30 days for the year 2020:

```pycon
>>> get_split_total_return(
...     vbt.Takeable(data.loc["2020":"2020"]),
...     _splitter="from_rolling",
...     _splitter_kwargs=dict(length="30d")
... )
split
0     0.321123
1    -0.088666
2    -0.250531
3     0.369418
4     0.093628
5    -0.059949
6     0.186424
7     0.020706
8    -0.069256
9     0.211424
10    0.372754
11    0.441005
dtype: float64
```

To avoid wrapping each object to be sliced with [Takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Takeable),
you can also provide a list of such arguments as `takeable_args`:

```pycon
>>> get_total_return_by_month = vbt.split(
...     get_total_return,
...     splitter="from_grouper",
...     splitter_kwargs=dict(by=vbt.RepEval("index.to_period('M')")),
...     takeable_args=["data"]
... )

>>> get_total_return_by_month(data.loc["2020":"2020"])
Open time
2020-01    0.298859
2020-02   -0.091746
2020-03   -0.248649
2020-04    0.297622
2020-05    0.070388
2020-06   -0.104131
2020-07    0.227844
2020-08   -0.012851
2020-09   -0.096073
2020-10    0.298694
2020-11    0.431230
2020-12    0.541364
Freq: M, dtype: float64
```

In this example, a new splitter is created from every data instance you provide.

You can also combine multiple decorators. For example, take the
`sma_crossover_perf` function from the first page, which we already decorated with
[@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized),
and decorate it so it splits the entire period into 60 percent for the training set and
40 percent for the test set:

```pycon
>>> cv_sma_crossover_perf = vbt.split(
...     sma_crossover_perf,
...     splitter="from_single",  # (1)!
...     splitter_kwargs=dict(split=0.6, set_labels=["train", "test"]),
...     takeable_args=["data"],
...     merge_func="concat",  # (2)!
... )
```

1. Use [Splitter.from_single](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_single)
to build a single split.

2. Any other arguments passed to `@split` will be forwarded to
[Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply).

Next, we will run the full parameter grid on the training set only.

```pycon
>>> train_perf = cv_sma_crossover_perf(
...     data.loc["2020":"2021"],
...     vbt.Param(np.arange(5, 50), condition="x < slow_window"),
...     vbt.Param(np.arange(5, 50)),
...     p_execute_kwargs=dict(
...         clear_cache=50,
...         collect_garbage=50
...     ),
...     _forward_kwargs_as={
...         "p_execute_kwargs": "_execute_kwargs"  # (1)!
...     },
...     _apply_kwargs=dict(set_="train")  # (2)!
... )
```

1. Both [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split) and
[@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) accept `execute_kwargs`.
To distinguish between them, we pass `p_execute_kwargs`, let
[@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split) rename it to
`_execute_kwargs` using `forward_kwargs_as`, and then forward the argument to
[@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized).

2. If you need to change any argument related to
[Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply),
you can pass it directly to [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split),
or include it in `_apply_kwargs` when calling an already decorated function.

[=100% "Split 1/1"]{: .candystripe .candystripe-animate }

```pycon
>>> train_perf
fast_window  slow_window
5            6              1.160003
              7              1.122994
              8              2.054193
              9              1.880043
              10             1.632951
                                  ...
46           48             1.362618
              49             1.415010
47           48             1.317795
              49             1.250835
48           49             0.160916
Length: 990, dtype: float64
```

We can now validate the optimization performance on the test set:

```pycon
>>> test_perf = cv_sma_crossover_perf(
...     data.loc["2020":"2021"],
...     train_perf.idxmax()[0],
...     train_perf.idxmax()[1],
...     _apply_kwargs=dict(set_="test")
... )
```

[=100% "Split 1/1"]{: .candystripe .candystripe-animate }

```pycon
>>> test_perf
1.2596407796960982
```

!!! tip
    If you want to receive a proper Series instead of a single value,
    disable `squeeze_one_split` and `squeeze_one_set` in
    [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply)
    using `_apply_kwargs`.

However, even this decorator is not the final step in automation: there is a special decorator
[@cv_split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.cv_split)
that combines [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
and [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) to run the full parameter
grid on the first set, and the best parameter combination on the remaining sets. How is
the best parameter combination chosen? The `selection` argument can be a template
that receives previous results (available as `grid_results` in the context) and returns the integer
position of the parameter combination you define as best. Additionally, the decorator
can return only the best results (`return_grid=False`), include the grid results
from the training set (`return_grid=True`), or provide the grid for all sets (`return_grid="all"`).

Since running the entire grid of parameter combinations can take a long time,
let's rewrite our pipeline with Numba to return the Sharpe ratio for a single parameter combination:

```pycon
>>> @njit(nogil=True)  # (1)!
>>> def sma_crossover_perf_nb(close, fast_window, slow_window, ann_factor):
...     fast_sma = vbt.nb.ma_nb(close, fast_window)  # (2)!
...     slow_sma = vbt.nb.ma_nb(close, slow_window)
...     entries = vbt.nb.crossed_above_nb(fast_sma, slow_sma)  # (3)!
...     exits = vbt.nb.crossed_above_nb(slow_sma, fast_sma)
...     sim_out = vbt.pf_nb.from_signals_nb(  # (4)!
...         target_shape=close.shape,
...         group_lens=np.full(close.shape[1], 1),
...         close=close,
...         long_entries=entries,
...         short_entries=exits,
...         save_returns=True
...     )
...     return vbt.ret_nb.sharpe_ratio_nb(  # (5)!
...         sim_out.in_outputs.returns, 
...         ann_factor
...     )
```

1. Disable [GIL](https://realpython.com/python-gil/) for multithreading.

2. Use [ma_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.ma_nb).

3. Use [crossed_above_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.crossed_above_nb).

4. Use [from_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signals_nb).

5. Use [sharpe_ratio_nb](https://vectorbt.pro/pvt_41531c19/api/returns/nb/#vectorbtpro.returns.nb.sharpe_ratio_nb).

Test the function on the full history:

```pycon
>>> sma_crossover_perf_nb(vbt.to_2d_array(data.close), 20, 30, 365)
array([1.04969317])
```

!!! note
    All Numba functions used here expect a two-dimensional NumPy array as input.

Finally, let's define and run CV in parallel:

```pycon
>>> cv_sma_crossover_perf = vbt.cv_split(
...     sma_crossover_perf_nb,
...     splitter="from_rolling",
...     splitter_kwargs=dict(
...         length=360,
...         split=0.5,
...         set_labels=["train", "test"]
...     ),
...     takeable_args=["close"],
...     parameterized_kwargs=dict(  # (1)!
...         engine="dask",
...         chunk_len="auto",  # (2)!
...     ),
...     merge_func="concat"  # (3)!
... )

>>> grid_perf, best_perf = cv_sma_crossover_perf(
...     vbt.to_2d_array(data.close),
...     vbt.Param(np.arange(5, 50), condition="x < slow_window"),
...     vbt.Param(np.arange(5, 50)),
...     pd.Timedelta(days=365) // data.index.freq,  # (4)!
...     _merge_kwargs=dict(wrapper=data.symbol_wrapper),  # (5)!
...     _index=data.index,  # (6)!
...     _return_grid="all"  # (7)!
... )
```

1. Keyword arguments for [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized).

2. Chunk length will be determined based on the number of CPUs.

3. The merging function passed to [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
will also be automatically passed to [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized).
Both decorators can use different merging functions if needed.

4. The annualization factor is calculated by dividing the year frequency by the data frequency.

5. Our Numba function returns a one-dimensional NumPy array (one value per column in the data),
so we need to pass the wrapper to assign each value to its corresponding column name.

6. `index` is an argument for [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
and can be used to provide an index if none can be extracted from the "takeable" arrays.

7. Runs the entire grid on each set. This will return both the grid results and the best results.
Note that the test set will still use the previous grid, not its own.

!!! info
    By default, the highest value is selected. To select the lowest value, set `selection="min"` in
    [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split).
    Also, remember to adjust the selection template if you use a different merging function.

[=100% "Split 9/9"]{: .candystripe .candystripe-animate }

That was fast! :dash:

!!! question
    Why is the performance so different compared to the previous version, which, by the way, uses the
    same Numba functions under the hood? Remember, when running a function a thousand times,
    even a 1-millisecond longer execution time adds up to a 1-second longer __total__ execution time.

Let's take a look at the CV results:

```pycon
>>> grid_perf
split  set    fast_window  slow_window  symbol 
0      train  5            6            BTCUSDT    1.771782
                            7            BTCUSDT    2.206458
                            8            BTCUSDT    2.705892
                            9            BTCUSDT    1.430768
                            10           BTCUSDT    0.851692
                                                        ...   
8      test   46           48           BTCUSDT    0.637332
                            49           BTCUSDT   -0.424650
              47           48           BTCUSDT   -0.214946
                            49           BTCUSDT    0.231712
              48           49           BTCUSDT   -0.351245
Length: 17820, dtype: float64

>>> best_perf
split  set    fast_window  slow_window  symbol 
0      train  19           34           BTCUSDT    4.392966
        test   19           34           BTCUSDT    0.535497
1      train  6            7            BTCUSDT    2.545991
        test   6            7            BTCUSDT   -1.101692
2      train  18           20           BTCUSDT    4.363491
        test   18           20           BTCUSDT    1.692070
3      train  14           18           BTCUSDT    3.615833
        test   14           18           BTCUSDT    0.035444
4      train  18           21           BTCUSDT    3.236440
        test   18           21           BTCUSDT    1.882290
5      train  20           27           BTCUSDT    3.371474
        test   20           27           BTCUSDT    2.574914
6      train  11           18           BTCUSDT    4.657549
        test   11           18           BTCUSDT   -2.067505
7      train  29           30           BTCUSDT    3.388797
        test   29           30           BTCUSDT    0.968127
8      train  25           28           BTCUSDT    2.149624
        test   25           28           BTCUSDT    1.256857
dtype: float64
```

For example, the test results show a negative correlation with the training results. This means that
the parameter combinations with the highest Sharpe often underperform when they had previously
outperformed. This actually makes sense, as BTC market regimes tend to change frequently.

```pycon
>>> best_train_perf = best_perf.xs("train", level="set")
>>> best_test_perf = best_perf.xs("test", level="set")
>>> best_train_perf.corr(best_test_perf)
-0.21641517083891232
```

To analyze further, let's look at the cross-set correlation for each parameter combination.
This shows how the performance of a parameter combination in the training set correlates
with its performance in the test set:

```pycon
>>> param_cross_set_corr = grid_perf\
...     .unstack("set")\
...     .groupby(["fast_window", "slow_window"])\
...     .apply(lambda x: x["train"].corr(x["test"]))
>>> param_cross_set_corr.vbt.heatmap(symmetric=True).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/param_cross_set_corr.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/param_cross_set_corr.dark.svg#only-dark){: .iimg loading=lazy }

Another way to analyze is by comparing the test performance of the best parameter combinations
to the test performance of all parameter combinations. Let's find the proportion of parameter
combinations that outperform the selected one in each split:

```pycon
>>> grid_test_perf = grid_perf.xs("test", level="set")
>>> grid_df = grid_test_perf.rename("grid").reset_index()  # (1)!
>>> del grid_df["fast_window"]  # (2)!
>>> del grid_df["slow_window"]
>>> best_df = best_test_perf.rename("best").reset_index()
>>> del best_df["fast_window"]
>>> del best_df["slow_window"]
>>> merged_df = pd.merge(grid_df, best_df, on=["split", "symbol"])  # (3)!
>>> grid_better_mask = merged_df["grid"] > merged_df["best"]  # (4)!
>>> grid_better_mask.index = grid_test_perf.index
>>> grid_better_cnt = grid_better_mask.groupby(["split", "symbol"]).mean()  # (5)!
>>> grid_better_cnt
split  symbol 
0      BTCUSDT    0.242424
1      BTCUSDT    0.988889
2      BTCUSDT    0.214141
3      BTCUSDT    0.404040
4      BTCUSDT    0.359596
5      BTCUSDT    0.963636
6      BTCUSDT    0.908081
7      BTCUSDT    0.342424
8      BTCUSDT    0.250505
dtype: float64
```

1. Convert the index columns to regular columns.

2. Remove the columns that should not be used for merging.

3. Merge both DataFrames by split and symbol.

4. Create a mask indicating where the grid results are better than the best results.

5. Use `sum()` for the count and `mean()` for the relative count.

The selected parameter combinations seem to outperform most others tested in the same period,
but some splits stand out—in splits `1`, `5`, and `6`, the selected parameter performed worse than
over 90% of the other combinations.

Finally, let's compare these results to the buy-and-hold baseline. To do this, we need to
extract prices for each split, but how can we do that without a splitter? It turns out
[@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
has an argument to return the splitter itself without running the pipeline :stuck_out_tongue:

```pycon
>>> cv_splitter = cv_sma_crossover_perf(
...     _index=data.index,  # (1)!
...     _return_splitter=True
... )
>>> stacked_close = cv_splitter.take(
...     data.close,
...     into="reset_stacked",
...     set_="test"  # (2)!
... )
>>> hold_pf = vbt.Portfolio.from_holding(stacked_close, freq="daily")
>>> hold_perf = hold_pf.sharpe_ratio
>>> hold_perf
split
0   -0.430642
1   -1.741407
2    3.408079
3   -0.556471
4    0.954291
5    3.241618
6    0.686198
7   -0.038013
8   -0.917722
Name: sharpe_ratio, dtype: float64
```

1. Pass the index or all other necessary arguments for parsing.

2. Select only the test set.

As we can see, the "taking" and "applying" approaches can be safely combined because the
underlying splitter is guaranteed to be built the same way and produce the same results,
unless the splitter method uses randomness and no seed has been set.

## Modeling

The [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter) class can also be
useful for cross-validating ML models. For example, you might use the
[SplitterCV](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/sklearn_/#vectorbtpro.generic.splitting.sklearn_.SplitterCV) class,
which acts as a regular scikit-learn cross-validator by subclassing `BaseCrossValidator`.
We will demonstrate its usage on a simple classification problem that predicts the best entry and exit times.

Before we begin, we need to decide on features and labels, which are the predictor and response variables,
respectively. Features are typically multi-column time series DataFrames, where each row contains several data
points (one per column) that should predict the same row in the labels. Labels are usually a single-column
time series Series to be predicted. Consider the following questions to guide your decision:

1. _"How can future performance be represented, preferably as a single number? Should it be the price at
the next bar, the average price change over the next week, a vector of weights for rebalancing,
a boolean signal, or something else?"_

2. _"What kind of historical data is likely to predict future performance? Should it be indicators, a
news sentiment index, past backtesting results, or something else?"_

3. _"Which ML model can handle this task?"_ (Remember, most models are limited to a few specific feature
and label formats.)

As an example, we will fit a [random forest classifier](https://en.wikipedia.org/wiki/Random_forest)
using all [TA-Lib](https://github.com/mrjbq7/ta-lib) indicators stacked as columns to predict
the binary labels generated by the label generator [TRENDLB](https://vectorbt.pro/pvt_41531c19/api/labels/generators/trendlb/#vectorbtpro.labels.generators.trendlb.TRENDLB),
where 1 means an uptrend and 0 means a downtrend. Sounds like fun :relieved:

First, run all the TA-Lib indicators on the data to obtain the feature set `X`:

```pycon
>>> X = data.run("talib")
>>> X.shape
(1902, 174)
```

This gives us 1902 rows (dates) and 174 columns (features).

Next, generate the labels `y` (using the same configuration as before):

```pycon
>>> trendlb = data.run("trendlb", 1.0, 0.5, mode="binary")
>>> y = trendlb.labels
>>> y.shape
(1902,)
```

Both the features and the labels contain NaNs, which we need to handle carefully.
If we remove rows with at least one NaN, we would lose all the data. Instead,
we first remove columns that are all NaNs or have only a single unique value. Also,
since `X` and `y` must have the same length, we need to filter rows in both datasets at the same time:

```pycon
>>> X = X.replace([-np.inf, np.inf], np.nan)
>>> invalid_column_mask = X.isnull().all(axis=0) | (X.nunique() == 1)
>>> X = X.loc[:, ~invalid_column_mask]
>>> invalid_row_mask = X.isnull().any(axis=1) | y.isnull()
>>> X = X.loc[~invalid_row_mask]
>>> y = y.loc[~invalid_row_mask]
>>> X.shape, y.shape
((1773, 144), (1773,))
```

!!! warning
    If you have used ML before, you will recognize the risk in the first logical operation:
    we are checking a condition for the entire column, which can introduce
    [look-ahead bias](https://www.investopedia.com/terms/l/lookaheadbias.asp).
    While our operation is not very dangerous since we only remove columns that would likely
    remain irrelevant in the future, other transformations like data normalization should always
    be included in a [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
    that is executed per split instead of once across all data.

We have successfully removed a total of 129 rows and 30 columns.

Next, we will define our classifier, which will learn `X` to predict `y`:

```pycon
>>> from sklearn.ensemble import RandomForestClassifier  # (1)!

>>> clf = RandomForestClassifier(random_state=42)
```

1. Use [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).

!!! question
    Why did we not rescale, normalize, or reduce the dimensionality of the features?
    Random forests are robust modeling techniques and can handle high noise levels
    and a large number of features.

To cross-validate the classifier, we will create a [SplitterCV](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/sklearn_/#vectorbtpro.generic.splitting.sklearn_.SplitterCV)
instance that splits the entire period into expanding windows with non-overlapping test periods of 180 bars:

```pycon
>>> cv = vbt.SplitterCV(
...     "from_expanding",  # (1)!
...     min_length=360,  # (2)!
...     offset=180,
...     split=-180,
...     set_labels=["train", "test"]
... )

>>> cv_splitter = cv.get_splitter(X)  # (3)!
>>> cv_splitter.plot().show()
```

1. Method of [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).

2. Other keyword arguments are passed to this method.

3. Get the actual splitter for plotting.

![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/sklsplitter.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/sklsplitter.dark.svg#only-dark){: .iimg loading=lazy }

Finally, we will run the classifier on each training period and check the accuracy of its predictions
on the corresponding test period. Although the accuracy score is the most basic classification
metric and has limitations, we will keep things simple for now:

```pycon
>>> from sklearn.model_selection import cross_val_score  # (1)!

>>> cross_val_score(clf, X, y, cv=cv, scoring="accuracy")
array([1.        , 0.20555556, 0.88333333, 0.72777778, 1.        ,
        0.92777778, 0.53333333, 0.30555556])
```

1. Use [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).

As we can see, only two splits underperform, and two splits even reach 100% accuracy.
How is this possible? Let's investigate! We need the raw predictions: we will use the actual splitter
to slice `X` and `y`, generate predictions for each test set using our classifier, and concatenate
all predictions into a single Series.

```pycon
>>> X_slices = cv_splitter.take(X)
>>> y_slices = cv_splitter.take(y)

>>> test_labels = []
>>> test_preds = []
>>> for split in X_slices.index.unique(level="split"):  # (1)!
...     X_train_slice = X_slices[(split, "train")]  # (2)!
...     y_train_slice = y_slices[(split, "train")]
...     X_test_slice = X_slices[(split, "test")]
...     y_test_slice = y_slices[(split, "test")]
...     slice_clf = clf.fit(X_train_slice, y_train_slice)  # (3)!
...     test_pred = slice_clf.predict(X_test_slice)  # (4)!
...     test_pred = pd.Series(test_pred, index=y_test_slice.index)
...     test_labels.append(y_test_slice)
...     test_preds.append(test_pred)

>>> test_labels = pd.concat(test_labels).rename("labels")  # (5)!
>>> test_preds = pd.concat(test_preds).rename("preds")
```

1. Iterate over the split labels.

2. Get the arrays for the current split.

3. Fit the classifier to the training data.

4. Generate predictions and wrap as a Series.

5. Concatenate all the prediction Series along rows.

Let's compare the actual labels (left tab) to the predictions (right tab):

=== "Labels"

    ```pycon
    >>> data.close.vbt.overlay_with_heatmap(test_labels).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/test_labels.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/test_labels.dark.svg#only-dark){: .iimg loading=lazy }

=== "Predictions"

    ```pycon
    >>> data.close.vbt.overlay_with_heatmap(test_preds).show()
    ```

    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/test_preds.light.svg#only-light){: .iimg loading=lazy }
    ![](https://vectorbt.pro/pvt_41531c19/assets/images/tutorials/cv/test_preds.dark.svg#only-dark){: .iimg loading=lazy }

The model appears to correctly classify many major uptrends and even issues an exit signal
at the most recent peak in a timely manner! Nevertheless, we should not rely solely on visual intuition.
Let's backtest the predictions.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     data.close[test_preds.index], 
...     test_preds == 1, 
...     test_preds == 0, 
...     direction="both"  # (1)!
... )
>>> pf.stats()
Start                         2018-05-12 00:00:00+00:00
End                           2022-04-20 00:00:00+00:00
Period                               1440 days 00:00:00
Start Value                                       100.0
Min Value                                     55.079685
Max Value                                   1238.365833
End Value                                    719.483655
Total Return [%]                             619.483655
Benchmark Return [%]                         388.524488
Total Time Exposure [%]                           100.0
Max Gross Exposure [%]                       532.441444
Max Drawdown [%]                              71.915142
Max Drawdown Duration                 244 days 00:00:00
Total Orders                                         20
Total Fees Paid                                     0.0
Total Trades                                         20
Win Rate [%]                                  52.631579
Best Trade [%]                               511.777894
Worst Trade [%]                              -27.856728
Avg Winning Trade [%]                          66.38609
Avg Losing Trade [%]                         -11.704147
Avg Winning Trade Duration            104 days 16:48:00
Avg Losing Trade Duration              35 days 05:20:00
Profit Factor                                  1.999834
Expectancy                                    32.802227
Sharpe Ratio                                   0.983656
Calmar Ratio                                   0.902507
Omega Ratio                                    1.202685
Sortino Ratio                                  1.627438
dtype: object
```

1. It is important that false-positive exit signals are just as costly as false-positive entry signals.

We have achieved some impressive statistics :star2:

If you are up for a challenge: build a pipeline to impute and (standard-)normalize the data,
[reduce the dimensionality](https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html)
of the features, and fit one of the [linear models](https://scikit-learn.org/stable/modules/linear_model.html)
to predict the average price change over the next `n` bars (making it a regression task).
Based on each prediction, you can then decide whether opening or closing a position is justified.

## Summary

Backtesting requires rethinking traditional cross-validation schemes centered around
ML, and VBT provides the necessary tools. The core of this functionality is the powerful
[Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter) class,
which not only provides time-series-safe splitting schemes but also lets you thoroughly
analyze generated splits and efficiently apply them to complex data. For example,
we can leverage its flexibility to either split data into slices for manual CV
or construct a pipeline and let the splitter handle slicing and execution for us.
There is even a decorator for parameterizing and cross-validating any Python function:
[@cv_split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.cv_split).
For ML tasks, the [SplitterCV](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/sklearn_/#vectorbtpro.generic.splitting.sklearn_.SplitterCV)
class offers a splitter-enhanced interface compatible with scikit-learn and many other packages,
including the scikit-learn compatible neural network library [skorch](https://github.com/skorch-dev/skorch),
which wraps PyTorch. As a result, validating both rule-based and ML-based models has never been easier :butterfly:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/tutorials/cross-validation/applications.py.txt){ .md-button target="blank_" }
[:material-notebook-outline: Notebook](https://github.com/polakowo/vectorbt.pro/blob/notebooks/CrossValidation.ipynb){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/more-tutorials/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
name: More tutorials
type: page
icon: material-link-variant-plus
---

# More tutorials

Explore additional tutorials on VBT provided by our partners.

## Qubit Quants

_Qubit Quants_ is an independent, professional, and open community dedicated to the research, 
development, and discussion of quantitative analytics, financial machine learning, and algorithmic 
trading.

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-gitbook:{ .lg .middle } __VectorBT PRO - Tutorial Series__](https://qubitquants.github.io/vbt-tuts-toc/index.html)

    ---

    ![](https://vectorbt.pro/pvt_41531c19/assets/partners/qubit-quants.png)

</div>

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-youtube:{ .lg .middle } __Discretionary Signals Backtesting using VectorBT PRO__](https://youtu.be/EQjdWAE613A?si=J6plzFmdBDSWBZoz)

    ---

    <iframe class="youtube-video" src="https://www.youtube.com/embed/EQjdWAE613A?si=kmphzzadn_Au9baW" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</div>

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-youtube:{ .lg .middle } __VectorBT PRO - Custom Plotly Dashboard__](https://youtu.be/Mek2Q6JZwTw?si=j7c_E_uPbo3m_y5m)

    ---

    <iframe class="youtube-video" src="https://www.youtube.com/embed/Mek2Q6JZwTw?si=4cfv-RyCWsjTI1e-" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</div>

## PyQuant News

_PyQuant News_ is a place for practitioners to level up their Python skills for quant finance, algorithmic 
trading, and data analysis, featuring free quant code.

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-gitbook:{ .lg .middle } __The PyQuant Newsletter__](https://pyquantnews.com/the-pyquant-newsletter/)

    ---

    ![](https://vectorbt.pro/pvt_41531c19/assets/partners/pqn.png)

</div>

<div class="grid cards" markdown>

- [:material-file-document:{ .lg .middle } __How to use VectorBT PRO to algorithmically find chart patterns__](https://pyquantnews.com/vectorbt-pro-algorithmically-find-chart-patterns/)

- [:material-file-document:{ .lg .middle } __Forecast future price trends (with projections)__](https://pyquantnews.com/forecast-future-price-trends-with-projections/)

- [:material-file-document:{ .lg .middle } __Easily cross-validate parameters to boost your trading strategy__](https://pyquantnews.com/easily-cross-validate-parameters-boost-strategy/)

- [:material-file-document:{ .lg .middle } __Backtest powerful intraday trading strategies__](https://www.pyquantnews.com/the-pyquant-newsletter/backtest-powerful-intraday-trading-strategies)

</div>

## Chad Thackray

_Chad Thackray_ creates educational programming content for traders on his YouTube channel. He
provides in-depth tutorials, coding insights, and practical applications to help improve trading strategies.

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-youtube:{ .lg .middle } __Take Profit Ladders In VectorBT PRO__](https://youtu.be/u4RABJMXhXc?si=wHrBS2HEVZ7btYsq)

    ---

    <iframe class="youtube-video" src="https://www.youtube.com/embed/u4RABJMXhXc?si=wkwT18xYQt4EUw0A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</div>

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-youtube:{ .lg .middle } __Cross Validation in VectorBT PRO__](https://youtu.be/_BSSPZplLHs?si=-22bt2hXW9pUBXvg)

    ---

    <iframe class="youtube-video" src="https://www.youtube.com/embed/_BSSPZplLHs?si=s-lqiyASBqeGigW9" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</div>

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-youtube:{ .lg .middle } __Multiple Timeframe Analysis in VectorBT PRO__](https://youtu.be/L6g3mslaTwE?si=viqfxQjMRjXSBGy7)

    ---

    <iframe class="youtube-video" src="https://www.youtube.com/embed/L6g3mslaTwE?si=viqfxQjMRjXSBGy7" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</div>

## Trade Science

_Trade Science_ is a YouTube channel dedicated to realistic technical analysis, systematic trading systems,
strategy and indicator reviews, and system optimization.

<div class="grid cards width-eighty" data-aos="fade-right" markdown>

- [:simple-youtube:{ .lg .middle } __How to do a Walk Forward Optimization in Python with Vectorbt__](https://youtu.be/RKE1ZXm00NY?si=mPDXX-tSC4X8rAtV)

    ---

    <iframe class="youtube-video" src="https://www.youtube.com/embed/RKE1ZXm00NY?si=dAEljjIS5trmIs1O" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

</div>

## Research

Below are research theses and papers that have utilized VBT:

<div class="grid cards" markdown>

- [:material-file-document:{ .lg .middle } __Improvements to Intraday Momentum Strategies Using Parameter Optimization and Different Exit Strategies__](https://dx.doi.org/10.2139/ssrn.5095349)

- [:material-file-document:{ .lg .middle } __Automation of Financial Portfolio Management using Machine Learning-Based Algorithmic Trading__](https://www.linkedin.com/in/martinandrovich/)

</div>

---
link: https://vectorbt.pro/pvt_41531c19/tutorials/to-be-continued/
parent: https://vectorbt.pro/pvt_41531c19/tutorials/
children:
  - https://vectorbt.pro/pvt_41531c19/tutorials/to-be-continued/#to-be-continued
name: To be continued...
type: page
---

Tutorials are a work in progress

---
link: https://vectorbt.pro/pvt_41531c19/documentation/overview/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
name: Overview
type: page
---

# Overview

Welcome to our documentation hub, your central resource for in-depth information about VBT's
main features and functionalities. Here, you will find a list of the major documentation sections:

<div class="grid cards" markdown>

- [:material-alphabetical-variant:{ .overview }__Fundamentals__](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/)

- [:material-cube-outline:{ .overview }__Building blocks__](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/)

- [:material-database:{ .overview }__Data__](https://vectorbt.pro/pvt_41531c19/documentation/data/)

- [:material-chart-timeline-variant:{ .overview }__Indicators__](https://vectorbt.pro/pvt_41531c19/documentation/indicators/)

- [:material-chart-areaspline:{ .overview }__Portfolio__](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/)

</div>

---
link: https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
name: Fundamentals
type: page
icon: material-alphabetical-variant
---

# Fundamentals

VBT was designed to address common performance challenges present in many backtesting libraries.
It is built on the idea that each trading strategy instance can be represented in a vectorized format.
This method allows multiple strategy instances to be combined into a single multi-dimensional array,
enabling highly efficient processing and straightforward analysis.

## Stack

Since trading data is time-series based, most aspects of backtesting can be represented as arrays. In 
particular, VBT works with [NumPy arrays](https://numpy.org/doc/stable/user/quickstart.html), which 
are ***very fast*** thanks to optimized, pre-compiled C code. NumPy arrays are supported by many 
scientific packages in the dynamic Python ecosystem, including Pandas, NumPy, and Numba. There is a good 
chance you have already used some of these packages!

While NumPy offers excellent performance, it is not always the most intuitive tool for time series 
analysis. Consider the following moving average example using NumPy:

```pycon
>>> from vectorbtpro import *

>>> def rolling_window(a, window):
...     shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)
...     strides = a.strides + (a.strides[-1],)
...     return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)

>>> np.mean(rolling_window(np.arange(10), 3), axis=1)
array([1., 2., 3., 4., 5., 6., 7., 8.])
```

While this approach is very fast, it can take some time to understand what is happening, and it requires 
experience to write such vectorized code correctly. What about other rolling functions used in more 
advanced indicators? And what about resampling, grouping, and other operations involving dates and times?

This is where [Pandas](https://pandas.pydata.org/docs/getting_started/overview.html) comes to the rescue! 
Pandas offers rich time series features, data alignment, NA-friendly statistics, groupby, merge and join 
methods, and many other useful tools. It has two primary data structures: 
[Series](https://pandas.pydata.org/docs/reference/api/pandas.Series.html) (one-dimensional) and
[DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) (two-dimensional).
You can think of them as NumPy arrays enhanced with valuable information like timestamps and column names.
Our moving average can be written in a single line:

```pycon
>>> index = vbt.date_range("2020-01-01", periods=10)
>>> sr = pd.Series(range(len(index)), index=index)
>>> sr.rolling(3).mean()
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64
```

VBT depends heavily on Pandas, but not in the way you might expect. Pandas has one major 
limitation for our purposes: it becomes slow when working with large datasets and user-defined functions. 
Many built-in functions like the rolling mean use [Cython](https://cython.org/) under the hood, making 
them fast enough. However, when you try to implement a more complex function, such as a rolling ranking 
metric involving multiple time series, things become complicated and slow. Additionally, what about 
functions that cannot be vectorized? For example, a portfolio strategy with money management cannot be 
simulated directly with vector calculations. In such cases, we need to write fast, iterative code that 
processes data element-by-element.

What if I told you there is a Python package that lets you run for-loops at machine code speed? And 
that it works seamlessly with NumPy and does not require you to heavily modify your Python code? This 
would solve many of our problems: our code would become incredibly fast while remaining easy to read. 
This package is [Numba](https://numba.pydata.org/numba-doc/latest/user/5minguide.html). Numba converts a 
subset of Python and NumPy code into efficient machine code.

```pycon
>>> @njit
... def moving_average_nb(a, window_len):
...     b = np.empty_like(a, dtype=float_)
...     for i in range(len(a)):
...         window_start = max(0, i + 1 - window_len)
...         window_end = i + 1
...         if window_end - window_start < window_len:
...             b[i] = np.nan
...         else:
...             b[i] = np.mean(a[window_start:window_end])
...     return b

>>> moving_average_nb(np.arange(10), 3)
array([nan, nan, 1., 2., 3., 4., 5., 6., 7., 8.])
```

Now we can clearly see what is happening: we loop over the time series one timestamp at a time, check
if there is enough data in the window, and if so, calculate its mean. Not only does Numba help 
produce more readable and less error-prone code, it is also as fast as 
[C](https://en.wikipedia.org/wiki/C_(programming_language))!

```pycon
>>> big_a = np.arange(1000000)
>>> %timeit moving_average_nb.py_func(big_a, 10)  # (1)!
6.54 s ± 142 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> %timeit np.mean(rolling_window(big_a, 10), axis=1)  # (2)!
24.7 ms ± 173 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> %timeit pd.Series(big_a).rolling(10).mean()  # (3)!
10.2 ms ± 309 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

>>> %timeit moving_average_nb(big_a, 10)  # (4)!
5.12 ms ± 7.21 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

1. Python.

2. NumPy.

3. Pandas.

4. Numba!

!!! tip
    If you are curious about how VBT uses Numba, look for any directory or file named `nb`.
    [This sub-package](https://github.com/polakowo/vectorbt.pro/blob/main/vectorbtpro/generic/nb/)
    contains all the basic functions, while
    [this module](https://github.com/polakowo/vectorbt.pro/blob/main/vectorbtpro/portfolio/nb/from_order_func.py)
    handles some advanced topics (:warning: adults only).

So, where is the catch? Unfortunately, Numba only understands NumPy, not Pandas. This means we lose the 
datetime index and other features essential for time series analysis. This is where VBT comes in: it 
replicates many Pandas functions using Numba and even adds new features to them. As a result, we not 
only make a subset of Pandas faster, but also more powerful!

Here is how it works:

1. Extract the NumPy array from the Pandas object.

2. Run a Numba-compiled function on this array.

3. Wrap the result back using Pandas.

```pycon
>>> arr = sr.values
>>> result = moving_average_nb(arr, 3)
>>> new_sr = pd.Series(result, index=sr.index, name=sr.name)
>>> new_sr
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64
```

Or, using VBT:

```pycon
>>> sr.vbt.rolling_mean(3)
2020-01-01    NaN
2020-01-02    NaN
2020-01-03    1.0
2020-01-04    2.0
2020-01-05    3.0
2020-01-06    4.0
2020-01-07    5.0
2020-01-08    6.0
2020-01-09    7.0
2020-01-10    8.0
Freq: D, dtype: float64
```

## Accessors

Notice how `vbt` is attached directly to the Series object? This is called 
[an accessor](https://pandas.pydata.org/docs/development/extending.html#registering-custom-accessors) –
a convenient way to extend Pandas objects without subclassing them. With an accessor, you can easily switch 
between native Pandas and VBT functionality. In addition, each VBT method is flexible with 
inputs and can work on both Series and DataFrames.

```pycon
>>> df = pd.DataFrame({'a': range(10), 'b': range(9, -1, -1)})
>>> df.vbt.rolling_mean(3)
      a    b
0  NaN  NaN
1  NaN  NaN
2  1.0  8.0
3  2.0  7.0
4  3.0  6.0
5  4.0  5.0
6  5.0  4.0
7  6.0  3.0
8  7.0  2.0
9  8.0  1.0
```

You can learn more about VBT's accessors [here](https://vectorbt.pro/pvt_41531c19/api/accessors/). For example, `rolling_mean`
is part of the accessor [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor),
which you can access directly using `vbt`. Another popular accessor, [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor),
is used for processing returns. It is a subclass of `GenericAccessor` and can be accessed using `vbt.returns`.

```pycon
>>> ret = pd.Series([0.1, 0.2, -0.1])
>>> ret.vbt.returns.total()
0.18800000000000017
```

!!! important
    Each accessor expects the data to be in a ready-to-use format. For example, the accessor
    for working with returns expects the data to be returns, not prices!

## Multidimensionality

Remember that VBT differs from traditional backtesters by handling trading data 
as multi-dimensional arrays. Specifically, VBT treats each column as a separate backtesting 
instance rather than a feature. Consider a simple OHLC DataFrame:

```pycon
>>> p1 = pd.DataFrame({
...     'open': [1, 2, 3, 4, 5],
...     'high': [2.5, 3.5, 4.5, 5.5, 6.5],
...     'low': [0.5, 1.5, 2.5, 3.5, 4.5],
...     'close': [2, 3, 4, 5, 6]
... }, index=vbt.date_range("2020-01-01", periods=5))
>>> p1
            open  high  low  close
2020-01-01     1   2.5  0.5      2
2020-01-02     2   3.5  1.5      3
2020-01-03     3   4.5  2.5      4
2020-01-04     4   5.5  3.5      5
2020-01-05     5   6.5  4.5      6
```

Here, the columns are separate features describing the same abstract object: price.
Although it may feel natural to pass this DataFrame to VBT (as you might with 
[scikit-learn](https://scikit-learn.org/stable/) and other ML tools that expect 
DataFrames with features as columns), this approach has several drawbacks in backtesting:

1. Features of different lengths or types are difficult to concatenate with NumPy.

2. Optional features cannot be properly represented as arrays.

3. Multiple backtests could be stacked into a 3-dimensional array (cube), but Pandas would not handle this correctly.

4. Non-changing features and constants would need to be converted into arrays and
replicated across all backtests, resulting in memory waste.

VBT manages this variability of features by processing them as separate arrays.
So, instead of passing one large DataFrame, you provide each feature independently:

```pycon
>>> single_pf = vbt.Portfolio.from_holding(
...     open=p1['open'], 
...     high=p1['high'], 
...     low=p1['low'], 
...     close=p1['close']
... )
>>> single_pf.value
2020-01-01    100.0
2020-01-02    150.0
2020-01-03    200.0
2020-01-04    250.0
2020-01-05    300.0
Freq: D, dtype: float64
```

Now, if you want to process multiple abstract objects, such as ticker symbols, you can simply pass 
DataFrames instead of Series:

```pycon
>>> p2 = pd.DataFrame({
...     'open': [6, 5, 4, 3, 2],
...     'high': [6.5, 5.5, 4.5, 3.5, 2.5],
...     'low': [4.5, 3.5, 2.5, 1.5, 0.5],
...     'close': [5, 4, 3, 2, 1]
... }, index=vbt.date_range("2020-01-01", periods=5))
>>> p2
            open  high  low  close
2020-01-01     6   6.5  4.5      5
2020-01-02     5   5.5  3.5      4
2020-01-03     4   4.5  2.5      3
2020-01-04     3   3.5  1.5      2
2020-01-05     2   2.5  0.5      1

>>> multi_open = pd.DataFrame({
...     'p1': p1['open'],
...     'p2': p2['open']
... })
>>> multi_high = pd.DataFrame({
...     'p1': p1['high'],
...     'p2': p2['high']
... })
>>> multi_low = pd.DataFrame({
...     'p1': p1['low'],
...     'p2': p2['low']
... })
>>> multi_close = pd.DataFrame({
...     'p1': p1['close'],
...     'p2': p2['close']
... })

>>> multi_pf = vbt.Portfolio.from_holding(
...     open=multi_open,
...     high=multi_high,
...     low=multi_low,
...     close=multi_close
... )
>>> multi_pf.value
                p1     p2
2020-01-01  100.0  100.0
2020-01-02  150.0   80.0
2020-01-03  200.0   60.0
2020-01-04  250.0   40.0
2020-01-05  300.0   20.0
```

Here, each column (sometimes called a "line" in VBT) in each feature DataFrame represents 
a separate backtesting instance and creates a separate equity curve. So, adding another backtest
is as easy as adding another column to the features :sparkles:

Keeping features separate has another major advantage: it lets us combine them easily.
Even better, we can combine all backtesting instances at once using vectorization.
For example, here we place an entry signal whenever the previous candle was green and 
an exit signal whenever the previous candle was red (this is a basic example for illustration):

```pycon
>>> candle_green = multi_close > multi_open
>>> prev_candle_green = candle_green.vbt.signals.fshift(1)
>>> prev_candle_green
                p1     p2
2020-01-01  False  False
2020-01-02   True  False
2020-01-03   True  False
2020-01-04   True  False
2020-01-05   True  False

>>> candle_red = multi_close < multi_open
>>> prev_candle_red = candle_red.vbt.signals.fshift(1)
>>> prev_candle_red
                p1     p2
2020-01-01  False  False
2020-01-02  False   True
2020-01-03  False   True
2020-01-04  False   True
2020-01-05  False   True
```

The Pandas objects `multi_close` and `multi_open` can be Series or DataFrames of any shape,
and our micro-pipeline will continue to work as expected.

## Labels

In the example above, we created our multi-OHLC DataFrames with two columns, `p1` and `p2`, so we can
easily identify them later during the analysis phase. For this reason, VBT ensures that these columns
are preserved throughout the entire backtesting pipeline, from signal generation to performance modeling.

But what if individual columns represent more complex configurations, such as those involving multiple
hyperparameter combinations? Storing complex objects as column labels would not work well in such cases.
Fortunately, Pandas offers [hierarchical columns](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html),
which are similar to regular columns but are stacked in multiple layers. Each level in this hierarchy
can help us identify a specific input or parameter.

Take a simple crossover strategy as an example: it depends on the lengths of the fast and slow windows.
Each of these hyperparameters becomes an additional dimension for manipulating data and is stored as a
separate column level. Below is a more complex example showing the column hierarchy of a MACD indicator:

```pycon
>>> macd = vbt.MACD.run(
...     multi_close,
...     fast_window=2,
...     slow_window=(3, 4),
...     signal_window=2,
...     macd_wtype="simple",
...     signal_wtype="weighted"
... )
>>> macd.signal
macd_fast_window               2             2  << fast window for MACD line
macd_slow_window               3             4  << slow window for MACD line
macd_signal_window             2             2  << window for signal line
macd_macd_wtype           simple        simple  << window type for MACD line
macd_signal_wtype       weighted      weighted  << window type for signal line   
                          p1   p2       p1   p2  << price
2020-01-01              NaN  NaN      NaN  NaN
2020-01-02              NaN  NaN      NaN  NaN
2020-01-03              NaN  NaN      NaN  NaN
2020-01-04              0.5 -0.5      NaN  NaN
2020-01-05              0.5 -0.5      1.0 -1.0
```

The columns above represent two different backtesting configurations that can now be easily analyzed and
compared using Pandas. This is a powerful way to analyze data. For example, you could group your
performance by `macd_fast_window` to see how the size of the fast window affects your strategy's
profitability. Pretty magical, right?

## Broadcasting

One of the most important concepts in VBT is broadcasting. Since VBT functions take
time series as independent arrays, they need to know how to connect elements across those arrays so that
there is 1) complete information, 2) across all arrays, and 3) at each time step.

If all arrays are the same size, VBT can easily perform operations on an element-by-element basis.
If any array is smaller in size, VBT tries to "stretch" it to match the length of the other arrays.
This approach is heavily inspired by (and internally based on) [NumPy's broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html).
The main difference from NumPy is that one-dimensional arrays are always specified per row,
since we are primarily working with time series data.

Why is broadcasting important? Because it allows you to pass array-like objects of any shape to almost
every function in VBT, whether they are constants or full DataFrames, and VBT will automatically
determine where each element belongs.

```pycon
>>> part_arrays = dict(
...     close=pd.DataFrame({  # (1)!
...         'a': [1, 2, 3, 4], 
...         'b': [4, 3, 2, 1]
...     }),
...     size=pd.Series([1, -1, 1, -1]),  # (2)!
...     direction=[['longonly', 'shortonly']],  # (3)!
...     fees=0.01  # (4)!
... )
>>> full_arrays = vbt.broadcast(part_arrays)

>>> full_arrays['close']
    a  b
0  1  4
1  2  3
2  3  2
3  4  1

>>> full_arrays['size']
    a  b
0  1  1
1 -1 -1
2  1  1
3 -1 -1

>>> full_arrays['direction']
          a          b
0  longonly  shortonly
1  longonly  shortonly
2  longonly  shortonly
3  longonly  shortonly

>>> full_arrays['fees']
      a     b
0  0.01  0.01
1  0.01  0.01
2  0.01  0.01
3  0.01  0.01
```

1. Specified per element. Does not broadcast.

2. Specified per row. Broadcasts along columns.

3. Specified per column. Broadcasts along rows.

4. Specified per entire array. Broadcasts along rows and columns.

!!! tip
    As a rule of thumb:

    1. If any array is a Pandas object, the output is always a Pandas object.

    2. If any array is two-dimensional, the output is always a two-dimensional array.

    3. If all arrays are constants or one-dimensional, the output is always a one-dimensional array.

    4. If the array is a one-dimensional NumPy array or Series, it is always specified per row.

    5. Lists and other sequences are converted to NumPy arrays before broadcasting.

Unlike NumPy and Pandas, VBT knows how to broadcast labels: if columns or individual column levels in both
objects are different, they are stacked together. For example, you can check whenever the fast moving average is
higher than the slow moving average, using the following window combinations: (2, 3) and (3, 4).

```pycon
>>> fast_ma = vbt.MA.run(multi_close, window=[2, 3], short_name='fast')
>>> slow_ma = vbt.MA.run(multi_close, window=[3, 4], short_name='slow')

>>> fast_ma.ma
fast_window    2    2    3    3
              p1   p2   p1   p2
2020-01-01   NaN  NaN  NaN  NaN
2020-01-02   2.5  4.5  NaN  NaN
2020-01-03   3.5  3.5  3.0  4.0
2020-01-04   4.5  2.5  4.0  3.0
2020-01-05   5.5  1.5  5.0  2.0

>>> slow_ma.ma
slow_window    3    3    4    4
              p1   p2   p1   p2
2020-01-01   NaN  NaN  NaN  NaN
2020-01-02   NaN  NaN  NaN  NaN
2020-01-03   3.0  4.0  NaN  NaN
2020-01-04   4.0  3.0  3.5  3.5
2020-01-05   5.0  2.0  4.5  2.5

>>> fast_ma.ma > slow_ma.ma  # (1)!
ValueError: Can only compare identically-labeled DataFrame objects

>>> fast_ma.ma.values > slow_ma.ma.values  # (2)!
array([[False, False, False, False],
        [False, False, False, False],
        [ True, False, False, False],
        [ True, False,  True, False],
        [ True, False,  True, False]])

>>> fast_ma.ma.vbt > slow_ma.ma  # (3)!
fast_window      2      2      3      3
slow_window      3      3      4      4
                p1     p2     p1     p2
2020-01-01   False  False  False  False
2020-01-02   False  False  False  False
2020-01-03    True  False  False  False
2020-01-04    True  False   True  False
2020-01-05    True  False   True  False
```

1. Comparison with Pandas fails.

2. Comparison with NumPy succeeds.

3. Comparison with VBT succeeds.

!!! tip
    Appending `.vbt` to a Pandas object on the left will broadcast both operands with
    VBT and execute the operation with NumPy/Numba. This gives you the ultimate combination of power 
    and convenience :firecracker:

In contrast to Pandas, VBT broadcasts rows and columns by their absolute positions, not by their labels.
This broadcasting style is very similar to how NumPy handles broadcasting:

```pycon
>>> df1 = pd.DataFrame({'a': [0], 'b': [1]})
>>> df2 = pd.DataFrame({'b': [0], 'a': [1]})
>>> df1 + df2  # (1)!
    a  b
0  1  1

>>> df1.values + df2.values
array([[0, 2]])

>>> df1.vbt + df2  # (2)!
    a  b
    b  a
0  0  2
```

1. Pandas connects column `a` in `df1` with column `a` in `df2`.

2. VBT connects the first column in `df1` with the first column in `df2`, regardless of their labels.

!!! important
    If you pass multiple arrays of data to VBT, make sure that their columns line up positionally!

    If your columns are not properly ordered, you will notice the result has multiple column levels with
    identical labels but different orders.

Another feature of VBT is its ability to broadcast objects with incompatible shapes but overlapping
multi-index levels, meaning those that share the same name or values. Continuing with the previous example,
let's check when the fast moving average is higher than the price:

```pycon
>>> fast_ma.ma > multi_close  # (1)!
ValueError: Can only compare identically-labeled DataFrame objects

>>> fast_ma.ma.values > multi_close.values  # (2)!
ValueError: operands could not be broadcast together with shapes (5,4) (5,2) 

>>> fast_ma.ma.vbt > multi_close  # (3)!
fast_window      2      2      3      3
                p1     p2     p1     p2
2020-01-01   False  False  False  False
2020-01-02   False   True  False  False
2020-01-03   False   True  False   True
2020-01-04   False   True  False   True
2020-01-05   False   True  False   True
```

1. Comparison with Pandas fails.

2. Comparison with NumPy fails.

3. Comparison with VBT succeeds.

And here is even more (stay with me): you can easily test multiple scalar-like hyperparameters
by passing them as a Pandas Index. Let's see whether the price is within certain thresholds:

```pycon
>>> above_lower = multi_close.vbt > vbt.Param([1, 2], name='lower')
>>> below_upper = multi_close.vbt < vbt.Param([3, 4], name='upper')
>>> above_lower.vbt & below_upper
lower           1      1      2      2
upper           3      3      4      4
                p1     p2     p1     p2
2020-01-01   True  False  False  False
2020-01-02  False  False   True  False
2020-01-03  False  False  False   True
2020-01-04  False   True  False  False
2020-01-05  False  False  False  False
```

As you can see, smart broadcasting is :gem: when it comes to merging information.
See [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast) to learn more about
broadcasting principles and new ways to combine arrays.

## Flexible indexing

Broadcasting many large arrays can use a lot of RAM and eventually slow down processing.
That's why VBT introduces the concept of "flexible indexing", which selects one element
from a one-dimensional or two-dimensional array of any shape. For example, if a one-dimensional array
has only one element and needs to be broadcast along 1000 rows, VBT will return that one
element regardless of which row is being queried, since this array would broadcast against any shape:

```pycon
>>> a = np.array([1])

>>> vbt.flex_select_1d_nb(a, 0)
1

>>> vbt.flex_select_1d_nb(a, 1)
1

>>> vbt.flex_select_1d_nb(a, 2)
1
```

This is equivalent to:

```pycon
>>> full_a = np.broadcast_to(a, (1000,))

>>> full_a[2]
1
```

Two-dimensional arrays offer more flexibility. Consider an example where you want to process 1000 columns,
and you have several parameters to apply to each element. Some parameters might be scalars that are
the same for every element, some might be one-dimensional arrays that repeat for each column, and some
might be the same for each row. Instead of broadcasting these arrays fully, you can simply keep the number
of their elements and expand them to two dimensions as needed so they will broadcast correctly with NumPy:

```pycon
>>> a = np.array([[0]])  # (1)!
>>> b = np.array([[1, 2, 3]])  # (2)!
>>> c = np.array([[4], [5], [6]])  # (3)!

>>> vbt.flex_select_nb(a, 2, 1)  # (4)!
0

>>> vbt.flex_select_nb(b, 2, 1)
2

>>> vbt.flex_select_nb(c, 2, 1)
6
```

1. Value per array. Same value for each element.

2. Value per column. Same values for each row.

3. Value per row. Same values for each column.

4. Query the element at the third row and second column.

One nice feature of this approach is that such an operation adds almost no additional memory overhead and
can broadcast in any direction, no matter how large the shape gets. This is one of the keys to how
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
can broadcast more than 50 arguments without any loss of memory efficiency or performance :wink:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/fundamentals.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
name: Building blocks
type: page
icon: material-cube-outline
---

# Building blocks

In the following sections, we will explore some sub-packages, modules, and especially classes that
serve as building blocks for advanced functionalities in VBT, such as
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio). To demonstrate this, we will
gradually build a custom class, `CorrStats`, which enables us to analyze the correlation between two
arrays in the most efficient and flexible way :brain:

## Utilities

```mermaid
flowchart TD;
    Config -->|inherits| Pickleable;
    Config -->|inherits| Prettified;
    Configured -->|inherits| Pickleable;
    Configured -->|inherits| Prettified;
    Configured -.->|references| Config;
    AttrResolverMixin;
```

(Reload the page if the diagram does not appear.)

VBT uses a modular project structure composed of several subpackages.
Each subpackage is designed for a specific area of analysis.

The [utils](https://vectorbt.pro/pvt_41531c19/api/utils/) subpackage provides a set of utilities that power every part of VBT :zap:
These utilities are loosely connected and offer small but powerful reusable code snippets that can
be used independently of other functionality.

!!! info
    The main reason we avoid importing third-party packages and instead implement many utilities
    from scratch is to maintain full control over execution and code quality.

### Formatting

VBT includes its own formatting engine that can pretty-print any Python object.
It is much more advanced than formatting with [JSON](https://en.wikipedia.org/wiki/JSON) because it
recognizes native Python data types and adds intelligent formatting for more structured data types,
such as `np.dtype` and `namedtuple`. In many cases, you can even convert the formatted string back
into a Python object using `eval`.

Let's beautify a nested dictionary using [prettify](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.prettify)
and then convert the string back into an object:

```pycon
>>> from vectorbtpro import *

>>> dct = {'planet' : {'has': {'plants': 'yes', 'animals': 'yes', 'cryptonite': 'no'}, 'name': 'Earth'}}
>>> print(vbt.prettify(dct))
{
    'planet': {
        'has': {
            'plants': 'yes',
            'animals': 'yes',
            'cryptonite': 'no'
        },
        'name': 'Earth'
    }
}

>>> eval(vbt.prettify(dct)) == dct
True
```

!!! tip
    Curious why we used `vbt.prettify` instead of `vbt.utils.formatting.prettify`?
    Any utility that may be useful to the end user can be accessed directly from `vbt`.

    To see which utilities are accessible from the root of the package, visit
    [vectorbtpro/utils/\_\_init\_\_.py](https://github.com/polakowo/vectorbt.pro/blob/main/vectorbtpro/utils/__init__.py)
    or any other subpackage, and look for the objects that are listed in `__all__`.

The [Prettified](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.formatting.Prettified) class implements the
abstract method [Prettified.prettify](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.Prettified.prettify),
which can be overridden by a subclass to pretty-print an instance using
[prettify](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.prettify).
Read below to learn how this method can be used to introspect instances of various classes.

### Pickling

Pickling is the process of converting a Python object into a byte stream to store it in a file or database.
The [Pickleable](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable) class enables pickling
of objects of any complexity using [Dill](https://dill.readthedocs.io/en/latest/)
(or [pickle](https://docs.python.org/3/library/pickle.html) if Dill is not installed).
Each subclass inherits ready-to-use methods for serializing, deserializing, saving to a file,
and loading from a file. This is especially powerful because it allows us to persist
objects containing any type of data, including instances of [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data)
and [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

### Configuring

VBT relies heavily on automation driven by specifications. The specification for most repetitive
tasks is usually stored in "configs," which serve as settings for specific tasks, data structures,
or even classes. This approach makes most parts of VBT transparent, easily traversable,
and programmatically changeable.

The [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config) class is like a dictionary on steroids:
it extends Python's `dict` with various configuration features, such as frozen keys,
read-only values, dot notation access to keys, and nested updates. The most notable feature
is the ability to reset a config to its initial state and even create checkpoints, which is
especially useful for settings. In addition, since [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config)
inherits from [Pickleable](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable), we can save any
configuration to disk, and by subclassing [Prettified](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.formatting.Prettified)
we can beautify it (this approach is used to generate the API reference):

```pycon
>>> print(vbt.Records.field_config)
Config(
    dtype=None,
    settings={
        'id': {
            'name': 'id',
            'title': 'Id'
        },
        'col': {
            'name': 'col',
            'title': 'Column',
            'mapping': 'columns'
        },
        'idx': {
            'name': 'idx',
            'title': 'Timestamp',
            'mapping': 'index'
        }
    }
)
```

Configs are very common structures in VBT. There are three main types of configs (that either
subclass or partially use [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config)) used throughout VBT:

1. [ReadonlyConfig](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.ReadonlyConfig) is used for configurations
that are not meant to be modified, such as [nb_config](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config),
which is used to attach a number of Numba-compiled functions to [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)
only once when importing VBT. Since modifications would have no effect anyway, these configs are locked.

2. [HybridConfig](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.HybridConfig) is used for configurations
that are intended to be modified. The best examples are [Portfolio.metrics](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.metrics)
and [Portfolio.subplots](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.subplots),
which list all the metrics and subplots supported by [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).
They can be easily changed and extended, and reset if you make breaking changes.

3. [SettingsConfig](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.SettingsConfig) holds the global settings
defined in [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/). It is a custom subclass of [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config)
that can change Plotly themes and convert any sub-configs of type `dict` to smaller settings
accessible via dot notation (`vbt.settings.portfolio.log` instead of `vbt.settings['portfolio']['log']`).

You can create a config just like a regular `dict`. To provide options for the config, use the `options_`
argument (note the trailing underscore):

```pycon
>>> cfg = vbt.Config(
...     hello="world",
...     options_=dict(readonly=True)
... )
>>> print(cfg)
Config(
    hello='world'
)

>>> cfg["change"] = "something"
TypeError: Config is read-only
```

In addition to the cases above, [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config)
is also used by the [Configured](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured) class,
which is a base class for most core classes in VBT. This read-only class holds a
config of type [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config) containing all
arguments passed during initialization. Any time you initialize a subclass of
[Configured](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured),
all named arguments you pass to the initializer (`__init__`) are stored inside
[Configured.config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.config).
In this way, the created instance is described and managed entirely by its config:

- You can copy or modify the config and pass it to the class to initialize another instance
  (`new_instance = ConfiguredClass(**old_instance.config)`).

- You can pickle both the class and the config to pickle the entire instance.

- You can compare instances based on their classes and configs.

The main requirement for all of this to work correctly is **immutability**. This brings us to an
important design decision: most classes in VBT to be immutable (read-only), and it is
discouraged to change any attribute unless it is listed in a special variable called `_writeable_attrs`.
There are several reasons why immutability is required:

1. Immutable instances can be easily recreated and manipulated using their configs.

2. Immutable instances are free of side effects and can have cached attributes.

3. Immutable instances can be hashed.

Let's create a custom class that returns some correlation statistics of two arrays.
Specifically, it will compute the Pearson correlation coefficient and its rolling version using Pandas.

```pycon
>>> class CorrStats(vbt.Configured):
...     def __init__(self, obj1, obj2):
...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)
...
...         self._obj1 = obj1
...         self._obj2 = obj2
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):
...         if isinstance(self.obj1, pd.Series):
...             return self.obj1.corr(self.obj2)
...         return self.obj1.corrwith(self.obj2)
...
...     def rolling_corr(self, window):
...         return self.obj1.rolling(window).corr(self.obj2)
```

This is how most configured classes in VBT, such as
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio), are designed.
Any argument passed to `CorrStats` is forwarded to
[Configured](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured) to create a new config:

```pycon
>>> index = vbt.date_range("2020", periods=5)
>>> df1 = pd.DataFrame({
...     'a': [1, 5, 2, 4, 3],
...     'b': [3, 2, 4, 1, 5]
... }, index=index)
>>> df2 = pd.DataFrame({
...     'a': [1, 2, 3, 4, 5],
...     'b': [5, 4, 3, 2, 1]
... }, index=index)

>>> corrstats = CorrStats(df1, df2)
>>> print(corrstats.config)
Config(
    obj1=<DataFrame object at 0x7fd490461400 of shape (5, 2)>,
    obj2=<DataFrame object at 0x7fd490461240 of shape (5, 2)>
)
```

Access to any attribute is read-only. If you try to set a read-only property or modify the config,
an expected error will be raised:

```pycon
>>> df3 = pd.DataFrame({
...     'a': [3, 2, 1, 5, 4],
...     'b': [4, 5, 1, 2, 3]
... }, index=index)
>>> corrstats.obj1 = df3
AttributeError: can't set attribute

>>> corrstats.config['obj1'] = df3
TypeError: Config is read-only
```

However, it will not (and cannot) throw an error when setting a private attribute (with a leading underscore)
or if any of the attributes are modified in place, which is a common pitfall to avoid.

```pycon
>>> corrstats._obj1 = df3  # (1)!
>>> corrstats.obj1.iloc[:] = df3  # (2)!
```

1. This would work, but at what cost?

2. This would work too, unfortunately.

!!! warning
    VBT assumes that the data in a configured instance always remains the same. If there is a
    change to the data, VBT will not register it, and this can result in erroneous outcomes later.

    The only approved way to change data in an instance is to create a new instance!

To change any data, pass the new value to [Configured.replace](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.replace),
which takes the same arguments as the class, but as keyword-only arguments. These are merged with
the old config and then passed as keyword arguments to the class for instantiation.

```pycon
>>> new_corrstats = corrstats.replace(obj1=df3)
>>> new_corrstats.obj1  # (1)!
            a  b
2020-01-01  3  4
2020-01-02  2  5
2020-01-03  1  1
2020-01-04  5  2
2020-01-05  4  3

>>> new_corrstats.obj2  # (2)!
            a  b
2020-01-01  1  5
2020-01-02  2  4
2020-01-03  3  3
2020-01-04  4  2
2020-01-05  5  1
```

1. `df1` has been replaced with `df3`.

2. `df2` remains unchanged.

Now that all of our data is stored inside a config, we can perform actions on the instance
just as we would on the config itself, such as saving to disk
(thanks to [Pickling](#pickling)):

```pycon
>>> corrstats.save('corrstats')  # (1)!

>>> corrstats = CorrStats.load('corrstats')
```

1. Saves the instance along with both objects.

### Attribute resolution

Attribute resolution is useful for accessing attributes by strings or based on specific logic,
which is implemented by the [AttrResolverMixin](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.AttrResolverMixin).
You can think of it as adding custom logic to the `getattr` operation.
It is used extensively in [StatsBuilderMixin](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)
and [PlotsBuilderMixin](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin)
to execute metrics and subplots as chains of commands. In other classes,
such as [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),
it is used for accessing shortcut properties, caching attribute access, and more.
It works with [deep_getattr](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.deep_getattr),
which accesses a chain of attributes provided as a specification.

Let's compute the minimum of the rolling mean using only Pandas and deep attribute resolution:

```pycon
>>> sr = pd.Series([1, 2, 3, 4, 5])
>>> attr_chain = [('rolling', (3,)), 'mean', 'min']
>>> vbt.deep_getattr(sr, attr_chain)
2.0
```

If any of the above operations were performed on a subclass of [AttrResolverMixin](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.AttrResolverMixin),
they could be easily preprocessed and postprocessed.

### Templating

Templates play an important role in the VBT stem. They allow you to postpone data
resolution until more information becomes available. There are several
templating classes, such as [Rep](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.Rep)
for replacing an entire string, and [Sub](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.Sub)
for substituting only parts of a string (those starting with `$`).

You can think of templates as callbacks that are executed at some point during execution,
usually after broadcasting or merging keyword arguments. Some functions provide
multiple possible substitution points; in such cases, they may try to substitute
the template multiple times until successful, or match the template with a specific
evaluation id (`eval_id`), if given. The actual evaluation is performed by
[substitute_templates](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.substitute_templates).

```pycon
>>> def some_function(*args, **kwargs):
...     context = {}
...     args = vbt.substitute_templates(args, context=context, strict=False)
...     kwargs = vbt.substitute_templates(kwargs, context=context, strict=False)
...     print(args)
...     print(kwargs)
...     
...     context['result'] = 100  # (1)!
...     args = vbt.substitute_templates(args, context=context)
...     kwargs = vbt.substitute_templates(kwargs, context=context)
...     print(args)
...     print(kwargs)

>>> some_function(vbt.Rep('result'), double_result=vbt.RepEval('result * 2'))
(Rep(template='result', context=None, strict=None, eval_id=None),)
{'double_result': RepEval(template='result * 2', context=None, strict=None, eval_id=None)}
(100,)
{'double_result': 200}
```

1. New context

## Base

```mermaid
flowchart TD;
    Grouper -->|inherits| Configured;
    ArrayWrapper -->|inherits| Configured;
    ArrayWrapper -->|inherits| PandasIndexer;
    ArrayWrapper -.->|references| Grouper;
    Wrapping -->|inherits| Configured;
    Wrapping -->|inherits| PandasIndexer;
    Wrapping -->|inherits| AttrResolverMixin;
    Wrapping -.->|references| ArrayWrapper;
    BaseAccessor -->|inherits| Wrapping;
```

The [base](https://vectorbt.pro/pvt_41531c19/api/base/) subpackage is the non-computational core of VBT.
It offers a variety of modules for working with and converting between Pandas and NumPy objects.
In particular, it provides functions and classes for broadcasting, combining, and wrapping
NumPy arrays, grouping columns, managing [MultiIndex](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html),
and more. These operations are essential for extending Pandas and reproducing some of its
functionality in custom classes.

### Grouping

Since VBT is often used to process multi-column data, where each column (or "line")
represents a separate backtesting instance, being able to group these columns into different
groups is an essential feature.

The [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper) class provides
functionality to validate and build groups from any Pandas Index, especially columns.
It can translate various metadata, such as [GroupBy objects](https://pandas.pydata.org/docs/reference/groupby.html)
and column levels, into special NumPy arrays that can be used by Numba-compiled functions
to aggregate multiple columns of data. This is particularly useful for multi-asset portfolios,
where each group contains one or more assets.

```pycon
>>> columns = pd.MultiIndex.from_tuples([
...     ('BTC-USD', 'group1'),
...     ('ETH-USD', 'group1'),
...     ('ADA-USD', 'group2'),
...     ('SOL-USD', 'group2')
... ], names=['symbol', 'group'])
>>> vbt.Grouper(columns, 'group').get_groups()
array([0, 0, 1, 1])
```

### Indexing

In VBT, the main purpose of indexing is to provide Pandas-style indexing to any custom class
that holds Pandas-like objects, allowing you to select rows, columns, and groups in each object.
This is done by forwarding a Pandas indexing operation to each Pandas-like object and instantiating
the class using them. This can be done easily with [Configured](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured).
With this approach, you can index complex classes that contain many Pandas-like objects using
a single command.

The main indexer class, [PandasIndexer](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer),
mimics a regular Pandas object by exposing the properties
[PandasIndexer.iloc](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.iloc),
[PandasIndexer.loc](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.loc),
and [PandasIndexer.xs](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer.xs).
To use this, simply subclass this class and override
[IndexingBase.indexing_func](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.IndexingBase.indexing_func),
which should accept `pd_indexing_func`, apply it to each Pandas-like object, and initialize a new instance.

Let's extend our previously created `CorrStats` class with Pandas indexing:

```pycon
>>> class CorrStats(vbt.Configured, vbt.PandasIndexer):
...     def __init__(self, obj1, obj2):
...         vbt.Configured.__init__(self, obj1=obj1, obj2=obj2)
...         vbt.PandasIndexer.__init__(self)
...
...         self._obj1 = obj1
...         self._obj2 = obj2
...
...     def indexing_func(self, pd_indexing_func):  # (1)!
...         return self.replace(
...             obj1=pd_indexing_func(self.obj1),
...             obj2=pd_indexing_func(self.obj2)
...         )
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):
...         if isinstance(self.obj1, pd.Series):
...             return self.obj1.corr(self.obj2)
...         return self.obj1.corrwith(self.obj2)
...
...     def rolling_corr(self, window):
...         return self.obj1.rolling(window).corr(self.obj2)

>>> corrstats = CorrStats(df1, df2)
>>> corrstats.corr()
a    0.3
b   -0.3
dtype: float64

>>> corrstats.loc['2020-01-01':'2020-01-03', 'a'].corr()  # (2)!
0.24019223070763066
```

1. Here, `pd_indexing_func` is simply `lambda x: x.loc[key]`.

2. Running the correlation coefficient on a subset of data.

We just indexed two Pandas objects as a single entity. Yay!

### Wrapping

Remember that VBT specializes in taking a Pandas object, extracting its NumPy array,
processing the array, and converting the results back to a Pandas format?
The last part is handled by the [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)
class, which captures all the necessary metadata, such as the index, columns, and number of dimensions,
and provides methods like [ArrayWrapper.wrap](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)
to convert a NumPy object back into a Pandas format.

The [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper) class integrates
many concepts we discussed earlier to behave like a (supercharged) Pandas object. Notably,
it uses [Grouping](#grouping) to build and manage groups of columns and [Indexing](#indexing)
to select rows, columns, and groups with Pandas-style indexing. Some of the most powerful features
of an array wrapper are 1) the ability to behave like a grouped object, which is not possible with Pandas alone,
and 2) the ability to translate a Pandas indexing operation into a range of integer arrays that can be
used to index NumPy arrays. This allows for indexing without having to hold Pandas objects,
just the wrapper.

You can construct a wrapper in several ways, with the easiest being from a Pandas object:

```pycon
>>> df = pd.DataFrame({
...     'a': range(0, 5),
...     'b': range(5, 10),
...     'c': range(10, 15),
...     'd': range(15, 20)
... }, index=index)
>>> wrapper = vbt.ArrayWrapper.from_obj(df)
>>> print(wrapper)
ArrayWrapper(
    index=<DatetimeIndex object at 0x7ff6d8528d68 of shape (5,)>,
    columns=<Index object at 0x7ff6d857fcc0 of shape (4,)>,
    ndim=2,
    freq=None,
    column_only_select=None,
    group_select=None,
    grouped_ndim=None,
    grouper=Grouper(
        index=<Index object at 0x7ff6d857fcc0 of shape (4,)>,
        group_by=None,
        allow_enable=True,
        allow_disable=True,
        allow_modify=True
    )
)
```

Now, let's create a function that sums all elements in each column using NumPy and
returns a standard Pandas object:

```pycon
>>> def sum_per_column(df):
...     wrapper = vbt.ArrayWrapper.from_obj(df)
...     result = np.sum(df.values, axis=0)
...     return wrapper.wrap_reduced(result)

>>> sum_per_column(df)
a    10
b    35
c    60
d    85
dtype: int64
```

The function above is already 20x faster than Pandas :exploding_head:

```pycon
>>> big_df = pd.DataFrame(np.random.uniform(size=(1000, 1000)))

>>> %timeit big_df.sum()
2.52 ms ± 3.5 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

>>> %timeit sum_per_column(big_df)
428 µs ± 1.5 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)
```

Since [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper) can manage groups of
columns, let's adapt our function to sum all elements over each group of columns:

```pycon
>>> def sum_per_group(df, group_by):
...     wrapper = vbt.ArrayWrapper.from_obj(df, group_by=group_by)
...     results = []
...     for group_idxs in wrapper.grouper.iter_group_idxs():
...         group_result = np.sum(df.values[:, group_idxs])
...         results.append(group_result)
...     return wrapper.wrap_reduced(results)

>>> sum_per_group(df, False)  # (1)!
a    10
b    35
c    60
d    85
dtype: int64

>>> sum_per_group(df, True)  # (2)!
190

>>> group_by = pd.Index(['group1', 'group1', 'group2', 'group2'])
>>> sum_per_group(df, group_by)  # (3)!
group1     45
group2    145
dtype: int64
```

1. No grouping (one group per column).

2. One group with all columns.

3. Multiple groups with multiple columns.

To avoid creating multiple array wrappers with the same metadata, the
[Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) class binds a single
instance of [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)
to manage any number of array-like objects with compatible shapes. Instead of accepting multiple
Pandas objects, it takes an array wrapper and any number of objects and arrays in any format
(preferably NumPy), and wraps them using this wrapper. In addition, any
[Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) subclass can use its wrapper
to perform Pandas indexing on any kind of object, including NumPy arrays. This is possible because
[ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper) can translate
a Pandas indexing operation into universal row, column, and group indices.

Returning to our `CorrStats` class, there are two issues with the current implementation:

1. Both metrics require Pandas objects with exactly the same layouts as inputs.

2. Both metrics are computed using Pandas, which is relatively slow.

Let's upgrade our `CorrStats` class to work on NumPy arrays and with an array wrapper:

```pycon
>>> class CorrStats(vbt.Wrapping):
...     _expected_keys = vbt.Wrapping._expected_keys | {"obj1", "obj2"}  # (1)!
...
...     @classmethod
...     def from_objs(cls, obj1, obj2):  # (2)!
...         (obj1, obj2), wrapper = vbt.broadcast(
...             obj1, obj2, 
...             to_pd=False,
...             return_wrapper=True
...         )
...         return cls(wrapper, obj1, obj2)
...
...     def __init__(self, wrapper, obj1, obj2):
...         vbt.Wrapping.__init__(self, wrapper, obj1=obj1, obj2=obj2)
...
...         self._obj1 = vbt.to_2d_array(obj1)
...         self._obj2 = vbt.to_2d_array(obj2)
...
...     def indexing_func(self, pd_indexing_func, **kwargs):  # (3)!
...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)
...         new_wrapper = wrapper_meta["new_wrapper"]
...         row_idxs = wrapper_meta["row_idxs"]
...         col_idxs = wrapper_meta["col_idxs"]
...         return self.replace(
...             wrapper=new_wrapper,
...             obj1=self.obj1[row_idxs, :][:, col_idxs],
...             obj2=self.obj2[row_idxs, :][:, col_idxs]
...         )
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):  # (4)!
...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)
...         return self.wrapper.wrap_reduced(out)
...
...     def rolling_corr(self, window):
...         out = vbt.nb.rolling_corr_nb(
...             self.obj1, self.obj2, 
...             window, minp=window)
...         return self.wrapper.wrap(out)
```

1. Specify the argument names that are passed to [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)
(or set `_expected_keys=None` to disable).

2. A convenient class method to broadcast objects, create a wrapper, and pass everything to the constructor.

3. The indexing method uses the wrapper to translate `pd_indexing_func` to arrays of selected
rows and columns, applies them to both NumPy arrays, and creates a new `CorrStats` instance.

4. Computation is performed on NumPy arrays, and the result is converted into Pandas.

As you may have noticed, we replaced the superclasses
[Configured](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured)
and [PandasIndexer](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer)
with the single superclass [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping),
which already inherits both. Another change is in the arguments taken by `CorrStats`:
instead of taking two Pandas objects, it now accepts a `wrapper` of type
[ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper) along with the NumPy
arrays `obj1` and `obj2`. This has several benefits: we keep the Pandas metadata consistent and managed
by a single variable, while all calculations are efficiently performed using only NumPy.
Whenever we need to present results, we can call
[ArrayWrapper.wrap_reduced](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap_reduced)
and [ArrayWrapper.wrap](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)
to convert them back into Pandas format, as is done inside the `CorrStats.corr`
and `CorrStats.rolling_corr` methods.

Since we do not want to require users (or ourselves) to create an array wrapper manually,
we also implemented the `CorrStats.from_objs` class method, which broadcasts both arrays and
instantiates `CorrStats`. With this, you can provide any array-like objects, and `CorrStats`
will automatically build the wrapper for you. Let's illustrate this by computing the correlation
coefficient for `df1` and `df2`, and then for `df1` and a parameterized `df2`:

```pycon
>>> df1.corrwith(df2)  # (1)!
a    0.3
b   -0.3
dtype: float64

>>> corrstats = CorrStats.from_objs(df1, df2)
>>> corrstats.corr()  # (2)!
a    0.3
b   -0.3
dtype: float64

>>> df2_sh = vbt.pd_acc.concat(
...     df2, df2.vbt.shuffle(seed=42), 
...     keys=['plain', 'shuffled'])
>>> df2_sh 
              plain    shuffled   
                a  b        a  b
2020-01-01     1  5        2  2
2020-01-02     2  4        5  4
2020-01-03     3  3        3  3
2020-01-04     4  2        1  5
2020-01-05     5  1        4  1

>>> df1.corrwith(df2_sh)  # (3)!
ValueError: cannot join with no overlapping index names

>>> corrstats = CorrStats.from_objs(df1, df2_sh)
>>> corrstats.corr()  # (4)!
plain     a    0.3
          b   -0.3
shuffled  a    0.4
          b   -0.9
dtype: float64
```

1. Using Pandas.

2. Using our class.

3. Pandas fails to join due to no overlapping index names.

4. VBT works thanks to smart broadcasting.

Here is why we switched from Pandas to Numba:

```pycon
>>> big_df1 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))
>>> big_df2 = pd.DataFrame(np.random.uniform(size=(1000, 1000)))

>>> %timeit big_df1.rolling(10).corr(big_df2)  # (1)!
271 ms ± 13.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

>>> corrstats = CorrStats.from_objs(big_df1, big_df2)
>>> %timeit corrstats.rolling_corr(10)  # (2)!
12 ms ± 50.5 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. Using Pandas.

2. Using our class.

There is also an improvement regarding indexing. Because `obj1` and `obj2` are no longer regular Pandas
objects, we cannot simply apply `pd_indexing_func` to them. Instead, we use the method
[ArrayWrapper.indexing_func_meta](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.indexing_func_meta)
to obtain the rows, columns, and groups that this operation would select. We then apply those arrays
to both NumPy objects. This approach is especially useful because we can now select any data from
the final shape.

```pycon
>>> corrstats = CorrStats.from_objs(df1, df2_sh)
>>> corrstats.loc['2020-01-02':'2020-01-05'].rolling_corr(3)  # (1)!
                          plain            shuffled
                    a         b         a         b
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03       NaN       NaN       NaN       NaN
2020-01-04 -0.327327  0.327327  0.327327 -0.981981
2020-01-05  0.500000 -0.240192 -0.654654 -0.960769
```

1. Compute the rolling correlation coefficient only for 2020-01-02 through 2020-01-05.

!!! note
    Not all classes support indexing on rows. To ensure you can select rows,
    check whether the instance property `column_only_select` is False.

This demonstrates how most high-tier classes in VBT are built. As a general rule:

* For performance, VBT typically follows this process:
Pandas :arrow_right: NumPy/Numba :arrow_right: Pandas. The first stage builds a wrapper
from Pandas objects, and the last stage uses the wrapper to present results to the user.

* Pandas metadata such as shape and index is accessible via the attribute `wrapper`.
For example, `wrapper.ndim` returns the number of dimensions held by the current instance.

* Do not call classes directly. Use class methods, which usually begin with the prefix `from_`.
The constructor `__init__` is most likely reserved for internal use during indexing.
(This is why we use `vbt.MA.run()` instead of `vbt.MA()`.)

* The wrapper enables indexing of any objects that have elements aligned by row or column,
even those with more complex layouts (see [Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records)).

### Base accessor

This subpackage also provides the [BaseAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor),
which exposes many basic operations to the end user and serves as the superclass for all other accessors.
It inherits from [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping), so it supports
everything you can do with a custom `CorrStats` class. Why is it called the "base" accessor?
Because it is the parent class for all other VBT accessors and provides core combining, reshaping,
and indexing features. This includes functions such as [BaseAccessor.to_2d_array](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.to_2d_array),
which converts a Pandas object to a two-dimensional NumPy array.

Accessing the accessor is straightforward:

```pycon
>>> df.vbt
<vectorbtpro.accessors.Vbt_DFAccessor at 0x7fe3c19f7d68>

>>> df.vbt.to_2d_array()
array([[ 0,  5, 10, 15],
        [ 1,  6, 11, 16],
        [ 2,  7, 12, 17],
        [ 3,  8, 13, 18],
        [ 4,  9, 14, 19]])
```

In this example, [Vbt_DFAccessor](https://vectorbt.pro/pvt_41531c19/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)
is the main accessor for DataFrames, and as you can see from its definition,
[BaseAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor) is one of its superclasses.

Perhaps the most interesting method is [BaseAccessor.combine](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine),
which allows you to broadcast and combine the current Pandas object with any number of other
array-like objects using the function `combine_func` (mainly with NumPy).

```pycon
>>> pd.Series([1, 2, 3]).vbt.combine(np.array([[4, 5, 6]]), np.add)
    0  1  2
0  5  6  7
1  6  7  8
2  7  8  9
```

[BaseAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor) also implements a range of
[unary](https://vectorbt.pro/pvt_41531c19/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.unary_magic_config)
and [binary](https://vectorbt.pro/pvt_41531c19/api/utils/magic_decorators/#vectorbtpro.utils.magic_decorators.binary_magic_config) magic
methods using this feature. For example, let's call `BaseAccessor.__add__`, which implements addition:

```pycon
>>> pd.Series([1, 2, 3]) + np.array([[4, 5, 6]])  # (1)!
ValueError: Length of values (1) does not match length of index (3)

>>> pd.Series([1, 2, 3]).vbt + np.array([[4, 5, 6]])  # (2)!
    0  1  2
0  5  6  7
1  6  7  8
2  7  8  9
```

1. Without `.vbt`, Pandas performs the addition.

2. With `.vbt`, VBT performs the addition.

!!! tip
    To learn more about :magic_wand: methods, see [A Guide to Python's Magic Methods](https://rszalski.github.io/magicmethods/).

All of these magic methods were added using class decorators. There are many class decorators
for various tasks in VBT. Usually, they take a config and attach many attributes at once
in an automated way.

## Generic

```mermaid
flowchart TD;
    Analyzable -->|inherits| Wrapping;
    Analyzable -->|inherits| StatsBuilderMixin;
    Analyzable -->|inherits| PlotsBuilderMixin;
    GenericAccessor -->|inherits| BaseAccessor;
    GenericAccessor -->|inherits| Analyzable;
```

The [generic](https://vectorbt.pro/pvt_41531c19/api/generic/) subpackage serves as the computational core of VBT. It includes modules for
processing and plotting time series and numeric data in a broader sense. Most importantly, it provides an
[arsenal](https://vectorbt.pro/pvt_41531c19/api/generic/nb/) of Numba-compiled functions to accelerate and extend Pandas!
These functions power many features of VBT, from indicators to portfolio analysis.
For now, let's focus on classes that could enhance our `CorrStats` class.

### Builder mixins

Builder [mixins](https://en.wikipedia.org/wiki/Mixin) are classes that, when subclassed by another
class, enable building specific functionality from that class's attributes. Two main examples are
[StatsBuilderMixin](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin)
and [PlotsBuilderMixin](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin).
The first provides the method [StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)
to compute various metrics. The second provides the method [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)
to display different subplots. Nearly every class that can analyze data subclasses both.

### Analyzing

The [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable) class combines
[Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) and the [Builder mixins](#builder-mixins).
It brings together everything mentioned above to build a solid foundation for seamless data
analysis. This is why it is subclassed by many high-level classes, such as
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
and [Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records).

So what are we waiting for? Let's adapt our `CorrStats` class to be analyzable!

```pycon
>>> class CorrStats(vbt.Analyzable):  # (1)!
...     _expected_keys = vbt.Analyzable._expected_keys | {"obj1", "obj2"}
...
...     @classmethod
...     def from_objs(cls, obj1, obj2):
...         (obj1, obj2), wrapper = vbt.broadcast(
...             obj1, obj2, 
...             to_pd=False,
...             return_wrapper=True
...         )
...         return cls(wrapper, obj1, obj2)
...
...     def __init__(self, wrapper, obj1, obj2):
...         vbt.Analyzable.__init__(self, wrapper, obj1=obj1, obj2=obj2)  # (2)!
...
...         self._obj1 = vbt.to_2d_array(obj1)
...         self._obj2 = vbt.to_2d_array(obj2)
...
...     def indexing_func(self, pd_indexing_func, **kwargs):
...         wrapper_meta = self.wrapper.indexing_func_meta(pd_indexing_func, **kwargs)
...         new_wrapper = wrapper_meta["new_wrapper"]
...         row_idxs = wrapper_meta["row_idxs"]
...         col_idxs = wrapper_meta["col_idxs"]
...         return self.replace(
...             wrapper=new_wrapper,
...             obj1=self.obj1[row_idxs, :][:, col_idxs],
...             obj2=self.obj2[row_idxs, :][:, col_idxs]
...         )
...
...     @property
...     def obj1(self):
...         return self._obj1
...
...     @property
...     def obj2(self):
...         return self._obj2
...
...     def corr(self):
...         out = vbt.nb.nancorr_nb(self.obj1, self.obj2)
...         return self.wrapper.wrap_reduced(out)
...
...     def rolling_corr(self, window):
...         out = vbt.nb.rolling_corr_nb(
...             self.obj1, self.obj2, 
...             window, minp=window)
...         return self.wrapper.wrap(out)
...
...     _metrics = vbt.HybridConfig(  # (3)!
...         corr=dict(
...             title='Corr. Coefficient',
...             calc_func='corr'
...         )
...     )
...
...     _subplots = vbt.HybridConfig(  # (4)!
...          rolling_corr=dict(
...              title=vbt.Sub("Rolling Corr. Coefficient (window=$window)"),
...              plot_func=vbt.Sub('rolling_corr($window).vbt.plot'),  # (5)!
...              pass_trace_names=False
...          )
...     )
```

1. Replace `Wrapping` with `Analyzable`.

2. Replace `Wrapping` with `Analyzable`.

3. Define default metrics for [StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats).

4. Define default subplots for [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots).

5. See [Templating](#templating) and [Attribute resolution](#attribute-resolution).

We made a few changes: we replaced `Wrapping` with `Analyzable` and added some metrics and subplots
based on `CorrStats.corr` and `CorrStats.rolling_corr`. That's all! Now we can pass any array-like
objects to `CorrStats.from_objs`, which will return an instance ready to analyze the correlation
between the objects. In particular, you can use `CorrStats.stats` and `CorrStats.plots`:

```pycon
>>> corrstats = CorrStats.from_objs(df1, df2)
>>> corrstats.stats(column='a')  # (1)!
Corr. Coefficient    0.3
Name: a, dtype: object

>>> corrstats['a'].stats()  # (2)!
Corr. Coefficient    0.3
Name: a, dtype: object

>>> corrstats.plots(template_context=dict(window=3)).show()  # (3)!
```

1. Compute metrics for all columns and show only `a`.

2. Compute and show metrics for only `a`.

3. Set the rolling window to 3 and show subplots for all columns.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/bblocks/analyzing.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/bblocks/analyzing.dark.svg#only-dark){: .iimg loading=lazy }

There is nothing more satisfying than not having to write boilerplate code.
Thanks to [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),
we can focus entirely on analysis while VBT takes care of everything else.

### Generic accessor

You do not have to look far to find a class that inherits from
[Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable):
the [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)
class extends [BaseAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor)
to provide statistics and plots for any numeric data. It is a one-size-fits-all class
whose goal is to replicate, accelerate, and extend Pandas core functionality.
It implements custom rolling, mapping, reducing, splitting, plotting, and many other
methods, which can be used with any Series or DataFrame.

In short, [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)
offers the following:

- Wraps many Numba-compiled functions from the [nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/) module and provides them
as methods that mimic some of Pandas' most popular functions. Some have meta versions
(accepting UDFs that take metadata instead of arrays) or can be used on grouped data.

- Wraps many data transformation utilities from [scikit-learn](https://scikit-learn.org/stable/).

- Displays a range of statistics similar to
[pandas.DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html).

- Wraps widgets from [plotting](https://vectorbt.pro/pvt_41531c19/api/generic/plotting/) and provides many custom plotting methods.
Some methods even support interactive controls for analyzing groups of data.

- Extracts basic events as [Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records), such as drawdowns.

Just like the [Base accessor](#base-accessor), the generic accessor uses
[class decorators](https://vectorbt.pro/pvt_41531c19/api/generic/decorators/) and
[configs](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.nb_config)
to attach many Numba-compiled and scikit-learn functions at once.

Usage is similar to `CorrStats`, except you can use the generic accessor directly on Pandas objects,
since it is directly subclassed by [Vbt_DFAccessor](https://vectorbt.pro/pvt_41531c19/api/accessors/#vectorbtpro.accessors.Vbt_DFAccessor)!

```pycon
>>> df.vbt.stats(column='a')
Start        2020-01-01 00:00:00
End          2020-01-05 00:00:00
Period           5 days 00:00:00
Count                          5
Mean                         2.0
Std                     1.581139
Min                            0
Median                       2.0
Max                            4
Min Index    2020-01-01 00:00:00
Max Index    2020-01-05 00:00:00
Name: a, dtype: object
```

## Records

```mermaid
flowchart TD;
    Records -.->|references| ColumnMapper;
    Records -.->|references| MappedArray;
    Records -->|inherits| Analyzable;
    MappedArray -.->|references| ColumnMapper;
    MappedArray -->|inherits| Analyzable;
```

Records are [structured arrays](https://numpy.org/doc/stable/user/basics.rec.html), which are NumPy arrays
that can hold different data types, much like a Pandas DataFrame. Records have a major advantage
over DataFrames: they are well supported by Numba, making it possible to generate and use
them efficiently. So, what is the drawback? Records do not have (index) labels, and their API is very
limited. As we [discussed](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/), VBT does not favor heterogeneous data
and instead prefers to work with multiple homogeneous arrays (such as splitting OHLC into O, H, L, and C).
Even so, records play an important role in our ecosystem as containers for event data.

Trading revolves around events: executing trades, combining them into positions, analyzing drawdowns,
and more. Each event is a complex piece of data that needs a container optimized for fast writes
and reads, especially inside Numba-compiled code (but do not use a list of dictionaries, as that is
**very inefficient**). Structured arrays are the data structure we need! Each event is a record that holds
all the necessary information, such as the column and row where it originally occurred.

Because structured arrays can be difficult to analyze, there is a dedicated class for this purpose:
[Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records)! By subclassing
[Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable), it wraps a structured
NumPy array and provides useful tools for analysis. Every
[Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records) instance can be indexed like a regular
Pandas object and can compute various metrics and plot graphs.

Let's generate [Drawdowns](https://vectorbt.pro/pvt_41531c19/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns) records
for two columns of time series data:

```pycon
>>> dd_df = pd.DataFrame({
...     'a': [10, 11, 12, 11, 12, 13, 12],
...     'b': [14, 13, 12, 11, 12, 13, 14]
... }, index=vbt.date_range("2020", periods=7))
>>> drawdowns = dd_df.vbt.drawdowns
>>> drawdowns.readable
    Drawdown Id Column Start Index Valley Index  End Index  Start Value  \
0            0      a  2020-01-03   2020-01-04 2020-01-05         12.0   
1            1      a  2020-01-06   2020-01-07 2020-01-07         13.0   
2            0      b  2020-01-01   2020-01-04 2020-01-07         14.0   

    Valley Value  End Value     Status  
0          11.0       12.0  Recovered  
1          12.0       12.0     Active  
2          11.0       14.0  Recovered  

>>> drawdowns['b'].readable  # (1)!
    Drawdown Id Column Start Index Valley Index  End Index  Start Value  \
0            0      b  2020-01-01   2020-01-04 2020-01-07         14.0   

    Valley Value  End Value     Status  
0          11.0       14.0  Recovered  
```

1. Select all records of column `b` and display them in a human-readable format.

That's a lot of information! Each field is a standard NumPy array, so where does all this rich
information come from? Surprisingly, the labels of the DataFrames above were automatically
generated from the metadata that
[Drawdowns](https://vectorbt.pro/pvt_41531c19/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns) contains.
This metadata is called a "field config," which is a regular
[Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config) describing each field
(for example, [Drawdowns.field_config](https://vectorbt.pro/pvt_41531c19/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns.field_config)).
This setup enables automating and enhancing the behavior of each field. The class
[Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records), which is the base for all record classes,
includes many methods to read and interpret this config.

```pycon
>>> drawdowns.status.values  # (1)!
array([1, 0, 1])

>>> drawdowns.get_apply_mapping_arr('status')  # (2)!
array(['Recovered', 'Active', 'Recovered'], dtype=object)
```

1. Raw data.

2. Data enhanced using the field config.

### Column mapper

Records are one-dimensional structured NumPy arrays. Records from multiple columns are concatenated
into a single array, so we need a way to group them by column or group. For example, we might want to
aggregate values by column. This is not a trivial task because finding which records correspond to a
specific column requires searching through all records, which can be slow if done repeatedly. The
[ColumnMapper](https://vectorbt.pro/pvt_41531c19/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper) class addresses
this by indexing all columns just once and caching the results
(see [ColumnMapper.col_map](https://vectorbt.pro/pvt_41531c19/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper.col_map)).
A column mapper provides at least two more advantages: it allows for grouping columns and enables
efficient [Indexing](#indexing).

```pycon
>>> drawdowns.col_mapper.col_map  # (1)!
(array([0, 1, 2]), array([2, 1]))
```

1. An array containing the indices of records ordered by column, and an array with the number of records
per column.

The column map above tells us that column `a` has two records at indices 0 and 1, while column `b` has one
record at index 2.

### Mapped arrays

If [Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records) is like our own DataFrame for events,
then [MappedArray](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray) is like our
own Series! Each field in records can be mapped into a *mapped* array. In fact, most calculations happen on
a mapped array. It is similar to [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor),
but it represents data in a completely different way: one-dimensional and clustered, rather than
two-dimensional and column-wise. We can even seemingly convert between both representations.
Why not simply convert a mapped array into a standard Series and do all analyses there?
There are several reasons:

1. Event data is often sparse. If 1,000,000 data points yield only 50 events, it is much faster to analyze
    50 values than to convert them back and manage 9,999,950 NaNs.

2. Multiple events, such as orders, can happen within the same bar, which cannot be efficiently represented
in Pandas.

Let's analyze the drawdown values in `drawdowns`:

```pycon
>>> dd_ma = drawdowns.drawdown
>>> dd_ma
<vectorbtpro.records.mapped_array.MappedArray at 0x7ff6d8514f98>

>>> dd_ma.values  # (1)!
array([-0.08333333, -0.07692308, -0.21428571])

>>> dd_ma.stats(column='a')
Start        2020-01-01 00:00:00
End          2020-01-07 00:00:00
Period           7 days 00:00:00
Count                          2
Mean                   -0.080128
Std                     0.004533
Min                    -0.083333
Median                 -0.080128
Max                    -0.076923
Min Index    2020-01-05 00:00:00
Max Index    2020-01-07 00:00:00
Name: a, dtype: object

>>> dd_ma.to_pd()  # (2)!
                    a         b
2020-01-01       NaN       NaN
2020-01-02       NaN       NaN
2020-01-03       NaN       NaN
2020-01-04       NaN       NaN
2020-01-05 -0.083333       NaN
2020-01-06       NaN       NaN
2020-01-07 -0.076923 -0.214286
```

1. Holds three values: two from `a` and one from `b`.

2. Can be converted into Pandas, but at a cost.

Thanks to [ColumnMapper](https://vectorbt.pro/pvt_41531c19/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper) and
[Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable), we can select rows
and columns from a mapped array just like from records or any regular Pandas object:

```pycon
>>> dd_ma['b'].values
array([-0.21428571])
```

## Summary

Thank you for following along all the way down here! The classes we just discussed form a solid
foundation for data analysis with VBT. They implement design patterns that you will encounter in
many other places throughout the codebase, making them easy to recognize and extend. In fact, the most
advanced class, [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio), is very similar
to our `CorrStats`.

You are now more than ready to use VBT, soldier :star2:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/building-blocks.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/data/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
children:
  - https://vectorbt.pro/pvt_41531c19/documentation/data/local/
  - https://vectorbt.pro/pvt_41531c19/documentation/data/remote/
  - https://vectorbt.pro/pvt_41531c19/documentation/data/synthetic/
  - https://vectorbt.pro/pvt_41531c19/documentation/data/scheduling/
name: Data
type: page
icon: material-database
---

# Data

VBT works with Pandas and NumPy arrays, but where do these arrays come from? Obtaining 
financial data manually can be challenging, especially when an exchange provides only a single batch 
of data at a time. In such situations, users need to handle tasks like iterating over time ranges, 
concatenating results, and aligning indexes and columns themselves. This process becomes even more 
complex when working with multiple symbols.

To simplify data retrieval and management, VBT implements the 
[Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class, which streamlines the handling of features 
(such as OHLC) and symbols (such as "BTC-USD"). This is a semi-abstract class, so you need to 
create a subclass and define custom logic in a few places to fully access its advanced features. 
Fortunately, a set of custom data classes is already available, but it is helpful to understand how 
to create your own data class as well.

The following graph illustrates the steps discussed below:

```mermaid
flowchart TD;
    dataclass["Data class"]
    fetching["Fetching"]
    pdobjs["Pandas objects"]
    wrapping["Wrapping"]
    dataobjs["Data objects"]
    merging["Merging"]
    alignment["Alignment"]
    dataobj["Data object"]
    updating["Updating"]
    transforming["Transforming"]
    resampling["Resampling"]
    indexing["Indexing"]
    getting["Getting"]
    pdobj["Pandas object"]
    running["Running"]
    output["Output"]
    
    dataclass --> fetching;
    fetching --> alignment;
    pdobjs --> wrapping;
    wrapping --> alignment;
    dataobjs --> merging;
    merging --> alignment;
    alignment -->|"creates new"| dataobj;
    dataobj --> updating;
    updating --> alignment;
    dataobj --> transforming;
    transforming --> alignment;
    dataobj --> getting;
    getting --> pdobj;
    dataobj --> running;
    running --> output;
    dataobj --> resampling;
    resampling -->|"creates new"| dataobj;
    dataobj --> indexing;
    indexing -->|"creates new"| dataobj;
```

(Reload the page if the diagram does not appear.)

## Fetching

The [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class defines an abstract class method 
[Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol) for generating, loading, 
or fetching data for a single symbol from any data source. You must override and implement this method 
yourself so that it returns a single (Pandas or NumPy) array based on certain parameters, such as the 
start date, end date, and frequency.

Here is a function that retrieves data for any symbol from Yahoo Finance using 
[yfinance](https://github.com/ranaroussi/yfinance):

```pycon
>>> from vectorbtpro import *

>>> def get_yf_symbol(symbol, period="max", start=None, end=None, **kwargs):
...     import yfinance as yf
...     if start is not None:
...         start = vbt.local_datetime(start)  # (1)!
...     if end is not None:
...         end = vbt.local_datetime(end)
...     return yf.Ticker(symbol).history(
...         period=period, 
...         start=start, 
...         end=end, 
...         **kwargs
...     )

>>> get_yf_symbol("BTC-USD", start="2020-01-01", end="2020-01-05")
                                  Open         High          Low        Close  \
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00  21167946112        0.0           0.0  
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

1. Converts to datetime using [to_datetime](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_datetime)

!!! info
    Why does the returned data start from `2019-12-31` instead of `2020-01-01`? The start and 
    end dates you provide are in your local timezone and are then converted to UTC. For example, 
    in the Europe/Berlin timezone, depending on the time of year, `2020-01-01` becomes 
    `2019-12-31 22:00:00` UTC, which is the date Yahoo Finance actually receives. To specify 
    a date directly in UTC, append "UTC": `2020-01-01 UTC`, or use a proper 
    [Timestamp](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html) instance.

Using the Pandas format is convenient for a single symbol, but what if you want data for multiple 
symbols? Remember, VBT expects you to provide each feature, such as open price or high price, 
as a separate variable. Each variable should have symbols as columns, which means you would need 
to fetch every symbol manually and reorganize their data. This can become especially troublesome 
if the symbols have mismatched indexes or columns.

Fortunately, there is a class method called [Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull) 
that solves most of these challenges around iterating, fetching, and merging symbols. It accepts one 
or multiple symbols, fetches each using 
[Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol), collects the data into 
a dictionary, and passes this dictionary to [Data.from_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data) 
for further processing and class instantiation.

Building on the example, let's subclass [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) and 
override the [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol) method 
to call our `get_yf_symbol` function:

```pycon
>>> class YFData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         return get_yf_symbol(symbol, **kwargs)
```

!!! tip
    You can replace `get_yf_symbol` with any function that returns array-like data!

That's it. `YFData` is now a fully functional data class capable of pulling data from Yahoo Finance 
and storing it:

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

The pulled data is stored in the [Data.data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.data)
dictionary, with symbols as keys and the values being the Pandas objects returned by 
[Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol):

```pycon
>>> yf_data.data["ETH-USD"]
                                  Open        High         Low       Close  \
Date                                                                        
2019-12-31 00:00:00+00:00  132.612274  133.732681  128.798157  129.610855   
2020-01-01 00:00:00+00:00  129.630661  132.835358  129.198288  130.802002   
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00   8936866397        0.0           0.0  
2020-01-01 00:00:00+00:00   7935230330        0.0           0.0  
2020-01-02 00:00:00+00:00   8032709256        0.0           0.0  
2020-01-03 00:00:00+00:00  10476845358        0.0           0.0  
2020-01-04 00:00:00+00:00   7430904515        0.0           0.0 
```

### Exception handling

If [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol) returns `None` or
an empty Pandas object or NumPy array, the symbol will be skipped entirely.
[Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull) will also catch any exception raised in 
[Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol) and skip the symbol if 
the argument `skip_on_error` is True (it is False by default!). Otherwise, it will abort the procedure.

In general, it is the responsibility of [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol) 
to handle issues. If you have many data points to fetch and the fetcher relies on a loop to concatenate
different batches of data, the best approach is to show the user a warning whenever an exception occurs 
and return the data that has been fetched up to the latest available point in time, similar to how 
this is implemented in [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData) 
and [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData). In such cases, VBT will 
replace missing data points with NaN or drop them entirely, while keeping track of the last index. You can 
wait until your connection is stable and then re-fetch any missing data with 
[Data.update](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update).

### Custom context

Along with the data, [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol) can 
return a dictionary containing custom keyword arguments that serve as the context for the fetch operation.
This context can later be accessed in the symbol dictionary [Data.returned_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.returned_kwargs).
For example, this context may include information on why the fetch failed, the amount of data left 
to fetch, or which rows the fetched data represents when reading from a local file (as done in 
[CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) when updating data).

As an example, let's save the current timestamp:

```pycon
>>> class YFData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         returned_kwargs = dict(timestamp=vbt.timestamp())
...         return get_yf_symbol(symbol, **kwargs), returned_kwargs

>>> yf_data = YFData.pull("BTC-USD", start="2020-01-01", end="2020-01-05")
>>> yf_data.returned_kwargs
symbol_dict({'BTC-USD': {'timestamp': Timestamp('2023-08-28 20:08:50.893763')}})
```

!!! info
    [symbol_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.symbol_dict) is a standard dictionary 
    where information is grouped by symbol.

## Alignment

Like most classes that hold data, the [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class 
inherits from [Analyzable](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#analyzing), allowing you to perform 
Pandas indexing directly on the class instance to select rows and columns across all Pandas 
objects stored within that instance. Performing a single Pandas indexing operation across multiple 
Pandas objects with different labels is not possible, so what happens if you fetch symbol data from 
different date ranges or with different columns? Whenever [Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull) 
passes the (unaligned) data dictionary to [Data.from_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data), 
it calls [Data.align_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.align_data), which does the following:

1. Converts any array-like data into a Pandas object.

2. Removes rows with duplicate indices except for the most recent one.

3. Calls [Data.prepare_tzaware_index](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.prepare_tzaware_index)
to convert each object's index into a timezone-aware index using 
[DataFrame.tz_localize](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tz_localize.html) 
and [DataFrame.tz_convert](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.tz_convert.html).

4. Calls [Data.align_index](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.align_index) to align the index 
labels of all objects based on a specific rule. By default, it creates the union of all indexes, sorts 
the resulting index, and fills missing data points in any object with NaN.

5. Calls [Data.align_columns](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.align_columns) to align column 
labels of all objects using a similar approach to index alignment.

6. Once all objects have the same index and columns, it creates a [wrapper](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#wrapping).

7. Finally, it passes all the prepared information to the class constructor for instantiation.

Let's walk through this workflow in practice:

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({  # (1)!
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     })
... )
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.
```

1. Use [symbol_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.symbol_dict) to specify any argument per symbol.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> yf_data.data["BTC-USD"]
                                  Open         High          Low        Close  \
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00          NaN          NaN          NaN          NaN   
2020-01-04 00:00:00+00:00          NaN          NaN          NaN          NaN   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  2.116795e+10        0.0           0.0  
2020-01-01 00:00:00+00:00  1.856566e+10        0.0           0.0  
2020-01-02 00:00:00+00:00  2.080208e+10        0.0           0.0  
2020-01-03 00:00:00+00:00           NaN        NaN           NaN  
2020-01-04 00:00:00+00:00           NaN        NaN           NaN  

>>> yf_data.data["ETH-USD"]
                                  Open        High         Low       Close  \
Date                                                                        
2019-12-31 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-01 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00           NaN        NaN           NaN  
2020-01-01 00:00:00+00:00           NaN        NaN           NaN  
2020-01-02 00:00:00+00:00  8.032709e+09        0.0           0.0  
2020-01-03 00:00:00+00:00  1.047685e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  7.430905e+09        0.0           0.0 
```

Notice how we now have the same index and columns across all Pandas objects. You can use this data in 
any VBT function without worrying about indexing errors.

### NaNs

If some rows are present in one symbol and missing in another, VBT will raise a warning 
with the message "Symbols have mismatching index". By default, any missing rows will be replaced with NaN.
To drop these rows or raise an error instead, use the `missing_index` argument:

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     }),
...     missing_index="drop"
... )
UserWarning: Symbols have mismatching index. Dropping missing data points.
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> yf_data.data["BTC-USD"]
                                  Open         High         Low        Close  \
Date                                                                          
2020-01-02 00:00:00+00:00  7202.55127  7212.155273  6935.27002  6985.470215   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  

>>> yf_data.data["ETH-USD"]
                                  Open        High        Low       Close  \
Date                                                                       
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.95491  127.410179   

                                Volume  Dividends  Stock Splits  
Date                                                            
2020-01-02 00:00:00+00:00  8032709256        0.0           0.0  
```

## Updating

Updating is a standard fetching operation used to update existing data points or 
add new ones. It requires specifying the first timestamp or row index of the update and 
assumes that the data points before this timestamp or row index have not changed.

Similarly to [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol), updating 
must be implemented manually by overriding the [Data.update_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update_symbol) 
method. Unlike the fetcher, the updater is an **instance** method and can access data fetched earlier. 
For example, it can access the keyword arguments originally passed to the fetcher, which are available in 
the symbol dictionary [Data.fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_kwargs).
These arguments can be used as default arguments, or overridden by any argument passed directly to the updater. 
Each data instance also has a symbol dictionary [Data.last_index](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.last_index), 
which holds the last fetched index for each symbol. You can use this index as the starting point for the next update.

Let's create a new `YFData` class that can also perform updates on the stored data:

```pycon
>>> class YFData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         return get_yf_symbol(symbol, **kwargs)
...
...     def update_symbol(self, symbol, **kwargs):
...         defaults = self.select_fetch_kwargs(symbol)  # (1)!
...         defaults["start"] = self.select_last_index(symbol)  # (2)!
...         kwargs = vbt.merge_dicts(defaults, kwargs)  # (3)!
...         return self.fetch_symbol(symbol, **kwargs)  # (4)!
```

1. Get the keyword arguments initially passed to [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol)
for this specific symbol.

2. Override the default value for the starting date. Note that changing the keys will not affect 
[Data.fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_kwargs), but be careful with mutable values!

3. Override the default arguments with new arguments in `kwargs` using [merge_dicts](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.merge_dicts).

4. Pass the final arguments to [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol).

Once the [Data.update_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update_symbol) method is implemented,
we can call the [Data.update](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update) method to iterate over
each symbol and update its data. Under the hood, this method aligns the index and column labels
of all returned Pandas objects, appends the new data to the old data through concatenation along rows,
and updates the last index for each symbol for use in the next data update. Finally, it produces a new
instance of [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) by using 
[Configured.replace](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.replace).

!!! important
    Updating data never overwrites the existing data instance but always returns a new instance.
    Remember that most classes in VBT are read-only to enable caching and prevent side effects.

First, let's fetch the same data as before:

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     })
... )
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

Even though both DataFrames end with the same date, our `YFData` instance knows that 
the `BTC-USD` symbol is 2 rows behind the `ETH-USD` symbol:

```pycon
>>> yf_data.last_index
symbol_dict({
    'BTC-USD': Timestamp('2020-01-02 00:00:00+0000', tz='UTC'), 
    'ETH-USD': Timestamp('2020-01-04 00:00:00+0000', tz='UTC')
})
```

We can also access the keyword arguments passed to the initial fetch operation:

```pycon
>>> yf_data.fetch_kwargs
symbol_dict({
    'BTC-USD': {'start': '2020-01-01', 'end': '2020-01-03'}, 
    'ETH-USD': {'start': '2020-01-03', 'end': '2020-01-05'}
})
```

The `start` argument for each symbol will be replaced by its corresponding entry in 
[Data.last_index](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.last_index), while the 
`end` argument can be overridden by any date you specify during the update.

!!! note
    If you do not specify the end date, VBT will update only the latest data point for each symbol.

Let's update both symbols up to the same date:

```pycon
>>> yf_data_updated = yf_data.update(end="2020-01-06")  # (1)!

>>> yf_data_updated.data["BTC-USD"]
                                  Open         High          Low        Close  \
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   
2020-01-05 00:00:00+00:00  7410.451660  7544.497070  7400.535645  7411.317383   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  2.116795e+10        0.0           0.0  
2020-01-01 00:00:00+00:00  1.856566e+10        0.0           0.0  
2020-01-02 00:00:00+00:00  2.080208e+10        0.0           0.0  
2020-01-03 00:00:00+00:00  2.811148e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  1.844427e+10        0.0           0.0  
2020-01-05 00:00:00+00:00  1.972507e+10        0.0           0.0  

>>> yf_data_updated.data["ETH-USD"]
                                  Open        High         Low       Close  \
Date                                                                        
2019-12-31 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-01 00:00:00+00:00         NaN         NaN         NaN         NaN   
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   
2020-01-05 00:00:00+00:00  135.072098  139.410202  135.045624  136.276779   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00           NaN        NaN           NaN  
2020-01-01 00:00:00+00:00           NaN        NaN           NaN  
2020-01-02 00:00:00+00:00  8.032709e+09        0.0           0.0  
2020-01-03 00:00:00+00:00  1.047685e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  7.430905e+09        0.0           0.0  
2020-01-05 00:00:00+00:00  7.526675e+09        0.0           0.0 
```

1. Same date for both symbols.

Each symbol has been updated separately based on its `last_index` value. The `BTC-USD` symbol 
has received new rows from `2020-01-02` to `2020-01-05`, while the `ETH-USD` symbol has received 
new rows only from `2020-01-04` to `2020-01-05`. Now, both symbols are synced up to the same ending date:

```pycon
>>> yf_data_updated.last_index
symbol_dict({
    'BTC-USD': Timestamp('2020-01-05 00:00:00+0000', tz='UTC'), 
    'ETH-USD': Timestamp('2020-01-05 00:00:00+0000', tz='UTC')
})
```

If the last index of the data update falls before the current `last_index` (meaning you want to 
update data in the middle), all data after the new last index will be removed:

```pycon
>>> yf_data_updated = yf_data_updated.update(start="2020-01-01", end="2020-01-02")

>>> yf_data_updated.data["BTC-USD"]
                                  Open         High          Low        Close  \
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  2.116795e+10        0.0           0.0  
2020-01-01 00:00:00+00:00  1.856566e+10        0.0           0.0 

>>> yf_data_updated.data["ETH-USD"]
                                  Open        High         Low       Close  \
Date                                                                        
2019-12-31 00:00:00+00:00  132.612274  133.732681  128.798157  129.610855   
2020-01-01 00:00:00+00:00  129.630661  132.835358  129.198288  130.802002   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2019-12-31 00:00:00+00:00  8.936866e+09        0.0           0.0  
2020-01-01 00:00:00+00:00  7.935230e+09        0.0           0.0  
```

!!! note
    The last data point of an update is treated as the most up-to-date, so
    no previously stored data can come after it.

### Concatenation

By default, the returned data instance contains all data: the old data concatenated with the new data.
To return only the updated data, disable `concat`:

```pycon
>>> yf_data_new = yf_data.update(end="2020-01-06", concat=False)

>>> yf_data_new.data["BTC-USD"]
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   
2020-01-05 00:00:00+00:00  7410.451660  7544.497070  7400.535645  7411.317383   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2020-01-02 00:00:00+00:00  2.080208e+10        0.0           0.0  
2020-01-03 00:00:00+00:00  2.811148e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  1.844427e+10        0.0           0.0  
2020-01-05 00:00:00+00:00  1.972507e+10        0.0           0.0  

>>> yf_data_new.data["ETH-USD"]
                                  Open        High         Low       Close  \
Date                                                                        
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179   
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707   
2020-01-04 00:00:00+00:00  134.168518  136.052719  133.040558  135.069366   
2020-01-05 00:00:00+00:00  135.072098  139.410202  135.045624  136.276779   

                                  Volume  Dividends  Stock Splits  
Date                                                              
2020-01-02 00:00:00+00:00  8.032709e+09        0.0           0.0  
2020-01-03 00:00:00+00:00  1.047685e+10        0.0           0.0  
2020-01-04 00:00:00+00:00  7.430905e+09        0.0           0.0  
2020-01-05 00:00:00+00:00  7.526675e+09        0.0           0.0  
```

The returned data instance skips two timestamps: `2019-12-31` and `2020-01-01`,
since these were not changed during that update. Even though the symbol `ETH-USD` only received 
new rows from `2020-01-04` to `2020-01-05`, it also contains the old data for `2020-01-02` and
`2020-01-03`. Why is that? Those timestamps were updated in the `BTC-USD` dataset, and because the
index must be aligned across all symbols, some old data is included to avoid setting NaNs.

## Getting

After data is fetched and a new [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instance is
created, accessing the data is straightforward using the
[Data.data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.data) dictionary or the
[Data.get](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get) method.

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

Get all features for one symbol:

```pycon
>>> yf_data.get(symbols="BTC-USD")
                                  Open         High          Low        Close  \
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00  21167946112        0.0           0.0  
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

Get specific features for one symbol:

```pycon
>>> yf_data.get(features=["High", "Low"], symbols="BTC-USD")
                                  High          Low
Date                                               
2019-12-31 00:00:00+00:00  7335.290039  7169.777832
2020-01-01 00:00:00+00:00  7254.330566  7174.944336
2020-01-02 00:00:00+00:00  7212.155273  6935.270020
2020-01-03 00:00:00+00:00  7413.715332  6914.996094
2020-01-04 00:00:00+00:00  7427.385742  7309.514160
```

Get one feature for all symbols:

```pycon
>>> yf_data.get(features="Close")
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121  129.610855
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Notice how symbols are now columns in the returned DataFrame? This is the format widely used in VBT.

Get multiple features for multiple symbols:

```pycon
>>> open_price, close_price = yf_data.get(features=["Open", "Close"])  # (1)!
```

1. Tuple with DataFrames, one per feature.

!!! tip
    As you may have noticed, VBT returns different formats depending on whether there is one or
    multiple features or symbols included in your data instance. To produce a consistent format regardless
    of the number of features or symbols, pass `features` or `symbols` as a list or another collection.

    For example, calling `yf_data.get(features="Close")` when there is only one symbol will return a Series
    instead of a DataFrame. To ensure VBT always returns a DataFrame, pass `features=["Close"]`.

### Magnet features

Magnet features are features with case-insensitive names that the [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data)
class can detect and query. These include static features such as OHLCV, as well as those that
can be computed dynamically, like VWAP, HLC/3, OHLC/4, and returns. Each feature is also associated
with an instance property that returns that feature for all symbols in a data instance. For example,
to get the close price and returns:

```pycon
>>> yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121  129.610855
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366

>>> yf_data.returns
symbol                      BTC-USD   ETH-USD
Date                                         
2019-12-31 00:00:00+00:00  0.000000  0.000000
2020-01-01 00:00:00+00:00  0.000914  0.009190
2020-01-02 00:00:00+00:00 -0.029819 -0.025931
2020-01-03 00:00:00+00:00  0.051452  0.053069
2020-01-04 00:00:00+00:00  0.008955  0.006690
```

## Running

Thanks to the clear structure of magnet features, we can use them when passing data to many functions
across VBT. Since most functions do not accept the `data` instance directly, but expect features
such as `close` to be provided separately, there is a need for a method that can recognize what a
function requires and feed the data accordingly. The [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run)
method serves this purpose: it accepts a function, parses its arguments, and if it recognizes a magnet
feature, it forwards it as needed. This is especially helpful for quickly running indicators, which are
automatically recognized by their names:

```pycon
>>> yf_data.run("sma", 3)
<vectorbtpro.indicators.factory.talib.SMA at 0x7ff75218b5b0>
```

When there are multiple third-party libraries with the same indicator name, it is a good idea
to provide a prefix with the library name to avoid confusion:

```pycon
>>> yf_data.run("talib_sma", 3)
<vectorbtpro.indicators.factory.talib.SMA at 0x7ff752111070>
```

This method also accepts the names of all simulation methods available in
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio), such as
[Portfolio.from_holding](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_holding):

```pycon
>>> yf_data.run("from_holding")
<vectorbtpro.portfolio.base.Portfolio at 0x7ff767e18970>
```

## Features and symbols

The [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class provides various dictionaries
that hold data for each symbol, as well as methods to help manipulate that data.

You can view the list of features and symbols using the
[Data.features](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.symbols) and
[Data.symbols](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.symbols) properties, respectively:

```pycon
>>> yf_data.features
['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']

>>> yf_data.symbols
['BTC-USD', 'ETH-USD']
```

There is also a [Data.single_key](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.single_key) flag,
which is True if the instance holds only one symbol of data (or one feature, if the instance is
feature-oriented). This affects [Getting](#getting), as discussed in the hints above.

```pycon
>>> yf_data.single_key
False
```

### Dicts

Each data instance contains at least five dictionaries:

1. [Data.data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.data) with the Pandas objects.

2. [Data.classes](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.classes) with the classes.

3. [Data.fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_kwargs) with the keyword arguments
passed to the fetcher.

4. [Data.returned_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.returned_kwargs) with the keyword arguments
returned by the fetcher.

5. [Data.last_index](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.last_index) with the last fetched index.

Each dictionary is a regular dictionary of either the type
[symbol_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.symbol_dict) (mostly when the instance is symbol-oriented)
or [feature_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.feature_dict) (mostly when the instance is feature-oriented).

```pycon
>>> yf_data.last_index["BTC-USD"]
Timestamp('2020-01-04 00:00:00+0000', tz='UTC')
```

!!! important
    Do not change the values of the above dictionaries in-place. Whenever you work with keyword arguments,
    make sure to create a new dict after selecting a symbol: `dict(data.fetch_kwargs[symbol])`.
    This will not change the parent dict if you want to modify the keyword arguments for some task.

### Selecting

You can select one or more symbols using [Data.select](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.select):

```pycon
>>> yf_data.select("BTC-USD")
<__main__.YFData at 0x7ff6a97f4b38>

>>> yf_data.select("BTC-USD").get()
                                  Open         High          Low        Close  \
Date                                                                            
2019-12-31 00:00:00+00:00  7294.438965  7335.290039  7169.777832  7193.599121   
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2019-12-31 00:00:00+00:00  21167946112        0.0           0.0  
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

The operation above creates a new `YFData` instance with only one symbol, `BTC-USD`.

!!! note
    Updating the data in a child instance will not affect the parent instance it was copied from
    because updating creates a new Pandas object. However, changing the data in-place will also
    change it in the parent instance. To make both instances fully independent, pass `copy_mode_="deep"`
    (see [Configured.replace](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.replace)).

!!! info
    If the instance is feature-oriented, this method applies to features instead of symbols.

### Renaming

You can rename symbols using [Data.rename](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.rename):

```pycon
>>> yf_data.rename({
...     "BTC-USD": "BTC/USD",
...     "ETH-USD": "ETH/USD"
... }).get("Close")
symbol                         BTC/USD     ETH/USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121  129.610855
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

!!! warning
    Renaming symbols may (and often will) break their updating. Use this only for getting.

!!! info
    If the instance is feature-oriented, this method applies to features instead of symbols.

### Classes

Classes are helpful when you want to introduce another level of abstraction over symbols,
such as dividing symbols into industries and sectors. This lets you analyze symbols within
their classes and also study entire classes. Classes can be provided to the fetcher using the
`classes` argument. They must be specified per symbol, unless there is only one class to apply to
all symbols. In the end, they will be converted into a (multi-)index and stacked on top of symbol
columns when you get the symbol wrapper using [Data.get_symbol_wrapper](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data.get_symbol_wrapper).
Each class can be provided as a string (which will be stored under the class name `symbol_class`),
or as a dictionary with class names as keys and their corresponding class values as values.

```pycon
>>> cls_yfdata = vbt.YFData.pull(
...     ["META", "GOOGL", "NFLX", "BAC", "WFC", "TLT", "SHV"],
...     classes=[
...         dict(class1="Equity", class2="Technology"),
...         dict(class1="Equity", class2="Technology"),
...         dict(class1="Equity", class2="Technology"),
...         dict(class1="Equity", class2="Financial"),
...         dict(class1="Equity", class2="Financial"),
...         dict(class1="Fixed Income", class2="Treasury"),
...         dict(class1="Fixed Income", class2="Treasury"),
...     ],
...     start="2010-01-01",
...     missing_columns="nan"
... )
>>> cls_yfdata.close
class1                         Equity                                     \
class2                     Technology                          Financial   
symbol                           META       GOOGL        NFLX        BAC   
Date                                                                       
2010-01-04 00:00:00-05:00         NaN   15.684434    7.640000  12.977036   
2010-01-05 00:00:00-05:00         NaN   15.615365    7.358571  13.398854   
2010-01-06 00:00:00-05:00         NaN   15.221722    7.617143  13.555999   
...                               ...         ...         ...        ...   
2023-08-24 00:00:00-04:00  286.750000  129.779999  406.929993  28.620001   
2023-08-25 00:00:00-04:00  285.500000  129.880005  416.029999  28.500000   
2023-08-28 00:00:00-04:00  290.260010  131.009995  418.059998  28.764999   

class1                               Fixed Income              
class2                                   Treasury              
symbol                           WFC          TLT         SHV  
Date                                                           
2010-01-04 00:00:00-05:00  19.073046    62.717960   99.920975  
2010-01-05 00:00:00-05:00  19.596645    63.123013   99.884712  
2010-01-06 00:00:00-05:00  19.624567    62.278038   99.893806  
...                              ...          ...         ...  
2023-08-24 00:00:00-04:00  41.430000    94.910004  110.389999  
2023-08-25 00:00:00-04:00  41.230000    95.220001  110.389999  
2023-08-28 00:00:00-04:00  41.880001    95.320000  110.400002  

[3436 rows x 7 columns]
```

In addition to providing classes to the fetcher, you can also replace them in any existing data
instance, which will return a new data instance:

```pycon
>>> new_cls_yfdata = cls_yfdata.replace(
...     classes=vbt.symbol_dict({
...         "META": dict(class1="Equity", class2="Technology"),
...         "GOOGL": dict(class1="Equity", class2="Technology"),
...         "NFLX": dict(class1="Equity", class2="Technology"),
...         "BAC": dict(class1="Equity", class2="Financial"),
...         "WFC": dict(class1="Equity", class2="Financial"),
...         "TLT": dict(class1="Fixed Income", class2="Treasury"),
...         "SHV": dict(class1="Fixed Income", class2="Treasury"),
...     })
... )
```

You can also use [Data.update_classes](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update_classes):

```pycon
>>> new_cls_yfdata = cls_yfdata.update_classes(
...     class1=vbt.symbol_dict({
...         "META": "Equity",
...         "GOOGL": "Equity",
...         "NFLX": "Equity",
...         "BAC": "Equity",
...         "WFC": "Equity",
...         "TLT": "Fixed Income",
...         "SHV": "Fixed Income",
...     }),
...     class2=vbt.symbol_dict({
...         "META": "Technology",
...         "GOOGL": "Technology",
...         "NFLX": "Technology",
...         "BAC": "Financial",
...         "WFC": "Financial",
...         "TLT": "Treasury",
...         "SHV": "Treasury",
...     })
... )
```

!!! info
    If the instance is feature-oriented and the dictionary with classes is of type
    [feature_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.feature_dict),
    the classes will be applied to features instead of symbols.

## Wrapping

You do not need data instances to work with VBT, since the main objects VBT operates on are
Pandas and NumPy arrays. However, it is often much more convenient to have all your data contained
within the same [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) container. This allows data to
be managed (aligned, resampled, transformed, etc.) in a standardized way. To wrap any custom Pandas
object with a [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class, you can use the class method
[Data.from_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data). This method can take either a
single Pandas object (which will be stored under the symbol `symbol`), a symbol dictionary with
multiple Pandas objects (one per symbol), or a feature dictionary with multiple Pandas objects (one
per feature).

The Series or DataFrame to be wrapped typically has columns representing features, such as OHLC,
rather than symbols like `BTCUSDT`. For example:

```pycon
>>> btc_df = pd.DataFrame({
...     "Open": [7194.89, 7202.55, 6984.42],
...     "High": [7254.33, 7212.15, 7413.71],
...     "Low": [7174.94, 6935.27, 6985.47],
...     "Close": [7200.17, 6985.47, 7344.88]
... }, index=vbt.date_range("2020-01-01", periods=3))
>>> btc_df
                Open     High      Low    Close
2020-01-01  7194.89  7254.33  7174.94  7200.17
2020-01-02  7202.55  7212.15  6935.27  6985.47
2020-01-03  6984.42  7413.71  6985.47  7344.88

>>> my_data = vbt.Data.from_data(btc_df)
>>> my_data.hlc3
2020-01-01 00:00:00+00:00    7209.813333
2020-01-02 00:00:00+00:00    7044.296667
2020-01-03 00:00:00+00:00    7248.020000
Freq: D, dtype: float64
```

You can also wrap multiple Pandas objects keyed by symbol:

```pycon
>>> eth_df = pd.DataFrame({
...     "Open": [127.41, 134.16, 135.07],
...     "High": [134.55, 136.05, 139.41],
...     "Low": [126.49, 133.04, 135.04],
...     "Close": [134.17, 135.06, 136.27]
... }, index=vbt.date_range("2020-01-03", periods=3))  # (1)!
>>> eth_df
              Open    High     Low   Close
2020-01-03  127.41  134.55  126.49  134.17
2020-01-04  134.16  136.05  133.04  135.06
2020-01-05  135.07  139.41  135.04  136.27

>>> my_data = vbt.Data.from_data({"BTCUSDT": btc_df, "ETHUSDT": eth_df})
>>> my_data.hlc3
symbol                         BTCUSDT     ETHUSDT
2020-01-01 00:00:00+00:00  7209.813333         NaN
2020-01-02 00:00:00+00:00  7044.296667         NaN
2020-01-03 00:00:00+00:00  7248.020000  131.736667
2020-01-04 00:00:00+00:00          NaN  134.716667
2020-01-05 00:00:00+00:00          NaN  136.906667
```

1. Use different dates to demonstrate alignment.

If your data has symbols as columns, enable `columns_are_symbols`:

```pycon
>>> hlc3_data = vbt.Data.from_data(my_data.hlc3, columns_are_symbols=True)
>>> hlc3_data.get()
symbol                         BTCUSDT     ETHUSDT
2020-01-01 00:00:00+00:00  7209.813333         NaN
2020-01-02 00:00:00+00:00  7044.296667         NaN
2020-01-03 00:00:00+00:00  7248.020000  131.736667
2020-01-04 00:00:00+00:00          NaN  134.716667
2020-01-05 00:00:00+00:00          NaN  136.906667
```

In this case, the instance will become feature-oriented. This means the DataFrame above will be
stored in a [feature_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.feature_dict), and the behavior of
symbols and features will be swapped across many methods. To make the instance symbol-oriented as in
most examples, also pass `invert_data=True`.

## Merging

As you may have already noticed, the process of aligning data is logically separated from the process
of fetching data. This allows you to merge and align any data retrospectively.

Instead of storing and managing all symbols as one monolithic entity, you can handle them separately
and merge them into a single data instance only when needed. This approach is especially useful when
symbols are distributed across multiple data classes, such as when mixing remote and local data
sources. For this, you can use the class method [Data.merge](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.merge),
which takes two or more data instances, merges their information, and forwards the merged result to
[Data.from_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data):

```pycon
>>> yf_data_btc = YFData.pull(
...     "BTC-USD",
...     start="2020-01-01",
...     end="2020-01-03"
... )
>>> yf_data_eth = YFData.pull(
...     "ETH-USD",
...     start="2020-01-03",
...     end="2020-01-05"
... )
>>> merged_yf_data = YFData.merge(yf_data_btc, yf_data_eth)
>>> merged_yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121         NaN
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00          NaN  134.171707
2020-01-04 00:00:00+00:00          NaN  135.069366
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.
```

The advantage of this method is that it not only merges different symbols across various data
instances, but it can also merge Pandas objects for the same symbol.

```pycon
>>> yf_data_btc1 = YFData.pull(
...     "BTC-USD",
...     start="2020-01-01",
...     end="2020-01-03"
... )
>>> yf_data_btc2 = YFData.pull(
...     "BTC-USD",
...     start="2020-01-05",
...     end="2020-01-07"
... )
>>> yf_data_eth = YFData.pull(
...     "ETH-USD",
...     start="2020-01-06",
...     end="2020-01-08"
... )
>>> merged_yf_data = YFData.merge(yf_data_btc1, yf_data_btc2, yf_data_eth)
>>> merged_yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2019-12-31 00:00:00+00:00  7193.599121         NaN
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215         NaN
2020-01-04 00:00:00+00:00  7410.656738         NaN
2020-01-05 00:00:00+00:00  7411.317383  136.276779
2020-01-06 00:00:00+00:00  7769.219238  144.304153
2020-01-07 00:00:00+00:00          NaN  143.543991
```

### Subclassing

We used [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) on the class `YFData`, which
automatically creates an instance of that class. Once you have an instance of `YFData`, you can
update the data just as before. But what if the data instances you want to merge come from different
data classes? If you used `YFData` to merge [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData)
and [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData)
instances, you would no longer be able to update the data objects, since the method
`YFData.update_symbol` is implemented specifically for symbols supported by Yahoo Finance.

In this case, you can either use [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data), which will
raise an error if you attempt to update, or create a subclass to handle updates using different
data providers. This is fairly easy if you know which symbol belongs to which data class;
simply call the appropriate `fetch_symbol` or `update_symbol` method:

```pycon
>>> bn_data_btc = vbt.BinanceData.pull(
...     "BTCUSDT",
...     start="2020-01-01",
...     end="2020-01-04")
>>> bn_data_btc.close
Open time
2020-01-01 00:00:00+00:00    7200.85
2020-01-02 00:00:00+00:00    6965.71
2020-01-03 00:00:00+00:00    7344.96
Freq: D, Name: Close, dtype: float64

>>> ccxt_data_eth = vbt.CCXTData.pull(
...     "ETH/USDT",
...     start="2020-01-03",
...     end="2020-01-06")
>>> ccxt_data_eth.close
Open time
2020-01-03 00:00:00+00:00    134.35
2020-01-04 00:00:00+00:00    134.20
2020-01-05 00:00:00+00:00    135.37
Freq: D, Name: Close, dtype: float64

>>> class MergedData(vbt.Data):
...     @classmethod
...     def fetch_symbol(cls, symbol, **kwargs):
...         if symbol.startswith("BN_"):
...             return vbt.BinanceData.fetch_symbol(symbol[3:], **kwargs)
...         if symbol.startswith("CCXT_"):
...             return vbt.CCXTData.fetch_symbol(symbol[5:], **kwargs)
...         raise ValueError(f"Unknown symbol '{symbol}'")
...
...     def update_symbol(self, symbol, **kwargs):
...         fetch_kwargs = self.select_fetch_kwargs(symbol)
...         fetch_kwargs["start"] = self.select_last_index(symbol)
...         kwargs = vbt.merge_dicts(fetch_kwargs, kwargs)
...         return self.fetch_symbol(symbol, **kwargs)

>>> merged_data = MergedData.merge(
...     bn_data_btc,
...     ccxt_data_eth,
...     rename={
...         "BTCUSDT": "BN_BTCUSDT",
...         "ETH/USDT": "CCXT_ETH/USDT"
...     },
...     missing_columns="drop"
... )
UserWarning: Symbols have mismatching index. Setting missing data points to NaN.
UserWarning: Symbols have mismatching columns. Dropping missing data points.

>>> merged_data = merged_data.update(end="2020-01-07")
>>> merged_data.close
symbol                     BN_BTCUSDT  CCXT_ETH/USDT
Open time                                           
2020-01-01 00:00:00+00:00     7200.85            NaN
2020-01-02 00:00:00+00:00     6965.71            NaN
2020-01-03 00:00:00+00:00     7344.96         134.35
2020-01-04 00:00:00+00:00     7354.11         134.20
2020-01-05 00:00:00+00:00     7358.75         135.37
2020-01-06 00:00:00+00:00     7758.00         144.15
```

We have just created a flexible data class that can fetch, update, and manage symbols from multiple
data providers. Great!

## Resampling

As a subclass of [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping), each data
instance stores the normalized metadata of all Pandas objects stored within that instance. This
metadata can be used to resample (i.e., change the time frame of) all Pandas objects at once. Since
many data classes, such as [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData),
have a fixed feature layout, you can define the resampling function for each feature in a special
configuration called the "feature config" (stored under
[Data.feature_config](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.feature_config)), and bind that
config to the class itself for use by all instances. Similar to field configs in
[Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records), this config can be attached to the
entire data class or to any of its instances. Whenever a new instance is created, the class config is
copied so that rewriting it will not affect the original class config.

Here is what the feature config of [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData)
looks like:

```pycon
>>> vbt.pprint(vbt.BinanceData.feature_config)
HybridConfig({
    'Quote volume': dict(
        resample_func=<function BinanceData.<lambda> at 0x7ff7648d7280>
    ),
    'Taker base volume': dict(
        resample_func=<function BinanceData.<lambda> at 0x7ff7648d7310>
    ),
    'Taker quote volume': dict(
        resample_func=<function BinanceData.<lambda> at 0x7ff7648d73a0>
    )
})
```

You might be wondering where the resampling functions are for all the OHLCV features. Those features
are universal and are recognized and resampled automatically.

Now, let's resample the entire daily BTC/USD data from Yahoo Finance to a monthly frequency:

```pycon
>>> full_yf_data = vbt.YFData.pull("BTC-USD")  # (1)!
>>> ms_yf_data = full_yf_data.resample("M")
>>> ms_yf_data.close
Date
2014-09-01 00:00:00+00:00      386.944000
2014-10-01 00:00:00+00:00      338.321014
2014-11-01 00:00:00+00:00      378.046997
                                      ...     
2023-06-01 00:00:00+00:00    30477.251953
2023-07-01 00:00:00+00:00    29230.111328
2023-08-01 00:00:00+00:00    25995.177734
Freq: MS, Name: Close, Length: 108, dtype: float64
```

1. Use the built-in Yahoo Finance class, which already knows how to resample features such as dividends.

Since VBT works just as well with custom target indexes as with built-in frequencies, you can
provide a custom index for resampling:

```pycon
>>> resampler = vbt.Resampler.from_date_range(
...     full_yf_data.wrapper.index,
...     start=full_yf_data.wrapper.index[0],
...     end=full_yf_data.wrapper.index[-1],
...     freq="Y",
...     silence_warnings=True
... )
>>> y_yf_data = full_yf_data.resample(resampler)
>>> y_yf_data.close
2014-12-31 00:00:00+00:00      426.619995
2015-12-31 00:00:00+00:00      961.237976
2016-12-31 00:00:00+00:00    12952.200195
2017-12-31 00:00:00+00:00     3865.952637
2018-12-31 00:00:00+00:00     7292.995117
2019-12-31 00:00:00+00:00    28840.953125
2020-12-31 00:00:00+00:00    47178.125000
2021-12-31 00:00:00+00:00    39194.972656
Freq: A-DEC, Name: Close, dtype: float64
```

!!! note
    When providing a custom index, VBT will aggregate all values after each index entry.
    The last entry aggregates all values up to infinity. See [GenericAccessor.resample_to_index](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_to_index).

If a data class does not have a fixed feature layout, such as [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData),
you need to adapt the feature config to each **data instance** rather than to the entire data class.
For example, if you convert `bn_data_btc` to a generic [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instance:

```pycon
>>> data_btc = vbt.Data.from_data(bn_data_btc.data, single_key=True)
>>> data_btc.resample("M")
ValueError: Cannot resample feature 'Quote volume'. Specify resample_func in feature_config.

>>> for k, v in vbt.BinanceData.feature_config.items():
...     data_btc.feature_config[k] = v
>>> data_btc.resample("M")
<vectorbtpro.data.base.Data at 0x7fc0dfce1630>
```

You can also perform this with a single copy operation using
[Data.use_feature_config_of](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.use_feature_config_of):

```pycon
>>> data_btc = vbt.Data.from_data(bn_data_btc.data, single_key=True)
>>> data_btc.use_feature_config_of(vbt.BinanceData)
>>> data_btc.resample("M").close
Open time
2020-01-01 00:00:00+00:00    7344.96
Freq: MS, Name: Close, dtype: float64
```

### Realignment

Just like resampling, realignment also changes the data frequency, but unlike resampling, it does
not aggregate the data. Instead, it includes only the most recent data point available at each step in
the target index. It uses [GenericAccessor.realign_opening](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_opening)
for "open" and [GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing)
for all other features. There are two common use cases for realignment: aligning multiple symbols
from different time zones to a single index, and upsampling data. Let's align symbols with different
timings:

```pycon
>>> data = vbt.YFData.pull(
...     ["BTC-USD", "AAPL"],
...     start="2020-01-01",
...     end="2020-01-04"
... )
>>> data.close
symbol                         BTC-USD       AAPL
Date                                             
2020-01-01 00:00:00+00:00  7200.174316        NaN
2020-01-02 00:00:00+00:00  6985.470215        NaN
2020-01-02 05:00:00+00:00          NaN  73.249016
2020-01-03 00:00:00+00:00  7344.884277        NaN
2020-01-03 05:00:00+00:00          NaN  72.536888

>>> new_index = data.index.ceil("D").drop_duplicates()
>>> new_data = data.realign(new_index, ffill=False)
>>> new_data.close
symbol                         BTC-USD       AAPL
Date                                             
2020-01-01 00:00:00+00:00  7200.174316        NaN
2020-01-02 00:00:00+00:00  6985.470215        NaN
2020-01-03 00:00:00+00:00  7344.884277  73.249016
2020-01-04 00:00:00+00:00          NaN  72.536888
```

## Transforming

The main challenge in transforming any data is that each symbol must have the same index and columns,
since we need to concatenate them into one Pandas object later. Therefore, any transformation must
be applied to each symbol in the same way. To enforce this, the
[Data.transform](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.transform) method concatenates the data
across all symbols and features into a single large DataFrame and passes it to a UDF for
transformation. After the transformation, the method splits the result back into multiple smaller
Pandas objects, one per symbol, aligns them, creates a new data wrapper based on the aligned index
and columns, and finally initializes a new data instance.

Let's drop any row that contains at least one NaN:

```pycon
>>> full_yf_data = YFData.pull(["BTC-USD", "ETH-USD"])
>>> full_yf_data.close.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 3268 entries, 2014-09-17 00:00:00+00:00 to 2023-08-28 00:00:00+00:00
Freq: D
Data columns (total 2 columns):
  #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
  0   BTC-USD  3268 non-null   float64
  1   ETH-USD  2119 non-null   float64
dtypes: float64(2)
memory usage: 76.6 KB

>>> new_full_yf_data = full_yf_data.transform(lambda df: df.dropna())
>>> new_full_yf_data.close.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 2119 entries, 2017-11-09 00:00:00+00:00 to 2023-08-28 00:00:00+00:00
Freq: D
Data columns (total 2 columns):
  #   Column   Non-Null Count  Dtype  
---  ------   --------------  -----  
  0   BTC-USD  2119 non-null   float64
  1   ETH-USD  2119 non-null   float64
dtypes: float64(2)
memory usage: 49.7 KB
```

You can also choose to pass only one feature or one symbol at a time by setting `per_feature=True`
and `per_symbol=True`, respectively. By enabling both arguments at once, you can instruct VBT
to pass only one feature and symbol combination as a Pandas Series at a time.

## Analysis

Each data class is a subclass of [Analyzable](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#analyzing),
making it both analyzable and indexable.

### Indexing

You can use Pandas indexing on a data instance to select rows and columns across all fetched Pandas
objects. Supported operations include
[`iloc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html),
[`loc`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.loc.html),
[`xs`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.xs.html), and
[`[]`](https://pandas.pydata.org/docs/user_guide/indexing.html#basics):

```pycon
>>> sub_yf_data = yf_data.loc["2020-01-01":"2020-01-03"]  # (1)!
>>> sub_yf_data
<__main__.YFData at 0x7fa9a0012396>

>>> sub_yf_data = sub_yf_data[["Open", "High", "Low", "Close"]]  # (2)!
>>> sub_yf_data
<__main__.YFData at 0x7fa9a0032358>

>>> sub_yf_data.data["BTC-USD"]
                                  Open         High          Low        Close
Date                                                                         
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277

>>> sub_yf_data.data["ETH-USD"]
                                  Open        High         Low       Close
Date                                                                     
2020-01-01 00:00:00+00:00  129.630661  132.835358  129.198288  130.802002
2020-01-02 00:00:00+00:00  130.820038  130.820038  126.954910  127.410179
2020-01-03 00:00:00+00:00  127.411263  134.554016  126.490021  134.171707
```

1. Select rows (in `loc`, the second date is inclusive!). Returns a new data instance.

2. Select columns. Returns a new data instance.

!!! note
    Do not try to select symbols this way. This notation is reserved for rows and columns only.
    Use [Data.select](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.select) instead.

!!! info
    If the instance is feature-oriented, this method will apply to features rather than symbols.

### Stats and plots

As with every [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)
instance, you can compute and plot various properties of the data stored in the instance.

Very often, simply calling [DataFrame.info](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.info.html)
and [DataFrame.describe](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html)
on any stored Series or DataFrame is enough to print a concise summary:

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start=vbt.symbol_dict({
...         "BTC-USD": "2020-01-01", 
...         "ETH-USD": "2020-01-03"
...     }),
...     end=vbt.symbol_dict({
...         "BTC-USD": "2020-01-03", 
...         "ETH-USD": "2020-01-05"
...     })
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> yf_data.data["BTC-USD"].info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 5 entries, 2019-12-31 00:00:00+00:00 to 2020-01-04 00:00:00+00:00
Freq: D
Data columns (total 7 columns):
  #   Column        Non-Null Count  Dtype  
---  ------        --------------  -----  
  0   Open          3 non-null      float64
  1   High          3 non-null      float64
  2   Low           3 non-null      float64
  3   Close         3 non-null      float64
  4   Volume        3 non-null      float64
  5   Dividends     3 non-null      float64
  6   Stock Splits  3 non-null      float64
dtypes: float64(7)
memory usage: 320.0 bytes

>>> yf_data.data["BTC-USD"].describe()
              Open         High          Low        Close        Volume  \
count     3.000000     3.000000     3.000000     3.000000  3.000000e+00   
mean   7230.627441  7267.258626  7093.330729  7126.414551  2.017856e+10   
std      55.394933    62.577102   136.908963   122.105641  1.408740e+09   
min    7194.892090  7212.155273  6935.270020  6985.470215  1.856566e+10   
25%    7198.721680  7233.242920  7052.523926  7089.534668  1.968387e+10   
50%    7202.551270  7254.330566  7169.777832  7193.599121  2.080208e+10   
75%    7248.495117  7294.810303  7172.361084  7196.886719  2.098501e+10   
max    7294.438965  7335.290039  7174.944336  7200.174316  2.116795e+10   

        Dividends  Stock Splits  
count        3.0           3.0  
mean         0.0           0.0  
std          0.0           0.0  
min          0.0           0.0  
25%          0.0           0.0  
50%          0.0           0.0  
75%          0.0           0.0  
max          0.0           0.0  
```

But since any data instance can capture multiple symbols,
[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)
can also provide information about symbols:

```pycon
>>> yf_data.stats()
Start                   2019-12-31 00:00:00+00:00
End                     2020-01-04 00:00:00+00:00
Period                            5 days 00:00:00
Total Symbols                                   2
Null Counts: BTC-USD                           14
Null Counts: ETH-USD                           14
Name: agg_stats, dtype: object

>>> yf_data.stats(column="Volume")  # (1)!
Start                   2019-12-31 00:00:00+00:00
End                     2020-01-04 00:00:00+00:00
Period                            5 days 00:00:00
Total Symbols                                   2
Null Counts: BTC-USD                            2
Null Counts: ETH-USD                            2
Name: Volume, dtype: object
```

1. Print stats for the `Volume` column only.

To plot the data, use the [Data.plot](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.plot) method. This method
produces an OHLC(V) chart when the Pandas object is a DataFrame with standard price features and a line
chart otherwise. The OHLC(V) chart can plot only one symbol of data, while the line chart can plot only
one feature of data. You can specify both using the `symbol` and `feature` arguments, respectively.

Because different symbols often have different starting values, you can use the `base` argument to rebase
the time series so they start from the same point on the chart:

```pycon
>>> yf_data = YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-06-01"
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> yf_data.plot(column="Close", base=100).show()  # (1)!
```

1. Since our data is symbol-oriented, `column` here is an alias for `feature`.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plot_base.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plot_base.dark.svg#only-dark){: .iimg loading=lazy }

!!! info
    This works only for line traces, since we cannot plot multiple OHLC(V) traces on the same chart.

As with most things, you can achieve the same result by chaining simple commands:

```pycon
>>> yf_data["Close"].get().vbt.rebase(100).vbt.plot()  # (1)!
```

1. This uses [GenericAccessor.rebase](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rebase)
and [GenericAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.plot).

Additionally, [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) can display a subplot for each symbol using
[PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots),
which internally uses [Data.plot](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.plot):

```pycon
>>> yf_data.plots().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots.dark.svg#only-dark){: .iimg loading=lazy }

By specifying a column, you can plot one feature per symbol of data:

```pycon
>>> yf_data.plots(column="Close").show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots_column.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots_column.dark.svg#only-dark){: .iimg loading=lazy }

You can select one or more symbols by passing them through the `template_context` dictionary:

```pycon
>>> yf_data.plots(
...     column="Close", 
...     template_context=dict(symbols=["BTC-USD"])
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots_symbol.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots_symbol.dark.svg#only-dark){: .iimg loading=lazy }

If you review the [Data.subplots](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.subplots) config, you will
see that only one subplot is defined as a template. During the resolution phase, the template is evaluated
and the subplot is expanded into multiple subplots—one for each symbol—each with the name `plot`,
prefixed by its index in the expansion. For illustration, let's change the colors of both lines and plot
their moving averages:

```pycon
>>> from vectorbtpro.utils.colors import adjust_opacity

>>> fig = yf_data.plots(
...     column="Close",
...     subplot_settings=dict(
...         plot_0=dict(trace_kwargs=dict(line_color="mediumslateblue")),
...         plot_1=dict(trace_kwargs=dict(line_color="limegreen"))
...     )
... )
>>> sma = vbt.talib("SMA").run(yf_data.close, vbt.Default(10))  # (1)!
>>> sma["BTC-USD"].real.rename("SMA(10)").vbt.plot(
...     trace_kwargs=dict(line_color=adjust_opacity("mediumslateblue", 0.7)),
...     add_trace_kwargs=dict(row=1, col=1),  # (2)!
...     fig=fig
... )
>>> sma["ETH-USD"].real.rename("SMA(10)").vbt.plot(
...     trace_kwargs=dict(line_color=adjust_opacity("limegreen", 0.7)),
...     add_trace_kwargs=dict(row=2, col=1),
...     fig=fig
... )
>>> fig.show()
```

1. Hide the `timeperiod` parameter from the column hierarchy by wrapping it with
[Default](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Default).

2. Specify which subplot to plot this SMA on.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots_colors.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/plots_colors.dark.svg#only-dark){: .iimg loading=lazy }

If you are up for a challenge, try subclassing the `YFData` class and overriding the
[Data.plot](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.plot) method so that it also computes and
plots the SMA over the time series. This would make the plotting process very flexible, as you could
display the SMA for every feature and symbol without worrying about the subplot's position or other details.

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/data/index.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/data/local/
parent: https://vectorbt.pro/pvt_41531c19/documentation/data/
name: Local
type: page
icon: material-folder-open
---

# Local

Making repeated requests to remote API endpoints can be costly, so caching data locally is important.
Fortunately, VBT provides several methods for managing local data.

## Pickling

Like any class that subclasses [Pickleable](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable),
you can save any [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instance to disk using
[Pickleable.save](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.save), and load it again with
[Pickleable.load](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.load). This process pickles the
entire Python object, including stored Pandas objects, symbol dictionaries, and settings:

```pycon
>>> from vectorbtpro import *

>>> yf_data = vbt.YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> yf_data.save("yf_data")  # (1)!

>>> yf_data = vbt.YFData.load("yf_data")  # (2)!
>>> yf_data = yf_data.update(end="2020-01-06")
>>> yf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
2020-01-05 00:00:00+00:00  7411.317383  136.276779
```

1. The `.pickle` extension is added to the file name automatically.

2. You can load the object back in a new runtime or even on another machine.
Just make sure you are using a compatible VBT version.

!!! important
    The class definition is not saved. If a new version of VBT introduces a breaking change
    to the [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) constructor, the object might not load.
    In this case, you can manually create a new instance:

    ```pycon
    >>> yf_data = vbt.YFData(**vbt.Configured.load("yf_data").config)
    ```

## Saving

While pickling is a fast and convenient way to store Python objects of any size, the pickled file is
essentially a black box that requires a Python interpreter to access its contents. This means it cannot
be imported by most other data-driven tools, which makes it unusable for many tasks. To overcome this
limitation, the [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class allows you to save only the
stored Pandas objects into one or more tabular format files.

### CSV

The first supported file format is [CSV](https://en.wikipedia.org/wiki/Comma-separated_values),
which is implemented by the instance method [Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv).
This method takes the path to the directory where the data should be stored (`path_or_buf`) and saves each
symbol in a separate file using [DataFrame.to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html).

By default, it appends the `.csv` extension to each symbol and saves the files in the current directory:

```pycon
>>> yf_data.to_csv()
```

!!! info
    Multiple symbols cannot be stored in a single CSV file.

You can list all CSV files in the current directory using [list_files](https://vectorbt.pro/pvt_41531c19/api/utils/path_/#vectorbtpro.utils.path_.list_files):

```pycon
>>> vbt.list_files("*.csv")
[PosixPath('ETH-USD.csv'), PosixPath('BTC-USD.csv')]
```

A cleaner approach is to save all the data in a separate directory:

```pycon
>>> vbt.remove_file("BTC-USD.csv")  # (1)!
>>> vbt.remove_file("ETH-USD.csv")

>>> yf_data.to_csv("data", mkdir_kwargs=dict(mkdir=True))  # (2)!
```

1. Delete the previously created CSV files from the current directory.

2. Save the files to the directory named `data`. If the directory does not exist, create a new one by
passing the keyword argument `mkdir_kwargs` to [check_mkdir](https://vectorbt.pro/pvt_41531c19/api/utils/path_/#vectorbtpro.utils.path_.check_mkdir).

To save the data as tab-separated values (TSV):

```pycon
>>> yf_data.to_csv("data", ext="tsv")

>>> vbt.list_files("data/*.tsv")
[PosixPath('data/BTC-USD.tsv'), PosixPath('data/ETH-USD.tsv')]
```

!!! tip
    You do not need to pass `sep`: VBT will recognize the extension and use the correct delimiter.
    You can still override this argument if you want to split the data by a custom character.

Similar to [Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull), you can provide
any argument as a feature/symbol dictionary of type [key_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.key_dict)
to define different rules for different symbols. For example, you can store the symbols from our example
in separate directories:

```pycon
>>> yf_data.to_csv(
...     vbt.key_dict({
...         "BTC-USD": "btc_data",
...         "ETH-USD": "eth_data"
...     }),
...     mkdir_kwargs=dict(mkdir=True)
... )
```

You can also specify the path to each file by using `path_or_buf` (the first argument):

```pycon
>>> yf_data.to_csv(
...     vbt.key_dict({
...         "BTC-USD": "data/btc_usd.csv",
...         "ETH-USD": "data/eth_usd.csv"
...     }),
...     mkdir_kwargs=dict(mkdir=True)
... )
```

To delete an entire directory, for example as part of a cleanup process:

```pycon
>>> vbt.remove_dir("btc_data", with_contents=True)
>>> vbt.remove_dir("eth_data", with_contents=True)
>>> vbt.remove_dir("data", with_contents=True)
```

### HDF

The second supported file format is [HDF](https://en.wikipedia.org/wiki/Hierarchical_Data_Format),
which is implemented by the instance method [Data.to_hdf](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_hdf).
Unlike [Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv), this method can store
multiple symbols in a single file, where symbols are distributed as HDF keys.

By default, it creates a new file with the same name as the data class and the `.h5` extension,
saving each symbol under a separate key using [DataFrame.to_hdf](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_hdf.html):

```pycon
>>> yf_data.to_hdf()

>>> vbt.list_files("*.h5")
[PosixPath('YFData.h5')]
```

To see the list of all groups and keys in an HDF file:

```pycon
>>> with pd.HDFStore("YFData.h5") as store:
...     print(store.keys())
['/BTC-USD', '/ETH-USD']
```

Use the `key` argument to manually specify the key for a particular symbol:

```pycon
>>> yf_data.to_hdf(
...     key=vbt.key_dict({
...         "BTC-USD": "btc_usd",
...         "ETH-USD": "eth_usd"
...     })
... )
```

!!! tip
    If there is only one symbol, you do not need to use [key_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.key_dict).
    You can simply pass `key="btc_usd"`.

You can also specify the path to each file by using `path_or_buf` (the first argument):

```pycon
>>> yf_data.to_hdf(
...     vbt.key_dict({
...         "BTC-USD": "btc_usd.h5",
...         "ETH-USD": "eth_usd.h5"
...     })
... )
```

You can combine the arguments `path_or_buf` and `key`.

All other arguments behave the same as for [Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv).

### Feather & Parquet

The third supported option is saving to a Feather or Parquet file, which are handled by
the instance methods [Data.to_feather](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_feather) and
[Data.to_parquet](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_parquet). Feather is
an unmodified raw columnar Arrow memory format designed for short-term storage. Parquet
is often more expensive to write but offers more layers of encoding and compression, making Parquet
files usually much smaller than Feather files. Another key difference is that you cannot
partition data with Feather, nor can you natively store the index. The index must be stored as a separate
column, which is handled automatically by VBT. Here we will show how to save to Parquet.

By default, the `.parquet` extension is appended to each symbol, and files are saved in the current directory:

```pycon
>>> yf_data.to_parquet()
```

!!! info
    Multiple symbols cannot be stored in a single Parquet file.

Other saving options are very similar to the CSV saving options.

In addition to saving each DataFrame to a separate Parquet file, you can partition the DataFrame
either by columns or by rows. Partitioning by columns is controlled by the `partition_cols` argument,
which takes a list of column names. The columns must be present in your data and are partitioned in the
order you provide. Partitioning by rows is controlled by the `partition_by` argument, which
accepts a grouping instruction such as a frequency, an index (including multi-index), a Pandas grouper or resampler,
or a VBT [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper) instance.
Together with `groupby_kwargs`, this creates a new `Grouper` instance for partitioning.
The groups are attached as columns to each DataFrame, and the names of these columns are provided as
`partition_cols`. By default, if there is only one column, it will be named "group";
if there are multiple columns, they will be named "group_{index}". To use your own column names,
enable `keep_groupby_names`.

!!! info
    When `partition_cols` or `partition_by` is provided, each symbol will be stored in a separate directory.

Let's partition our data into two-day groups and save each DataFrame to its own Parquet directory:

```pycon
>>> yf_data.to_parquet(partition_by="2 days")
```

To visualize the directory tree for the `BTC-USD` symbol:

```pycon
>>> vbt.print_dir_tree("BTC-USD")
BTC-USD
├── group=2020-01-01%2000%3A00%3A00.000000000
│   └── 190335d948d04504b42a8d35ffb08f47-0.parquet
└── group=2020-01-03%2000%3A00%3A00.000000000
    └── 190335d948d04504b42a8d35ffb08f47-0.parquet

2 directories, 2 files
```

### SQL

The fourth supported option is saving to a SQL database, which is implemented by the instance method
[Data.to_sql](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_sql). This requires
[SQLAlchemy](https://www.sqlalchemy.org/) to be installed. The method takes the engine (object, name,
or URL) and saves each symbol as a separate table in the database managed by the engine using
[`pd.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).
You can create the engine manually using SQLAlchemy's `create_engine` function, or pass a URL and
it will be created for you; in this case, the engine will be disposed at the end of the call.

Let's create a SQL database file in the working directory and store the data there:

```pycon
>>> SQLITE_URL = "sqlite:///yf_data.db"

>>> yf_data.to_sql(SQLITE_URL)
```

If you want to continue working with the same engine and avoid creating another engine object,
pass `return_engine=True` to return the engine object:

```pycon
>>> engine = yf_data.to_sql(SQLITE_URL, if_exists="replace", return_engine=True)
```

You can also specify the schema using the `schema` argument. Note that some databases,
such as SQLite, do not support the concept of a schema. If the schema does not exist, it will be
created automatically.

```pycon
>>> POSTGRESQL_URL = "postgresql://postgres:postgres@localhost:5432"

>>> yf_data.to_sql(POSTGRESQL_URL, schema="yf_data")
```

Use the `table` argument to manually specify the table name for a particular symbol:

```pycon
>>> yf_data.to_sql(
...     POSTGRESQL_URL,
...     table=vbt.key_dict({
...         "BTC-USD": "BTC_USD",
...         "ETH-USD": "ETH_USD"
...     })
... )
```

!!! info
    If the index is datetime-like and/or there are datetime-like columns, the method will localize or convert
    them to UTC first. To adjust this behavior, set the `to_utc` argument to False to deactivate,
    to "index" to apply it to the index only, or to "columns" to apply it to columns only.
    Additionally, the UTC timezone will be removed if `remove_utc_tz` is True (default) since some
    databases do not support timezone-aware timestamps; other timezones are not affected.

### DuckDB

!!! tip
    The previous method (using SQLAlchemy) can also be used to write to a DuckDB database.
    For this, you need to install the [duckdb-engine](https://pypi.org/project/duckdb-engine/) extension.

The fifth supported option is very similar to the previous one and allows you to save data to a DuckDB
database or to CSV, Parquet, or JSON files. It is implemented by the instance method
[Data.to_duckdb](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_duckdb). This requires
[DuckDB](https://duckdb.org/) to be installed. The method takes the connection (object or URL)
and saves each symbol as a separate table in the database managed by the connection, or to file(s)
using a SQL query. The connection can be created manually using DuckDB's `connect` function,
or you can pass a URL and it will be created for you. If neither a connection nor connection-related
keyword arguments are provided, the default in-memory connection is used.

Let's create a DuckDB database file in the working directory and store the data there:

```pycon
>>> DUCKDB_URL = "database.duckdb"

>>> yf_data.to_duckdb(DUCKDB_URL)
```

You can also specify the catalog and schema using the `catalog` and `schema` arguments, respectively.
If the schema does not exist, it will be created automatically.

```pycon
>>> yf_data.to_duckdb(DUCKDB_URL, schema="yf_data")
```

Use the `table` argument to manually specify the table name for a particular symbol:

```pycon
>>> yf_data.to_duckdb(
...     DUCKDB_URL,
...     table=vbt.key_dict({
...         "BTC-USD": "BTC_USD",
...         "ETH-USD": "ETH_USD"
...     })
... )
```

There is also an option to save each DataFrame to a CSV, Parquet, or JSON file rather than to
the database itself. To do this, use `write_format`, `write_path`, and `write_options`.
This operation is performed using [`COPY TO`](https://duckdb.org/docs/sql/statements/copy.html).
If `write_path` is a directory (which is the working directory by default), each DataFrame
will be saved to a file based on the specified format. The format is not needed if `write_path` points
to a file with a recognizable extension. The options argument can be used to specify
writing options; it can be either a string as in DuckDB's documentation, such as
`HEADER 1, DELIMITER ','`, or a dictionary that will be translated into such a string by
VBT, such as `dict(header=1, delimiter=',')`.

Let's save all data to Parquet files:

```pycon
>>> yf_data.to_duckdb(
...     DUCKDB_URL,
...     write_path="data",
...     write_format="parquet",
...     mkdir_kwargs=dict(mkdir=True)
... )
```

!!! info
    See the notes in the SQL section.

## Loading

To import any previously stored data in a tabular format, you can use Pandas or VBT's preset
data classes, which are specifically designed for this purpose.

### CSV

You can import each CSV dataset manually using [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html):

```pycon
>>> yf_data.to_csv()

>>> pd.read_csv("BTC-USD.csv", index_col=0, parse_dates=True)
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0 
```

To combine the imported datasets and wrap them with [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data),
you can use [Data.from_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data):

```pycon
>>> btc_usd = pd.read_csv("BTC-USD.csv", index_col=0, parse_dates=True)
>>> eth_usd = pd.read_csv("ETH-USD.csv", index_col=0, parse_dates=True)

>>> data = vbt.Data.from_data({"BTC-USD": btc_usd, "ETH-USD": eth_usd})
>>> data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

To make it easier for users and eliminate the need to manually search for, fetch, and merge CSV data,
VBT provides the [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) class,
which can recursively search directories for CSV files, resolve path expressions using
[glob](https://docs.python.org/3/library/glob.html), translate matched paths into symbols,
and import and join tabular data—all automatically with a single command.
It is a subclass of [FileData](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData),
which enables all of these features.

At the core of the path matching functionality is the class method
[FileData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull), which iterates
over the specified paths. For each one, it finds the matching absolute paths using the class method
[FileData.match_path](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.match_path), then calls
the abstract class method [FileData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.fetch_key)
to pull the data from the file at that path.

To see how [FileData.match_path](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.match_path)
works, create a directory named `data` and store several empty files in it:

```pycon
>>> vbt.make_dir("data", exist_ok=True)
>>> vbt.make_file("data/file1.csv")
>>> vbt.make_file("data/file2.tsv")
>>> vbt.make_file("data/file3")

>>> vbt.make_dir("data/sub-data", exist_ok=True)
>>> vbt.make_file("data/sub-data/file1.csv")
>>> vbt.make_file("data/sub-data/file2.tsv")
>>> vbt.make_file("data/sub-data/file3")
```

To view the directory structure you just created:

```pycon
>>> vbt.print_dir_tree("data")
data
├── file1.csv
├── file2.tsv
├── file3
└── sub-data
    ├── file1.csv
    ├── file2.tsv
    └── file3

1 directories, 6 files
```

Match all files in a directory:

```pycon
>>> vbt.FileData.match_path("data")
[PosixPath("data/file1.csv"),
  PosixPath("data/file2.tsv"),
  PosixPath("data/file3")]
```

Match all CSV files in a directory:

```pycon
>>> vbt.FileData.match_path("data/*.csv")
[PosixPath("data/file1.csv")]
```

Match all CSV files in a directory recursively:

```pycon
>>> vbt.FileData.match_path("data/**/*.csv")
[PosixPath("data/file1.csv"), PosixPath("data/sub-data/file1.csv")]
```

For more details, see the [glob](https://docs.python.org/3/library/glob.html) documentation.

Returning to [FileData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull): this method
can match one or multiple path expressions as shown above, provided either as `symbols` (if `paths` is None)
or as `paths`. When you pass paths as symbols, the method calls
[FileData.path_to_symbol](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.path_to_symbol)
on each matched path to extract the symbol name (by default, this is the stem of the path):

```pycon
>>> vbt.CSVData.pull("BTC-USD.csv").symbols
["BTC-USD"]

>>> vbt.CSVData.pull(["BTC-USD.csv", "ETH-USD.csv"]).symbols
["BTC-USD", "ETH-USD"]

>>> vbt.CSVData.pull("*.csv").symbols
["BTC-USD", "ETH-USD"]

>>> vbt.CSVData.pull(["BTC/USD", "ETH/USD"], paths="*.csv").symbols  # (1)!
["BTC/USD", "ETH/USD"]

>>> vbt.CSVData.pull(  # (2)!
...     ["BTC/USD", "ETH/USD"], 
...     paths=["BTC-USD.csv", "ETH-USD.csv"]
... ).symbols
["BTC/USD", "ETH/USD"]
```

1. Explicitly specify the symbols.

2. Explicitly specify both the symbols and the paths.

!!! note
    Remember to filter by the `.csv`, `.tsv`, or any other extension in your path expression.

When you use a wildcard like `*.csv`, VBT will sort the matched paths (for each path expression).
To disable sorting, set `sort_paths` to False. If you want to turn off the path matching mechanism entirely,
you can set `match_paths` to False, which will send all arguments directly to
[CSVData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.fetch_key):

```pycon
>>> vbt.CSVData.pull(
...     ["BTC/USD", "ETH/USD"], 
...     paths=vbt.key_dict({
...         "BTC/USD": "BTC-USD.csv",
...         "ETH/USD": "ETH-USD.csv"
...     }),
...     match_paths=False
... ).symbols
["BTC/USD", "ETH/USD"]
```

!!! tip
    Instead of paths, you can use any object type supported by the `filepath_or_buffer` argument in
    [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

To summarize the techniques described above, let's create an empty directory called `data` again,
write the `BTC-USD` symbol to a CSV file and the `ETH-USD` symbol to a TSV file, then load both
datasets with a single `fetch` call:

```pycon
>>> vbt.remove_dir("data", with_contents=True)

>>> yf_data.to_csv(
...     "data",
...     ext=vbt.key_dict({
...         "BTC-USD": "csv",
...         "ETH-USD": "tsv"
...     }), 
...     mkdir_kwargs=dict(mkdir=True)
... )

>>> csv_data = vbt.CSVData.pull(["data/*.csv", "data/*.tsv"])  # (1)!
```

1. The delimiter is automatically detected based on the file's extension.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> csv_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

!!! note
    Providing two paths with wildcards (`*`) does not guarantee you will get exactly two symbols:
    each wildcard may match more than one path. Think of the two expressions above as being OR'ed
    into a single expression `data/*.{csv,tsv}` (which, unfortunately, is not supported by
    [glob](https://docs.python.org/3/library/glob.html)).

Last but not least is regex matching with `match_regex`. This tells VBT to iterate over all
matched paths and further validate them using a regular expression:

```pycon
>>> vbt.CSVData.pull(
...     "data/**/*",  # (1)!
...     match_regex=r"^.*\.(csv|tsv)$"  # (2)!
... ).close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

1. Recursively collect the paths of all files in any subdirectory of `data`.

2. Filter out any paths that do not end with `.csv` or `.tsv`.

Any other argument is passed directly to
[CSVData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.fetch_key)
and then to [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).

#### Chunking

Instead of reading everything into memory at once, Pandas allows you to read data in chunks.
For CSV files, this means you can load only a subset of lines into memory at any given time.
While this is a great feature for working with large datasets, chunking does not have many benefits
when your goal is simply to load all the data into memory anyway.

However, chunking becomes very useful for data filtering! The class
[CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData), as well as the function
[pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) that it uses,
do not have arguments for skipping rows based on their content—only by row index.
For example, to skip any data before `2020-01-03`, you would need to load all the data into memory first.
But if the dataset is too large, this could exhaust your system's RAM. To work around this,
you can split the data into chunks and check conditions on each chunk separately.

You have two options:

1. Use `chunksize` to divide data into chunks of fixed length.

2. Use `iterator` to return an iterator so you can read chunks of variable length.

Both options make [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
to return an iterator of type `TextFileReader`. To take advantage of this,
[CSVData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.fetch_key) accepts a
user-defined function `chunk_func` that should 1) accept the iterator, 2) select, process, and
concatenate chunks, and 3) return a Series or DataFrame.

Let's fetch only the rows whose date ends with an even day:

```pycon
>>> csv_data = vbt.CSVData.pull(
...     ["data/*.csv", "data/*.tsv"],
...     chunksize=1,  # (1)!
...     chunk_func=lambda iterator: pd.concat([
...         df 
...         for df in iterator
...         if (df.index.day % 2 == 0).all()
...     ], axis=0)
... )
```

1. Each chunk will be a DataFrame with one row.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> csv_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

!!! note
    Chunking is most useful when memory usage is a greater concern than speed.

### HDF

Each HDF dataset can be imported manually using [pandas.read_hdf](https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html):

```pycon
>>> yf_data.to_hdf()

>>> pd.read_hdf("YFData.h5", key="BTC-USD")
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

Just like [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) for CSV data,
VBT provides a preset class [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData)
specifically for reading HDF files. It shares the same parent class
[FileData](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData)
and uses its fetcher [FileData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull).
Unlike CSV datasets, where each file stores one dataset, HDF datasets are stored by key within a single HDF file.
Since groups and keys in HDF files follow a [POSIX](https://en.wikipedia.org/wiki/POSIX)-style
hierarchy with `/` separators, you can query them just like you would query directories and files
in a typical file system.

Let's demonstrate this by using [HDFData.match_path](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData.match_path),
which extends [FileData.match_path](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.match_path)
with proper discovery and handling of HDF groups and keys:

```pycon
>>> vbt.HDFData.match_path("YFData.h5")
[PosixPath("YFData.h5/BTC-USD"), PosixPath("YFData.h5/ETH-USD")]
```

As you can see, the HDF file is now treated as a directory, while groups and keys are treated
as subdirectories and files, respectively. This makes importing HDF datasets just as easy as importing CSV datasets:

```pycon
>>> vbt.HDFData.pull("YFData.h5/BTC-USD").symbols  # (1)!
["BTC-USD"]

>>> vbt.HDFData.pull("YFData.h5").symbols  # (2)!
["BTC-USD", "ETH-USD"]

>>> vbt.HDFData.pull("YFData.h5/BTC*").symbols  # (3)!
["BTC-USD"]

>>> vbt.HDFData.pull("*.h5/BTC-*").symbols  # (4)!
["BTC-USD"]
```

1. Matches the key `BTC-USD` in `YFData.h5`.

2. Matches all keys in `YFData.h5`.

3. Matches all keys starting with `BTC` in `YFData.h5`.

4. Matches all keys starting with `BTC` in all HDF files.

Any other argument works the same as for [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData),
but now is passed directly to [HDFData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData.fetch_key)
and then to [pandas.read_hdf](https://pandas.pydata.org/docs/reference/api/pandas.read_hdf.html).

#### Chunking

Chunking for HDF files works just like it does for CSV files, with two exceptions: the data must be
saved as a [PyTables](https://www.pytables.org/) Table structure by using `format="table"`, and
the iterator will be of type `TableIterator` instead of `TextFileReader`.

```pycon
>>> yf_data.to_hdf(format="table")

>>> hdf_data = vbt.HDFData.pull(
...     "YFData.h5",
...     chunksize=1,
...     chunk_func=lambda iterator: pd.concat([
...         df 
...         for df in iterator
...         if (df.index.day % 2 == 0).all()
...     ], axis=0)
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> hdf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

### Feather & Parquet

Each Parquet dataset can be manually imported using
[pandas.read_parquet](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html):

```pycon
>>> yf_data.to_parquet()

>>> pd.read_parquet("BTC-USD.parquet")
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0  
```

The same applies for partitioned datasets:

```pycon
>>> yf_data2 = vbt.YFData.pull(
...     ["BNB-USD", "XRP-USD"], 
...     start="2020-01-01", 
...     end="2020-01-05"
... )
>>> yf_data2.to_parquet(partition_by="2D")

>>> pd.read_parquet("BNB-USD")  # (1)!
                                Open       High        Low      Close  \
Date                                                                    
2020-01-01 00:00:00+00:00  13.730962  13.873946  13.654942  13.689083   
2020-01-02 00:00:00+00:00  13.698126  13.715548  12.989974  13.027011   
2020-01-03 00:00:00+00:00  13.035329  13.763709  13.012638  13.660452   
2020-01-04 00:00:00+00:00  13.667442  13.921914  13.560008  13.891512   

                              Volume  Dividends  Stock Splits  \
Date                                                            
2020-01-01 00:00:00+00:00  172980718        0.0           0.0   
2020-01-02 00:00:00+00:00  156376427        0.0           0.0   
2020-01-03 00:00:00+00:00  173683857        0.0           0.0   
2020-01-04 00:00:00+00:00  182230374        0.0           0.0   

                                                    group  
Date                                                      
2020-01-01 00:00:00+00:00  2020-01-01 00:00:00.000000000  
2020-01-02 00:00:00+00:00  2020-01-01 00:00:00.000000000  
2020-01-03 00:00:00+00:00  2020-01-03 00:00:00.000000000  
2020-01-04 00:00:00+00:00  2020-01-03 00:00:00.000000000  
```

1. The path points to a directory.

Just like with other classes for loading data from local files, VBT provides preset classes
[FeatherData](https://vectorbt.pro/pvt_41531c19/api/data/custom/feather/#vectorbtpro.data.custom.feather.FeatherData)
and [ParquetData](https://vectorbt.pro/pvt_41531c19/api/data/custom/parquet/#vectorbtpro.data.custom.parquet.ParquetData) for reading
Feather and Parquet files, respectively. They share the same parent class
[FileData](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData) and use its fetcher
[FileData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull).

First, let's discover any Parquet files or directories stored in the current working directory using
[ParquetData.list_paths](https://vectorbt.pro/pvt_41531c19/api/data/custom/parquet/#vectorbtpro.data.custom.parquet.ParquetData.list_paths).
This function searches for files with the ".parquet" extension as well as directories containing
partitioned datasets that follow the [Hive partitioning scheme](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.HivePartitioning.html).

```pycon
>>> vbt.ParquetData.list_paths()
[PosixPath('BNB-USD'),
  PosixPath('BTC-USD.parquet'),
  PosixPath('ETH-USD.parquet'),
  PosixPath('XRP-USD')]
  
>>> vbt.ParquetData.is_parquet_file("BTC-USD.parquet")
True

>>> vbt.ParquetData.is_parquet_dir("BNB-USD")
True
```

This makes importing Parquet datasets just as easy as importing any other file-based dataset:

```pycon
>>> vbt.ParquetData.pull("BTC-USD.parquet").symbols  # (1)!
['BTC-USD']

>>> vbt.ParquetData.pull(["BTC-USD.parquet", "ETH-USD.parquet"]).symbols
['BTC-USD', 'ETH-USD']

>>> vbt.ParquetData.pull("*.parquet").symbols  # (2)!
['BTC-USD', 'ETH-USD']

>>> vbt.ParquetData.pull("BNB-USD").symbols  # (3)!
['BNB-USD']

>>> vbt.ParquetData.pull(["BNB-USD", "XRP-USD"]).symbols
['BNB-USD', 'XRP-USD']

>>> vbt.ParquetData.pull().symbols  # (4)!
['BNB-USD', 'BTC-USD', 'ETH-USD', 'XRP-USD']
```

1. Pull a single-file dataset.

2. Pull all single-file datasets.

3. Pull a partitioned dataset.

4. Pull all single-file and partitioned datasets.

You might be wondering what happens to the "group" column in partitioned datasets.
Whenever a partitioned dataset is pulled and VBT detects one or more partition groups named
"group" or "group_{index}", these columns are automatically ignored because they were
most likely generated by user-defined row partitioning with `partition_by`. Such groups are
referred to as default groups.

```pycon
>>> vbt.ParquetData.list_partition_cols("BNB-USD")
['group']

>>> vbt.ParquetData.is_default_partition_col("group")
True

>>> vbt.ParquetData.pull("BNB-USD").features
['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']
```

You can disable this behavior by setting `keep_partition_cols` to True:

```pycon
>>> vbt.ParquetData.pull("BNB-USD", keep_partition_cols=True).features
['Open',
  'High',
  'Low',
  'Close',
  'Volume',
  'Dividends',
  'Stock Splits',
  'group']
```

### SQL

Each SQL table can be manually imported using [pandas.read_sql_table](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_table.html):

```pycon
>>> pd.read_sql_table(
...     "BTC-USD", 
...     POSTGRESQL_URL, 
...     schema="yf_data",
...     index_col="Date", 
...     parse_dates={"Date": dict(utc=True)}
... )
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0 
```

You can also execute any query using [pandas.read_sql_query](https://pandas.pydata.org/docs/reference/api/pandas.read_sql_query.html):

```pycon
>>> pd.read_sql_query(
...     'SELECT * FROM yf_data."BTC-USD"', 
...     POSTGRESQL_URL, 
...     index_col="Date", 
...     parse_dates={"Date": dict(utc=True)}
... )
                                  Open         High          Low        Close  \
Date                                                                            
2020-01-01 00:00:00+00:00  7194.892090  7254.330566  7174.944336  7200.174316   
2020-01-02 00:00:00+00:00  7202.551270  7212.155273  6935.270020  6985.470215   
2020-01-03 00:00:00+00:00  6984.428711  7413.715332  6914.996094  7344.884277   
2020-01-04 00:00:00+00:00  7345.375488  7427.385742  7309.514160  7410.656738   

                                Volume  Dividends  Stock Splits  
Date                                                             
2020-01-01 00:00:00+00:00  18565664997        0.0           0.0  
2020-01-02 00:00:00+00:00  20802083465        0.0           0.0  
2020-01-03 00:00:00+00:00  28111481032        0.0           0.0  
2020-01-04 00:00:00+00:00  18444271275        0.0           0.0 
```

But you might be asking: how do we know what schemas and tables are stored in our database?
We can call [SQLData.list_schemas](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.list_schemas)
and [SQLData.list_tables](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.list_tables), respectively.
Most methods in [SQLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData), including these two,
require that the engine be provided.

```pycon
>>> vbt.SQLData.list_schemas(engine=POSTGRESQL_URL)
['information_schema', 'public', 'yf_data']

>>> vbt.SQLData.list_tables(engine=POSTGRESQL_URL)
['information_schema:_pg_foreign_data_wrappers',
  'information_schema:_pg_foreign_servers',
  ...
  'yf_data:BTC-USD',
  'yf_data:ETH-USD']

>>> vbt.SQLData.list_tables(engine=POSTGRESQL_URL, schema="yf_data")
['BTC-USD', 'ETH-USD']
```

To fetch the actual data, you can use [SQLData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.fetch_key),
which calls `pd.read_sql_query` and performs many pre-processing and post-processing steps
under the hood. For example, it can inspect the database and map column indices to names,
which is not natively supported by the Pandas method. This is useful when specifying information
per column based on position, such as when providing `index_col` (which, by the way,
defaults to 0 - the first column). It also properly handles datetime indexes and columns,
and can automatically retrieve all tables under a schema or even from the entire database.

Let's pull all the tables we stored previously, both implicitly and explicitly:

```pycon
>>> vbt.SQLData.pull(engine=SQLITE_URL).symbols   # (1)!
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(["BTC-USD", "ETH-USD"], engine=SQLITE_URL).symbols  # (2)!
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(engine=POSTGRESQL_URL, schema="yf_data").symbols  # (3)!
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(["yf_data:BTC-USD", "yf_data:ETH-USD"], engine=POSTGRESQL_URL).symbols  # (4)!
['yf_data:BTC-USD', 'yf_data:ETH-USD']
```

1. Pull all tables from the database.

2. Pull specific tables from the database.

3. Pull all tables under a schema from the database.

4. Pull tables from the database, specifying the schema for each.

You can also specify information per feature or symbol by providing it as an instance of
[key_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.key_dict):

```pycon
>>> vbt.SQLData.pull(
...     ["BTCUSD", "ETHUSD"], 
...     schema="yf_data",
...     table=vbt.key_dict({
...         "BTCUSD": "BTC-USD",
...         "ETHUSD": "ETH-USD",
...     }),
...     engine=POSTGRESQL_URL
... ).symbols
['BTCUSD', 'ETHUSD']
```

This is especially useful for executing custom queries:

```pycon
>>> vbt.SQLData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": 'SELECT * FROM yf_data."BTC-USD"',
...         "ETH-USD": 'SELECT * FROM yf_data."ETH-USD"',
...     }), 
...     index_col="Date",
...     engine=POSTGRESQL_URL
... ).symbols
['BTC-USD', 'ETH-USD']
```

!!! note
    When executing a custom query, most preprocessings are not available because the 
    query cannot be easily introspected. For example, you must provide a column name under `index_col`
    (or False to avoid using any column as an index).

Because different engines have different configurations, and you may not want to repeatedly
pass them when pulling, you can save the respective configuration to the global settings.
First, create an engine name, and then save all the keyword arguments you would normally
pass to [SQLData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.fetch_key)
under this engine name in `vbt.settings.data.engines`. This is easily accomplished using
[SQLData.set_engine_settings](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.set_engine_settings),
which takes an engine name and the keyword arguments to save. If the engine name is new,
make sure to set `populate_` to True.

```pycon
>>> vbt.SQLData.set_engine_settings(
...     engine_name="sqlite",
...     populate_=True,
...     engine=SQLITE_URL
... )

>>> vbt.SQLData.set_engine_settings(
...     engine_name="postgresql",
...     populate_=True,
...     engine=POSTGRESQL_URL,
...     schema="yf_data"
... )
```

If any argument is None during fetching, it will be taken from these settings first.
You only need to provide an engine name as `engine` or `engine_name`:

```pycon
>>> vbt.SQLData.pull(engine_name="sqlite").symbols
['BTC-USD', 'ETH-USD']

>>> vbt.SQLData.pull(engine_name="postgresql").symbols
['BTC-USD', 'ETH-USD']
```

You can also save specific arguments you want to use under each engine name.
For example, let's define the default engine name:

```pycon
>>> vbt.SQLData.set_custom_settings(engine_name="postgresql")

>>> vbt.SQLData.pull().symbols
['BTC-USD', 'ETH-USD']
```

To fetch specific columns, use the `columns` argument:

```pycon
>>> vbt.SQLData.pull(columns=["High", "Low"]).features
['High', 'Low']
```

!!! tip
    If you want to pull a single column and keep it as a DataFrame, set `squeeze` to False.

Unlike the Pandas method, you can also filter by any start and end condition.
When `align_dates` is True (default), and `start` and/or `end` is provided,
[SQLData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.fetch_key)
will fetch just one row of data first. It will check whether the index of this row
is datetime-like, and if so, will treat the provided `start` and/or `end` as datetime-like
objects. This means converting them to `datetime` objects and then localizing or converting them
to the timezone of the index.

!!! note
    If you used [Data.to_sql](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_sql) to save the data,
    be sure to use the same `to_utc` option when pulling as you did when saving.

```pycon
>>> vbt.SQLData.pull(start="2020-01-03").close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Most databases either do not support timezones or store data in UTC, so the default
behavior is to localize any timezone-naive datetime to UTC. The user is then responsible
for providing the correct timezone. If a timezone is provided via `tz` and the provided
datetime is timezone-naive, it will be localized to `tz` and then converted to the timezone
of the index. If `to_utc` is True, it will also be converted to UTC. If the index has no timezone,
the provided datetime will be converted either to `tz` or to UTC (if `to_utc` is True),
and then the timezone will be removed.

Let's demonstrate using a custom timezone by saving and then pulling the price of AAPL:

```pycon
>>> aapl_data = vbt.YFData.fetch("AAPL", start="2022", end="2023")
>>> aapl_data.close
Date
2022-01-03 00:00:00-05:00    180.190979
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171844
                                    ...
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
2022-12-30 00:00:00-05:00    129.378006
Name: Close, Length: 251, dtype: float64

>>> aapl_data.to_sql("sqlite")  # (1)!
>>> aapl_data.to_sql("postgresql")  # (2)!

>>> vbt.SQLData.pull(
...     "AAPL", 
...     engine_name="sqlite", 
...     start="2022-12-23",
...     end="2022-12-30",
...     tz="America/New_York"
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64

>>> vbt.SQLData.pull(
...     "AAPL", 
...     engine_name="postgresql", 
...     start="2022-12-23",
...     end="2022-12-30",
...     tz="America/New_York"
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64
```

1. The first argument can also be an engine name from the global settings.

2. The schema defined in the global settings will be used by default.

All datetime pre-processing can be disabled by turning off `align_dates`:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL", 
...     start="2022-12-23",
...     end="2022-12-30",
...     tz="America/New_York",
...     align_dates=False
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64
```

When you need to execute a custom query, filtering must be performed inside the query itself:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL", 
...     query="""
...         SELECT *
...         FROM yf_data."AAPL" 
...         WHERE yf_data."AAPL"."Date" >= :start AND yf_data."AAPL"."Date" < :end
...     """,
...     tz="America/New_York",
...     index_col="Date",
...     params={
...         "start": vbt.datetime("2022-12-23", tz="America/New_York"),
...         "end": vbt.datetime("2022-12-30", tz="America/New_York")
...     }
... ).close
Date
2022-12-23 00:00:00-05:00    131.299820
2022-12-27 00:00:00-05:00    129.477585
2022-12-28 00:00:00-05:00    125.504539
2022-12-29 00:00:00-05:00    129.059372
Name: Close, dtype: float64
```

To filter by row number, a column with row numbers must be included first. This can be done
automatically by using [Data.to_sql](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_sql) with
`attach_row_number` set to True.

```pycon
>>> aapl_data.to_sql("sqlite", attach_row_number=True, if_exists="replace")
```

Then, when pulling, you can use `start_row` and `end_row` directly:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL", 
...     start_row=5,
...     end_row=10,
...     tz="America/New_York"
... ).close
Date
2022-01-10 00:00:00-05:00    170.469116
2022-01-11 00:00:00-05:00    173.330231
2022-01-12 00:00:00-05:00    173.775711
2022-01-13 00:00:00-05:00    170.469116
2022-01-14 00:00:00-05:00    171.340347
Freq: D, Name: Close, dtype: float64
```

#### Chunking

Chunking for SQL databases works the same way as it does for CSV files:

```pycon
>>> vbt.SQLData.pull(
...     "AAPL",
...     chunksize=1,
...     chunk_func=lambda iterator: pd.concat([
...         df 
...         for df in iterator
...         if df.index.is_month_start.all()
...     ], axis=0),
...     tz="America/New_York"
... ).close
Date
2022-02-01 00:00:00-05:00    172.864914
2022-03-01 00:00:00-05:00    161.774826
2022-04-01 00:00:00-04:00    172.787796
2022-06-01 00:00:00-04:00    147.627930
2022-07-01 00:00:00-04:00    137.919113
2022-08-01 00:00:00-04:00    160.334793
2022-09-01 00:00:00-04:00    157.028458
2022-11-01 00:00:00-04:00    149.761551
2022-12-01 00:00:00-05:00    147.679932
Name: Close, dtype: float64
```

### DuckDB

!!! tip
    The previous method (using SQLAlchemy) can also be used to read from a DuckDB database.
    To do this, you will need to install the [duckdb-engine](https://pypi.org/project/duckdb-engine/) extension.

To fetch data using DuckDB, you can use the convenient class 
[DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData). This class provides
methods for discovering catalogs, schemas, and tables, as well as methods for fetching the data itself,
such as [DuckDBData.fetch_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData.fetch_key).
Let's delete the existing DuckDB database, create a new one by setting its URL globally,
add some data, and review the stored objects:

```pycon
>>> vbt.remove_file(DUCKDB_URL)

>>> vbt.DuckDBData.set_custom_settings(
...     connection=DUCKDB_URL
... )

>>> yf_data.to_duckdb(schema="yf_data")

>>> vbt.DuckDBData.list_catalogs()
['database']

>>> vbt.DuckDBData.list_schemas()
['main', 'yf_data']

>>> vbt.DuckDBData.list_tables()
['yf_data:BTC-USD', 'yf_data:ETH-USD']
```

To fetch the data, use [DuckDBData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData.pull).
If you provide one or more keys, they will be used as table names. Without any arguments, the method
will first determine which tables are stored in the database and then pull them:

```pycon
>>> vbt.DuckDBData.pull("yf_data:BTC-USD").symbols
['yf_data:BTC-USD']

>>> vbt.DuckDBData.pull("BTC-USD", schema="yf_data").symbols
['BTC-USD']

>>> vbt.DuckDBData.pull(["BTC-USD", "ETH-USD"], schema="yf_data").symbols
['BTC-USD', 'ETH-USD']

>>> vbt.DuckDBData.pull().symbols
['yf_data:BTC-USD', 'yf_data:ETH-USD']
```

If you want to keep symbol names and table names separate, you can provide each table 
explicitly using a dictionary with [key_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.key_dict):

```pycon
>>> vbt.DuckDBData.pull(
...     ["BTC", "ETH"], 
...     table=vbt.key_dict(BTC="BTC-USD", ETH="ETH-USD"),
...     schema="yf_data"
... ).symbols
['BTC', 'ETH']
```

You can follow the same approach to execute custom queries:

```pycon
>>> vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": 'SELECT * FROM yf_data."BTC-USD"', 
...         "ETH-USD": 'SELECT * FROM yf_data."ETH-USD"'
...     })
... ).symbols
['BTC-USD', 'ETH-USD']
```

In addition to querying tables, you can also read CSV, Parquet, and JSON files.
The reading behavior is similar to the writing process shown in
[Data.to_duckdb](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_duckdb),
where there are three arguments: `read_format`, `read_path`, and `read_options`.
The first argument specifies the format of the file(s), the second specifies the file path(s),
and the third controls the options for reading the files. If `read_format` is not provided,
it is parsed from the file extension and used to call the corresponding `read_{format}` function in
the SQL query, for example, [`read_parquet`](https://duckdb.org/docs/data/parquet/overview)
for Parquet files.

```pycon
>>> vbt.DuckDBData.pull(read_path="data").symbols
['BTC-USD', 'ETH-USD']

>>> vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"],
...     read_path=vbt.key_dict({
...          "BTC-USD": "data/BTC-USD.parquet",
...          "ETH-USD": "data/ETH-USD.parquet"
...     })
... ).symbols
['BTC-USD', 'ETH-USD']

>>> vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"],
...     query=vbt.key_dict({
...          "BTC-USD": "SELECT * FROM read_parquet('data/BTC-USD.parquet')",
...          "ETH-USD": "SELECT * FROM read_parquet('data/ETH-USD.parquet')"
...     })
... ).symbols
['BTC-USD', 'ETH-USD']
```

## Updating

Updating local data works similarly to updating remote data, but it only affects the contents of the 
local file or database. When updating, the system will download new data, append it to the local dataset, 
and, if necessary, remove any duplicates. As a result, the file or database will always reflect the most 
up-to-date data available according to the update logic. 

You can trigger an update manually, or you can configure automatic updates by setting schedules or making 
calls from your code. The specifics depend on how you have configured your data pipeline and storage. 

Be sure to use the appropriate tools and methods provided by your data source and storage format to ensure
reliable and consistent updates.

### CSV & HDF

Tabular data, such as CSV and HDF files, can be read line by line, allowing you to monitor data updates.
The classes [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) and
[HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) can be updated like any other preset data
class by tracking the last row index in [Data.returned_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.returned_kwargs).
Whenever an update is triggered, this index is used as the starting row for reading the dataset. After the
update, the end row becomes the new last row index.

Let's separately download data for `BTC-USD` and `ETH-USD`, save them to a single HDF file,
and read the entire file using [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData):

```pycon
>>> yf_data_btc = vbt.YFData.pull(
...     "BTC-USD", 
...     start="2020-01-01", 
...     end="2020-01-03"
... )
>>> yf_data_eth = vbt.YFData.pull(
...     "ETH-USD", 
...     start="2020-01-03", 
...     end="2020-01-05"
... )

>>> yf_data_btc.to_hdf("data.h5", key="yf_data_btc")
>>> yf_data_eth.to_hdf("data.h5", key="yf_data_eth")

>>> hdf_data = vbt.HDFData.pull(["BTC-USD", "ETH-USD"], paths="data.h5")
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> hdf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215         NaN
2020-01-03 00:00:00+00:00          NaN  134.171707
2020-01-04 00:00:00+00:00          NaN  135.069366
```

Now, let's look at the last row index in each dataset:

```pycon
>>> hdf_data.returned_kwargs
key_dict({'BTC-USD': {'last_row': 1}, 'ETH-USD': {'last_row': 1}})
```

We see that the third row in each dataset is the new start row (1 row for the header and
1 row for the data). Let's append new data to the `BTC-USD` dataset and then update our
[HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) instance:

```pycon
>>> yf_data_btc = yf_data_btc.update(end="2020-01-06")
>>> yf_data_btc.to_hdf("data.h5", key="yf_data_btc")

>>> hdf_data = hdf_data.update()
>>> hdf_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316         NaN
2020-01-02 00:00:00+00:00  6985.470215         NaN
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
2020-01-05 00:00:00+00:00  7411.317383         NaN
```

The `BTC-USD` dataset has been updated with 3 new data points, while the `ETH-USD` dataset remains
unchanged. This is reflected in the last row index:

```pycon
>>> hdf_data.returned_kwargs
key_dict({
    'BTC-USD': {'last_row': 4, 'tz_convert': datetime.timezone.utc}, 
    'ETH-USD': {'last_row': 1, 'tz_convert': datetime.timezone.utc}
})
```

You can repeat this workflow as often as needed.

### Feather & Parquet

Feather and Parquet classes do not have any `start` or `end` arguments to select a date range to pull,
but you can still load all the data and append any differences to what you already have. This operation
is efficient because reading these formats is highly optimized. In newer versions of Pandas, you can also
filter partitions using the `filters` argument (supported only for Parquet):

```pycon
>>> parquet_data = vbt.ParquetData.pull(
...     "BNB-USD", 
...     filters=[("group", "<", "2020-01-03")]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
Name: Close, dtype: float64

>>> parquet_data = parquet_data.update(
...     filters=[("group", ">=", "2020-01-03")]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
2020-01-03 00:00:00+00:00    13.660452
2020-01-04 00:00:00+00:00    13.891512
Freq: D, Name: Close, dtype: float64
```

In newer versions of PyArrow, you can use the same argument to select rows:

```pycon
>>> parquet_data = vbt.ParquetData.pull(
...     "BNB-USD", 
...     filters=[("Date", "<", vbt.timestamp("2020-01-03", tz="UTC"))]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
Name: Close, dtype: float64

>>> parquet_data = parquet_data.update(
...     filters=[("Date", ">=", vbt.timestamp("2020-01-03", tz="UTC"))]
... )
>>> parquet_data.close
Date
2020-01-01 00:00:00+00:00    13.689083
2020-01-02 00:00:00+00:00    13.027011
2020-01-03 00:00:00+00:00    13.660452
2020-01-04 00:00:00+00:00    13.891512
Freq: D, Name: Close, dtype: float64
```

!!! important
    There is an issue with PyArrow starting from version 12.0.0 that makes it impossible
    to filter by timezone-aware timestamps: https://github.com/apache/arrow/issues/37355.
    This issue is said to be resolved in version 14.0.0.

### SQL

The class [SQLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData) can be updated
in two ways: by using the last row number or the last index. The first method works only if you have
attached a column with row numbers, the name of this column is known and stored in
[Data.returned_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.returned_kwargs) (this happens
automatically), and index-based filtering (`start` and/or `end`) is not used. If these conditions are met,
this method will be used by default; it will retrieve the last row number from the DataFrame and
pass it as `start_row`.

```pycon
>>> aapl_data.to_sql("postgresql", attach_row_number=True, if_exists="replace")

>>> sql_data = vbt.SQLData.pull(
...     "AAPL", 
...     end_row=5,
...     tz="America/New_York"
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
Freq: D, Name: Close, dtype: float64

>>> sql_data = sql_data.update(end_row=10)
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
2022-01-10 00:00:00-05:00    170.469131
2022-01-11 00:00:00-05:00    173.330261
2022-01-12 00:00:00-05:00    173.775726
2022-01-13 00:00:00-05:00    170.469131
2022-01-14 00:00:00-05:00    171.340317
Freq: B, Name: Close, dtype: float64
```

The second approach is enabled if the first approach is disabled and row-based filtering 
(`start_row` and/or `end_row`) is not used. It will extract the last index from 
[Data.last_index](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.last_index) and pass it as `start`, 
regardless of the index data type.

```pycon
>>> aapl_data.to_sql("postgresql", attach_row_number=False, if_exists="replace")

>>> sql_data = vbt.SQLData.pull(
...     "AAPL", 
...     end="2022-01-08",
...     tz="America/New_York"
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
Freq: D, Name: Close, dtype: float64

>>> sql_data = sql_data.update(end="2022-01-15")
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
2022-01-10 00:00:00-05:00    170.469131
2022-01-11 00:00:00-05:00    173.330261
2022-01-12 00:00:00-05:00    173.775726
2022-01-13 00:00:00-05:00    170.469131
2022-01-14 00:00:00-05:00    171.340317
Freq: B, Name: Close, dtype: float64
```

If the data was originally pulled using a custom query, both approaches will be disabled,
and you will need to implement either method manually.

```pycon
>>> sql_data = vbt.SQLData.pull(
...     "AAPL", 
...     query="""
...         SELECT *
...         FROM yf_data."AAPL" 
...         WHERE yf_data."AAPL"."Date" >= :start AND yf_data."AAPL"."Date" < :end
...     """,
...     tz="America/New_York",
...     index_col="Date",
...     params={
...         "start": vbt.datetime("2022-01-01", tz="America/New_York"),
...         "end": vbt.datetime("2022-01-08", tz="America/New_York")
...     }
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190964
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171814
2022-01-06 00:00:00-05:00    170.281021
2022-01-07 00:00:00-05:00    170.449341
Freq: D, Name: Close, dtype: float64

>>> sql_data = sql_data.update(
...     params={
...         "start": vbt.datetime("2022-01-08", tz="America/New_York"),
...         "end": vbt.datetime("2022-01-18", tz="America/New_York")
...     }
... )
>>> sql_data.close
Date
2022-01-03 00:00:00-05:00    180.190948
2022-01-04 00:00:00-05:00    177.904068
2022-01-05 00:00:00-05:00    173.171844
2022-01-06 00:00:00-05:00    170.281006
2022-01-07 00:00:00-05:00    170.449326
2022-01-10 00:00:00-05:00    170.469116
2022-01-11 00:00:00-05:00    173.330231
2022-01-12 00:00:00-05:00    173.775742
2022-01-13 00:00:00-05:00    170.469116
2022-01-14 00:00:00-05:00    171.340332
Freq: B, Name: Close, dtype: float64
```

### DuckDB

!!! tip
    The previous method (using SQLAlchemy) can also be used to update from a DuckDB database.
    To do this, you must install the [duckdb-engine](https://pypi.org/project/duckdb-engine/) extension.

To update an existing [DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData)
with new data, you can use the `start` and `end` arguments, or construct a SQL query that returns
the desired data range.

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     end="2020-01-03",
...     schema="yf_data"
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(end=None)
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Alternatively, you can do this manually using a custom SQL query:

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * FROM yf_data."BTC-USD" 
...             WHERE "Date" < TIMESTAMP '2020-01-03 00:00:00.000000'
...         """, 
...         "ETH-USD": """
...             SELECT * FROM yf_data."ETH-USD" 
...             WHERE "Date" < TIMESTAMP '2020-01-03 00:00:00.000000'
...         """
...     })
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * FROM yf_data."BTC-USD" 
...             WHERE "Date" >= TIMESTAMP '2020-01-03 00:00:00.000000'
...         """, 
...         "ETH-USD": """
...             SELECT * FROM yf_data."ETH-USD" 
...             WHERE "Date" >= TIMESTAMP '2020-01-03 00:00:00.000000'
...         """
...     })
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

You can also use prepared statements as shown below:

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * FROM yf_data."BTC-USD" 
...             WHERE "Date" >= $start AND "Date" < $end
...         """, 
...         "ETH-USD": """
...             SELECT * FROM yf_data."ETH-USD" 
...             WHERE "Date" >= $start AND "Date" < $end
...         """
...     }),
...     params=dict(
...         start=vbt.datetime("2020-01-01"),
...         end=vbt.datetime("2020-01-03")
...     )
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(
...     params=dict(
...         start=vbt.datetime("2020-01-03"),
...         end=vbt.datetime("2020-01-05")
...     )
... )
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

Similarly, you can filter based on the dynamically generated row number:

```pycon
>>> duckdb_data = vbt.DuckDBData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     query=vbt.key_dict({
...         "BTC-USD": """
...             SELECT * EXCLUDE (Row) FROM (
...                 SELECT row_number() OVER () AS "Row", * FROM yf_data."BTC-USD"
...             )
...             WHERE Row >= $start_row AND Row < $end_row
...         """, 
...         "ETH-USD": """
...             SELECT * EXCLUDE (Row) FROM (
...                 SELECT row_number() OVER () AS "Row", * FROM yf_data."ETH-USD"
...             )
...             WHERE Row >= $start_row AND Row < $end_row
...         """
...     }),
...     params=dict(start_row=1, end_row=3)
... )
>>> duckdb_data.close
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179

>>> duckdb_data = duckdb_data.update(params=dict(start_row=3, end_row=5))
symbol                         BTC-USD     ETH-USD
Date                                              
2020-01-01 00:00:00+00:00  7200.174316  130.802002
2020-01-02 00:00:00+00:00  6985.470215  127.410179
2020-01-03 00:00:00+00:00  7344.884277  134.171707
2020-01-04 00:00:00+00:00  7410.656738  135.069366
```

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/data/local.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/data/remote/
parent: https://vectorbt.pro/pvt_41531c19/documentation/data/
name: Remote
type: page
icon: material-web
---

# Remote

Data classes that subclass [RemoteData](https://vectorbt.pro/pvt_41531c19/api/data/custom/remote/#vectorbtpro.data.custom.remote.RemoteData) are designed
to retrieve (mainly OHLCV) data from remote sources. Unlike classes intended for locally stored data,
they communicate with remote API endpoints and must manage authentication, authorization, throttling,
and other related mechanisms. Additionally, the total amount of data to be retrieved is often unknown in advance,
and most data providers impose API rate limits and restrict the amount of data returned per request.
As a result, it is often necessary to fetch data in smaller batches and concatenate them correctly.
Fortunately, VBT provides several preset data classes that can handle all these tasks automatically.

## Arguments

Most remote data classes share the following arguments:

| Argument           | Description                                                                                                                                                                                                                                                                                                                                                       |
|--------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| `client`           | Client object required to make a request. Typically, the data class provides an additional class method, `resolve_client`, to create the client based on the keyword arguments in `client_config` if the client is `None`. If a client is already provided, this step is skipped. You do not need to call `resolve_client` directly; it is handled automatically. |
| `client_config`    | Keyword arguments used to instantiate the client.                                                                                                                                                                                                                                                                                                                 |
| `start`            | Start datetime (inclusive). Will be converted to a `datetime.datetime` using [to_tzaware_datetime](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_tzaware_datetime) and may be further processed to match the format required by the data provider.                                                                                                         |
| `end`              | End datetime (exclusive). Will be converted to a `datetime.datetime` using [to_tzaware_datetime](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_tzaware_datetime) and may be further processed to match the format required by the data provider.                                                                                                           |
| `tz`               | Target timezone of the index (internally becomes `tz_convert`). If `start` and/or `end` do not have a timezone, this timezone will also be used to localize them.                                                                                                                                                                                                 |
| `timeframe`        | Timeframe provided as a human-readable string (such as `1 day`) with a multiplier (`1`) and a unit (`day`). Will be parsed into a standardized format using [split_freq_str](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.split_freq_str).                                                                                                                   |
| `limit`            | Maximum number of data items to return per request.                                                                                                                                                                                                                                                                                                               |
| `delay`            | Delay in milliseconds between requests. Helps handle API rate limits.                                                                                                                                                                                                                                                                                             | 
| `retries`          | Number of retries in case of connectivity issues or other request-specific errors. Usually only applied if the data class can collect data in batches.                                                                                                                                                                                                            |
| `show_progress`    | Whether to show the progress bar of type [ProgressBar](https://vectorbt.pro/pvt_41531c19/api/utils/pbar/#vectorbtpro.utils.pbar.ProgressBar). Usually only applied if the data class can collect data in batches.                                                                                                                                                                                  | 
| `pbar_kwargs`      | Keyword arguments used to configure the progress bar.                                                                                                                                                                                                                                                                                                             |
| `silence_warnings` | Whether to silence all warnings to avoid being overwhelmed with messages such as those related to timeouts.                                                                                                                                                                                                                                                       |
| `exchange`         | Exchange to fetch from, if the data class supports multiple exchanges. If so, settings can also be customized per exchange.                                                                                                                                                                                                                                       |

To view the list of arguments accepted by a remote data class fetcher, you can check the API reference,
use the Python `help` command, or use VBT's helper function
[phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp)
on the class method [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol),
which creates a query for a single symbol and returns a Series or DataFrame:

```pycon
>>> from vectorbtpro import *

>>> vbt.phelp(vbt.CCXTData.fetch_symbol)
CCXTData.fetch_symbol(
    symbol,
    exchange=None,
    exchange_config=None,
    start=None,
    end=None,
    timeframe=None,
    tz=None,
    find_earliest_date=None,
    limit=None,
    delay=None,
    retries=None,
    fetch_params=None,
    show_progress=None,
    pbar_kwargs=None,
    silence_warnings=None,
    return_fetch_method=False
):
    Override `vectorbtpro.data.base.Data.fetch_symbol` to fetch a symbol from CCXT.
    
    Args:
        symbol (str): Symbol.
    
            Symbol can be in the `EXCHANGE:SYMBOL` format, in this case `exchange` argument will be ignored.
        exchange (str or object): Exchange identifier or an exchange object.
    
            See `CCXTData.resolve_exchange`.
        exchange_config (dict): Exchange config.
    
            See `CCXTData.resolve_exchange`.
        start (any): Start datetime.
    
            See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
        end (any): End datetime.
    
            See `vectorbtpro.utils.datetime_.to_tzaware_datetime`.
        timeframe (str): Timeframe.
    
            Allows human-readable strings such as "15 minutes".
        tz (any): Timezone.
    
            See `vectorbtpro.utils.datetime_.to_timezone`.
        find_earliest_date (bool): Whether to find the earliest date using `CCXTData.find_earliest_date`.
        limit (int): The maximum number of returned items.
        delay (float): Time to sleep after each request (in milliseconds).
    
            !!! note
                Use only if `enableRateLimit` is not set.
        retries (int): The number of retries on failure to fetch data.
        fetch_params (dict): Exchange-specific keyword arguments passed to `fetch_ohlcv`.
        show_progress (bool): Whether to show the progress bar.
        pbar_kwargs (dict): Keyword arguments passed to `vectorbtpro.utils.pbar.ProgressBar`.
        silence_warnings (bool): Whether to silence all warnings.
        return_fetch_method (bool): Required by `CCXTData.find_earliest_date`.
    
    For defaults, see `custom.ccxt` in `vectorbtpro._settings.data`.
    Global settings can be provided per exchange id using the `exchanges` dictionary.
```

As shown above, the class [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData) accepts
the exchange object, timeframe, start date, end date, and other keyword arguments.

!!! tip
    The class method [Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull) usually accepts the
    same arguments as [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol).

### Settings

But why are all argument values set to `None`? Remember that `None` has a special meaning and tells
VBT to retrieve the argument's default value from the [global settings](https://vectorbt.pro/pvt_41531c19/api/_settings/).
Specifically, you should look at the settings defined for CCXT, which can be found in the dictionary
under `custom.ccxt` in [settings.data](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.data):

```pycon
>>> vbt.pprint(vbt.settings.data["custom"]["ccxt"])
Config(
    exchange='binance',
    exchange_config=dict(
        enableRateLimit=True
    ),
    start=None,
    end=None,
    timeframe='1d',
    tz='UTC',
    find_earliest_date=False,
    limit=1000,
    delay=None,
    retries=3,
    show_progress=True,
    pbar_kwargs=dict(),
    fetch_params=dict(),
    exchanges=dict(),
    silence_warnings=False
)
```

Another way to get the settings is by using the method [Data.get_settings](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get_settings):

```pycon
>>> vbt.pprint(vbt.CCXTData.get_settings(path_id="custom"))
```

!!! tip
    Data classes register two path ids: `base` and `custom`. The `base` id manages the settings
    for the base class [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data), while the `custom` id
    manages settings for any subclass of [CustomData](https://vectorbt.pro/pvt_41531c19/api/data/custom/custom/#vectorbtpro.data.custom.custom.CustomData).

By using the default arguments, you will pull the symbol's entire daily history from Binance.

To set a default value, you can change the config directly. For example, to change the exchange to BitMEX:

```pycon
>>> vbt.settings.data["custom"]["ccxt"]["exchange"] = "bitmex"
```

Even easier, just like you used [Data.get_settings](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get_settings)
to get the settings dictionary, you can use [Data.set_settings](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.set_settings)
to set them:

```pycon
>>> vbt.CCXTData.set_settings(path_id="custom", exchange="bitmex")
>>> vbt.settings.data["custom"]["ccxt"]["exchange"]
'bitmex'
```

!!! note
    Overriding keys in the dictionary returned by
    [Data.get_settings](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get_settings)
    will not have any effect.

What if you made a mistake? No need to worry! You can reset the settings at any time:

```pycon
>>> vbt.CCXTData.reset_settings(path_id="custom")
>>> vbt.settings.data["custom"]["ccxt"]["exchange"]
'binance'
```

!!! tip
    This only resets the settings for this particular class, not for all of VBT.

### Start and end

Specifying dates and times is usually very easy thanks to the built-in datetime parser
[to_tzaware_datetime](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_tzaware_datetime), which can
parse dates and times from various objects, including human-readable strings like `1 day ago`:

```pycon
>>> vbt.local_datetime("1 day ago")
datetime.datetime(2023, 8, 28, 20, 57, 49, 77715, tzinfo=tzlocal())
```

Let's illustrate this by fetching the last 10 minutes of `BTC/USDT` and `ETH/USDT`:

```pycon
>>> ccxt_data = vbt.CCXTData.pull(
...     ["BTC/USDT", "ETH/USDT"],
...     start="10 minutes ago UTC",
...     end="now UTC",
...     timeframe="1m"
... )
```

!!! note
    Different remote data classes may use different symbol notations, such as `BTC/USDT` for CCXT,
    `BTC-USD` for Yahoo Finance, `BTCUSDT` for Binance, `X:BTCUSD` for Polygon.io, and more.

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> ccxt_data.close
symbol                     BTC/USDT  ETH/USDT
Open time                                    
2023-08-29 18:50:00+00:00  27990.00   1738.22
2023-08-29 18:51:00+00:00  27973.54   1737.43
2023-08-29 18:52:00+00:00  27981.32   1737.53
2023-08-29 18:53:00+00:00  27964.75   1736.64
2023-08-29 18:54:00+00:00  27972.27   1737.15
2023-08-29 18:55:00+00:00  27963.03   1737.10
2023-08-29 18:56:00+00:00  27962.01   1736.70
2023-08-29 18:57:00+00:00  27970.29   1736.82
2023-08-29 18:58:00+00:00  27986.34   1737.00
2023-08-29 18:59:00+00:00  27987.54   1736.80
```

!!! tip
    Dates and times are resolved in [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol).
    When fetching high-frequency data, make sure to provide resolved start and end times using
    [to_tzaware_datetime](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_tzaware_datetime).
    Otherwise, by the time the first symbol is fetched, the resolved times for the next symbol may
    already be different.

### Timeframe

The timeframe format is standardized across the entire VBT codebase, including all preset data
classes. This is accomplished by the function [split_freq_str](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.split_freq_str),
which splits a timeframe string into a multiplier and a unit:

```pycon
>>> vbt.dt.split_freq_str("15 minutes")
(15, 'm')

>>> vbt.dt.split_freq_str("daily")
(1, 'D')

>>> vbt.dt.split_freq_str("1wk")
(1, 'W')

>>> vbt.dt.split_freq_str("annually")
(1, 'Y')
```

After splitting, each preset data class converts the resulting multiplier and unit into the format
accepted by its API. For example, in [PolygonData](https://vectorbt.pro/pvt_41531c19/api/data/custom/polygon/#vectorbtpro.data.custom.polygon.PolygonData),
the unit `"m"` is translated to `"minute"`, while in [AlpacaData](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData)
it is translated to `TimeFrameUnit.Minute`. Note that units like `"m"` are for internal use only
and should not be used directly in Pandas. For instance, using `"m"` to construct a date offset
(for use in [pandas.DataFrame.resample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html))
creates a month end, while using it to construct a timedelta creates a minute:

```pycon
>>> from pandas.tseries.frequencies import to_offset

>>> to_offset("1m")
<MonthEnd>

>>> pd.Timedelta("1m")
Timedelta('0 days 00:01:00')
```

In these cases, you should use [to_offset](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_offset) and
[to_timedelta](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_timedelta) respectively,
which are in-house functions that can handle many common formats:

```pycon
>>> vbt.offset("1m")
<Minute>

>>> vbt.timedelta("1m")
Timedelta('0 days 00:01:00')
```

Let's pull 30-minute `BTC/USDT` data for the current day:

```pycon
>>> ccxt_data = vbt.CCXTData.pull(
...     "BTC/USDT",
...     start="today midnight UTC",
...     timeframe="30 minutes"
... )
>>> ccxt_data.get()
                                Open      High       Low     Close       Volume
Open time
2023-08-29 00:00:00+00:00  26120.00  26135.20  26092.91  26092.91    244.82583
2023-08-29 00:30:00+00:00  26092.92  26165.99  26084.07  26140.44    372.98168
2023-08-29 01:00:00+00:00  26140.44  26206.24  26105.78  26127.64    595.85951
...                             ...       ...       ...       ...          ...
2023-08-29 18:00:00+00:00  27892.08  27973.14  27835.84  27949.82   1217.90593
2023-08-29 18:30:00+00:00  27949.83  28020.73  27910.70  27971.79   1422.90243
2023-08-29 19:00:00+00:00  27971.78  27982.65  27900.87  27910.01    220.80118
```

### Client

Many APIs require a client to make requests. Data classes based on such APIs usually include a
class method called `resolve_client` that is used to resolve the client, which is called before
pulling each symbol. If the client is not provided by the user (`None`), this method creates
it automatically based on the `client_config`. This config can include a variety of information,
such as API keys or connection parameters. For example, here is the default client of
[BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData):

```pycon
>>> binance_client = vbt.BinanceData.resolve_client()
>>> binance_client
<binance.client.Client at 0x7f893a193af0>
```

To provide information to this client, you can pass keyword arguments directly:

```pycon
>>> binance_client = vbt.BinanceData.resolve_client(
...     api_key="YOUR_KEY",
...     api_secret="YOUR_SECRET"
... )
>>> binance_client
<binance.client.Client at 0x7f89183512e0>
```

Since the client is created automatically, you can pass all client-related information using
the `client_config` argument during fetching:

```pycon
>>> binance_data = vbt.BinanceData.pull(
...     "BTCUSDT",
...     client_config=dict(
...         api_key="YOUR_KEY",
...         api_secret="YOUR_SECRET"
...     )
... )
>>> binance_data.get()
                                Open      High       Low     Close  \
Open time
2017-08-17 00:00:00+00:00   4261.48   4485.39   4200.74   4285.08
2017-08-18 00:00:00+00:00   4285.08   4371.52   3938.77   4108.37
2017-08-19 00:00:00+00:00   4108.37   4184.69   3850.00   4139.98
...                             ...       ...       ...       ...
2023-08-27 00:00:00+00:00  26017.38  26182.23  25966.11  26101.77
2023-08-28 00:00:00+00:00  26101.78  26253.99  25864.50  26120.00
2023-08-29 00:00:00+00:00  26120.00  28142.85  25922.00  27895.97

                                  Volume  Quote volume  Trade count  \
Open time
2017-08-17 00:00:00+00:00    795.150377  3.454770e+06         3427
2017-08-18 00:00:00+00:00   1199.888264  5.086958e+06         5233
2017-08-19 00:00:00+00:00    381.309763  1.549484e+06         2153
...                                 ...           ...          ...
2023-08-27 00:00:00+00:00  12099.642160  3.155484e+08       349090
2023-08-28 00:00:00+00:00  22692.626550  5.910089e+08       523057
2023-08-29 00:00:00+00:00  65076.334610  1.768909e+09       968856

                            Taker base volume  Taker quote volume
Open time
2017-08-17 00:00:00+00:00         616.248541        2.678216e+06
2017-08-18 00:00:00+00:00         972.868710        4.129123e+06
2017-08-19 00:00:00+00:00         274.336042        1.118002e+06
...                                      ...                 ...
2023-08-27 00:00:00+00:00        6170.486640        1.609182e+08
2023-08-28 00:00:00+00:00       10912.405490        2.842262e+08
2023-08-29 00:00:00+00:00       33459.465920        9.098352e+08

[2204 rows x 9 columns]
```

However, if you run [BinanceData.resolve_client](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData.resolve_client),
you will notice that it takes time to instantiate a client, and you probably do not want to wait that
long for every single symbol you are trying to fetch. A better approach is to instantiate a client
manually just once and then pass it via the `client` argument, which will reuse the client and
make fetching much faster:

```pycon
>>> binance_data = vbt.BinanceData.pull(
...     "BTCUSDT",
...     client=binance_client
... )
```

!!! info
    This also enables reusing the client or client config during updating since any argument passed to
    the fetcher is stored inside the dictionary
    [Data.fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_kwargs),
    which is used by the updater.

!!! warning
    However, this also means that sharing the data object with anyone may expose your credentials!

To maintain security, the recommended approach is to set any credentials and clients globally, as
previously discussed. This way, they will not be stored inside the data instance.

```pycon
>>> vbt.BinanceData.set_settings(
...     path_id="custom",
...     client=binance_client
... )
```

!!! tip
    See the API documentation of the specific data class for more examples.

### Saving

To save any remote data instance, see [this documentation](https://vectorbt.pro/pvt_41531c19/documentation/data/local/). In short,
pickling is preferred because it also saves all arguments that were passed to the fetcher,
such as the selected timeframe. These arguments are important when updating because without them,
you would need to provide them manually each time you attempt to update the data.

```pycon
>>> binance_data = vbt.BinanceData.pull(
...     "BTCUSDT",
...     start="today midnight UTC",
...     timeframe="1 hour"
... )
>>> binance_data.save("binance_data")

>>> binance_data = vbt.BinanceData.load("binance_data")
>>> vbt.pprint(binance_data.fetch_kwargs)
symbol_dict(
    BTCUSDT=dict(
        start='today midnight UTC',
        timeframe='1 hour',
        silence_warnings=False
    )
)
```

As you can see, all arguments were saved along with the data instance. But if you do not plan on
updating the data, you can save the arrays themselves to one or multiple CSV files or HDF keys,
one per symbol:

```pycon
>>> binance_data.to_csv()  # (1)!

>>> csv_data = vbt.CSVData.pull("BTCUSDT.csv")
```

1. The data will be saved into a CSV file named `{symbol}.csv`.

But what if you want to update the data stored locally in a CSV or HDF file?
The fetching-related keyword arguments no longer include the timeframe or other parameters.
They only include arguments important for the data class holding the data—
[CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData):

```pycon
>>> vbt.pprint(csv_data.fetch_kwargs)
symbol_dict(
    BTCUSDT=dict(
        path=PosixPath('BTCUSDT.csv')
    )
)
```

If you use the update method on this data instance, it will try to update using the local data,
not remote data. To update from a remote endpoint, you need to switch the data class
back to the original class—[BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData)
in this case. For this, you can use the method [Data.switch_class](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.switch_class),
which can also clear all the fetching-related and returned keyword arguments that are
related to the CSV file:

```pycon
>>> binance_data = csv_data.switch_class(
...     new_cls=vbt.BinanceData,
...     clear_fetch_kwargs=True,
...     clear_returned_kwargs=True
... )
>>> type(binance_data)
vectorbtpro.data.custom.binance.BinanceData
```

Next, use the method [Data.update_fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update_fetch_kwargs)
to update the fetching keyword arguments with the timeframe to avoid setting it again whenever
you update:

```pycon
>>> binance_data = binance_data.update_fetch_kwargs(timeframe="1 hour")
>>> vbt.pprint(binance_data.fetch_kwargs)
symbol_dict(
    BTCUSDT=dict(
        timeframe='1 hour'
    )
)
```

Is there an easier way? Absolutely! The class methods
[RemoteData.from_csv](https://vectorbt.pro/pvt_41531c19/api/data/custom/remote/#vectorbtpro.data.custom.remote.RemoteData.from_csv)
and [RemoteData.from_hdf](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.RemoteData.from_hdf)
are available from all data classes and automatically perform all the steps above:

```pycon
>>> binance_data = vbt.BinanceData.from_csv(
...     "BTCUSDT.csv",
...     fetch_kwargs=dict(timeframe="1 hour")
... )

>>> type(binance_data)
<class 'vectorbtpro.data.custom.binance.BinanceData'>

>>> vbt.pprint(binance_data.fetch_kwargs)
symbol_dict(
    BTCUSDT=dict(
        timeframe='1 hour'
    )
)
```

### Updating

Updating a data instance is generally straightforward:

```pycon
>>> binance_data = binance_data.update()
```

!!! note
    Updating the current data instance always returns a new data instance.

Under the hood, the updater first overrides the start date with the latest date in the index,
and then calls the fetcher. This lets you specify or override any argument that was originally used
when fetching. Also note that it will only pull new data if the end date is not fixed. If you used
the end date `2022-01-01` when fetching, it will be used again when updating. Be sure to set
`end` to `"now"` or `"now UTC"` if you want to get fresh data. Let's first fetch the history
for the year 2020, and then append the history for the year 2021:

```pycon
>>> binance_data = vbt.BinanceData.pull(
...     "BTCUSDT",
...     start="2020-01-01",
...     end="2021-01-01"
... )
>>> binance_data = binance_data.update(end="2022-01-01")  # (1)!
>>> binance_data.wrapper.index
DatetimeIndex(['2020-01-01 00:00:00+00:00', '2020-01-02 00:00:00+00:00',
                '2020-01-03 00:00:00+00:00', '2020-01-04 00:00:00+00:00',
                '2020-01-05 00:00:00+00:00', '2020-01-06 00:00:00+00:00',
                ...
                '2021-12-26 00:00:00+00:00', '2021-12-27 00:00:00+00:00',
                '2021-12-28 00:00:00+00:00', '2021-12-29 00:00:00+00:00',
                '2021-12-30 00:00:00+00:00', '2021-12-31 00:00:00+00:00'],
              dtype='datetime64[ns, UTC]', name='Open time', length=731, freq='D')
```

1. Without overriding, the argument `end` will default to the value passed to the fetcher—`2021-01-01`.

## From URL

Although [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) is designed for local file systems,
you can use a few tricks to pull remote data with it as well! Remember that it uses
[pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)?
This function has a `filepath_or_buffer` argument, which can accept a URL.
To use this feature, simply disable the path matching mechanism by setting `match_paths` to False.

Here is an example of pulling S&P 500 index data:

```pycon
>>> url = "https://datahub.io/core/s-and-p-500/r/data.csv"
>>> csv_data = vbt.CSVData.pull(url, match_paths=False)
>>> csv_data.get()
                              SP500  Dividend  Earnings  Consumer Price Index  \
Date                                                                           
1871-01-01 00:00:00+00:00     4.44      0.26      0.40                 12.46   
1871-02-01 00:00:00+00:00     4.50      0.26      0.40                 12.84   
1871-03-01 00:00:00+00:00     4.61      0.26      0.40                 13.03   
...                            ...       ...       ...                   ...   
2018-02-01 00:00:00+00:00  2705.16     49.64       NaN                248.99   
2018-03-01 00:00:00+00:00  2702.77     50.00       NaN                249.55   
2018-04-01 00:00:00+00:00  2642.19       NaN       NaN                249.84   

                            Long Interest Rate  Real Price  Real Dividend  \
Date                                                                       
1871-01-01 00:00:00+00:00                5.32       89.00           5.21   
1871-02-01 00:00:00+00:00                5.32       87.53           5.06   
1871-03-01 00:00:00+00:00                5.33       88.36           4.98   
...                                       ...         ...            ...   
2018-02-01 00:00:00+00:00                2.86     2714.34          49.81   
2018-03-01 00:00:00+00:00                2.84     2705.82          50.06   
2018-04-01 00:00:00+00:00                2.80     2642.19            NaN   

                            Real Earnings   PE10  
Date                                             
1871-01-01 00:00:00+00:00           8.02    NaN  
1871-02-01 00:00:00+00:00           7.78    NaN  
1871-03-01 00:00:00+00:00           7.67    NaN  
...                                  ...    ...  
2018-02-01 00:00:00+00:00            NaN  32.12  
2018-03-01 00:00:00+00:00            NaN  31.99  
2018-04-01 00:00:00+00:00            NaN  31.19  

[1768 rows x 9 columns]
```

### AWS S3

Here is another example for AWS S3:

```pycon
>>> import boto3
>>> s3_client = boto3.client("s3")  # (1)!

>>> symbols = ["BTCUSDT", "ETHUSDT"]
>>> paths = vbt.symbol_dict({ 
...     s: s3_client.get_object(
...         Bucket="binance", 
...         Key=f"data/{s}.csv")["Body"]  # (2)!
...     for s in symbols
... })
>>> s3_data = vbt.CSVData.pull(symbols, paths=paths, match_paths=False)  # (3)!
>>> s3_data.close
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2017-08-17 00:00:00+00:00   4285.08   302.00
2017-08-18 00:00:00+00:00   4108.37   293.96
2017-08-19 00:00:00+00:00   4139.98   290.91
2017-08-20 00:00:00+00:00   4086.29   299.10
2017-08-21 00:00:00+00:00   4016.00   323.29
...                             ...      ...
2022-02-14 00:00:00+00:00  42535.94  2929.75
2022-02-15 00:00:00+00:00  44544.86  3183.52
2022-02-16 00:00:00+00:00  43873.56  3122.30
2022-02-17 00:00:00+00:00  40515.70  2891.87
2022-02-18 00:00:00+00:00  39892.83  2768.74

[1647 rows x 2 columns]
```

1. See [Python, Boto3, and AWS S3: Demystified](https://realpython.com/python-boto3-aws-s3/).

2. Adapt the code to your specific bucket and keys.

3. Each path in `paths` will become `filepath_or_buffer`.

You could load both datasets using [pandas.read_csv](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)
directly, but wrapping them with [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) lets you
take advantage of the powerful [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class in VBT.
For example, you can update your remote datasets whenever new data points arrive, which is a real :gem:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/data/remote.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/data/synthetic/
parent: https://vectorbt.pro/pvt_41531c19/documentation/data/
name: Synthetic
type: page
icon: material-creation
---

# Synthetic

Synthetic data refers to data that could have been generated by financial markets but was not. Synthetic
price and return data help address the financial small data problem and are useful for various purposes,
including testing new investment strategies and supporting data-hungry ML models. They also enable you
to identify behavioral differences and outliers between real and simulated markets. For example, if your
model performs well on a subset of real-world data, you can test it against synthetic data to check
whether you have unintentionally introduced look-ahead bias or other significant weaknesses into the
model.

To facilitate the generation of synthetic data, VBT provides the class
[SyntheticData](https://vectorbt.pro/pvt_41531c19/api/data/custom/synthetic/#vectorbtpro.data.custom.synthetic.SyntheticData). This class
takes a start date, end date, and frequency, then builds a datetime-like Pandas Index. Next, it calls the
abstract class method [SyntheticData.generate_key](https://vectorbt.pro/pvt_41531c19/api/data/custom/synthetic/#vectorbtpro.data.custom.synthetic.SyntheticData.generate_key),
which takes the key and the index, generates new data, and returns a Series or DataFrame ready to be
used by [Data](https://vectorbt.pro/pvt_41531c19/api/data/custom/#vectorbtpro.data.base.Data). Here, the key is either a feature or a symbol,
depending on the data orientation set by the user. You must override this method and implement your own
data generation logic.

!!! note
    If your logic depends on the data orientation (that is, whether features or symbols should be generated),
    you should be more specific and override [SyntheticData.generate_symbol](https://vectorbt.pro/pvt_41531c19/api/data/custom/synthetic/#vectorbtpro.data.custom.synthetic.SyntheticData.generate_symbol)
    and/or [SyntheticData.generate_feature](https://vectorbt.pro/pvt_41531c19/api/data/custom/synthetic/#vectorbtpro.data.custom.synthetic.SyntheticData.generate_feature).

There are two preset classes: [RandomData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random/#vectorbtpro.data.custom.random.RandomData),
which uses cumulative normally-distributed returns, and [GBMData](https://vectorbt.pro/pvt_41531c19/api/data/custom/gbm/#vectorbtpro.data.custom.gbm.GBMData),
which utilizes the [Geometric Brownian Motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion).
Both generators are basic but are quite useful for testing models. However, one limitation is that real
asset prices often make dramatic moves in response to new information. To address this, we will create a
data generator based on the [Levy alpha-stable distribution](https://en.wikipedia.org/wiki/Stable_distribution)!

```pycon
>>> from vectorbtpro import *  # (1)!
>>> from scipy.stats import levy_stable

>>> def geometric_levy_price(alpha, beta, drift, vol, shape):  # (2)!
...     _rvs = levy_stable.rvs(alpha, beta,loc=0, scale=1, size=shape)
...     _rvs_sum = np.cumsum(_rvs, axis=0)
...     return np.exp(vol * _rvs_sum + (drift - 0.5 * vol ** 2))

>>> class LevyData(vbt.SyntheticData):  # (3)!
...
...     _settings_path = dict(custom="data.custom.levy")  # (4)!
...
...     @classmethod
...     def generate_key(
...         cls, 
...         key, 
...         index, 
...         columns, 
...         start_value=None,  # (5)!
...         alpha=None, 
...         beta=None, 
...         drift=None, 
...         vol=None, 
...         seed=None,
...         **kwargs
...     ):
...         start_value = cls.resolve_custom_setting(start_value, "start_value")  # (6)!
...         alpha = cls.resolve_custom_setting(alpha, "alpha")
...         beta = cls.resolve_custom_setting(beta, "beta")
...         drift = cls.resolve_custom_setting(drift, "drift")
...         vol = cls.resolve_custom_setting(vol, "vol")
...         seed = cls.resolve_custom_setting(seed, "seed")
...         if seed is not None:
...             np.random.seed(seed)
...
...         shape = (len(index), len(columns))
...         out = geometric_levy_price(alpha, beta, drift, vol, shape)
...         out = start_value * out
...         return pd.DataFrame(out, index=index, columns=columns)

>>> LevyData.set_custom_settings(  # (7)!
...     populate_=True,
...     start_value=100., 
...     alpha=1.68, 
...     beta=0.01, 
...     drift=0.0, 
...     vol=0.01, 
...     seed=None
... )
```

1. Imports `np`, `pd`, `njit`, and `vbt`.

2. Generation function. See [Asset price mimicry](https://medium.com/codex/generating-synthetic-price-data-2dd1e82fe5cf).

3. Subclass [SyntheticData](https://vectorbt.pro/pvt_41531c19/api/data/custom/synthetic/#vectorbtpro.data.custom.synthetic.SyntheticData).

4. Specify the path to global settings for this class and assign it a new identifier "custom".
There is one more identifier already registered: "base", which points to general settings.

5. Set most arguments to None to pull their default values from the global settings. You can also hard-code
the values here if you decide not to use global settings.

6. Access the global settings and replace None arguments with their default values.

7. Populate the global settings for this class.

Now, let's try it out by generating and plotting the close price for several symbols:

```pycon
>>> levy_data = LevyData.pull(
...     "Close",
...     keys_are_features=True,
...     columns=pd.Index(["BTC/USD", "ETH/USD", "XRP/USD"], name="symbol"),
...     start="2020-01-01 UTC",
...     end="2021-01-01 UTC",
...     seed=42)
>>> levy_data.get()
symbol                        BTC/USD     ETH/USD     XRP/USD
2020-01-01 00:00:00+00:00   99.218626  101.893255  100.371131
2020-01-02 00:00:00+00:00  100.062835   99.537102   97.857226
2020-01-03 00:00:00+00:00   95.321467  100.474547   98.246993
2020-01-04 00:00:00+00:00   96.493680   96.455981   99.797874
2020-01-05 00:00:00+00:00   98.489931   95.658733   98.892301
...                               ...         ...         ...
2020-12-27 00:00:00+00:00  189.477849   91.730109   55.055316
2020-12-28 00:00:00+00:00  190.620767   89.452822   59.555616
2020-12-29 00:00:00+00:00  187.641089   92.164802   60.034154
2020-12-30 00:00:00+00:00  188.287168   92.245270   59.188719
2020-12-31 00:00:00+00:00  185.500114   91.701142   58.443060

[366 rows x 3 columns]

>>> levy_data.get().vbt.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/synthetic_levy.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/synthetic_levy.dark.svg#only-dark){: .iimg loading=lazy }

Great job! We have built our own data generator that simulates sudden large price changes.

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/data/synthetic.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/data/scheduling/
parent: https://vectorbt.pro/pvt_41531c19/documentation/data/
name: Scheduling
type: page
icon: material-timer-outline
---

# Scheduling

Most data sources are not idle; they continuously generate new data. To stay up-to-date, we can use a 
schedule manager (or even a simple while loop) to periodically run tasks such as data capturing and 
manipulation.

## Updating

We can easily schedule data updates using [DataUpdater](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater).
This class accepts a data instance of type [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) and a
schedule manager of type [ScheduleManager](https://vectorbt.pro/pvt_41531c19/api/utils/schedule_/#vectorbtpro.utils.schedule_.ScheduleManager).
It periodically triggers an update that replaces the old data instance with a new one. The new instance 
can be accessed under [DataUpdater.data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater.data). 
The update occurs in the method [DataUpdater.update](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater.update),
which can be overridden to implement custom logic when new data arrives. Since the updater class is a 
subclass of [Configured](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured), it also automatically 
updates its config when `data` changes.

!!! important
    This is one of the few classes in VBT that is not read-only. Do not rely on caching within it!

Let's use this simple yet powerful class to update and plot the last 10 minutes of a Binance ticker
every 10 seconds for 5 minutes. First, pull the latest 10 minutes of data:

```pycon
>>> from vectorbtpro import *

>>> data = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="10 minutes ago UTC", 
...     end="now UTC", 
...     timeframe="1m"
... )

>>> data.close
Open time
2022-02-19 20:09:00+00:00    40005.78
2022-02-19 20:10:00+00:00    40001.80
2022-02-19 20:11:00+00:00    40006.45
2022-02-19 20:12:00+00:00    40003.68
2022-02-19 20:13:00+00:00    40022.24
2022-02-19 20:14:00+00:00    40026.73
2022-02-19 20:15:00+00:00    40048.88
2022-02-19 20:16:00+00:00    40044.92
2022-02-19 20:17:00+00:00    40044.03
2022-02-19 20:18:00+00:00    40049.93
Freq: T, Name: Close, dtype: float64
```

Next, subclass [DataUpdater](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater) to accept
the figure and update it along with the data. To make sure nothing is missed visually, after each
update, append the figure's PNG image to a GIF file:

```pycon
>>> import imageio.v2 as imageio

>>> class OHLCFigUpdater(vbt.DataUpdater):
...     _expected_keys = None
...
...     def __init__(self, data, fig, writer=None, display_last=None, 
...                  stop_after=None, **kwargs):
...         vbt.DataUpdater.__init__(  # (1)!
...             self, 
...             data, 
...             writer=writer,  # (2)!
...             display_last=display_last,
...             stop_after=stop_after,
...             **kwargs
...         )
...
...         self._fig = fig
...         self._writer = writer
...         self._display_last = display_last
...         self._stop_after = stop_after
...         self._start_dt = vbt.utc_datetime()  # (3)!
...
...     @property  # (4)!
...     def fig(self):
...         return self._fig
...
...     @property
...     def writer(self):
...         return self._writer
...
...     @property
...     def display_last(self):
...         return self._display_last
...
...     @property
...     def stop_after(self):
...         return self._stop_after
...
...     @property
...     def start_dt(self):
...         return self._start_dt
...         
...     def update(self, **kwargs):
...         vbt.DataUpdater.update(self, **kwargs)  # (5)!
...         
...         df = self.data.get()
...         if self.display_last is not None:
...             df = df[df.index[-1] - self.display_last:]  # (6)!
...
...         trace = self.fig.data[0]
...         with self.fig.batch_update():
...             trace.x = df["Close"].index  # (7)!
...             trace.open = df["Open"].values
...             trace.high = df["High"].values
...             trace.low = df["Low"].values
...             trace.close = df["Close"].values
...
...         if self.writer is not None:
...             fig_data = imageio.imread(self.fig.to_image(format="png"))
...             self.writer.append_data(fig_data)  # (8)!
...
...         if self.stop_after is not None:
...             now_dt = vbt.utc_datetime()
...             if now_dt - self.start_dt >= self.stop_after:
...                 raise vbt.CancelledError  # (9)!
```

1. Call the constructor of `DataUpdater`.

2. Pass all class-specific keyword arguments to include them in the 
[config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.config).

3. Register the start time.

4. Properties prevent the user (and the program) from overwriting the object, following a convention in VBT.

5. Call `DataUpdater.update`; otherwise, the data will not update.

6. Get the OHLC data within a specific time period (optional).

7. Update the data of the trace (see [Candlestick Charts](https://plotly.com/python/candlestick-charts/)).

8. Append the figure data to the GIF file (optional).

9. Stop once the job has run for a certain amount of time by raising 
[CancelledError](https://vectorbt.pro/pvt_41531c19/api/utils/schedule_/#vectorbtpro.utils.schedule_.CancelledError) (optional).

Do not forget to enable logging if needed:

```pycon
>>> import logging

>>> logging.basicConfig(level = logging.INFO)
```

Finally, run the `OHLCFigUpdater` job every 10 seconds:

```pycon
>>> fig = data.plot(ohlc_type="candlestick", plot_volume=False)
>>> fig  # (1)!

>>> with imageio.get_writer("ohlc_fig_updater.gif", duration=250, loop=0) as writer:  # (2)!
...     ohlc_fig_updater = OHLCFigUpdater(
...         data=data, 
...         fig=fig, 
...         writer=writer,
...         display_last=pd.Timedelta(minutes=10),
...         stop_after=pd.Timedelta(minutes=5)
...     )
...     ohlc_fig_updater.update_every(10)  # (3)!
INFO:vectorbtpro.utils.schedule_:Starting schedule manager with jobs [Every 10 seconds do update() (last run: [never], next run: 2022-02-19 21:18:38)]
INFO:vectorbtpro.data.updater:New data has 10 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:18:00+00:00
INFO:vectorbtpro.data.updater:New data has 10 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:18:00+00:00
INFO:vectorbtpro.data.updater:New data has 11 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:19:00+00:00
INFO:vectorbtpro.data.updater:New data has 11 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:19:00+00:00
INFO:vectorbtpro.data.updater:New data has 11 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:19:00+00:00
INFO:vectorbtpro.data.updater:New data has 11 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:19:00+00:00
INFO:vectorbtpro.data.updater:New data has 11 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:19:00+00:00
INFO:vectorbtpro.data.updater:New data has 12 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:20:00+00:00
INFO:vectorbtpro.data.updater:New data has 12 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:20:00+00:00
INFO:vectorbtpro.data.updater:New data has 12 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:20:00+00:00
INFO:vectorbtpro.data.updater:New data has 12 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:20:00+00:00
INFO:vectorbtpro.data.updater:New data has 12 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:20:00+00:00
INFO:vectorbtpro.data.updater:New data has 13 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:21:00+00:00
INFO:vectorbtpro.data.updater:New data has 13 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:21:00+00:00
INFO:vectorbtpro.data.updater:New data has 13 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:21:00+00:00
INFO:vectorbtpro.data.updater:New data has 13 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:21:00+00:00
INFO:vectorbtpro.data.updater:New data has 13 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:21:00+00:00
INFO:vectorbtpro.data.updater:New data has 14 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:22:00+00:00
INFO:vectorbtpro.data.updater:New data has 14 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:22:00+00:00
INFO:vectorbtpro.data.updater:New data has 14 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:22:00+00:00
INFO:vectorbtpro.data.updater:New data has 14 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:22:00+00:00
INFO:vectorbtpro.data.updater:New data has 14 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:22:00+00:00
INFO:vectorbtpro.data.updater:New data has 15 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:23:00+00:00
INFO:vectorbtpro.data.updater:New data has 15 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:23:00+00:00
INFO:vectorbtpro.data.updater:New data has 15 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:23:00+00:00
INFO:vectorbtpro.data.updater:New data has 15 rows from 2022-02-19 20:09:00+00:00 to 2022-02-19 20:23:00+00:00
INFO:vectorbtpro.utils.schedule_:Stopping schedule manager
```

1. Run these two lines in a separate cell to see the updates in real time.

2. Each frame is shown for 250 milliseconds (4 frames per second), and the GIF loops indefinitely.

3. Uses [DataUpdater.update_every](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater.update_every).

To stop the updater early, simply interrupt the execution.

!!! tip
    To run the job in the background, set `in_background` to True. You can then manually stop the
    execution by calling `ohlc_fig_updater.schedule_manager.stop()`.

After the data updater has finished running, you can access all the data:

```pycon
>>> ohlc_fig_updater.data.close
Open time
2022-02-19 20:09:00+00:00    40005.78
2022-02-19 20:10:00+00:00    40001.80
2022-02-19 20:11:00+00:00    40006.45
2022-02-19 20:12:00+00:00    40003.68
2022-02-19 20:13:00+00:00    40022.24
2022-02-19 20:14:00+00:00    40026.73
2022-02-19 20:15:00+00:00    40048.88
2022-02-19 20:16:00+00:00    40044.92
2022-02-19 20:17:00+00:00    40044.03
2022-02-19 20:18:00+00:00    40045.36
2022-02-19 20:19:00+00:00    40047.68
2022-02-19 20:20:00+00:00    40036.74
2022-02-19 20:21:00+00:00    40037.69
2022-02-19 20:22:00+00:00    40039.92
2022-02-19 20:23:00+00:00    40041.62
Freq: T, Name: Close, dtype: float64
```

Here is the generated GIF:

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/ohlc_fig_updater.light.gif#only-light){: .iimg loading=lazy style="width:700px" }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/data/ohlc_fig_updater.dark.gif#only-dark){: .iimg loading=lazy style="width:700px" }

!!! tip
    The smallest time unit for [ScheduleManager](https://vectorbt.pro/pvt_41531c19/api/utils/schedule_/#vectorbtpro.utils.schedule_.ScheduleManager)
    is one second. For high-precision job scheduling, use a loop with a timer.

## Saving

Regular updates with [DataUpdater](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater) keep all data
in memory at all times. But what if you do not need to access the entire dataset? What if your main
goal is to collect as much data as possible from an exchange and write each update directly to disk in a
tabular format instead of processing it? This approach allows you to create one script to write 
data updates to a file and another script to regularly read that file and perform a job.
Additionally, this method improves data collection resilience to errors because every new batch
of data is saved immediately.

If this workflow fits your needs, the [DataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver) 
class is an excellent choice for such tasks. It subclasses the 
[DataUpdater](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater) class and adds two abstract methods:
[DataSaver.init_save_data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.init_save_data) and 
[DataSaver.save_data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.save_data), which handle saving 
the initial data and each new batch of data, respectively, to a file.

The [DataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver) workflow is straightforward.
First, it requires a data instance `data` with some initial data. When you call 
[DataSaver.update_every](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.update_every) with 
`init_save=True`, it saves this data to a file using [DataSaver.init_save_data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.init_save_data).
Once the initial data is persisted, on each call to [DataSaver.update](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.update),
it fetches the next data update using [Data.update](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update)
with `concat=False` to avoid storing the entire dataset in memory. It then calls 
[DataSaver.save_data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.save_data) to **append** the new 
data to the file. This process repeats until the program is stopped by the user or the system.

There are two preset subclasses of [DataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver):

1. [CSVDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.CSVDataSaver) for writing data updates using 
[Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv).

2. [HDFDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.HDFDataSaver) for writing data updates using 
[Data.to_hdf](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_hdf).

Let's pull 1-minute `BTCUSDT` data from Binance and write it to a CSV file every 10 seconds:

```pycon
>>> data = vbt.BinanceData.pull(
...     "BTCUSDT", 
...     start="10 minutes ago UTC", 
...     end="now UTC", 
...     timeframe="1m"
... )

>>> csv_saver = vbt.CSVDataSaver(data)
>>> csv_saver.update_every(10, init_save=True)  # (1)!
INFO:vectorbtpro.data.saver:Saved initial 10 rows from 2022-02-21 23:25:00+00:00 to 2022-02-21 23:34:00+00:00
INFO:vectorbtpro.utils.schedule_:Starting schedule manager with jobs [Every 10 seconds do update() (last run: [never], next run: 2022-02-22 00:34:55)]
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:34:00+00:00 to 2022-02-21 23:34:00+00:00
INFO:vectorbtpro.data.saver:Saved 2 rows from 2022-02-21 23:34:00+00:00 to 2022-02-21 23:35:00+00:00
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:35:00+00:00 to 2022-02-21 23:35:00+00:00
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:35:00+00:00 to 2022-02-21 23:35:00+00:00
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:35:00+00:00 to 2022-02-21 23:35:00+00:00
```

1. Use `init_save` to tell VBT to save the initial data before updating.

!!! important
    If the initial data has not yet been saved to disk, pass `init_save=True` to save it first.
    Otherwise, only the subsequent updates will be saved!

!!! note
    Remember to set up logging as shown in the previous example to view log messages.

Now, let's interrupt the execution and take a look at the data in `csv_saver`:

```pycon
INFO:vectorbtpro.utils.schedule_:Stopping schedule manager

>>> csv_saver.data.close
Open time
2022-02-21 23:35:00+00:00    37185.95
Name: Close, dtype: float64
```

As you can see, unlike the data updater we used previously, the data saver keeps only the latest 
received batch of data in memory, which is needed for the next update. All previously fetched data 
is now stored in a CSV file. Let's take a look:

```pycon
>>> pd.read_csv("BTCUSDT.csv", index_col=0, parse_dates=True)["Close"]  # (1)!
Open time
2022-02-21 23:25:00+00:00    37247.29
2022-02-21 23:26:00+00:00    37296.44
2022-02-21 23:27:00+00:00    37190.16
2022-02-21 23:28:00+00:00    37135.50
2022-02-21 23:29:00+00:00    37186.11
2022-02-21 23:30:00+00:00    37056.19
2022-02-21 23:31:00+00:00    37079.47
2022-02-21 23:32:00+00:00    37181.49
2022-02-21 23:33:00+00:00    37288.48
2022-02-21 23:34:00+00:00    37209.83
2022-02-21 23:34:00+00:00    37240.21
2022-02-21 23:34:00+00:00    37245.64
2022-02-21 23:35:00+00:00    37248.12
2022-02-21 23:35:00+00:00    37213.55
2022-02-21 23:35:00+00:00    37168.76
2022-02-21 23:35:00+00:00    37185.95
Name: Close, dtype: float64
```

1. Use Pandas to view the file without any post-processing.

There are many duplicate index entries. Why is that? Remember, every time we request an update, we
try not only to fetch new data points but also to refresh the most recent ones. If we request
10 updates during a single 1-minute candle, we will get 10 different data points with the same timestamp.
Overwriting any row in a CSV file is very inefficient, as you would need to scan the entire
file just to remove one line. As a result, new data simply gets appended to the file. When we want
to fetch the full dataset, [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) will
automatically remove any duplicates:

```pycon
>>> vbt.CSVData.pull("BTCUSDT.csv").close
Open time
2022-02-21 23:25:00+00:00    37247.29
2022-02-21 23:26:00+00:00    37296.44
2022-02-21 23:27:00+00:00    37190.16
2022-02-21 23:28:00+00:00    37135.50
2022-02-21 23:29:00+00:00    37186.11
2022-02-21 23:30:00+00:00    37056.19
2022-02-21 23:31:00+00:00    37079.47
2022-02-21 23:32:00+00:00    37181.49
2022-02-21 23:33:00+00:00    37288.48
2022-02-21 23:34:00+00:00    37245.64
2022-02-21 23:35:00+00:00    37185.95
Freq: T, Name: Close, dtype: float64
```

To clean the CSV file from duplicates, read the data using 
[CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) and write it back:

```pycon
>>> vbt.CSVData.pull("BTCUSDT.csv").to_csv()

>>> pd.read_csv("BTCUSDT.csv", index_col=0, parse_dates=True)["Close"]
Open time
2022-02-21 23:25:00+00:00    37247.29
2022-02-21 23:26:00+00:00    37296.44
2022-02-21 23:27:00+00:00    37190.16
2022-02-21 23:28:00+00:00    37135.50
2022-02-21 23:29:00+00:00    37186.11
2022-02-21 23:30:00+00:00    37056.19
2022-02-21 23:31:00+00:00    37079.47
2022-02-21 23:32:00+00:00    37181.49
2022-02-21 23:33:00+00:00    37288.48
2022-02-21 23:34:00+00:00    37245.64
2022-02-21 23:35:00+00:00    37185.95
Name: Close, dtype: float64
```

!!! info
    The above step is optional and mainly helps save disk space.
    You should perform it only occasionally, mainly when exporting the CSV file to another 
    program for analysis.

You can resume the saving process at any time:

```pycon
>>> csv_saver.update_every(10)  # (1)!
INFO:vectorbtpro.utils.schedule_:Starting schedule manager with jobs [Every 10 seconds do update() (last run: [never], next run: 2022-02-22 00:35:55)]
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:35:00+00:00 to 2022-02-21 23:35:00+00:00
INFO:vectorbtpro.data.saver:Saved 2 rows from 2022-02-21 23:35:00+00:00 to 2022-02-21 23:36:00+00:00
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:36:00+00:00 to 2022-02-21 23:36:00+00:00
INFO:vectorbtpro.utils.schedule_:Stopping schedule manager
```

1. We do not need `init_save` here since this data is already on disk.

!!! note
    If your data provider offers only a limited time window of high-frequency data, avoid pausing
    the saving process for too long, or you may end up with missing data.

If you want to resume the saving process even after restarting your runtime, it is a good idea
to pickle and save the data saver instance to disk:

```pycon
>>> csv_saver.save("csv_saver")  # (1)!
```

1. Using [Pickleable.save](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.save).

You can then continue in a new runtime:

```pycon
>>> import logging
>>> logging.basicConfig(level = logging.INFO)

>>> from vectorbtpro import *
>>> csv_saver = vbt.CSVDataSaver.load("csv_saver")
>>> csv_saver.update_every(10)
INFO:vectorbtpro.utils.schedule_:Starting schedule manager with jobs [Every 10 seconds do update() (last run: [never], next run: 2022-02-22 00:36:45)]
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:36:00+00:00 to 2022-02-21 23:36:00+00:00
INFO:vectorbtpro.data.saver:Saved 1 rows from 2022-02-21 23:36:00+00:00 to 2022-02-21 23:36:00+00:00
INFO:vectorbtpro.data.saver:Saved 2 rows from 2022-02-21 23:36:00+00:00 to 2022-02-21 23:37:00+00:00
INFO:vectorbtpro.utils.schedule_:Stopping schedule manager
```

How can we specify exactly where the data should be stored? [DataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver)
accepts two arguments: `save_kwargs` and `init_save_kwargs`, which are passed to 
[DataSaver.save_data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.save_data) and 
[DataSaver.init_save_data](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.DataSaver.init_save_data), respectively.
For example, in [CSVDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.CSVDataSaver), these keyword arguments
are further forwarded to [Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv).
To change the directory path, for instance, you can simply do:

```pycon
>>> csv_saver = vbt.CSVDataSaver(
...     save_kwargs=dict(
...         path_or_buf="data",
...         mkdir_kwargs=dict(mkdir=True)
...     )
... )
```

However, this is not the only way to provide keyword arguments for saving. If you look at the arguments
accepted by the method [DataUpdater.update_every](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater.update_every),
you will see `save_kwargs` and `init_save_kwargs` again, which are passed down to their respective methods.
These arguments have higher priority and override any arguments with the same name provided to the constructor.
This approach allows you to customize how the data is saved each time you resume the operation.

The same principle applies to [HDFDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.saver.HDFDataSaver).

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/data/scheduling.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/indicators/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
children:
  - https://vectorbt.pro/pvt_41531c19/documentation/indicators/development/
  - https://vectorbt.pro/pvt_41531c19/documentation/indicators/analysis/
  - https://vectorbt.pro/pvt_41531c19/documentation/indicators/parsers/
name: Indicators
type: page
icon: material-chart-timeline-variant
---

# Indicators

The [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) class is 
one of the most powerful components in the VBT ecosystem. It can wrap any indicator function, 
making it both parameterizable and analyzable.

## Pipeline

An indicator is a pipeline that performs the following steps:

* Accepts input arrays (for example, opening and closing prices).

* Accepts parameters either as scalars or arrays (for example, window size).

* Accepts other relevant arguments and keyword arguments.

* Broadcasts input arrays against each other or to a specific shape.

* Broadcasts parameters against each other to create a fixed set of parameter combinations.

* Calculates output arrays for each parameter combination using the input arrays,
producing the same shape (for example, a rolling average).

* Concatenates the output arrays of all parameter combinations along the columns.

* Converts the results back to the Pandas format.

Let's manually create an indicator that takes two time series, computes their normalized moving
averages, and returns the difference between the two. We will test different shapes as well as 
parameter combinations to see how broadcasting can be leveraged:

```pycon
>>> from vectorbtpro import *

>>> def mov_avg_crossover(ts1, ts2, w1, w2):
...     ts1, ts2 = vbt.broadcast(ts1, ts2)  # (1)!
...
...     w1, w2 = vbt.broadcast(  # (2)!
...         vbt.to_1d_array(w1), 
...         vbt.to_1d_array(w2))
...
...     ts1_mas = []
...     for w in w1:
...         ts1_mas.append(ts1.vbt.rolling_mean(w) / ts1)  # (3)!
...     ts2_mas = []
...     for w in w2:
...         ts2_mas.append(ts2.vbt.rolling_mean(w) / ts2)
...
...     ts1_ma = pd.concat(ts1_mas, axis=1)  # (4)!
...     ts2_ma = pd.concat(ts2_mas, axis=1)
...
...     ts1_ma.columns = vbt.combine_indexes((  # (5)!
...         pd.Index(w1, name="ts1_window"), 
...         ts1.columns))
...     ts2_ma.columns = vbt.combine_indexes((
...         pd.Index(w2, name="ts2_window"), 
...         ts2.columns))
...
...     return ts1_ma.vbt - ts2_ma  # (6)!

>>> def generate_index(n):  # (7)!
...     return vbt.date_range("2020-01-01", periods=n)

>>> ts1 = pd.Series([1, 2, 3, 4, 5, 6, 7], index=generate_index(7))
>>> ts2 = pd.DataFrame({
...     'a': [5, 4, 3, 2, 3, 4, 5],
...     'b': [2, 3, 4, 5, 4, 3, 2]
... }, index=generate_index(7))
>>> w1 = 2
>>> w2 = [3, 4]

>>> mov_avg_crossover(ts1, ts2, w1, w2)
ts1_window                                       2
ts2_window                   3                   4
                    a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429
```

1. Both time series are converted to Pandas objects with the same shape: `(7, 2)`.

2. Window `w1` becomes `[2, 2]` and window `w2` becomes `[3, 4]`. This results in two parameter
combinations: `(2, 3)` and `(2, 4)`.

3. Calculate the normalized moving average for each window.

4. Concatenate the DataFrames along the column axis.

5. Create a new column hierarchy using the window values as the top level.

6. Calculate the difference between the two concatenated DataFrames.

7. Helper function to generate a datetime-like index.

Pretty neat! We just built a flexible pipeline that can handle arbitrary input and parameter 
combinations. The resulting DataFrame shows each column as a specific window combination applied 
to each column in both `ts1` and `ts2`. But is this pipeline user-friendly? :thinking:  
Dealing with broadcasting, output concatenation, and column hierarchies makes this process 
very similar to working with regular Pandas code.

The pipeline above can be easily standardized using 
[IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline).
This method conveniently prepares inputs, parameters, and columns. However, you still need to perform 
the calculation and output concatenation yourself by providing a `custom_func`.  
Let's update the example:

```pycon
>>> def custom_func(ts1, ts2, w1, w2):
...     ts1_mas = []
...     for w in w1:
...         ts1_mas.append(vbt.nb.rolling_mean_nb(ts1, w) / ts1)  # (1)!
...     ts2_mas = []
...     for w in w2:
...         ts2_mas.append(vbt.nb.rolling_mean_nb(ts2, w) / ts2)
...
...     ts1_ma = np.column_stack(ts1_mas)  # (2)!
...     ts2_ma = np.column_stack(ts2_mas)
...
...     return ts1_ma - ts2_ma  # (3)!

>>> outputs = vbt.IndicatorBase.run_pipeline(
...     num_ret_outputs=1,
...     custom_func=custom_func,
...     inputs=dict(ts1=ts1, ts2=ts2),
...     params=dict(w1=w1, w2=w2)
... )
>>> outputs
(<vectorbtpro.base.wrapping.ArrayWrapper at 0x7fb188993160>,
  [array([[1, 1],
          [2, 2],
          [3, 3],
          [4, 4],
          [5, 5],
          [6, 6],
          [7, 7]]),
  array([[5, 2],
          [4, 3],
          [3, 4],
          [2, 5],
          [3, 4],
          [4, 3],
          [5, 2]])],
  array([0, 1, 0, 1]),
  [],
  [array([[        nan,         nan,         nan,         nan],
          [        nan,         nan,         nan,         nan],
          [-0.5       ,  0.08333333,         nan,         nan],
          [-0.625     ,  0.075     , -0.875     ,  0.175     ],
          [ 0.01111111, -0.18333333, -0.1       , -0.1       ],
          [ 0.16666667, -0.41666667,  0.16666667, -0.41666667],
          [ 0.12857143, -0.57142857,  0.22857143, -0.82142857]])],
  [[2, 2], [3, 4]],
  [Int64Index([2, 2, 2, 2], dtype='int64'),
  Int64Index([3, 3, 4, 4], dtype='int64')],
  [])
```

1. Use [rolling_mean_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_mean_nb).

2. Concatenate the NumPy arrays along the column axis.

3. Calculate the difference between the two concatenated NumPy arrays.

With much less code, we performed the entire calculation using only NumPy and Numba—a big win!
But what is this complex output?

This raw output is designed for internal use by VBT and is not intended for direct use. It contains 
metadata necessary for working with the indicator. Additionally, if you review the source of this 
function, you will see that it accepts many different arguments. This complexity provides great 
flexibility, as each argument corresponds to a specific step in the pipeline. But do not worry: we 
will not use this function directly.

## Factory

Instead, we will use [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory),
which simplifies [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
by providing a unified interface and various automations. Let's use the factory 
to wrap our `custom_func`:

```pycon
>>> MADiff = vbt.IF(
...     class_name='MADiff',
...     input_names=['ts1', 'ts2'],
...     param_names=['w1', 'w2'],
...     output_names=['diff'],
... ).with_custom_func(custom_func)

>>> madiff = MADiff.run(ts1, ts2, w1, w2)
>>> madiff.diff
madiff_w1                                        2
madiff_w2                    3                   4
                    a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429
```

!!! tip
    `vbt.IF` is a shortcut for [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory).

As you can see, [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) 
takes the specification for our indicator and creates a Python class that knows how to communicate with 
[IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline) 
and manage and format its results. Specifically, it attaches the class method `MADiff.run`, which 
works just like `custom_func` but prepares and forwards all arguments to 
[IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline) 
under the hood. Whenever we call the `run` method, it initializes and returns an instance of `MADiff` 
containing all the input and output data.

You may wonder: *"Why does the factory create a class instead of a function? Wouldn't 
an indicator function be more intuitive?"* If you have read [Building blocks](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks),
you may already be familiar with the class [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),
the main class for analyzing data. The indicator class created by the factory is a subclass of 
[Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable), so you not only 
get access to the output, but also to many methods for analyzing this output. For example,
the factory automatically provides `crossed_above`, `cross_below`, `stats`, and many other 
methods for each input and output in the indicator:

```pycon
>>> madiff.diff_stats(column=(2, 3, 'a'))
Start        2020-01-01 00:00:00
End          2020-01-07 00:00:00
Period           7 days 00:00:00
Count                          5
Mean                    -0.16373
Std                     0.371153
Min                       -0.625
Median                  0.011111
Max                     0.166667
Min Index    2020-01-04 00:00:00
Max Index    2020-01-06 00:00:00
Name: (2, 3, a), dtype: object
```

### Workflow

The main goal of [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
is to create a stand-alone indicator class that includes a `run` method for executing the indicator. 
To accomplish this, it needs to know what inputs, parameters, and outputs to expect. You can provide 
this information using `input_names`, `param_names`, and other arguments in the constructor:

```pycon
>>> MADiff_factory = vbt.IF(
...     class_name='MADiff',
...     input_names=['ts1', 'ts2'],
...     param_names=['w1', 'w2'],
...     output_names=['diff'],
... )
>>> MADiff_factory.Indicator
vectorbtpro.indicators.factory.MADiff
```

When initialized, it builds the skeleton of our indicator class, which is a type of
[IndicatorBase](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase). This is accessible via 
[IndicatorFactory.Indicator](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.Indicator).
Even though the factory creates the constructor for this class and attaches various
properties and methods, we cannot run the indicator yet:

```pycon
>>> MADiff_factory.Indicator.run()
NotImplementedError: 
```

This is because we still need to provide the calculation function. There are several methods 
starting with the prefix `with_`. The fundamental method, used by all others, is 
[IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func)
(which we used earlier). It overrides the abstract `run` method to execute the indicator using 
[IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
and returns a fully functional indicator class:

```pycon
>>> MADiff = MADiff_factory.with_custom_func(custom_func)
>>> MADiff
vectorbtpro.indicators.factory.MADiff
```

The calculation function has now been successfully attached, so we can run this indicator!

## Factory methods

Factory methods come in two forms: instance and class methods. Instance methods, with the prefix `with_`, 
such as [IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func),
require instantiating the indicator factory. This means you need to call `vbt.IF(...)` and manually provide 
the required information as we did with `MADiff`.  
Class methods, with the prefix `from_`, such as [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr),
can (semi-)automatically parse the required information.

### From custom function

The method [IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func)
accepts a "custom function", which is the most flexible way to define an indicator. However, with this
flexibility comes added responsibility: as the user, you must handle iterating through parameters, manage
caching, and concatenate columns for each parameter combination (usually using
[apply_and_concat](https://vectorbt.pro/pvt_41531c19/api/base/combining/#vectorbtpro.base.combining.apply_and_concat)). You must also ensure
that each output array has the correct number of columns, equal to the number of columns in the input
arrays multiplied by the number of parameter combinations. In addition, your custom function receives
commands passed by the pipeline, so it is up to you to properly process those commands.

For example, if your custom function needs the index and columns alongside the NumPy arrays, you can
instruct the pipeline to pass the wrapper by setting `pass_wrapper=True` in `with_custom_func`. This and
all other arguments are forwarded directly to
[IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline),
which handles communication with your custom function.

### From apply function

The method [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func)
greatly simplifies indicator development. It creates a `custom_func` that handles caching, iteration over
parameters with [apply_and_concat](https://vectorbt.pro/pvt_41531c19/api/base/combining/#vectorbtpro.base.combining.apply_and_concat),
output concatenation with [column_stack](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.column_stack), and
then passes this function to [IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func).
Your only task is to write an "apply function," which accepts a single parameter combination and performs
the calculation. The resulting outputs are automatically concatenated along the column axis.

!!! note
    An apply function has mostly the same signature as a custom function, but the parameters
    are individual values instead of multiple values.

Let's create our indicator using an apply function:

```pycon
>>> def apply_func(ts1, ts2, w1, w2):
...     ts1_ma = vbt.nb.rolling_mean_nb(ts1, w1) / ts1
...     ts2_ma = vbt.nb.rolling_mean_nb(ts2, w2) / ts2
...     return ts1_ma - ts2_ma

>>> MADiff = vbt.IF(
...     class_name='MADiff',
...     input_names=['ts1', 'ts2'],
...     param_names=['w1', 'w2'],
...     output_names=['diff'],
... ).with_apply_func(apply_func)

>>> madiff = MADiff.run(ts1, ts2, w1, w2)
>>> madiff.diff
madiff_w1                                        2
madiff_w2                    3                   4
                    a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429
```

That's all you need! Under the hood, the code creates a custom function that iterates over all
parameter combinations and calls `apply_func` on each one. If you print `ts1`, `ts2`, `w1`, and `w2`,
you would see that `ts1` and `ts2` remain the same, while `w1` and `w2` are now individual values.
This design allows you to simplify your code, working with one set of parameters at a time without 
worrying about multiple parameter combinations.

Another advantage of this method is that apply functions are a natural fit in VBT :monkey:, so you
can use most regular and Numba-compiled functions that take two-dimensional NumPy arrays directly as
apply functions. For example, let's build an indicator for rolling covariance:

```pycon
>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(vbt.nb.rolling_cov_nb)

>>> rollcov = RollCov.run(ts1, ts2, [2, 3])
>>> rollcov.rollcov
rollcov_w            2                   3
                a     b         a         b
2020-01-01   NaN   NaN       NaN       NaN
2020-01-02 -0.25  0.25       NaN       NaN
2020-01-03 -0.25  0.25 -0.666667  0.666667
2020-01-04 -0.25  0.25 -0.666667  0.666667
2020-01-05  0.25 -0.25  0.000000  0.000000
2020-01-06  0.25 -0.25  0.666667 -0.666667
2020-01-07  0.25 -0.25  0.666667 -0.666667
```

In this example, both input arrays and the window parameter are passed directly to
[rolling_cov_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_cov_nb).

#### Custom iteration

We can easily emulate `apply_func` using `custom_func` and
[apply_and_concat](https://vectorbt.pro/pvt_41531c19/api/base/combining/#vectorbtpro.base.combining.apply_and_concat).
For example, if we need the index of the current iteration or want access to all parameter
combinations:

```pycon
>>> from vectorbtpro.base.combining import apply_and_concat

>>> def apply_func(i, ts1, ts2, w):  # (1)!
...     return vbt.nb.rolling_cov_nb(ts1, ts2, w[i])

>>> def custom_func(ts1, ts2, w):
...     return apply_and_concat(len(w), apply_func, ts1, ts2, w)  # (2)!

>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_custom_func(custom_func)
```

1. Unlike our previous `apply_func`, an apply function used in
[apply_and_concat](https://vectorbt.pro/pvt_41531c19/api/base/combining/#vectorbtpro.base.combining.apply_and_concat)
must take the index of the iteration and select the parameters manually using this index.

2. [apply_and_concat](https://vectorbt.pro/pvt_41531c19/api/base/combining/#vectorbtpro.base.combining.apply_and_concat)
requires the number of iterations, which is simply the length of any parameter array.

The same result can be achieved using [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func)
and `select_params=False`:

```pycon
>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(apply_func, select_params=False)
```

#### Execution

Since the same apply function is called multiple times—once per parameter combination—we can
use one of VBT's preset execution engines to distribute these calls sequentially (default),
across multiple threads, or across multiple processes. In fact, the function
[apply_and_concat](https://vectorbt.pro/pvt_41531c19/api/base/combining/#vectorbtpro.base.combining.apply_and_concat),
which is used to iterate over all parameter combinations, handles this automatically by forwarding
all calls to the executor function [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute).
By passing keyword arguments in `execute_kwargs`, we can define how to distribute these calls.
For example, to disable the progress bar:

```pycon
>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(vbt.nb.rolling_cov_nb)

>>> RollCov.run(
...     ts1, ts2, np.full(100, 2),
...     execute_kwargs=dict(show_progress=False)
... )
```

[=100% "Iteration 100/100"]{: .candystripe .candystripe-animate }

#### Numba

When the apply function is Numba-compiled, the indicator factory also makes the parameter selection
function Numba-compiled (with the GIL released), allowing for multithreading.
This behavior can be disabled by setting `jit_select_params` to False. The keyword
arguments used to configure the Numba-compiled function can be supplied via the `jit_kwargs` argument.

!!! note
    Setting `jit_select_params` will remove all keyword arguments since variable keyword arguments
    are not supported by Numba (yet). To pass keyword arguments to the apply function anyway,
    set `remove_kwargs` to False or use the `kwargs_as_args` argument, which specifies which keyword
    arguments should be supplied as (variable) positional arguments.

Additionally, you can explicitly set `jitted_loop` to True to loop over each parameter combination
in a Numba loop. This can speed up iteration for shallow inputs with a large number of columns,
but may slow it down otherwise.

!!! note
    In this case, the execution will be performed by Numba, so you cannot use `execute_kwargs` anymore.

#### Debugging

Sometimes it is not clear which arguments are being passed to `apply_func`.
Debugging in this case is usually simple: just replace your apply function with a generic
function that accepts variable arguments and prints them.

```pycon
>>> def apply_func(*args, **kwargs):
...     for i, arg in enumerate(args):
...         print("arg {}: {}".format(i, type(arg)))
...     for k, v in kwargs.items():
...         print("kwarg {}: {}".format(k, type(v)))
...     raise NotImplementedError

>>> RollCov = vbt.IF(
...     class_name='RollCov',
...     input_names=['ts1', 'ts2'],
...     param_names=['w'],
...     output_names=['rollcov'],
... ).with_apply_func(apply_func, select_params=False)

>>> try:
...     RollCov.run(ts1, ts2, [2, 3], some_arg="some_value")
... except:
...     pass
arg 0: <class 'int'>
arg 1: <class 'numpy.ndarray'>
arg 2: <class 'numpy.ndarray'>
arg 3: <class 'list'>
kwarg some_arg: <class 'str'>
```

### From parsing

Parsers offer the most convenient way to build indicator classes. For example, there are dedicated
parser methods for third-party technical analysis packages that can automatically or semi-automatically
derive the specification of each indicator. Additionally, a powerful expression parser can help you avoid
writing complex Python functions for simple indicators. Let's *express* our indicator as an expression:

```pycon
>>> MADiff = vbt.IF.from_expr(
...     "rolling_mean(@in_ts1, @p_w1) / @in_ts1 - rolling_mean(@in_ts2, @p_w2) / @in_ts2",
...     factory_kwargs=dict(class_name="MADiff")  # (1)!
... )
>>> madiff = MADiff.run(ts1, ts2, w1, w2)
>>> madiff.out
madiff_w1                                        2
madiff_w2                    3                   4
                    a         b         a         b
2020-01-01       NaN       NaN       NaN       NaN
2020-01-02       NaN       NaN       NaN       NaN
2020-01-03 -0.500000  0.083333       NaN       NaN
2020-01-04 -0.625000  0.075000 -0.875000  0.175000
2020-01-05  0.011111 -0.183333 -0.100000 -0.100000
2020-01-06  0.166667 -0.416667  0.166667 -0.416667
2020-01-07  0.128571 -0.571429  0.228571 -0.821429
```

1. We can still override any information passed to the factory class.

Notice that we did not need to call `vbt.IF(...)`? [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr)
is a class method that parses `input_names` and other information directly from the expression and
creates a factory instance using only this information. It is amazing how we reduced our first
implementation with `mov_avg_crossover` to just this while still enjoying all the features, right?

## Run methods

Once you have built your indicator class, it is time to run it. The primary method for executing an
indicator is the class method [IndicatorBase.run](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run).
This method accepts positional and keyword arguments based on the specifications provided to the
[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory).
These arguments include input arrays, in-place output arrays, and parameters. Any additional arguments
are forwarded down to [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline), 
which may use them to set up the pipeline or forward them further down to the custom function
and, if provided, the apply function.

To see which arguments the `run` method accepts, use [phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp):

```pycon
>>> vbt.phelp(MADiff.run)
MADiff.run(
    ts1,
    ts2,
    w1,
    w2,
    short_name='madiff',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `MADiff` indicator.
    

    * Inputs: `ts1`, `ts2`

    * Parameters: `w1`, `w2`

    * Outputs: `out`
    
    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.
    
    Other keyword arguments are passed to `MADiff.run_pipeline`.
```

We can see that `MADiff.run` takes two input time series, `ts1` and `ts2`, two parameters, `w1` and `w2`,
and produces a single output time series, `diff`. When you call the class method, it runs the indicator
and returns a new instance of `MADiff` with all data ready for analysis. Specifically,
you can access the output as a regular instance attribute, `MADiff.diff`.

The second method for running indicators is [IndicatorBase.run_combs](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_combs).
This method accepts the same inputs as the method above, but computes all combinations of the given
parameters using a combinatorial function and returns **multiple** indicator instances that can be
combined with each other. This is useful for comparing multiple indicators of the **same type** but with
different parameters, such as when testing a moving average crossover, which involves two
[MA](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/ma/#vectorbtpro.indicators.custom.ma.MA) instances applied to the same time series:

```pycon
>>> ts = pd.Series([3, 2, 1, 2, 3])
>>> fast_ma, slow_ma = vbt.MA.run_combs(
...     ts, [2, 3, 4], 
...     short_names=['fast_ma', 'slow_ma'])
>>> fast_ma.ma_crossed_above(slow_ma)
fast_ma_window             2      3
slow_ma_window      3      4      4
0               False  False  False
1               False  False  False
2               False  False  False
3               False  False  False
4                True   True  False
```

In the example above, [MA.run_combs](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/ma/#vectorbtpro.indicators.custom.MA.ma.run_combs)
generated the combinations of `window` using [itertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations)
with `r=2`. The first set of window combinations was passed to the first instance, and the second set
to the second instance. The same example can be replicated using only the `run` method:

```pycon
>>> windows = [2, 3, 4]
>>> fast_windows, slow_windows = zip(*combinations(windows, 2))
>>> fast_ma = vbt.MA.run(ts, fast_windows, short_name='fast_ma')
>>> slow_ma = vbt.MA.run(ts, slow_windows, short_name='slow_ma')
>>> fast_ma.ma_crossed_above(slow_ma)
fast_ma_window             2      3
slow_ma_window      3      4      4
0               False  False  False
1               False  False  False
2               False  False  False
3               False  False  False
4                True   True  False
```

The main advantage of a single `run_combs` call over multiple `run` calls is that it does not need to
re-compute each combination, thanks to smart caching.

!!! note
    `run_combs` should only be used for combining multiple indicators. To test multiple
    parameter combinations, use `run` and provide parameters as lists.

## Preset indicators

VBT provides a collection of preset, fully Numba-compiled indicators (such as 
[ATR](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/atr/#vectorbtpro.indicators.custom.atr.ATR)) that benefit from 
manual caching, extension, and plotting. You can use them as inspiration
for how to create indicators in a classic yet efficient way.

!!! note
    VBT uses SMA and EMA, while other technical analysis libraries and TradingView use
    Wilder's method. There is no right or wrong method.
    See [different smoothing methods](https://www.macroption.com/atr-calculation/).

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/indicators/index.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/indicators/development/
parent: https://vectorbt.pro/pvt_41531c19/documentation/indicators/
name: Development
type: page
icon: material-order-bool-descending-variant
---

# Development

VBT offers a wide range of functions and arguments to simplify indicator development. All you
need is an indicator function and a way to specify how it should be handled.

## Parameters

[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) allows
you to define any parameter grids you need. An indicator can have one or more parameters. Each
parameter can accept one or more values, which can be scalars (such as integers), arrays, or any
other objects.

If an indicator has multiple parameters, and one or more of them have several values, their values
will broadcast together. For example, if the parameter `w1` has only a single value `2` and the
parameter `w2` has two values `3` and `4`, then `w1` will be stretched to two values: `2` and `2`.
This allows the indicator to [zip](https://realpython.com/python-zip-function/) both parameters and
create two parameter combinations: `(2, 3)` and `(2, 4)`. The indicator will then iterate through
these combinations and apply a function to each one. The example below illustrates broadcasting:

```pycon
>>> from vectorbtpro import *

>>> def broadcast_params(*params):
...     return list(zip(*vbt.broadcast(*[vbt.to_1d_array(p) for p in params])))

>>> broadcast_params(2, 3)
[(2, 3)]

>>> broadcast_params([2, 3], 4)
[(2, 4), (3, 4)]

>>> broadcast_params(2, [3, 4])
[(2, 3), (2, 4)]

>>> broadcast_params([2, 3], [4, 5])
[(2, 4), (3, 5)]

>>> broadcast_params([2, 3], [4, 5, 6])
ValueError: Could not broadcast shapes: {0: (2,), 1: (3,)}
```

!!! note
    Do not confuse broadcasting with a product operation. The product of `[2, 3]` and `[4, 5]`
    would result in 4 combinations: `[2, 4]`, `[2, 5]`, `[3, 4]`, and `[3, 5]`. In broadcasting,
    smaller arrays are simply stretched to match the length of larger arrays for zipping.

To illustrate how parameters are used in indicators, here is a simple example. This indicator returns 1
when the rolling mean is above an upper bound, -1 when it is below a lower bound, and 0 when it is
between the upper and lower bounds:

```pycon
>>> def apply_func(ts, window, lower, upper):
...     out = np.full_like(ts, np.nan, dtype=float_)
...     ts_mean = vbt.nb.rolling_mean_nb(ts, window)
...     out[ts_mean >= upper] = 1
...     out[ts_mean <= lower] = -1
...     out[(ts_mean > lower) & (ts_mean < upper)] = 0
...     return out

>>> Bounded = vbt.IF(
...     class_name="Bounded",
...     input_names=['ts'],
...     param_names=['window', 'lower', 'upper'],
...     output_names=['out']
... ).with_apply_func(apply_func)

>>> def generate_index(n):
...     return vbt.date_range("2020-01-01", periods=n)

>>> ts = pd.DataFrame({
...     'a': [5, 4, 3, 2, 3, 4, 5],
...     'b': [2, 3, 4, 5, 4, 3, 2]
... }, index=generate_index(7))
>>> bounded = Bounded.run(ts, 2, 3, 5)
```

To retrieve the list of parameter names:

```pycon
>>> bounded.param_names
('window', 'lower', 'upper')
```

The broadcasted values of each parameter are accessible as attributes of the indicator instance,
using the parameter name followed by `_list`:

```pycon
>>> bounded.window_list
[2]
```

By default, when `per_column` is set to False, each parameter combination is applied to every
column in the input. For example, if your input array has 20 columns and you want to test 5
parameter combinations, you will get `20 * 5 = 100` columns in total.

A single parameter combination:

```pycon
>>> Bounded.run(
...     ts,
...     window=2,
...     lower=3,
...     upper=5
... ).out  # (1)!
bounded_window           2
bounded_lower            3
bounded_upper            5
                    a    b
2020-01-01        NaN  NaN
2020-01-02        0.0 -1.0
2020-01-03        0.0  0.0
2020-01-04       -1.0  0.0
2020-01-05       -1.0  0.0
2020-01-06        0.0  0.0
2020-01-07        0.0 -1.0
```

1. `2 * 1 = 2` output columns.

Multiple parameter combinations:

```pycon
>>> Bounded.run(
...     ts,
...     window=[2, 3],
...     lower=3,
...     upper=5
... ).out  # (1)!
bounded_window           2         3
bounded_lower            3         3
bounded_upper            5         5
                    a    b    a    b
2020-01-01        NaN  NaN  NaN  NaN
2020-01-02        0.0 -1.0  NaN  NaN
2020-01-03        0.0  0.0  0.0 -1.0
2020-01-04       -1.0  0.0 -1.0  0.0
2020-01-05       -1.0  0.0 -1.0  0.0
2020-01-06        0.0  0.0 -1.0  0.0
2020-01-07        0.0 -1.0  0.0 -1.0
```

1. `2 * 2 = 4` output columns.

Product of parameter combinations:

```pycon
>>> Bounded.run(
...     ts,
...     window=[2, 3],
...     lower=[3, 4],
...     upper=5,
...     param_product=True
... ).out  # (1)!
bounded_window                     2                   3
bounded_lower            3         4         3         4
bounded_upper            5         5         5         5
                    a    b    a    b    a    b    a    b
2020-01-01        NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
2020-01-02        0.0 -1.0  0.0 -1.0  NaN  NaN  NaN  NaN
2020-01-03        0.0  0.0 -1.0 -1.0  0.0 -1.0 -1.0 -1.0
2020-01-04       -1.0  0.0 -1.0  0.0 -1.0  0.0 -1.0 -1.0
2020-01-05       -1.0  0.0 -1.0  0.0 -1.0  0.0 -1.0  0.0
2020-01-06        0.0  0.0 -1.0 -1.0 -1.0  0.0 -1.0 -1.0
2020-01-07        0.0 -1.0  0.0 -1.0  0.0 -1.0 -1.0 -1.0
```

1. `2 * 2 * 2 = 8` output columns.

You can build more complex parameter combinations using the
[generate_param_combs](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.generate_param_combs) function.
For example, if the lower bound should always remain below the upper bound, you can control this
relationship using [itertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations).
After that, you can create a Cartesian product with the window using
[itertools.product](https://docs.python.org/3/library/itertools.html#itertools.product).

```pycon
>>> bound_combs_op = (combinations, [3, 4, 5], 2)
>>> product_op = (product, [2, 3], bound_combs_op)
>>> windows, lowers, uppers = vbt.generate_param_combs(product_op)  # (1)!

>>> Bounded.run(
...     ts,
...     window=windows,
...     lower=lowers,
...     upper=uppers
... ).out  # (2)!
bounded_window                               2                             3
bounded_lower                      3         4                   3         4
bounded_upper            4         5         5         4         5         5
                    a    b    a    b    a    b    a    b    a    b    a    b
2020-01-01        NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN  NaN
2020-01-02        1.0 -1.0  0.0 -1.0  0.0 -1.0  NaN  NaN  NaN  NaN  NaN  NaN
2020-01-03        0.0  0.0  0.0  0.0 -1.0 -1.0  1.0 -1.0  0.0 -1.0 -1.0 -1.0
2020-01-04       -1.0  1.0 -1.0  0.0 -1.0  0.0 -1.0  1.0 -1.0  0.0 -1.0 -1.0
2020-01-05       -1.0  1.0 -1.0  0.0 -1.0  0.0 -1.0  1.0 -1.0  0.0 -1.0  0.0
2020-01-06        0.0  0.0  0.0  0.0 -1.0 -1.0 -1.0  1.0 -1.0  0.0 -1.0 -1.0
2020-01-07        1.0 -1.0  0.0 -1.0  0.0 -1.0  1.0 -1.0  0.0 -1.0 -1.0 -1.0
```

1. Each tuple represents a combinatoric operation. Tuples can accept other tuples as inputs.

2. `2 * 2 * 2 * 3 / 2 = 12` output columns.

One parameter combination per column:

```pycon
>>> Bounded.run(
...     ts,
...     window=[2, 3],
...     lower=[3, 4],
...     upper=5,
...     per_column=True
... ).out  # (1)!
bounded_window      2    3
bounded_lower       3    4
bounded_upper       5    5
                    a    b
2020-01-01        NaN  NaN
2020-01-02        0.0  NaN
2020-01-03        0.0 -1.0
2020-01-04       -1.0 -1.0
2020-01-05       -1.0  0.0
2020-01-06        0.0 -1.0
2020-01-07        0.0 -1.0
```

1. The number of output columns matches the number of input columns.

### Defaults

Any argument passed to [IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func)
that is not listed among the arguments of [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
is intended to be used as a default argument for the calculation function. Since most methods, including
[IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func),
call this method, you can easily set parameter defaults by passing them along with the function:

```pycon
>>> Bounded = vbt.IF(
...     class_name="Bounded",
...     input_names=['ts'],
...     param_names=['window', 'lower', 'upper'],
...     output_names=['out']
... ).with_apply_func(apply_func, window=2, lower=3, upper=4)

>>> Bounded.run(ts).out
              a    b
2020-01-01  NaN  NaN
2020-01-02  1.0 -1.0
2020-01-03  0.0  0.0
2020-01-04 -1.0  1.0
2020-01-05 -1.0  1.0
2020-01-06  0.0  0.0
2020-01-07  1.0 -1.0

>>> Bounded.run(ts, upper=[5, 6]).out
bounded_upper           5         6
                    a    b    a    b
2020-01-01       NaN  NaN  NaN  NaN
2020-01-02       0.0 -1.0  0.0 -1.0
2020-01-03       0.0  0.0  0.0  0.0
2020-01-04      -1.0  0.0 -1.0  0.0
2020-01-05      -1.0  0.0 -1.0  0.0
2020-01-06       0.0  0.0  0.0  0.0
2020-01-07       0.0 -1.0  0.0 -1.0
```

The reason why the parameters `window` and `lower` do not appear in the column hierarchy above
is that default values are hidden by default. To display them, set `hide_default` to False:

```pycon
>>> Bounded.run(ts, hide_default=False).out
bounded_window           2     
bounded_lower            3     
bounded_upper            4     
                    a    b
2020-01-01        NaN  NaN
2020-01-02        1.0 -1.0
2020-01-03        0.0  0.0
2020-01-04       -1.0  1.0
2020-01-05       -1.0  1.0
2020-01-06        0.0  0.0
2020-01-07        1.0 -1.0
```

### Array-like

Some parameters are intended to be specified per row, column, or element of the input.
By default, if you pass a parameter value as an array, the indicator will treat this array
as a list of multiple values, one per input. To make the indicator interpret this array as a single value,
set the flag `is_array_like` to True in `param_settings`. To automatically broadcast the parameter value
to the input shape, set `bc_to_input` to True, 0 (index axis), or 1 (column axis).

In our example, the parameter `window` can be broadcast per column, and both parameters
`lower` and `upper` can be broadcast per element. To enable this, we need
to rewrite `apply_func` to apply the rolling mean on each column instead of the entire input:

```pycon
>>> def apply_func(ts, window, lower, upper):  # (1)!
...     out = np.full_like(ts, np.nan, dtype=float_)
...     ts_means = []
...     for col in range(ts.shape[1]):
...         ts_means.append(vbt.nb.rolling_mean_1d_nb(ts[:, col], window[col]))
...     ts_mean = np.column_stack(ts_means)
...     out[ts_mean >= upper] = 1
...     out[ts_mean <= lower] = -1
...     out[(ts_mean > lower) & (ts_mean < upper)] = 0
...     return out

>>> Bounded = vbt.IF(
...     class_name="Bounded",
...     input_names=['ts'],
...     param_names=['window', 'lower', 'upper'],
...     output_names=['out']
... ).with_apply_func(
...     apply_func,
...     param_settings=dict(
...         window=dict(is_array_like=True, bc_to_input=1, per_column=True),
...         lower=dict(is_array_like=True, bc_to_input=True),
...         upper=dict(is_array_like=True, bc_to_input=True)
...     )
... )
```

1. All parameters are now arrays that can broadcast against `ts`.

Both bound parameters can now be passed as a scalar (one value for the entire input), a one-dimensional
array (one value per row or column, depending on whether the input is a Series or a DataFrame),
a two-dimensional array (one value per element), or a list containing any of these. This approach allows
for maximum parameter flexibility.

For example, let's build a grid of two parameter combinations:

```pycon
>>> Bounded.run(
...     ts,
...     window=[np.array([2, 3]), 4],  # (1)!
...     lower=np.array([[1, 2]]),  # (2)!
...     upper=np.array([6, 5, 4, 3, 4, 5, 6]),  # (3)!
... ).out
bounded_window         2       3               4
bounded_lower    array_0 array_0 array_1 array_1  
bounded_upper    array_0 array_0 array_1 array_1
                        a       b       a       b
2020-01-01           NaN     NaN     NaN     NaN
2020-01-02           0.0     NaN     NaN     NaN
2020-01-03           0.0     0.0     NaN     NaN
2020-01-04           0.0     1.0     1.0     1.0
2020-01-05           0.0     1.0     0.0     1.0
2020-01-06           0.0     0.0     0.0     0.0
2020-01-07           0.0     0.0     0.0     0.0
```

1. One combination with a value per column and one with a value per input.

2. Value per column.

3. Value per row.

Our `apply_func` gets called twice, once for each parameter combination in `window`.
If you print the shapes of the passed arguments, you will see that each window array
now matches the number of columns in `ts`, while each bound array exactly matches the shape of `ts`:

```plaintext
Combination 1:
(7, 2)
(2,)
(7, 2)
(7, 2)

Combination 2:
(7, 2)
(2,)
(7, 2)
(7, 2)
```

### Lazy broadcasting

Broadcasting a large number of parameters to the input shape can use a lot of memory,
especially when the arrays are materialized. Fortunately, VBT can preserve the original (smaller)
dimensions of each parameter array and give you full control over broadcasting.
This requires setting `keep_flex` to True in `broadcast_kwargs`, which will make the factory first
check whether the array can be broadcast, and then expand it to either one or two dimensions in the most
memory-efficient way. There are two configs in [configs](https://vectorbt.pro/pvt_41531c19/api/indicators/configs) for this purpose:
one for column-wise broadcasting and one for element-wise broadcasting.

```pycon
>>> def apply_func(ts, window, lower, upper):
...     window = np.broadcast_to(window, ts.shape[1])  # (1)!
...     lower = np.broadcast_to(lower, ts.shape)
...     upper = np.broadcast_to(upper, ts.shape)
... 
...     out = np.full_like(ts, np.nan, dtype=float_)
...     ts_means = []
...     for col in range(ts.shape[1]):
...         ts_means.append(vbt.nb.rolling_mean_1d_nb(ts[:, col], window[col]))
...     ts_mean = np.column_stack(ts_means)
...     out[ts_mean >= upper] = 1
...     out[ts_mean <= lower] = -1
...     out[(ts_mean > lower) & (ts_mean < upper)] = 0
...     return out

>>> Bounded = vbt.IF(
...     class_name="Bounded",
...     input_names=['ts'],
...     param_names=['window', 'lower', 'upper'],
...     output_names=['out']
... ).with_apply_func(
...     apply_func,
...     param_settings=dict(
...         window=vbt.flex_col_param_config,
...         lower=vbt.flex_elem_param_config,
...         upper=vbt.flex_elem_param_config
...     )
... )
```

1. Bring to the full shape.

Well done! This is the most flexible and memory-efficient way to implement an indicator.
Instead of broadcasting all array-like parameter values immediately, we delay this operation
until it is actually needed.

#### With Numba

The implementation above is very flexible, but it is not the most optimized because it iterates over the
input shape multiple times. As a bonus, let's rewrite our `apply_func` to be Numba-compiled:
this version will iterate over columns and rows, select each parameter value
[flexibly](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/#flexible-indexing) without any broadcasting,
and fill the output array step by step.

```pycon
>>> @njit
... def apply_func_nb(ts, window, lower, upper):
...     out = np.full_like(ts, np.nan, dtype=float_)
...
...     for col in range(ts.shape[1]):
...         _window = vbt.flex_select_1d_pc_nb(window, col)  # (1)!
...
...         for row in range(ts.shape[0]):
...             window_start = max(0, row + 1 - _window)
...             window_end = row + 1
...             if window_end - window_start >= _window:
...                 _lower = vbt.flex_select_nb(lower, row, col)  # (2)!
...                 _upper = vbt.flex_select_nb(upper, row, col)  # (3)!
...
...                 mean = np.nanmean(ts[window_start:window_end, col])  # (4)!
...                 if mean >= _upper:
...                     out[row, col] = 1
...                 elif mean <= _lower:
...                     out[row, col] = -1
...                 elif _lower < mean < _upper:
...                     out[row, col] = 0
...     return out
```

1. Get the window defined for this column.

2. Get the lower bound defined for this element.

3. Get the upper bound defined for this element.

4. Get the mean of this window.

!!! tip
    This is perfectly valid Python code. Even if you remove the `@njit` decorator, it will still work!

Remember that executing code compiled with Numba can provide performance increases
many times greater than standard Python and even Pandas :snail:

### Parameterless

Indicators can also be parameterless, such as [OBV](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/obv/#vectorbtpro.indicators.custom.obv.OBV).

## Inputs

[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) allows you 
to pass none, one, or multiple inputs. If multiple inputs are provided, it will try to broadcast them 
into a single shape using [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast)
(see [Broadcasting](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/#broadcasting)).

Remember, in VBT each column represents a separate backtest. So, to work with multiple
pieces of data such as OHLCV, you should provide them as separate Pandas objects, not as a single
monolithic DataFrame (see [Multidimensionality](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/#multidimensionality)).

Let's create a parameterless indicator that calculates the position of the closing price relative to
the candle:

```pycon
>>> def apply_func(high, low, close):
...     return (close - low) / (high - low)

>>> RelClose = vbt.IF(
...     input_names=['high', 'low', 'close'],
...     output_names=['out']
... ).with_apply_func(apply_func)

>>> close = pd.Series([1, 2, 3, 4, 5], index=generate_index(5))
>>> high = close * 1.2
>>> low = close * 0.8

>>> rel_close = RelClose.run(high, low, close)
>>> rel_close.out
2020-01-01    0.5
2020-01-02    0.5
2020-01-03    0.5
2020-01-04    0.5
2020-01-05    0.5
dtype: float64
```

To see the list of input names:

```pycon
>>> rel_close.input_names
('high', 'low', 'close')
```

You can access any (broadcasted and tiled) input array as an attribute of the indicator instance:

```pycon
>>> rel_close.high
2020-01-01    1.2
2020-01-02    2.4
2020-01-03    3.6
2020-01-04    4.8
2020-01-05    6.0
dtype: float64
```

!!! note
    The input array attached to the indicator instance may look different from what you passed in:
    1) it has been broadcasted with the other inputs, and 2) when you access the attribute, it is
    automatically tiled by the number of parameter combinations to make it easier to compare with
    outputs. To access the original array, prepend an underscore (`_high`).

To demonstrate broadcasting, let's pass `high` as a scalar, `low` as a Series, and `close` as a DataFrame
(even if this combination does not make sense):

```pycon
>>> high = 10
>>> low = pd.Series([1, 2, 3, 4, 5], index=generate_index(5))
>>> close = pd.DataFrame({
...     'a': [3, 2, 1, 2, 3],
...     'b': [5, 4, 3, 4, 5]
... }, index=generate_index(5))
>>> RelClose.run(high, low, close).out
                    a         b
2020-01-01  0.222222  0.444444
2020-01-02  0.000000  0.250000
2020-01-03 -0.285714  0.000000
2020-01-04 -0.333333  0.000000
2020-01-05 -0.400000  0.000000
```

!!! tip
    By default, if all inputs are Series, they are automatically converted into two-dimensional NumPy
    arrays. This provides a unified array interface, as most VBT functions mainly
    work with two-dimensional data. To keep their original dimensions, set `to_2d` to False in
    [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func)
    or any other factory method.

To change broadcasting rules, you can pass a dict called `broadcast_kwargs`, which is unpacked and
forwarded to [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast). For example,
let's tell the broadcaster to cast all three arrays to `np.float16`:

```pycon
>>> RelClose.run(
...     high, low, close,
...     broadcast_kwargs=dict(require_kwargs=dict(dtype=np.float16))
... ).out.dtypes
a    float16
b    float16
dtype: object
```

!!! tip
    Remember that any additional keyword arguments passed to a `run` method are
    forwarded to [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline).
    This allows you to set up the pipeline during both indicator creation and execution.

Since all arrays are passed directly to [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast),
you can also wrap any of them using the [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO) class
to override broadcasting rules for that particular array only:

```pycon
>>> RelClose.run(
...     vbt.BCO(high, require_kwargs=dict(dtype=np.float16)), 
...     vbt.BCO(low, require_kwargs=dict(dtype=np.float16)), 
...     vbt.BCO(close, require_kwargs=dict(dtype=np.float16))
... ).out.dtypes
a    float16
b    float16
dtype: object
```

### One dim

Sometimes, adapting your indicator function to work with two-dimensional data is not straightforward.
For example, when using a TA-Lib indicator in `apply_func`, you may need to pass only one column at a
time. To instruct [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func)
to split any input and in-place output (Pandas or NumPy) array by column, use the `takes_1d` argument:

```pycon
>>> import talib

>>> def apply_func_1d(close, timeperiod):
...     return talib.SMA(close.astype(np.double), timeperiod)

>>> SMA = vbt.IF(
...     input_names=['ts'],
...     param_names=['timeperiod'],
...     output_names=['sma']
... ).with_apply_func(apply_func_1d, takes_1d=True)

>>> sma = SMA.run(ts, [3, 4])
>>> sma.sma
custom_timeperiod                   3         4
                          a         b    a    b
2020-01-01              NaN       NaN  NaN  NaN
2020-01-02              NaN       NaN  NaN  NaN
2020-01-03         4.000000  3.000000  NaN  NaN
2020-01-04         3.000000  4.000000  3.5  3.5
2020-01-05         2.666667  4.333333  3.0  4.0
2020-01-06         3.000000  4.000000  3.0  4.0
2020-01-07         4.000000  3.000000  3.5  3.5
```

!!! note
    Do not confuse this with `per_column`, which also splits by column but applies one parameter
    combination to one column instead of all columns.

### Defaults

Just like with parameters, you can define defaults for your inputs:

```pycon
>>> RelClose = vbt.IF(
...     input_names=['high', 'low', 'close'],
...     output_names=['out']
... ).with_apply_func(
...     apply_func,
...     high=0,
...     low=10
... )

>>> RelClose.run(close).out
              a    b
2020-01-01  0.7  0.5
2020-01-02  0.8  0.6
2020-01-03  0.9  0.7
2020-01-04  0.8  0.6
2020-01-05  0.7  0.5
```

However, unlike parameters, setting inputs to scalars is often not ideal.
Instead, you may want to set them to other inputs, which is possible using 
[Ref](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Ref):

```pycon
>>> RelClose = vbt.IF(
...     input_names=['high', 'low', 'close'],
...     output_names=['out']
... ).with_apply_func(
...     apply_func,
...     high=vbt.Ref('close'),
...     low=vbt.Ref('close')
... )

>>> RelClose.run(high=high, close=close).out
              a    b
2020-01-01  0.0  0.0
2020-01-02  0.0  0.0
2020-01-03  0.0  0.0
2020-01-04  0.0  0.0
2020-01-05  0.0  0.0
```

### Using Pandas

Working only with NumPy arrays is not always the best choice: sometimes you want to take
advantage of Pandas metadata or VBT's Pandas extensions. To prevent conversion of Pandas 
objects to NumPy arrays, you can set `keep_pd` to True.

For example, let's create an indicator that takes a DataFrame and normalizes it against the mean of
each group of columns. The interesting part is that the `group_by` for grouping columns will be a
parameter!

```pycon
>>> def apply_func(ts, group_by):
...     return ts.vbt.demean(group_by=group_by)

>>> Demeaner = vbt.IF(
...     input_names=['ts'],
...     param_names=['group_by'],
...     output_names=['out']
... ).with_apply_func(apply_func, keep_pd=True)

>>> ts_wide = pd.DataFrame({
...     'a': [1, 2, 3, 4, 5],
...     'b': [5, 4, 3, 2, 1],
...     'c': [3, 2, 1, 2, 3],
...     'd': [1, 2, 3, 2, 1]
... }, index=generate_index(5))
>>> demeaner = Demeaner.run(ts_wide, group_by=[(0, 0, 1, 1), True])
>>> demeaner.out
custom_group_by                tuple_0                True
                      a    b    c    d    a    b    c    d
2020-01-01         -2.0  2.0  1.0 -1.0 -1.5  2.5  0.5 -1.5
2020-01-02         -1.0  1.0  0.0  0.0 -0.5  1.5 -0.5 -0.5
2020-01-03          0.0  0.0 -1.0  1.0  0.5  0.5 -1.5  0.5
2020-01-04          1.0 -1.0  0.0  0.0  1.5 -0.5 -0.5 -0.5
2020-01-05          2.0 -2.0  1.0 -1.0  2.5 -1.5  0.5 -1.5
```

Alternatively, you can have [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
pass inputs as NumPy arrays, along with a [wrapper](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#wrapping)
that includes the Pandas metadata:

```pycon
>>> def apply_func(ts, group_by, wrapper):  # (1)!
...     group_map = wrapper.grouper.get_group_map(group_by=group_by)
...     return vbt.nb.demean_nb(ts, group_map)

>>> Demeaner = vbt.IF(
...     input_names=['ts'],
...     param_names=['group_by'],
...     output_names=['out']
... ).with_apply_func(apply_func, pass_wrapper=True)
```

1. `ts` is a two-dimensional NumPy array, which can be used in most Numba-compiled 
functions provided by VBT.

### Inputless

What if an indicator does not take any input arrays? For example, you may want to create an indicator
that takes an input shape, creates one or more output arrays of that shape, and fills them using
information provided as additional arguments. To do this, you can require the user to provide an input
shape using `require_input_shape`.

Let's define a generator that emulates random returns and generates a synthetic price.
This is a parameterized way of implementing [RandomData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random/#vectorbtpro.data.custom.random.RandomData):

```pycon
>>> def apply_func(input_shape, start, mean, std):
...     rand_returns = np.random.normal(mean, std, input_shape)
...     return start * np.cumprod(1 + rand_returns, axis=0)

>>> RandPrice = vbt.IF(
...     class_name="RandPrice",
...     param_names=['start', 'mean', 'std'],
...     output_names=['out']
... ).with_apply_func(
...     apply_func,
...     require_input_shape=True,
...     start=100,  # (1)!
...     mean=0,  # (2)!
...     std=0.01,  # (3)!
...     seed=42  # (4)!
... )

>>> RandPrice.run((5, 2)).out
            0           1
0  100.496714   99.861736
1  101.147620  101.382660
2  100.910779  101.145285
3  102.504375  101.921510
4  102.023143  102.474495
```

1. Default starting value.

2. Default mean ("center") of the distribution.

3. Default standard deviation (spread or "width") of the distribution.

4. [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
supports random seeds.

!!! info
    When `require_input_shape` is True, [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
    prepends an `input_shape` argument to the `run` method's signature. Without this argument,
    the `apply_func` itself must determine the input shape.

However, having integer columns and index is not very convenient. Fortunately, VBT allows you to
pass `input_index` and `input_columns`!

```pycon
>>> RandPrice.run(
...     (5, 2),
...     input_index=generate_index(5),
...     input_columns=['a', 'b'],
...     mean=[-0.1, 0.1]
... ).out
randprice_mean                  -0.1                     0.1
                        a          b           a           b
2020-01-01      90.496714  89.861736  109.536582  109.534270
2020-01-02      82.033180  82.244183  120.755278  118.392000
2020-01-03      73.637778  73.827201  130.747876  129.565496
2020-01-04      67.436898  67.011056  142.498409  142.929202
2020-01-05      60.376609  60.673526  155.454330  155.203528
```

You can even build an indicator that decides on the output shape dynamically.
Let's create a fun indicator that returns an array with a random shape:

```pycon
>>> def custom_func(min_rows=1, max_rows=5, min_cols=1, max_cols=3):  # (1)!
...     n_rows = np.random.randint(min_rows, max_rows)
...     n_cols = np.random.randint(min_cols, max_cols)
...     return np.random.uniform(size=(n_rows, n_cols))

>>> RandShaped = vbt.IF(
...     output_names=['out']
... ).with_custom_func(custom_func)

>>> RandShaped.run(seed=42).out
          0         1
0  0.950714  0.731994
1  0.598658  0.156019
2  0.155995  0.058084

>>> RandShaped.run(seed=43).out
0    0.609067
dtype: float64

>>> RandShaped.run(seed=44).out
          0        1
0  0.104796  0.74464
```

1. We use `custom_func` instead of `apply_func` because this indicator cannot be parameterized, and
no iteration takes place.

## Outputs

[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
supports returning one or more outputs. There are two types of outputs: regular outputs and
in-place outputs (also called "in-place outputs").

### Regular

Regular outputs are arrays explicitly returned by the calculation function. Each output must have
the same shape and match the number of columns in the input, multiplied by the number of parameter
combinations. This requirement only needs special attention when using `custom_func`, as `apply_func`
handles the tiling automatically. If there is only one output, an array must be returned. If there are
multiple outputs, a tuple containing multiple arrays must be returned.

Let's demonstrate multiple regular outputs by computing and returning the entries and exits
from a moving average crossover:

```pycon
>>> def apply_func(ts, fastw, sloww, minp=None):
...     fast_ma = vbt.nb.rolling_mean_nb(ts, fastw, minp=minp)
...     slow_ma = vbt.nb.rolling_mean_nb(ts, sloww, minp=minp)
...     entries = vbt.nb.crossed_above_nb(fast_ma, slow_ma)
...     exits = vbt.nb.crossed_above_nb(slow_ma, fast_ma)
...     return (fast_ma, slow_ma, entries, exits)  # (1)!

>>> CrossSig = vbt.IF(
...     class_name="CrossSig",
...     input_names=['ts'],
...     param_names=['fastw', 'sloww'],
...     output_names=['fast_ma', 'slow_ma', 'entries', 'exits']
... ).with_apply_func(apply_func)

>>> ts2 = pd.DataFrame({
...     'a': [1, 2, 3, 2, 1, 2, 3],
...     'b': [3, 2, 1, 2, 3, 2, 1]
... }, index=generate_index(7))
>>> cross_sig = CrossSig.run(ts2, 2, 4)
```

1. Also return the fast and slow moving average for analysis.

!!! important
    Any output registered in `output_names` must have the same shape as the broadcasted inputs.
    This requirement makes it possible to index the indicator instance.

To get the list of output names:

```pycon
>>> cross_sig.output_names
('fast_ma', 'slow_ma', 'entries', 'exits')
```

Any (broadcasted and tiled) output array can be accessed as an attribute of the indicator instance:

```pycon
>>> cross_sig.entries
crosssig_fastw      2      2
crosssig_sloww      4      4
                    a      b
2020-01-01      False  False
2020-01-02      False  False
2020-01-03      False  False
2020-01-04      False  False
2020-01-05      False   True
2020-01-06      False  False
2020-01-07       True  False
```

### In-place

In-place outputs are arrays that are not returned but modified in place. They act as regular inputs
when entering the pipeline and as regular outputs when exiting. In particular:

1. They broadcast together with regular inputs if provided. Otherwise, an empty array is created.

2. They are tiled by the number of hyperparameter combinations.

3. Each tile is modified in place (they are **not** returned).

4. After calculation, all the tiles are concatenated to form an output.

By default, in-place outputs are created as empty arrays with uninitialized floating point values.
This allows for the creation of optional outputs that, if not written to, do not consume much memory.
Since not all outputs should have the `float` data type, you can pass `dtype` in the
`in_output_settings`.

Let's modify the indicator above by converting both signal arrays to in-place outputs:

```pycon
>>> def apply_func(ts, entries, exits, fastw, sloww, minp=None):
...     fast_ma = vbt.nb.rolling_mean_nb(ts, fastw, minp=minp)
...     slow_ma = vbt.nb.rolling_mean_nb(ts, sloww, minp=minp)
...     entries[:] = vbt.nb.crossed_above_nb(fast_ma, slow_ma)  # (1)!
...     exits[:] = vbt.nb.crossed_above_nb(slow_ma, fast_ma)
...     return (fast_ma, slow_ma)  # (2)!

>>> CrossSig = vbt.IF(
...     class_name="CrossSig",
...     input_names=['ts'],
...     in_output_names=['entries', 'exits'],
...     param_names=['fastw', 'sloww'],
...     output_names=['fast_ma', 'slow_ma']
... ).with_apply_func(
...     apply_func,
...     in_output_settings=dict(
...         entries=dict(dtype=np.bool_),  # (3)!
...         exits=dict(dtype=np.bool_)
...     )
... )
>>> cross_sig = CrossSig.run(ts2, 2, 4)
```

1. Both arrays are modified in place.

2. Both arrays are no longer returned.

3. Instruct [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
to initialize both signal arrays with the boolean data type.

If you print the `output_names`, you will see that `entries` and `exits` are no longer included:

```pycon
>>> cross_sig.output_names
('fast_ma', 'slow_ma')
```

To see all in-place output arrays, use the `in_output_names` attribute:

```pycon
>>> cross_sig.in_output_names
('entries', 'exits')
```

Both signal arrays can be accessed as usual:

```pycon
>>> cross_sig.entries
crosssig_fastw             2
crosssig_sloww             4
                    a      b
2020-01-01      False  False
2020-01-02      False  False
2020-01-03      False  False
2020-01-04      False  False
2020-01-05      False   True
2020-01-06      False  False
2020-01-07       True  False
```

!!! tip
    An interesting scenario occurs when there are no regular outputs, only in-place outputs.
    In this case, you should set `output_names` to an empty list, modify all arrays in place,
    and return `None`. See the example below.

#### Defaults

You might be wondering: *"Why should we bother using in-place outputs when we can just return regular
outputs?"* The answer is that we can provide custom data and overwrite it without using additional
memory. Consider the following example, where we keep the first `n` signals in a boolean time
series:

```pycon
>>> @njit
... def apply_func_nb(signals, n):
...     for col in range(signals.shape[1]):
...         n_found = 0
...         for row in range(signals.shape[0]):
...             if signals[row, col]:
...                 if n_found >= n:
...                     signals[row, col] = False
...                 else:
...                     n_found += 1

>>> FirstNSig = vbt.IF(
...     class_name="FirstNSig",
...     in_output_names=['signals'],
...     param_names=['n']
... ).with_apply_func(apply_func_nb)

>>> signals = pd.Series([False, True, True, True, False])
>>> first_n_sig = FirstNSig.run([1, 2, 3], signals=signals)
>>> first_n_sig.signals
firstnsig_n      1      2      3
0            False  False  False
1             True   True   True
2            False   True   True
3            False  False   True
4            False  False  False
```

As you can see, one array fulfills the job of two, and this is done without modifying the passed
`signals` array!

```pycon
>>> signals
0    False
1     True
2     True
3     True
4    False
dtype: bool
```

!!! note
    Unlike regular inputs, none of the in-place outputs is required when running an indicator,
    so they appear in the signature of the `run` method as keyword arguments with `None` as the
    default value. Be sure to pass each in-place output as a keyword argument after other positional
    arguments (such as inputs and parameters).

### Extra

Any additional output returned by `custom_func` that is not registered in `output_names`
is returned in a raw format along with the indicator instance. Such outputs can be objects
of any type, especially arrays whose shapes differ from those of the inputs.
They are not included in the indicator instance because the indicator factory does not know how
to wrap, index, and analyze them; only the user does. For example, let's return the rolling
mean along with its maximum in each column:

```pycon
>>> def custom_func(ts, window):  # (1)!
...     ts_mas = []
...     ts_ma_maxs = []
...     for w in window:
...         ts_ma = vbt.nb.rolling_mean_nb(ts, w)
...         ts_mas.append(ts_ma)
...         ts_ma_maxs.append(np.nanmax(ts_ma, axis=0))
...     return np.column_stack(ts_mas), np.concatenate(ts_ma_maxs)

>>> MAMax = vbt.IF(
...     class_name='MAMax',
...     input_names=['ts'],
...     param_names=['window'],
...     output_names=['ma'],
... ).with_custom_func(custom_func)

>>> ma_ind, ma_max = MAMax.run(ts2, [2, 3])  # (2)!
>>> ma_ind
mamax_window         2                   3
                a    b         a         b
2020-01-01    NaN  NaN       NaN       NaN
2020-01-02    1.5  2.5       NaN       NaN
2020-01-03    2.5  1.5  2.000000  2.000000
2020-01-04    2.5  1.5  2.333333  1.666667
2020-01-05    1.5  2.5  2.000000  2.000000
2020-01-06    1.5  2.5  1.666667  2.333333
2020-01-07    2.5  1.5  2.000000  2.000000

>>> ma_ind.wrapper.wrap_reduced(ma_max)  # (3)!
mamax_window   
2             a    2.500000
              b    2.500000
3             a    2.333333
              b    2.333333
dtype: float64
```

1. `apply_func` does not support extra outputs, only `custom_func`.

2. Extra outputs are returned along with the indicator instance.

3. Wrap the reduced array using the wrapper of the indicator instance.

### Lazy

Use the `lazy_outputs` argument when constructing an indicator to define lazy outputs—
outputs that are computed from "normal" outputs and are only calculated when explicitly requested.
They are available as regular cacheable properties of the indicator instance and can be of
any type. Continuing with the previous example, let's add a cached property that returns
the maximum of the rolling mean:

```pycon
>>> MAMax = vbt.IF(
...     class_name='MAMax',
...     input_names=['ts'],
...     param_names=['window'],
...     output_names=['ma'],
...     lazy_outputs=dict(
...         ma_max=vbt.cached_property(lambda self: self.ma.max())
...     )
... ).with_apply_func(vbt.nb.rolling_mean_nb)

>>> ma_ind = MAMax.run(ts2, [2, 3])
>>> ma_ind.ma_max
mamax_window   
2             a    2.500000
              b    2.500000
3             a    2.333333
              b    2.333333
dtype: float64
```

!!! tip
    You can achieve the same result by subclassing `MAMax` and defining the property in the subclass.

## Custom arguments

Sometimes, you may need to pass arguments that are not inputs, in-place outputs, or parameters.

### Optional

If you review the `apply_func` of `CrossSig`, you will notice it takes another optional argument, `minp`,
which controls the minimum number of observations in a window required to have a value.
Listing a keyword argument with its default in `custom_func` or `apply_func` is one way to
provide a default value. Another way is to make the argument positional and provide its default 
to [IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func)
or another factory method. The default can also be set during execution in the `run` method.

### Variable

Variable arguments, often appearing as `*args`, are used to accept a *variable* number of arguments.
To enable variable arguments, you need to set `var_args` to True. The reason is as follows: when
[IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func)
builds the `run` method, it must rearrange the arguments so that required arguments appear
before optional arguments. Without the `var_args` flag, the `run` method does not expect any 
additional positional arguments to be passed, which can lead to an error or, even worse, a corrupted result.

Let's add a variable number of inputs:

```pycon
>>> def custom_func(*arrs):
...     out = None
...     for arr in arrs:
...         if out is None:
...             out = arr
...         else:
...             out += arr
...     return out

>>> VarArgAdder = vbt.IF(
...     output_names=['out']  # (1)!
... ).with_custom_func(custom_func, var_args=True)

>>> VarArgAdder.run(
...     pd.Series([1, 2, 3]),
...     pd.Series([10, 20, 30]),
...     pd.Series([100, 200, 300])
... ).out
0    111
1    222
2    333
dtype: int64
```

1. No inputs.

!!! note
    The indicator above is effectively inputless: inputs that are not registered in `input_names` 
    will not broadcast automatically and are not available as attributes of an indicator instance.

### Positional

Positional arguments are handled in the same way as variable arguments.

### Keyword-only

You can set `keyword_only_args` to True to require that all arguments be used as keyword-only arguments.
This can help avoid accidentally placing arguments in the wrong position. 

For example, consider the `RelClose` indicator:

```pycon
>>> def apply_func(high, low, close):
...     return (close - low) / (high - low)

>>> RelClose = vbt.IF(
...     input_names=['high', 'low', 'close'],
...     output_names=['out']
... ).with_apply_func(apply_func)

>>> RelClose.run(close, high, low).out  # (1)!
                    a         b
2020-01-01  1.285714  1.800000
2020-01-02  1.000000  1.333333
2020-01-03  0.777778  1.000000
2020-01-04  0.750000  1.000000
2020-01-05  0.714286  1.000000

>>> RelClose = vbt.IF(
...     input_names=['high', 'low', 'close'],
...     output_names=['out']
... ).with_apply_func(apply_func, keyword_only_args=True)

>>> RelClose.run(close, high, low).out  # (2)!
TypeError: run() takes 1 positional argument but 4 were given

>>> RelClose.run(close=close, high=high, low=low).out  # (3)!
                    a         b
2020-01-01  0.222222  0.444444
2020-01-02  0.000000  0.250000
2020-01-03 -0.285714  0.000000
2020-01-04 -0.333333  0.000000
2020-01-05 -0.400000  0.000000
```

1. `close` is incorrectly passed as the high price—no error is thrown!

2. An error is shown, meaning you must use keyword arguments.

3. `close` is now correctly passed as the closing price.

## Built-in caching

[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) 
reuses calculation artifacts whenever possible. Since it was originally designed
for hyperparameter optimization and parameter combinations may get repeated,
preventing the repeated processing of the same parameter combination is essential for 
good performance.

First, look at a typical raw output by passing repeated parameter combinations
and setting `return_raw` to True:

```pycon
>>> raw = vbt.MA.run(
...     ts2, 
...     window=[2, 2, 3], 
...     wtype=["simple", "simple", "exp"],  # (1)!
...     return_raw=True)
>>> raw
([array([[     nan,      nan,      nan,      nan,      nan,      nan],
          [1.5     , 2.5     , 1.5     , 2.5     ,      nan,      nan],
          [2.5     , 1.5     , 2.5     , 1.5     , 2.25    , 1.75    ],
          [2.5     , 1.5     , 2.5     , 1.5     , 2.125   , 1.875   ],
          [1.5     , 2.5     , 1.5     , 2.5     , 1.5625  , 2.4375  ],
          [1.5     , 2.5     , 1.5     , 2.5     , 1.78125 , 2.21875 ],
          [2.5     , 1.5     , 2.5     , 1.5     , 2.390625, 1.609375]])],
  [(2, 0), (2, 0), (3, 2)],
  2,
  [])
```

1. Three parameter combinations, with two of them being identical.

The raw output consists of:

1. The list of returned output arrays.

2. The list of zipped parameter combinations.

3. The number of input columns.

4. Other objects returned along with output arrays but not listed in `output_names`.

!!! info
    A raw output represents the context of running an indicator. If any parameter combination
    appears in the list of zipped parameter combinations, it means that it was actually run, not cached.

You can see that the calculation function was executed for the same parameter combination twice.
This is not a problem if your calculation is fast enough that you do not mind re-running
the same procedure. However, if your indicator is very complex and slow to compute,
you can instruct [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
to run the indicator on unique parameter combinations only by passing `run_unique`:

```pycon
>>> raw = vbt.MA.run(
...     ts2, 
...     window=[2, 2, 3], 
...     wtype=["simple", "simple", "exp"], 
...     return_raw=True, 
...     run_unique=True, 
...     silence_warnings=True)  # (1)!
>>> raw
([array([[     nan,      nan,      nan,      nan],
          [1.5     , 2.5     ,      nan,      nan],
          [2.5     , 1.5     , 2.25    , 1.75    ],
          [2.5     , 1.5     , 2.125   , 1.875   ],
          [1.5     , 2.5     , 1.5625  , 2.4375  ],
          [1.5     , 2.5     , 1.78125 , 2.21875 ],
          [2.5     , 1.5     , 2.390625, 1.609375]])],
  [(2, 0), (3, 2)],
  2,
  [])
```

1. Without `silence_warnings`, you would get a warning that the raw output contains
only unique parameter combinations. You can ignore this.

Let's compare the performance of running the same parameter combination repeatedly
with and without `run_unique`:

```pycon
>>> a = np.random.uniform(size=(1000,))

>>> %timeit vbt.MA.run(a, np.full(1000, 2), run_unique=False)
11.6 ms ± 1.26 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)

>>> %timeit vbt.MA.run(a, np.full(1000, 2), run_unique=True)
5.91 ms ± 220 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

!!! tip
    The moving average is one of the fastest indicators available. Try this example on a more complex 
    indicator to see the impact of built-in caching.

As a rule of thumb:

* Enable `run_unique` if input arrays have few columns, the calculation function is slow, and 
there are duplicates among parameter combinations.

* Disable `run_unique` if input arrays have many columns, the calculation function is very fast, or 
if two identical parameter combinations can lead to different results (for example, when using
a `custom_func` that makes decisions based on the entire parameter grid, or when there is some 
randomness involved).

!!! note
    `run_unique` is disabled by default.

### Reusing cache

Internally, `run_unique` uses the raw output computed from unique parameter combinations to produce 
the output for all parameter combinations. But what if you already have your own raw output?
You can pass it as `use_raw`. This does not call the calculation function, but instead stacks raw outputs 
in the order their parameter combinations appear in the requested grid. If some requested parameter 
combinations cannot be found in `use_raw`, it will raise an error:

```pycon
>>> raw = vbt.MA.run(
...     ts2, 
...     window=[2, 3], 
...     wtype=["simple", "exp"],
...     return_raw=True)
>>> vbt.MA.run(ts2, 2, "simple", use_raw=raw).ma
ma_window            2     
ma_wtype        simple     
                a    b
2020-01-01    NaN  NaN
2020-01-02    1.5  2.5
2020-01-03    2.5  1.5
2020-01-04    2.5  1.5
2020-01-05    1.5  2.5
2020-01-06    1.5  2.5
2020-01-07    2.5  1.5

>>> vbt.MA.run(ts2, 2, "exp", use_raw=raw).ma
ValueError: (2, 2) is not in list
```

This allows you to pre-compute indicators.

## Manual caching

Another performance boost can be achieved by caching manually, which must be implemented
inside `custom_func`. Additionally, `custom_func` must accept a `return_cache` argument 
to return the cache and a `use_cache` argument to reuse the cache 
(similar to `return_raw` and `use_raw`). Fortunately,
[IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func)
accepts a `cache_func` and provides a `custom_func` that meets these requirements.

Consider this scenario: you want to calculate the relative distance between two 
computationally expensive rolling windows. You have already decided on the value for the first window, 
and want to test thousands of values for the second window. Without caching, and even with `run_unique` 
enabled, the first rolling window will be recalculated many times, wasting resources:

```pycon
>>> def roll_mean_expensive_nb(ts, w):
...     for i in range(100):
...         out = vbt.nb.rolling_mean_nb(ts, w)
...     return out

>>> def apply_func(ts, w1, w2):
...     roll_mean1 = roll_mean_expensive_nb(ts, w1)
...     roll_mean2 = roll_mean_expensive_nb(ts, w2)
...     return (roll_mean2 - roll_mean1) / roll_mean1

>>> RelMADist = vbt.IF(
...     class_name="RelMADist",
...     input_names=['ts'],
...     param_names=['w1', 'w2'],
...     output_names=['out'],
... ).with_apply_func(apply_func)

>>> RelMADist.run(ts2, 2, 3).out
relmadist_w1                   2
relmadist_w2                   3
                      a         b
2020-01-01         NaN       NaN
2020-01-02         NaN       NaN
2020-01-03   -0.200000  0.333333
2020-01-04   -0.066667  0.111111
2020-01-05    0.333333 -0.200000
2020-01-06    0.111111 -0.066667
2020-01-07   -0.200000  0.333333

>>> %timeit RelMADist.run(ts2, 2, np.arange(2, 1000))
294 ms ± 52.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

To avoid this, pre-compute all unique rolling windows in `cache_func` and use them in `apply_func`:

```pycon
>>> def cache_func(ts, w1, w2):  # (1)!
...     cache_dict = dict()
...     for w in w1 + w2:
...         if w not in cache_dict:
...             cache_dict[w] = roll_mean_expensive_nb(ts, w)
...     return cache_dict

>>> def apply_func(ts, w1, w2, cache_dict):  # (2)!
...     return (cache_dict[w2] - cache_dict[w1]) / cache_dict[w1]

>>> RelMADist = vbt.IF(
...     class_name="RelMADist",
...     input_names=['ts'],
...     param_names=['w1', 'w2'],
...     output_names=['out'],
... ).with_apply_func(apply_func, cache_func=cache_func)

>>> RelMADist.run(ts2, 2, 3).out
relmadist_w1                   2
relmadist_w2                   3
                      a         b
2020-01-01         NaN       NaN
2020-01-02         NaN       NaN
2020-01-03   -0.200000  0.333333
2020-01-04   -0.066667  0.111111
2020-01-05    0.333333 -0.200000
2020-01-06    0.111111 -0.066667
2020-01-07   -0.200000  0.333333

>>> %timeit RelMADist.run(ts2, 2, np.arange(2, 1000))
119 ms ± 335 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

1. `cache_func` accepts the same arguments as `apply_func`, but parameters are now lists 
instead of single values.

2. `apply_func` accepts the output of `cache_func` as its last argument. If there are multiple
outputs, they all must appear as separate arguments.

This method cuts processing time by half!

### Per column

What happens when you pass `per_column=True` to apply each parameter combination per column?
Internally, [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func) 
splits any input, in-place output, and parameter array per column, passing one element from each to `apply_func` 
at a time. However, the same splitting cannot be done for `cache_func`, because you would
then get 1) a list of input arrays instead of a single array (which would cause an error if the caching 
function was Numba-compiled, since Numba does not allow the same argument with two different types),
and 2) each input array in the list could be different, so maintaining a single caching dictionary with 
parameter combinations as keys would not be sufficient.

To handle this edge case, VBT passes input and in-place output arrays in their regular shape 
(not split), and also provides a `per_column` argument set to True, so `cache_func` knows
that each parameter corresponds to only one column in the input. In the caching function, you can 
use this flag to decide how to proceed. Usually, you simply disable caching and perform all calculations 
directly in the apply function.

```pycon
>>> def cache_func(ts, w1, w2, per_column=False):
...     if per_column:
...         return None
...     cache_dict = dict()
...     for w in w1 + w2:
...         if w not in cache_dict:
...             cache_dict[w] = roll_mean_expensive_nb(ts, w)
...     return cache_dict

>>> def apply_func(ts, w1, w2, cache_dict=None):  # (1)!
...     if cache_dict is None:
...         roll_mean1 = roll_mean_expensive_nb(ts, w1)
...         roll_mean2 = roll_mean_expensive_nb(ts, w2)
...     else:
...         roll_mean1 = cache_dict[w1]
...         roll_mean2 = cache_dict[w2]
...     return (roll_mean2 - roll_mean1) / roll_mean1
...     

>>> RelMADist = vbt.IF(
...     class_name="RelMADist",
...     input_names=['ts'],
...     param_names=['w1', 'w2'],
...     output_names=['out'],
... ).with_apply_func(apply_func, cache_func=cache_func)

>>> RelMADist.run(ts2, 2, 3).out
relmadist_w1                   2
relmadist_w2                   3
                      a         b
2020-01-01         NaN       NaN
2020-01-02         NaN       NaN
2020-01-03   -0.200000  0.333333
2020-01-04   -0.066667  0.111111
2020-01-05    0.333333 -0.200000
2020-01-06    0.111111 -0.066667
2020-01-07   -0.200000  0.333333

>>> RelMADist.run(ts2, [2, 2], [3, 4], per_column=True).out
relmadist_w1                   2
relmadist_w2         3         4
                      a         b
2020-01-01         NaN       NaN
2020-01-02         NaN       NaN
2020-01-03   -0.200000       NaN
2020-01-04   -0.066667  0.333333
2020-01-05    0.333333 -0.200000
2020-01-06    0.111111 -0.200000
2020-01-07   -0.200000  0.333333
```

1. Make cache optional.

This design is even better than the previous one because caching is now optional,
and any other function can call `apply_func` without needing to handle caching.
This approach also works with Numba.

### Reusing cache

Similar to raw outputs, you can force [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
and `custom_func` to return the cache, allowing you to reuse it in other calculations or even indicators. 
The clear advantage of this method is that you are not limited to a fixed set of parameter combinations, 
but can instead use the actual parameter values, providing more control over performance management.

```pycon
>>> cache = RelMADist.run(
...     ts2, 
...     w1=2, 
...     w2=np.arange(2, 1000), 
...     return_cache=True)

>>> %timeit RelMADist.run( \
...     ts2, \
...     w1=np.arange(2, 1000), \
...     w2=np.arange(2, 1000), \
...     use_cache=cache)
7.7 ms ± 153 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
```

## Stacking

Like regular functions, indicators can depend on each other. To build a stacked
indicator, the first step is to merge their inputs and parameters. Consider the classic
moving average crossover, where we want to use the TA-Lib `SMA` indicator twice: once for
the fast moving average and once for the slow moving average. By checking the arguments accepted by the 
indicator's `run` method, we see that it takes a time series `close` and a parameter `timeperiod`.
Since both moving averages use the same time series, our only input is `close`.
However, the parameter `timeperiod` should be different for each moving average, so we need to define two
parameters: `timeperiod1` and `timeperiod2` (feel free to use any other names).

```pycon
>>> vbt.phelp(vbt.talib('SMA').run)
SMA.run(
    close,
    timeperiod=Default(value=30),
    short_name='sma',
    hide_params=None,
    hide_default=True,
    **kwargs
):
    Run `SMA` indicator.
    

    * Inputs: `close`

    * Parameters: `timeperiod`

    * Outputs: `real`
    
    Pass a list of parameter names as `hide_params` to hide their column levels.
    Set `hide_default` to False to show the column levels of the parameters with a default value.
    
    Other keyword arguments are passed to `SMA.run_pipeline`.

>>> def apply_func(close, timeperiod1, timeperiod2):
...     fast_ma = vbt.talib('SMA').run(close, timeperiod1)
...     slow_ma = vbt.talib('SMA').run(close, timeperiod2)
...     entries = fast_ma.real_crossed_above(slow_ma)
...     exits = fast_ma.real_crossed_below(slow_ma)
...     return (fast_ma.real, slow_ma.real, entries, exits)

>>> MACrossover = vbt.IF(
...     class_name="CrossSig",
...     input_names=['close'],
...     param_names=['timeperiod1', 'timeperiod2'],
...     output_names=['fast_ma', 'slow_ma', 'entries', 'exits'],
... ).with_apply_func(apply_func)

>>> MACrossover.run(ts2, 2, 3).entries
crosssig_timeperiod1             2
crosssig_timeperiod2             3
                          a      b
2020-01-01            False  False
2020-01-02            False  False
2020-01-03            False  False
2020-01-04            False  False
2020-01-05            False   True
2020-01-06            False  False
2020-01-07             True  False
```

This implementation does have one drawback: it needlessly creates two indicator instances
and repeatedly converts between NumPy arrays and Pandas objects. An ideal implementation would
use only NumPy and Numba. Fortunately, any indicator constructed by
[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) 
supports the `return_raw` argument, which allows you to access the actual NumPy array(s)
returned by the calculation function.

```pycon
>>> def sma(close, timeperiod):
...     return vbt.talib('SMA').run(close, timeperiod, return_raw=True)[0][0]

>>> def apply_func(close, timeperiod1, timeperiod2):
...     fast_ma = sma(close, timeperiod1)
...     slow_ma = sma(close, timeperiod2)
...     entries = vbt.nb.crossed_above_nb(fast_ma, slow_ma)
...     exits = vbt.nb.crossed_above_nb(slow_ma, fast_ma)
...     return (fast_ma, slow_ma, entries, exits)
```

Looking for another approach? Any indicator class created by 
[IndicatorFactory.with_custom_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_custom_func) 
has a `custom_func` attribute to access the custom function. Similarly, any indicator class 
created by [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.with_apply_func)
has an `apply_func` attribute to access the apply function. This lets you call
an indicator's `custom_func` from your own `custom_func` and its `apply_func` from 
your own `apply_func`. Note that the `apply_func` of all parsed indicators is created dynamically 
with `pass_packed` set to True, so it accepts arguments in the packed form:

```pycon
>>> vbt.phelp(vbt.talib('SMA').apply_func)
apply_func(
    input_tuple,
    in_output_tuple,
    param_tuple,
    **_kwargs
)

>>> def sma(close, timeperiod):
...     return vbt.talib('SMA').apply_func((close,), (), (timeperiod,))
```

This approach is as fast as it gets!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/indicators/development.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/indicators/analysis/
parent: https://vectorbt.pro/pvt_41531c19/documentation/indicators/
name: Analysis
type: page
icon: material-chart-timeline-variant-shimmer
---

# Analysis

To analyze an indicator, use the indicator instance returned by the `run` method.

## Helper methods

Whenever you create an instance of [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory),
it builds and sets up an indicator class. During this process, the factory attaches many useful
attributes to the class. For example, for each item in `input_names`, `in_output_names`, `output_names`,
and `lazy_outputs`, it creates and attaches a set of comparison and combination methods.
The properties of any of these attributes can be controlled using the `attr_settings` dictionary.

Let's modify the `CrossSig` class created earlier by combining entries and exits into a single signal
array. We will also return an enumerated array that indicates the signal type. Additionally,
we will specify the data type of each array in the `attr_settings` dictionary:

```pycon
>>> from vectorbtpro import *

>>> SignalType = namedtuple('SigType', ['Entry', 'Exit'])(0, 1)  # (1)!

>>> def apply_func(ts, fastw, sloww, minp=None):
...     fast_ma = vbt.nb.rolling_mean_nb(ts, fastw, minp=minp)
...     slow_ma = vbt.nb.rolling_mean_nb(ts, sloww, minp=minp)
...     entries = vbt.nb.crossed_above_nb(fast_ma, slow_ma)
...     exits = vbt.nb.crossed_above_nb(slow_ma, fast_ma)
...     signals = entries | exits
...     signal_type = np.full(ts.shape, -1, dtype=int_)  # (2)!
...     signal_type[entries] = SignalType.Entry
...     signal_type[exits] = SignalType.Exit
...     return (fast_ma, slow_ma, signals, signal_type)

>>> CrossSig = vbt.IF(
...     class_name="CrossSig",
...     input_names=['ts'],
...     param_names=['fastw', 'sloww'],
...     output_names=['fast_ma', 'slow_ma', 'signals', 'signal_type'],
...     attr_settings=dict(
...         fast_ma=dict(dtype=float_),
...         slow_ma=dict(dtype=float_),
...         signals=dict(dtype=np.bool_),
...         signal_type=dict(dtype=SignalType),
...     )
... ).with_apply_func(apply_func)

>>> def generate_index(n):
...     return vbt.date_range("2020-01-01", periods=n)

>>> ts = pd.DataFrame({
...     'a': [1, 2, 3, 2, 1, 2, 3],
...     'b': [3, 2, 1, 2, 3, 2, 1]
... }, index=generate_index(7))
>>> cross_sig = CrossSig.run(ts, 2, 3)
```

1. Enum values in VBT usually start at 0 and increase incrementally.

2. In VBT, a missing value is typically represented as `-1`.

We can explore the helper methods that were attached using Python's `dir` command:

```pycon
>>> dir(cross_sig)
...
'fast_ma',
'fast_ma_above',
'fast_ma_below',
'fast_ma_crossed_above',
'fast_ma_crossed_below',
'fast_ma_equal',
'fast_ma_stats',
...
'signal_type',
'signal_type_readable',
'signal_type_stats',
...
'signals',
'signals_and',
'signals_or',
'signals_stats',
'signals_xor',
...
'slow_ma',
'slow_ma_above',
'slow_ma_below',
'slow_ma_crossed_above',
'slow_ma_crossed_below',
'slow_ma_equal',
'slow_ma_stats',
...
'ts',
'ts_above',
'ts_below',
'ts_crossed_above',
'ts_crossed_below',
'ts_equal',
'ts_stats',
```

One helper method that appears for each array is `stats`, which calls 
[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)
on the accessor that matches the data type of the array:

```pycon
>>> cross_sig.fast_ma_stats(column=(2, 3, 'a'))  # (1)!
Start        2020-01-01 00:00:00
End          2020-01-07 00:00:00
Period           7 days 00:00:00
Count                          6
Mean                         2.0
Std                     0.547723
Min                          1.5
Median                       2.0
Max                          2.5
Min Index    2020-01-02 00:00:00
Max Index    2020-01-03 00:00:00
Name: (2, 3, a), dtype: object
```

1. Uses [GenericAccessor.stats](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.stats).

You can also perform the same operation manually:

```pycon
>>> cross_sig.fast_ma.vbt.stats(column=(2, 3, 'a'))
```

### Numeric

The factory generated the comparison methods `above`, `below`, and `equal` for the numeric arrays.
Each of these methods is based on [combine_objs](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.combine_objs),
which in turn builds on [BaseAccessor.combine](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine).
All operations are performed strictly with NumPy. Another benefit is that VBT's own broadcasting
is used, allowing you to combine arrays with any array-like object as long as their shapes can be
broadcast together. You can also compare with multiple objects at once by passing them as a tuple or
list.

Let's return True when the fast moving average is above a range of thresholds:

```pycon
>>> cross_sig.fast_ma_above([2, 3])
crosssig_fast_ma_above             2             3
crosssig_fastw                     2             2
crosssig_sloww                     3             3
                            a      b      a      b
2020-01-01              False  False  False  False
2020-01-02              False   True  False  False
2020-01-03               True  False  False  False
2020-01-04               True  False  False  False
2020-01-05              False   True  False  False
2020-01-06              False   True  False  False
2020-01-07               True  False  False  False
```

Or, manually:

```pycon
>>> cross_sig.fast_ma.vbt > vbt.Param([2, 3], name='crosssig_fast_ma_above')
```

Additionally, the factory attached the methods `crossed_above` and `crossed_below`.
These are based on [GenericAccessor.crossed_above](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_above)
and [GenericAccessor.crossed_below](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_below)
respectively.

```pycon
>>> cross_sig.fast_ma_crossed_above(cross_sig.slow_ma)
crosssig_fastw             2
crosssig_sloww             3
                    a      b
2020-01-01      False  False
2020-01-02      False  False
2020-01-03      False  False
2020-01-04      False  False
2020-01-05      False   True
2020-01-06      False  False
2020-01-07       True  False
```

Or, manually:

```pycon
>>> cross_sig.fast_ma.vbt.crossed_above(cross_sig.slow_ma)
```

### Boolean

The factory generated the comparison methods `and`, `or`, and `xor` for the boolean arrays.
Similar to those generated for numeric arrays, these are also based on
[combine_objs](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.combine_objs):

```pycon
>>> other_signals = pd.Series([False, False, False, False, True, False, False])
>>> cross_sig.signals_and(other_signals)
crosssig_fastw             2
crosssig_sloww             3
                    a      b
2020-01-01      False  False
2020-01-02      False  False
2020-01-03      False  False
2020-01-04      False  False
2020-01-05       True   True
2020-01-06      False  False
2020-01-07      False  False
```

Or, manually:

```pycon
>>> cross_sig.signals.vbt & other_signals
```

### Enumerated

Enumerated (or categorical) arrays, such as our `signal_type`, contain integer data that can be mapped
to specific categories using a named tuple or another enum. Unlike numeric and boolean arrays,
comparing them with other arrays is not meaningful. As a result, there is only one attached method,
`readable`, which displays the array in a human-readable format:

```pycon
>>> cross_sig.signal_type_readable
crosssig_fastw             2
crosssig_sloww             3
                    a      b
2020-01-01       None   None
2020-01-02       None   None
2020-01-03       None   None
2020-01-04       None   None
2020-01-05       Exit  Entry
2020-01-06       None   None
2020-01-07      Entry   Exit
```

!!! tip
    In VBT, if `-1` is not included in the enum, it automatically indicates a missing value and
    is replaced by `None`.

Or, manually:

```pycon
>>> cross_sig.signal_type.vbt(mapping=SignalType).apply_mapping()
```

## Indexing

Each indicator class inherits from [Analyzable](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#analyzing), allowing you
to use Pandas indexing on the indicator instance to select rows and columns across all Pandas objects.
Supported operations include `iloc`, `loc`, `xs`, and `__getitem__`.

```pycon
>>> cross_sig = CrossSig.run(ts, [2, 3], [3, 4], param_product=True)

>>> cross_sig.loc["2020-01-03":, (2, 3, 'a')]  # (1)!
<vectorbtpro.indicators.factory.CrossSig at 0x7fcaf8e2f5c0>

>>> cross_sig.loc["2020-01-03":, (2, 3, 'a')].signals
2020-01-03    False
2020-01-04    False
2020-01-05     True
2020-01-06    False
2020-01-07     True
Name: (2, 3, a), dtype: bool
```

1. Using a single Pandas indexing operation.

Additionally, [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
uses the class factory function [build_param_indexer](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.build_param_indexer)
to generate an indexing class that enables Pandas-style indexing on each parameter.
Since the indicator class inherits from this indexing class, you can use `*param_name*_loc`
to select one or more parameter values.

```pycon
>>> cross_sig.fastw_loc[2].sloww_loc[3]['a']  # (1)!
<vectorbtpro.indicators.factory.CrossSig at 0x7fcac80742b0>

>>> cross_sig.fastw_loc[2].sloww_loc[3]['a'].signals
2020-01-01    False
2020-01-02    False
2020-01-03    False
2020-01-04    False
2020-01-05     True
2020-01-06    False
2020-01-07     True
Name: a, dtype: bool
```

1. Using two parameter and one Pandas indexing operation. Each of these operations returns
a separate instance of `CrossSig`.

All of this allows you to access rows and columns by labels, integer positions, and parameters,
offering complete flexibility :man_cartwheeling:

## Stats and plots

As with any [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable) instance,
you can compute and plot various properties of the input and output data stored in the instance.

Metrics can be set in two ways: by passing them through the `metrics` argument, or by
subclassing the indicator class. The same applies to the `stats_defaults` argument, which
can be specified as either a dictionary or a function, and defines the default settings for
[StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats).
Subplots can be defined similarly to metrics, but are set using the `subplots` and
`plots_defaults` arguments, and invoked through [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots).

Let's define some metrics and subplots for `CrossSig`:

```pycon
>>> metrics = dict(
...     start=dict(  # (1)!
...         title='Start',
...         calc_func=lambda self: self.wrapper.index[0],
...         agg_func=None
...     ),
...     end=dict(  # (2)!
...         title='End',
...         calc_func=lambda self: self.wrapper.index[-1],
...         agg_func=None
...     ),
...     period=dict(  # (3)!
...         title='Period',
...         calc_func=lambda self: len(self.wrapper.index),
...         apply_to_timedelta=True,
...         agg_func=None
...     ),
...     fast_stats=dict(  # (4)!
...         title="Fast Stats",
...         calc_func=lambda self:
...         self.fast_ma.describe()\
...         .loc[['count', 'mean', 'std', 'min', 'max']]\
...         .vbt.to_dict(orient='index_series')
...     ),
...     slow_stats=dict(
...         title="Slow Stats",
...         calc_func=lambda self:
...         self.slow_ma.describe()\
...         .loc[['count', 'mean', 'std', 'min', 'max']]\
...         .vbt.to_dict(orient='index_series')
...     ),
...     num_entries=dict(  # (5)!
...         title="Entries",
...         calc_func=lambda self:
...         np.sum(self.signal_type == SignalType.Entry)
...     ),
...     num_exits=dict(
...         title="Exits",
...         calc_func=lambda self:
...         np.sum(self.signal_type == SignalType.Exit)
...     )
... )

>>> def plot_mas(self, column=None, add_trace_kwargs=None, fig=None):  # (6)!
...     ts = self.select_col_from_obj(self.ts, column).rename('TS')  # (7)!
...     fast_ma = self.select_col_from_obj(self.fast_ma, column).rename('Fast MA')
...     slow_ma = self.select_col_from_obj(self.slow_ma, column).rename('Slow MA')
...     ts.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...     fast_ma.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)  # (8)!
...     slow_ma.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)

>>> def plot_signals(self, column=None, add_trace_kwargs=None, fig=None):
...     signal_type = self.select_col_from_obj(self.signal_type, column)
...     entries = (signal_type == SignalType.Entry).rename('Entries')
...     exits = (signal_type == SignalType.Exit).rename('Exits')
...     entries.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...     exits.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)

>>> subplots = dict(
...     mas=dict(
...         title="Moving averages",
...         plot_func=plot_mas
...     ),
...     signals=dict(
...         title="Signals",
...         plot_func=plot_signals
...     )
... )

>>> CrossSig = vbt.IF(
...     class_name="CrossSig",
...     input_names=['ts'],
...     param_names=['fastw', 'sloww'],
...     output_names=['fast_ma', 'slow_ma', 'signals', 'signal_type'],
...     attr_settings=dict(
...         fast_ma=dict(dtype=float_),
...         slow_ma=dict(dtype=float_),
...         signals=dict(dtype=np.bool_),
...         signal_type=dict(dtype=SignalType),
...     ),
...     metrics=metrics,  # (9)!
...     subplots=subplots
... ).with_apply_func(apply_func)

>>> cross_sig = CrossSig.run(ts, [2, 3], 4)
```

1. Index of the first data point.

2. Index of the last data point.

3. The number of data points multiplied by the index frequency.

4. Takes `fast_ma`, generates descriptive statistics using Pandas, and converts them to a dictionary
with Series (per statistic) using [BaseAccessor.to_dict](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.to_dict).
The `dict` type tells VBT to make multiple metrics from one.

5. Number of entry signals.

6. [PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots)
can automatically recognize the arguments your plotting function takes and pass the needed information.
For example, it detects `self` and passes the indicator instance.

7. Since we are plotting multiple time series, we cannot plot multiple columns at once, so we must
select the provided column from each time series using [Wrapping.select_col_from_obj](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.select_col_from_obj).

8. `fig` is the figure and `add_trace_kwargs` contains subplot settings for plotting this time series.
Note that the function does not return anything; the figure is modified in place.

9. Metric and subplot dictionaries are passed to the constructor.

Calculate the metrics:

```pycon
>>> cross_sig.stats(column=(2, 4, 'a'))
Start                2020-01-01 00:00:00
End                  2020-01-07 00:00:00
Period                   7 days 00:00:00
Fast Stats: count                    6.0
Fast Stats: mean                     2.0
Fast Stats: std                 0.547723
Fast Stats: min                      1.5
Fast Stats: max                      2.5
Slow Stats: count                    4.0
Slow Stats: mean                     2.0
Slow Stats: std                      0.0
Slow Stats: min                      2.0
Slow Stats: max                      2.0
Entries                                1
Exits                                  1
Name: (2, 4, a), dtype: object
```

Plot the subplots:

```pycon
>>> cross_sig.plots(column=(2, 4, 'a')).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/plots.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/plots.dark.svg#only-dark){: .iimg loading=lazy }

We have created a smart indicator, yay! :partying_face:

## Extending

Indicator classes can be extended and modified just like regular Python classes, by subclassing.
Let's turn the newly created `plot_mas` and `plot_signals` functions into methods of the indicator
class, so we can plot each graph separately. We will also redefine the `subplots`
configuration to reflect this change:

```pycon
>>> class SmartCrossSig(CrossSig):
...     def plot_mas(self, column=None, add_trace_kwargs=None, fig=None):
...         ts = self.select_col_from_obj(self.ts, column).rename('TS')
...         fast_ma = self.select_col_from_obj(self.fast_ma, column).rename('Fast MA')
...         slow_ma = self.select_col_from_obj(self.slow_ma, column).rename('Slow MA')
...         fig = ts.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...         fast_ma.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...         slow_ma.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...         return fig  # (1)!
...
...     def plot_signals(self, column=None, add_trace_kwargs=None, fig=None):
...         signal_type = self.select_col_from_obj(self.signal_type, column)
...         entries = (signal_type == SignalType.Entry).rename('Entries')
...         exits = (signal_type == SignalType.Exit).rename('Exits')
...         fig = entries.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...         exits.vbt.plot(add_trace_kwargs=add_trace_kwargs, fig=fig)
...         return fig
...
...     subplots = vbt.HybridConfig(  # (2)!
...         mas=dict(
...             title="Moving averages",
...             plot_func='plot_mas'  # (3)!
...         ),
...         signals=dict(
...             title="Signals",
...             plot_func='plot_signals'
...         )
...     )

>>> smart_cross_sig = SmartCrossSig.run(ts, [2, 3], 4)
>>> smart_cross_sig.plot_signals(column=(2, 4, 'a')).show()
```

1. Since both plotting functions can now be called by the user, we need to return the figure
if it was not provided.

2. When setting `metrics` and `subplots` manually, we need to use [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config)
instead of `dict`.

3. Since both functions are now class attributes, we can specify them as strings to take advantage of
VBT's [attribute resolution](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#attribute-resolution).

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/plot_signals.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/plot_signals.dark.svg#only-dark){: .iimg loading=lazy }

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/indicators/analysis.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/indicators/parsers/
parent: https://vectorbt.pro/pvt_41531c19/documentation/indicators/
name: Parsers
type: page
icon: material-code-tags
---

# Parsers

[IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
uses a set of parsers to simplify the process of creating indicators. These include parsers for
third-party indicators as well as an advanced expression parser.

!!! info
    Each parser method is a class method with the prefix `from_`, so you do not need to
    create or pass any information to the indicator factory using `vbt.IF(...)` -
    the method handles this for you!

## TA-Lib

[IndicatorFactory.from_talib](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_talib)
can parse [TA-Lib](https://github.com/mrjbq7/ta-lib) indicators. When you provide the name of an indicator,
the method retrieves the TA-Lib abstract function and then checks the `info` dictionary to determine the
input, parameter, and output names. After creating a factory instance, it builds an apply function that can
run the indicator function on two-dimensional inputs, instead of only the one-dimensional inputs that TA-Lib
supports by default.

To view the list of all supported indicators:

```pycon
>>> from vectorbtpro import *

>>> vbt.IF.list_talib_indicators()  # (1)!
{'ACOS',
  'AD',
  'ADD',
  ...
  'WCLPRICE',
  'WILLR',
  'WMA'}
```

1. Uses [IndicatorFactory.list_talib_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_talib_indicators).

To retrieve an indicator:

```pycon
>>> vbt.IF.from_talib('RSI')  # (1)!
vectorbtpro.indicators.factory.talib.RSI
```

1. Uses [IndicatorFactory.from_talib](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_talib).

Or, you can use a shortcut:

```pycon
>>> vbt.talib('RSI')
vectorbtpro.indicators.factory.talib.RSI
```

### Skipping NaN

TA-Lib indicators often do not handle missing data well. For example, a single NaN in a time series can
cause all subsequent values to become NaN:

```pycon
>>> price = vbt.RandomData.pull(
...     start='2020-01-01', 
...     end='2020-06-01', 
...     timeframe='1H',
...     seed=42
... ).get()
>>> price_na = price.copy()
>>> price_na.iloc[2] = np.nan  # (1)!

>>> SMA = vbt.talib("SMA")
>>> sma = SMA.run(price_na, timeperiod=10)
>>> sma.real
2019-12-31 22:00:00+00:00   NaN
2019-12-31 23:00:00+00:00   NaN
2020-01-01 00:00:00+00:00   NaN
2020-01-01 01:00:00+00:00   NaN
2020-01-01 02:00:00+00:00   NaN
...                         ...
2020-05-31 18:00:00+00:00   NaN
2020-05-31 19:00:00+00:00   NaN
2020-05-31 20:00:00+00:00   NaN
2020-05-31 21:00:00+00:00   NaN
2020-05-31 22:00:00+00:00   NaN
Freq: H, Name: 10, Length: 3649, dtype: float64
```

1. Make a single value missing.

To address this, you can instruct the indicator factory to run the indicator only on non-NA values,
then insert the output values at their original positions:

```pycon
>>> sma = SMA.run(price_na, timeperiod=10, skipna=True)
>>> sma.real
2019-12-31 22:00:00+00:00           NaN
2019-12-31 23:00:00+00:00           NaN
2020-01-01 00:00:00+00:00           NaN
2020-01-01 01:00:00+00:00           NaN
2020-01-01 02:00:00+00:00           NaN
...                                 ...
2020-05-31 18:00:00+00:00    213.169260
2020-05-31 19:00:00+00:00    212.477181
2020-05-31 20:00:00+00:00    211.911416
2020-05-31 21:00:00+00:00    211.310849
2020-05-31 22:00:00+00:00    210.899923
Freq: H, Name: 10, Length: 3649, dtype: float64
```

!!! tip
    Another method is to forward fill NaN values before running an indicator, but this can distort the
    results. Only use this approach when it is truly appropriate.

### Resampling

Another feature provided by the indicator factory is support for parameterized time frames.

Here is how it works:

1. The wrapper downsamples input arrays to the target time frame.

2. It runs the indicator on the downsampled input arrays.

3. Output arrays are then upsampled back to the original time frame.

This allows you to pack multiple time frames into a single two-dimensional array:

```pycon
>>> sma = SMA.run(
...     price_na, 
...     timeperiod=10, 
...     skipna=True, 
...     timeframe=["1h", "4h", "1d"]
... )
>>> sma.real
sma_timeperiod                                             10
sma_timeframe                      1h          4h          1d
2019-12-31 22:00:00+00:00         NaN         NaN         NaN
2019-12-31 23:00:00+00:00         NaN         NaN         NaN
2020-01-01 00:00:00+00:00         NaN         NaN         NaN
2020-01-01 01:00:00+00:00         NaN         NaN         NaN
2020-01-01 02:00:00+00:00         NaN         NaN         NaN
...                               ...         ...         ...
2020-05-31 18:00:00+00:00  213.169260  215.561805  206.104351
2020-05-31 19:00:00+00:00  212.477181  214.422456  206.104351
2020-05-31 20:00:00+00:00  211.911416  214.422456  206.104351
2020-05-31 21:00:00+00:00  211.310849  214.422456  206.104351
2020-05-31 22:00:00+00:00  210.899923  214.422456  206.104351

[3649 rows x 3 columns]
```

!!! note
    If some timestamps are missing, VBT may have trouble parsing the source index frequency.
    To specify the frequency directly, pass
    `broadcast_kwargs=dict(wrapper_kwargs=dict(freq="1h"))`, for example.
    Without the source frequency, VBT will upsample the downsampled arrays between each pair
    of timestamps in the source index, rather than relying on its frequency, which might be
    undesirable.

### Plotting

You can also plot each indicator. This is done programmatically by parsing the output flags of the
indicator. For example, here is how to use `STOCH`:

```pycon
>>> STOCH = vbt.talib('STOCH')
>>> STOCH.output_flags
OrderedDict([('slowk', ['Dashed Line']), ('slowd', ['Dashed Line'])])

>>> ohlc = price.resample('1d').ohlc()
>>> stoch = STOCH.run(ohlc['high'], ohlc['low'], ohlc['close'])
>>> stoch.plot().show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/stoch.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/stoch.dark.svg#only-dark){: .iimg loading=lazy }

To see the arguments that the `plot` function accepts, use [phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp):

```pycon
>>> vbt.phelp(STOCH.plot)
plot(
    self,
    column=None,
    limits=None,
    add_shape_kwargs=None,
    add_trace_kwargs=None,
    fig=None,
    slowk_trace_kwargs=None,
    slowd_trace_kwargs=None,
    **layout_kwargs
):
    Plot the outputs of the indicator based on their flags.
            
    Args:
        column (str): Name of the column to plot.
        limits (tuple of float): Tuple of the lower and upper limit.
        add_shape_kwargs (dict): Keyword arguments passed to `fig.add_shape` when adding the range between both limits.
        add_trace_kwargs (dict): Keyword arguments passed to `fig.add_trace` when adding each trace.
        slowk_trace_kwargs (dict): Keyword arguments passed to the trace of `slowk`.
        slowd_trace_kwargs (dict): Keyword arguments passed to the trace of `slowd`.
        fig (Figure or FigureWidget): Figure to add the traces to.
        **layout_kwargs: Keyword arguments passed to `fig.update_layout`.
```

Now, let's create a plot with two subplots: OHLC above, and %D and %K below. We will also change
the style of both output lines from dashed to solid and display a range between an oversold limit
of 20 and an overbought limit of 80:

```pycon
>>> fig = vbt.make_subplots(
...     rows=2, 
...     cols=1, 
...     shared_xaxes=True,  # (1)!
...     vertical_spacing=0.05)
>>> ohlc.vbt.ohlcv.plot(
...     add_trace_kwargs=dict(row=1, col=1),  # (2)!
...     fig=fig,
...     xaxis=dict(rangeslider_visible=False))  # (3)!
>>> stoch.plot(
...     limits=(20, 80),
...     add_trace_kwargs=dict(row=2, col=1),  # (4)!
...     slowk_trace_kwargs=dict(line=dict(dash=None)),  # (5)!
...     slowd_trace_kwargs=dict(line=dict(dash=None)),
...     fig=fig)
>>> fig.show()
```

1. All time series use the same index.

2. First subplot.

3. Passed directly to `fig.update_layout`.

4. Second subplot.

5. Passed directly to `fig.add_trace`.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/stoch_subplots.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/stoch_subplots.dark.svg#only-dark){: .iimg loading=lazy }

## Pandas TA

[IndicatorFactory.from_pandas_ta](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_pandas_ta)
can parse [Pandas TA](https://pypi.org/project/pandas-ta/) indicators. Since Pandas TA indicators do not
have metadata attached to each indicator, a method called
[IndicatorFactory.parse_pandas_ta_config](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.parse_pandas_ta_config)
is used. This method reads the signature of an indicator function to determine the input and parameter
names and their default values. It also passes several dozen rows of sample data to the function to
identify the number and names of the outputs.

!!! note
    If any indicator raises an error while parsing, try increasing the number of rows passed to the
    indicator function. For example, use `parse_kwargs=dict(test_index_len=150)`.

To get the list of all supported indicators:

```pycon
>>> vbt.IF.list_pandas_ta_indicators()  # (1)!
{'ABERRATION',
  'ACCBANDS',
  'AD',
  ...
  'XSIGNALS',
  'ZLMA',
  'ZSCORE'}
```

1. Using [IndicatorFactory.list_pandas_ta_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_pandas_ta_indicators).

To get an indicator:

```pycon
>>> vbt.IF.from_pandas_ta('RSI')  # (1)!
vectorbtpro.indicators.factory.pandas_ta.RSI
```

1. Using [IndicatorFactory.from_pandas_ta](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_pandas_ta).

Or, use the shortcut:

```pycon
>>> vbt.pandas_ta('RSI')
vectorbtpro.indicators.factory.pandas_ta.RSI
```

## TA

[IndicatorFactory.from_ta](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_ta)
can parse [TA](https://github.com/bukosabino/ta) indicators. Similar to Pandas TA, TA indicators must be
explicitly parsed to obtain the context of each indicator function. Since every indicator is a class,
there is a method called [IndicatorFactory.parse_ta_config](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.parse_ta_config)
that reads the signature, docstring, and attributes of the class to determine the input, parameter,
and output names, as well as their defaults.

To get the list of all supported indicators:

```pycon
>>> vbt.IF.list_ta_indicators()  # (1)!
{'ADXIndicator',
  'AccDistIndexIndicator',
  'AroonIndicator',
  ...
  'VortexIndicator',
  'WMAIndicator',
  'WilliamsRIndicator'
```

1. Using [IndicatorFactory.list_ta_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_ta_indicators).

To get an indicator:

```pycon
>>> vbt.IF.from_ta('RSIIndicator')  # (1)!
vectorbtpro.indicators.factory.ta.RSIIndicator
```

1. Using [IndicatorFactory.from_ta](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_ta).

Or, use the shortcut:

```pycon
>>> vbt.ta('RSIIndicator')
vectorbtpro.indicators.factory.ta.RSIIndicator
```

## Expressions

Expressions are a brand-new way to define indicators of any complexity using regular strings.
The main advantage of expressions over custom and apply functions is that VBT can easily
introspect the code of an indicator and add many useful automations.

Expressions are converted into full-featured indicators by a hybrid method,
[IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr).
Why hybrid? It is both a class method and an instance method. You can call this method on an instance
if you want full control over the indicator's specification, or on a class if you want the entire
specification to be parsed for you. Let's try both approaches while building an ATR indicator!

### Instance method

Here is a semi-automated implementation using the instance method:

```pycon
>>> expr = """
... tr0 = abs(high - low)
... tr1 = abs(high - fshift(close))
... tr2 = abs(low - fshift(close))
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, n)
... tr, atr
... """
>>> ATR = vbt.IF(
...     class_name='ATR',
...     input_names=['high', 'low', 'close'],
...     param_names=['n'],
...     output_names=['tr', 'atr']
... ).from_expr(expr, n=14)

>>> atr = ATR.run(ohlc['high'], ohlc['low'], ohlc['close'])
>>> atr.atr
2019-12-31 00:00:00+00:00          NaN
2020-01-01 00:00:00+00:00          NaN
2020-01-02 00:00:00+00:00          NaN
2020-01-03 00:00:00+00:00          NaN
2020-01-04 00:00:00+00:00          NaN
...                                ...
2020-05-27 00:00:00+00:00    13.394434
2020-05-28 00:00:00+00:00    13.338482
2020-05-29 00:00:00+00:00    13.480809
2020-05-30 00:00:00+00:00    13.003231
2020-05-31 00:00:00+00:00    12.888624
Freq: D, Length: 153, dtype: float64
```

The expression `expr` is regular Python code without extensions that is evaluated using Python's `eval`
command. All function names are resolved by the parser before evaluation.

### Class method

Here is a fully-automated implementation using the class method and annotations:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(@in_high - @in_low)
... tr1 = abs(@in_high - fshift(@in_close))
... tr2 = abs(@in_low - fshift(@in_close))
... @out_tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... @out_atr = wwm_mean_1d(@out_tr, @p_n)
... @out_tr, @out_atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14)
```

In the first example, we provided all the required information manually by constructing
a new instance of [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory).
The parser identified each input and parameter name in the expression and replaced them with actual arrays.
In the second example, we used annotations to give the parser hints about the meaning of each variable.
Whenever the parser finds a substring starting with `@`, it knows the variable has a special meaning
for constructing a factory instance. The prefixes `@_in`, `@_p`, and `@_out` indicate an input,
parameter, and output, respectively. The names appear in the order they appear in the expression
(apart from OHLCV, where H always comes after O):

```pycon
>>> ATR.input_names
('high', 'low', 'close')
```

The parser can also detect information not starting with a special character.
For example, inputs such as `open`, `high`, `low`, `close`, and `volume` are recognized automatically,
so there is no need to annotate them. These are called magnet inputs, and you can specify them via the
`magnet_inputs` argument. If no outputs use annotations and the expression is a multiline string with
the last line containing a tuple of valid variable names, you do not need to annotate the outputs either.
Also, as shown above, you can provide the class name on the first line, followed by a colon:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - fshift(close))
... tr2 = abs(low - fshift(close))
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, @p_n)
... tr, atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14)
>>> ATR.input_names
('high', 'low', 'close')

>>> ATR.output_names
('tr', 'atr')
```

What about functions? The parser identifies functions by searching various modules and packages.
In this example, `abs` and `nanmax` are from NumPy, while `wwm_mean_1d` is found among generic
Numba-compiled functions in [nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/) (even without the `_nb` suffix).
See the API for [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr)
for full details. To avoid naming conflicts, you can explicitly access the NumPy, Pandas, and VBT
modules as `np`, `pd`, and `vbt`:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - vbt.nb.fshift_nb(close))
... tr2 = abs(low - vbt.nb.fshift_nb(close))
... tr = np.nanmax(np.column_stack((tr0, tr1, tr2)), axis=1)
... atr = vbt.nb.wwm_mean_1d_nb(tr, n)
... tr, atr
... """
```

### TA-Lib

Another automation covers TA-Lib indicators: VBT will replace any variable annotated
with `@talib` with an actual TA-Lib indicator function that can operate on both one-dimensional and
two-dimensional data!

```pycon
>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - fshift(close))
... tr2 = abs(low - fshift(close))
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = @talib_ema(tr, 2 * n - 1)  # Wilder's EMA
... tr, atr
... """
```

### Context

How can you define your own functions and rules? Any additional keyword argument passed to
[IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr)
acts as a context for evaluation and can replace a variable with the same name. Let's define our
own function `shift_close`, which is an alias for [fshift_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.fshift_nb):

```pycon
>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - shift_close(close))
... tr2 = abs(low - shift_close(close))
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, @p_n)
... tr, atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14, shift_close=vbt.nb.fshift_nb)
```

You can also create functions that depend on the evaluation context. In the example above,
you can have `shift_close` accept the context and retrieve the number of periods to shift the closing price
(just as an example):

```pycon
>>> def shift_close(close, context):
...     return vbt.nb.fshift_nb(close, context.get('shift', 1))

>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - shift_close(close))
... tr2 = abs(low - shift_close(close))
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, @p_n)
... tr, atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14, shift_close=shift_close, shift=2)
```

The context will be automatically passed to the function once `context` has been recognized in its
arguments. Moreover, you can also make `shift_close` retrieve the closing price itself.
Notice how `shift_close` takes no arguments in the expression:

```pycon
>>> def shift_close(context):
...     return vbt.nb.fshift_nb(context['close'], context.get('shift', 1))

>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - shift_close())
... tr2 = abs(low - shift_close())
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, @p_n)
... tr, atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14, shift_close=shift_close)
```

If you run this, you will receive an error indicating that `close` was not found in the context. This happens
because the input `close` is not "visible" in the expression, so it was not added to the list of input names.
To make any input, in-place output, or parameter visible—even if it is not included directly in the expression—you
must notify VBT that a function depends on it. You do this using a dictionary called
`func_mapping`, which maps functions to the magnet names they depend on:

```pycon
>>> func_mapping = dict(
...     shift_close=dict(
...         func=shift_close,
...         magnet_inputs=['close']
...     )
... )
>>> ATR = vbt.IF.from_expr(expr, n=14, func_mapping=func_mapping)
```

Since `shift_close` depends only on the context, you can instruct the parser to call it
before evaluation and only once, effectively caching its result. To do this, use
`res_func_mapping` instead of `func_mapping`:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - shifted_close)
... tr2 = abs(low - shifted_close)
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, @p_n)
... tr, atr
... """
>>> res_func_mapping = dict(
...     shifted_close=dict(
...         func=shift_close,
...         magnet_inputs=['close']
...     )
... )
>>> ATR = vbt.IF.from_expr(expr, n=14, res_func_mapping=res_func_mapping)
```

Notice that `shifted_close` no longer has parentheses—it is now an array.

### Settings

But that's not all. What if you want to override any information passed to
[IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr)
from within the expression itself? You can! Define a dictionary anywhere in the expression and annotate
it with `@settings({...})`. The dictionary inside the parentheses will be evaluated with Python's `eval`
function before the main evaluation and will be merged over the default settings of the factory method.

Let's rewrite the [instance method](#instance-method) example using only an expression:

```pycon
>>> expr = """
... @settings(dict(
...     factory_kwargs=dict(
...         class_name='ATR',
...         input_names=['high', 'low', 'close'],
...         param_names=['n'],
...         output_names=['tr', 'atr']
...     ),
...     n=14
... ))
...
... tr0 = abs(high - low)
... tr1 = abs(high - fshift(close))
... tr2 = abs(low - fshift(close))
... tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... atr = wwm_mean_1d(tr, n)
... tr, atr
... """
>>> ATR = vbt.IF.from_expr(expr)
```

### Stacking

Remember, you can use any Python code in your expressions—even other indicators.
To make using indicators easier, there is a convenient annotation `@res`, which takes
the name of an indicator and creates an automatically resolved function from it, similar to
`shifted_close` above. This function becomes an entry in `res_func_mapping`, and the indicator's
input, in-place output, and parameter names are added to the entry's magnet lists. This means you do not
need to worry about passing the correct information to the indicator—vectorbt handles it for you!

Let's illustrate this by defining basic SuperTrend bands:

```pycon
>>> expr = """
... SuperTrend[st]:
... avg_price = (high + low) / 2
... up = avg_price + @p_mult * @res_talib_atr
... down = avg_price - @p_mult * @res_talib_atr
... up, down
... """  # (1)!
>>> SuperTrend = vbt.IF.from_expr(expr, mult=3, atr_timeperiod=10)

>>> SuperTrend.input_names  # (2)!
('high', 'low', 'close')

>>> SuperTrend.param_names  # (3)!
('mult', 'atr_timeperiod')

>>> SuperTrend.output_names
('up', 'down')

>>> st = SuperTrend.run(ohlc['high'], ohlc['low'], ohlc['close'])
>>> st.up
2019-12-31 00:00:00+00:00           NaN
2020-01-01 00:00:00+00:00           NaN
2020-01-02 00:00:00+00:00           NaN
2020-01-03 00:00:00+00:00           NaN
2020-01-04 00:00:00+00:00           NaN
...                                 ...
2020-05-27 00:00:00+00:00    240.698333
2020-05-28 00:00:00+00:00    245.683118
2020-05-29 00:00:00+00:00    251.211675
2020-05-30 00:00:00+00:00    256.406868
2020-05-31 00:00:00+00:00    250.201819
Freq: D, Length: 153, dtype: float64
```

1. A short name for the indicator can be provided in square brackets following the class name.

2. The TA-Lib ATR indicator depends on `high`, `low`, and `close`.

3. Our indicator depends on the multiplier `mult`, while ATR also depends on `timeperiod`.

What happens if two indicators have overlapping inputs, parameters, or other arguments?
Every argument except for the inputs receives a prefix with the indicator's short name
([IndicatorBase.short_name](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.short_name)).
Under the hood, VBT checks the signature of the indicator's `run` method to see
whether there is an argument with the same name in the context (and remember the prefix).

By default, the resolved function returns raw output as one or more NumPy arrays.
If the indicator has more than one output, you can use regular indexing to select a specific array,
such as `@res_talib_macd[0]`. Let's disable raw outputs for ATR and access the `real` Pandas object
from its indicator instance instead:

```pycon
>>> expr = """
... SuperTrend[st]:
... avg_price = (high + low) / 2
... up = avg_price + @p_mult * @res_talib_atr.real.values
... down = avg_price - @p_mult * @res_talib_atr.real.values
... up, down
... """
>>> SuperTrend = vbt.IF.from_expr(
...     expr,
...     mult=3,
...     atr_timeperiod=10,
...     atr_kwargs=dict(return_raw=False))  # (1)!
```

1. If you run `vbt.phelp(vbt.talib('ATR').run)`, you will see that additional keyword arguments
are passed as `**kwargs`, so you can specify `atr_kwargs` to target those variable arguments.

### One-liners

There is nothing more satisfying than defining an indicator in one line :drooling_face:

```pycon
>>> AvgPrice = vbt.IF.from_expr("AvgPrice: @out_avg_price:(high + low) / 2")

>>> AvgPrice.run(ohlc['high'], ohlc['low']).avg_price
2019-12-31 00:00:00+00:00    100.496714
2020-01-01 00:00:00+00:00    100.216791
2020-01-02 00:00:00+00:00     92.735610
2020-01-03 00:00:00+00:00     90.353978
2020-01-04 00:00:00+00:00     91.676538
...                                 ...
2020-05-27 00:00:00+00:00    200.426358
2020-05-28 00:00:00+00:00    205.655007
2020-05-29 00:00:00+00:00    210.587055
2020-05-30 00:00:00+00:00    217.806298
2020-05-31 00:00:00+00:00    212.041686
Freq: D, Length: 153, dtype: float64
```

Notice how the output annotation `@out` is no longer bound to any variable and is now written similarly
to the class name, with a trailing colon followed by the output expression.
If there are multiple outputs, separate their output expressions with a comma.
Here is a single-line expression for basic SuperTrend bands with multiple outputs:

```pycon
>>> SuperTrend = vbt.IF.from_expr(
...     "SuperTrend[st]: @out_up:@res_avg_price + @p_mult * @res_talib_atr, "
...     "@out_down:@res_avg_price - @p_mult * @res_talib_atr",  # (1)!
...     avg_price=AvgPrice,
...     atr_timeperiod=10, 
...     mult=3)
>>> st = SuperTrend.run(ohlc['high'], ohlc['low'], ohlc['close'])

>>> fig = ohlc.vbt.ohlcv.plot()
>>> st.up.rename('Upper').vbt.plot(fig=fig)
>>> st.down.rename('Lower').vbt.plot(fig=fig)
>>> fig.show()
```

1. Split the expression for better readability.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/supertrend.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/indicators/supertrend.dark.svg#only-dark){: .iimg loading=lazy }

### Using Pandas

Like many other factory methods, [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr)
passes inputs and in-place outputs as two-dimensional NumPy arrays. You can enable the `keep_pd` flag to work
with Pandas objects. Let's run our ATR indicator using only Pandas:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(high - low)
... tr1 = abs(high - close.shift())
... tr2 = abs(low - close.shift())
... tr = pd.concat((tr0, tr1, tr2), axis=1).max(axis=1)
... atr = tr.ewm(alpha=1 / @p_n, adjust=False, min_periods=@p_n).mean()
... tr, atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14, keep_pd=True)
>>> atr = ATR.run(ohlc['high'], ohlc['low'], ohlc['close'])
>>> atr.atr
2019-12-31 00:00:00+00:00          NaN
2020-01-01 00:00:00+00:00          NaN
2020-01-02 00:00:00+00:00          NaN
2020-01-03 00:00:00+00:00          NaN
2020-01-04 00:00:00+00:00          NaN
                                ...    
2020-05-27 00:00:00+00:00    13.394434
2020-05-28 00:00:00+00:00    13.338482
2020-05-29 00:00:00+00:00    13.480809
2020-05-30 00:00:00+00:00    13.003231
2020-05-31 00:00:00+00:00    12.888624
Freq: D, Length: 153, dtype: float64
```

!!! note
    Unlike the previous NumPy-only expressions, this expression will not work for
    multiple columns of input data.

For simpler expressions, you can instruct the parser to use
[pandas.eval](https://pandas.pydata.org/docs/reference/api/pandas.eval.html) instead of Python's `eval`.
This provides multi-threading and other performance benefits for large inputs, since `pd.eval`
switches to [NumExpr](https://github.com/pydata/numexpr) by default:

```pycon
>>> AvgPrice = vbt.IF.from_expr(
...     "AvgPrice: @out_avg_price:(high + low) / 2",
...     use_pd_eval=True
... )

>>> AvgPrice.run(ohlc['high'], ohlc['low']).avg_price
2019-12-31 00:00:00+00:00    100.496714
2020-01-01 00:00:00+00:00    100.216791
2020-01-02 00:00:00+00:00     92.735610
2020-01-03 00:00:00+00:00     90.353978
2020-01-04 00:00:00+00:00     91.676538
                                ...    
2020-05-27 00:00:00+00:00    200.426358
2020-05-28 00:00:00+00:00    205.655007
2020-05-29 00:00:00+00:00    210.587055
2020-05-30 00:00:00+00:00    217.806298
2020-05-31 00:00:00+00:00    212.041686
Freq: D, Length: 153, dtype: float64
```

### Debugging

To view the expression after parsing all annotations, set `return_clean_expr` to True:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(@in_high - @in_low)
... tr1 = abs(@in_high - fshift(@in_close))
... tr2 = abs(@in_low - fshift(@in_close))
... @out_tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... @out_atr = wwm_mean_1d(@out_tr, @p_n)
... @out_tr, @out_atr
... """
>>> print(vbt.IF.from_expr(expr, n=14, return_clean_expr=True))
tr0 = abs(__in_high - __in_low)
tr1 = abs(__in_high - fshift(__in_close))
tr2 = abs(__in_low - fshift(__in_close))
__out_tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
__out_atr = wwm_mean_1d(__out_tr, __p_n)
__out_tr, __out_atr
```

Additionally, just as in regular Python code, you can place `print` statements to
explore the state at each step of execution:

```pycon
>>> expr = """
... ATR:
... tr0 = abs(@in_high - @in_low)
... print('tr0: ', tr0.shape)
... tr1 = abs(@in_high - fshift(@in_close))
... print('tr1: ', tr1.shape)
... tr2 = abs(@in_low - fshift(@in_close))
... print('tr2: ', tr2.shape)
... @out_tr = nanmax(column_stack((tr0, tr1, tr2)), axis=1)
... print('tr: ', @out_tr.shape)
... @out_atr = wwm_mean_1d(@out_tr, @p_n)
... print('atr: ', @out_atr.shape)
... @out_tr, @out_atr
... """
>>> ATR = vbt.IF.from_expr(expr, n=14)
>>> atr = ATR.run(ohlc['high'], ohlc['low'], ohlc['close'])
tr0:  (153, 1)
tr1:  (153, 1)
tr2:  (153, 1)
tr:  (153,)
atr:  (153,)
```

## WorldQuant's Alphas

[IndicatorFactory.from_wqa101](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_wqa101)
uses the expression parser to parse and execute [101 Formulaic Alphas](https://arxiv.org/pdf/1601.00991.pdf).
Each alpha expression is defined in [wqa101_expr_config](https://vectorbt.pro/pvt_41531c19/api/indicators/expr/#vectorbtpro.indicators.expr.wqa101_expr_config),
while most functions and resolved functions used in the alpha expressions are defined in
[expr_func_config](https://vectorbt.pro/pvt_41531c19/api/indicators/expr/#vectorbtpro.indicators.expr.expr_func_config) and 
[expr_res_func_config](https://vectorbt.pro/pvt_41531c19/api/indicators/expr/#vectorbtpro.indicators.expr.expr_res_func_config), respectively.

To get an indicator:

```pycon
>>> WQA53 = vbt.IF.from_wqa101(53)
>>> wqa53 = WQA53.run(ohlc['open'], ohlc['high'], ohlc['low'], ohlc['close'])
>>> wqa53.out
2019-12-31 00:00:00+00:00         NaN
2020-01-01 00:00:00+00:00         NaN
2020-01-02 00:00:00+00:00         NaN
2020-01-03 00:00:00+00:00         NaN
2020-01-04 00:00:00+00:00         NaN
                                ...   
2020-05-27 00:00:00+00:00    0.193719
2020-05-28 00:00:00+00:00   -0.858778
2020-05-29 00:00:00+00:00    0.452096
2020-05-30 00:00:00+00:00    0.376475
2020-05-31 00:00:00+00:00   -0.539368
Freq: D, Length: 153, dtype: float64
```

Or, use a shortcut:

```pycon
>>> vbt.wqa101(53)
vectorbtpro.indicators.factory.wqa101.WQA53
```

Replicating an alpha indicator is straightforward: look up its expression in the config and 
pass it to [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr):

```pycon
>>> WQA53 = vbt.IF.from_expr("-delta(((close - low) - (high - close)) / (close - low), 9)")
>>> wqa53 = WQA53.run(ohlc['open'], ohlc['high'], ohlc['low'], ohlc['close'])
>>> wqa53.out
2019-12-31 00:00:00+00:00         NaN
2020-01-01 00:00:00+00:00         NaN
2020-01-02 00:00:00+00:00         NaN
2020-01-03 00:00:00+00:00         NaN
2020-01-04 00:00:00+00:00         NaN
                                ...   
2020-05-27 00:00:00+00:00    0.193719
2020-05-28 00:00:00+00:00   -0.858778
2020-05-29 00:00:00+00:00    0.452096
2020-05-30 00:00:00+00:00    0.376475
2020-05-31 00:00:00+00:00   -0.539368
Freq: D, Length: 153, dtype: float64
```

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/indicators/parsers.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/portfolio/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
children:
  - https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/
  - https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-signals/
name: Portfolio
type: page
icon: material-chart-areaspline
---

# Portfolio

A portfolio refers to any combination of financial assets held by a trader. In VBT, a
"portfolio" is a multidimensional structure designed to simulate and track multiple independent as
well as dependent portfolio instances. The primary function of a portfolio is to apply trading
logic to a set of inputs and simulate a realistic trading environment, known as "simulation". The
outputs of this simulation are orders and other information that users can use to assess the
portfolio's performance, a process also referred to as "reconstruction" or "post-analysis".
Both phases are isolated, which allows for a variety of interesting use cases in quantitative
analysis and data science.

The main class for simulating and analyzing portfolios (that is, actual backtesting) is
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio). This is a standard Python
class that subclasses [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable)
and provides access to a variety of Numba-compiled functions. It is structured similarly to other
analyzable classes, featuring diverse class methods for instantiation from different types of inputs
(such as [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
which accepts signals). This stateful class can wrap and index any Pandas-like objects it contains,
compute metrics, and display (sub-)plots for quick introspection of the stored data.

## Simulation

So, what is a simulation? It is simply a sophisticated loop! :doughnut:

A typical simulation in VBT takes some inputs (such as signals), gradually iterates over their
rows (representing time steps in the real world) using a for-loop, and at each row runs the trading
logic by issuing and executing orders. It then updates the current state of the trading environment,
such as the cash balance and position size. This process mirrors how we would approach algorithmic
trading in reality: at each minute, hour, or day (each row), we decide what to do (the trading
logic) and place an order if we wish to change our market position.

Now, let's discuss execution. The core of VBT's backtesting engine is entirely Numba-compiled
for optimal performance. The engine's functionality is distributed across many functions within the
[portfolio.nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/) sub-package, covering everything from core order execution
commands to the calculation of P&L in trade records. It is important to note that these functions
are not intended for direct use (unless specifically required); instead, they are called by Python
functions higher in the stack. These higher-level functions handle proper pre-processing of input
data and post-processing of output data.

In the following sections, we will discuss order execution and processing, and we will gradually
implement a collection of simple pipelines to better illustrate various simulation concepts.

## Primitive commands

Keep in mind that VBT is an exceptionally raw backtester: its primary commands are "buy"
:green_circle: and "sell" :red_circle:. This means that any strategy that can be expressed as a set
of these commands is supported out of the box. It also means that more complex orders, such as
limit and stop-loss orders, must be implemented manually. In contrast to other backtesting
frameworks, where processing is monolithic and functionality is written in an
[object-oriented manner](https://en.wikipedia.org/wiki/Object-oriented_programming), Numba forces
VBT to implement most functionality in a procedural way.

!!! info
    Even though Numba supports OOP by compiling Python classes with `@jitclass`, they are treated as
    functions, must be statically typed, and have performance drawbacks that prevent us from
    adopting them at this time.

Functions related to order execution are primarily found in [portfolio.nb.core](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/).
The functions implementing our two primary commands are [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb)
and [sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.sell_nb). In addition to the
requested size and price of an order, the main input for each of these functions is the current
account state of type [AccountState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState).
This includes the cash balance, position size, and other details about the current environment.
Whenever we buy or sell, the function creates and returns a new state of the same type. It also
returns an order result of type [OrderResult](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderResult),
which includes the filled size, slippage-adjusted price, transaction fee, order side, status
information indicating whether the order succeeded or failed, and helpful details about any failure.

### Buying

The buy operation consists of two distinct actions: "long-buy," implemented by
[long_buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.long_buy_nb), and "short-buy,"
implemented by [short_buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.short_buy_nb).
The first opens or increases a long position, while the second reduces a short position. By chaining
these two actions, we can reverse a short position, which is handled automatically by
[buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb). This function checks the
current position (if any) and calls the appropriate function.

Suppose we have $100 available and want to buy 1 share at a price of $15:

```pycon
>>> from vectorbtpro import *

>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,  # (1)!
...     locked_cash=0.0,  # (2)!
...     free_cash=100.0  # (3)!
... )
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state,
...     size=1.0,
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=1.0,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=85.0,
    position=1.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=85.0
)
```

1. Debt is non-zero only when leveraging or shorting.

2. Locked cash is non-zero only when leveraging or shorting.

3. Free cash differs from the regular cash balance only when leveraging or shorting.

The returned state shows that we spent $15 and increased our position by 1 share. The order result
contains details about the executed order: we bought 1 share for $15, with no transaction fees.
Since order side and status are fields of the named tuples
[OrderSide](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderSide) and
[OrderStatus](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderStatus), we can look up what
those numbers mean as follows:

```pycon
>>> vbt.pf_enums.OrderSide._fields[order_result.side]
'Buy'

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Filled'
```

!!! info
    If any value is `-1` and cannot be found in the named tuple, the information is unavailable.

Now, with the new state, let's execute a transaction that uses up the remaining cash:

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state,  # (1)!
...     size=np.inf,  # (2)!
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.666666666666667,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=0.0,
    position=6.666666666666667,
    debt=0.0,
    locked_cash=0.0,
    free_cash=0.0
)
```

1. Use the previous account state as input.

2. Infinity indicates use of the entire balance.

Since VBT was originally designed for cryptocurrency and fractional shares, the default
behavior is to buy as much as possible (here, `5.67`), even if this amount is less than requested.
But what if we want to buy only whole shares? Let's specify a size granularity of 1 to ensure only
integer amounts are allowed:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state,
...     size=np.inf,
...     price=15.0,
...     size_granularity=1
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.0,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=10.0,
    position=6.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=10.0
)
```

Now we have bought exactly 6 shares. With the new account state, let's repeat the transaction:

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     new_account_state,  # (1)!
...     size=np.inf,
...     price=15.0,
...     size_granularity=1
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=np.nan,
    price=np.nan,
    fees=np.nan,
    side=-1,
    status=1,
    status_info=5
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=10.0,
    position=6.0,
    debt=0.0,
    free_cash=10.0
)
```

1. Use the account state from the previous operation.

The account state remains unchanged. The presence of NaNs in the order result suggests a failed
order. To look up the meaning of the status and status information values, we can use
[OrderStatus](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderStatus) and
[OrderStatusInfo](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderStatusInfo):

```pycon
>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Ignored'

>>> vbt.pf_enums.OrderStatusInfo._fields[order_result.status_info]
'SizeZero'

>>> vbt.pf_enums.status_info_desc[order_result.status_info]  # (1)!
'Size is zero'
```

1. A list is available with more detailed descriptions of different statuses.

Here, the status "Size is zero" means that, after considering the cash balance and applying size
granularity, the (potentially) filled order size is zero, so the order is ignored. Ignored orders
have no effect on the trading environment and are simply, well, *ignored*. Sometimes, when a specific
requirement cannot be met, the status will be "Rejected," indicating the request could not be
fulfilled and an error can be thrown if desired.

For example, let's try to buy more than is possible:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=1000.0, 
...     price=15.0,
...     allow_partial=False
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=np.nan,
    price=np.nan,
    fees=np.nan,
    side=-1,
    status=2,
    status_info=12
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=100.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=100.0
)

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Rejected'

>>> vbt.pf_enums.status_info_desc[order_result.status_info]
'Final size is less than requested'
```

There are many other parameters to control execution. Let's use 50% of the cash, and apply 1% in
fees and slippage:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0,
...     fees=0.01,  # (1)!
...     slippage=0.01,  # (2)!
...     percent=0.5  # (3)!
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=3.2676534980230696,
    price=15.15,
    fees=0.4950495049504937,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=50.0,
    position=3.2676534980230696,
    debt=0.0,
    locked_cash=0.0,
    free_cash=50.0
)
```

1. 0.01 = 1%. This is always paid in cash. To specify fixed fees, use `fixed_fees` instead.

2. 0.01 = 1%. This is applied to the price. By artificially increasing the price, you always put
yourself at a disadvantage, but this can be useful to make backtesting more realistic.

3. 0.01 = 1%. This is applied to the resources used to open or increase a position.

The final fees and the price adjusted with slippage are shown in the order result.

Whenever we place an order, we can specify any price. As a result, it is possible that the provided
price is, perhaps by mistake, higher than the highest price of that bar or lower than the lowest
price of that bar. Also, if the user wants to use the closing price and specifies slippage,
this could result in unrealistic prices. To prevent such mistakes, the function performs an OHLC
check. For this, we need to specify the `price_area` argument of type
[PriceArea](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceArea) with the price boundaries,
and indicate what should happen if a boundary is violated using `price_area_vio_mode` of type
[PriceAreaVioMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceAreaVioMode):

```pycon
>>> price_area = vbt.pf_enums.PriceArea(
...     open=10,
...     high=14,
...     low=8,
...     close=12
... )
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state,
...     size=np.inf,
...     price=np.inf,
...     price_area=price_area,
...     price_area_vio_mode=vbt.pf_enums.PriceAreaVioMode.Error
... )
ValueError: Adjusted order price is above the highest price
```

### Selling

The sell operation consists of two distinct actions: "long-sell," implemented by
[long_sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.long_sell_nb),
and "short-sell," implemented by [short_sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.short_sell_nb).
The first reduces a long position, while the second opens or increases a short position. By chaining
these two actions, we can reverse a long position. This is handled automatically by
[sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.sell_nb), which checks the current
position (if any) and calls the appropriate function.

The function for selling accepts the same arguments as buying, but uses them in the opposite
direction. Let's remove 2 shares from a position of 10 shares:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state,
...     size=2.0,
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=2.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=30.0,
    position=8.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=30.0
)
```

The size in the order result remains positive, but the side has changed from 0 to 1:

```pycon
>>> vbt.pf_enums.OrderSide._fields[order_result.side]
'Sell'
```

### Shorting

Shorting is a regular sell operation with [sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.sell_nb),
but with one key difference: it now involves the debt and the locked cash balance. When we short,
we borrow shares and sell them to buyers at the market price. This increases the cash balance,
turns the position size negative, and registers the received cash as debt, which is subtracted from
the free cash balance. When we buy back some shares, the debt decreases in proportion to the value of
the shares bought back, while the free cash may increase or decrease depending on whether the repurchase
price was higher or lower than the average short-selling price. Once we cover the short position
entirely, the debt becomes zero and the free cash returns to the same level as the regular cash balance.

!!! note
    You should not treat debt as an absolute amount of cash you owe, since you owe shares, not cash.
    It is used to calculate the average leverage and entry price of the short position, which then
    helps determine changes in the free cash balance with each trade.

To borrow any shares, we need a positive free cash balance to use as collateral. The exact amount
of free cash required for shorting depends on the margin; by default, you need to have funds available
in your margin account equal to the value of the shares to be borrowed. For example, if you short a stock
and the new position is worth $100, you are required to have the $100 obtained from the short sale plus
an additional $100 in cash, for a total of $200. Depending on the definition, this is a 100% (before sale)
or 200% (after sale) initial margin requirement. Maintenance margin and liquidation checks are currently
the responsibility of the user.

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf,  # (1)!
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.666666666666667,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=200.0,
    position=-6.666666666666667,
    debt=100.0,
    locked_cash=100.0,
    free_cash=0.0
)
```

1. Short-sell as much as possible.

!!! info
    Infinity is a special value in VBT and usually means "go as far as you can".

Here is what happened. First, we moved all available free cash ($100) into the locked cash balance,
so it is now collateral for the shorting operation. Because the default leverage is 1, we have borrowed
shares worth $100, which has been added to the regular cash balance and also recorded as debt. This
corresponds to (minus = borrowed) 6.67 shares. However, since we have doubled the cash balance, it
could be used for other assets. To prevent this, all operations use only the free cash balance. Since
the free cash is now zero, how can we buy back the borrowed shares? Remember, the debt and the locked
cash together represent the total amount of cash we used at the start. By adding these amounts to the
free cash, we get our cash limit for the current buy operation, which matches the regular cash if we
are only dealing with one asset.

To adjust the margin, use the `leverage` argument. For example, setting it to 2 will allow us to
borrow twice as many shares as can be covered by the current free cash:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf,
...     price=15.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=13.333333333333334,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=300.0,
    position=-13.333333333333334,
    debt=200.0,
    locked_cash=100.0,
    free_cash=0.0
)
```

The debt-to-locked-cash ratio is now 2, which matches the leverage we specified.

!!! info
    You can specify a different leverage for each short-sell order, even within the same position.

Let's try running the same operation again, but this time using the new account state:

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=np.inf,
...     price=15.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=np.nan,
    price=np.nan,
    fees=np.nan,
    side=-1,
    status=2,
    status_info=6
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=300.0,
    position=-13.333333333333334,
    debt=200.0,
    locked_cash=100.0,
    free_cash=0.0
)

>>> vbt.pf_enums.OrderStatus._fields[order_result.status]
'Rejected'

>>> vbt.pf_enums.status_info_desc[order_result.status_info]
'Not enough cash'
```

We see that VBT prevents the free cash balance from going negative.

To order any quantity possible, we can use unlimited leverage:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=1000, 
...     price=15.0, 
...     leverage=np.inf
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=1000.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=15100.0,
    position=-1000.0,
    debt=15000.0,
    locked_cash=100.0,
    free_cash=0.0
)
```

What is the effective leverage of this operation?

```pycon
>>> new_account_state.debt / new_account_state.locked_cash
150.0
```

If we calculate the current portfolio value, it still defaults to the initial cash,
since no transaction costs were involved and no additional trades were made:

```pycon
>>> new_account_state.cash + new_account_state.position * order_result.price
100.0
```

As we can see, the positive cash balance and the negative position size keep the total value balanced.
Now, let's illustrate buying back some shares using [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb).
First, we will borrow 10 shares with 2x leverage and sell them at $10 per share:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=10.0, 
...     price=15.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=250.0,
    position=-10.0,
    debt=150.0,
    locked_cash=75.0,
    free_cash=25.0
)
```

Let's buy back 5 shares at $30 per share (my condolences):

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=30.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=30.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=100.0,
    position=-5.0,
    debt=75.0,
    locked_cash=37.5,
    free_cash=-12.5
)
```

We executed the order for $150, which was deducted from the regular cash balance. The position has
been reduced by half, resulting in 5 borrowed shares. Along with the position, the debt and locked
cash have also been reduced by half. Given the absolute amount of released debt ($75), we can
calculate the P&L by subtracting the total spent cash from the total released debt, resulting in
-$75. This operation also released some locked cash - $37.5 - which was added back to our
free cash balance, yielding -$37.5 and making it negative. A negative free cash balance means we
will not be able to buy any other assets apart from reducing short positions, which could release
additional funds. Profits and losses are shared among all assets within the same group with cash
sharing. Even with negative free cash, we can still buy back more shares, since the sum of
`debt`, `locked_cash`, and `free_cash` is greater than zero.

In addition to using debt and locked cash to compute effective leverage, we can also calculate
the average entry price of the entire position:

```pycon
>>> new_account_state2.debt / abs(new_account_state2.position)
15.0
```

Suppose instead the price drops to $10 per share (my congratulations!):

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=10.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=200.0,
    position=-5.0,
    debt=75.0,
    locked_cash=37.5,
    free_cash=87.5
)
```

We see that the debt and locked cash have decreased to the same levels as before (because
we bought back the same number of shares), but the free cash balance is now $87.5, netting $25
in profit! The calculation is straightforward: take the total amount of spent cash
($5 * 10 = $50) and subtract it from the total released debt (0.5 * $150 = $75) to get the P&L.
When we add the P&L, released locked cash (0.5 * $75 = $37.5), and existing free cash ($25),
we get the new free cash of $87.5, which is immediately available for all other assets.

Let's compute the equity to confirm the profit:

```pycon
>>> st0 = account_state
>>> st1 = new_account_state2
>>> avg_entry_price = st1.debt / abs(st1.position)  # (1)!
>>> new_equity = st1.cash + st1.position * avg_entry_price
>>> new_equity - st0.cash
25.0
```

1. The remaining shares are still valued at $15.

Let's close out the open short position using the same price:

```pycon
>>> order_result, new_account_state3 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state2, 
...     size=5.0, 
...     price=10.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state3)
AccountState(
    cash=150.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=150.0
)
```

The free cash balance now equals the regular cash balance, and we are debt-free! Additionally,
the last two operations have brought us $50 in profit, or (15 - 10) * 10 = $50.

Finally, let's try to close the position using an extremely high price!

```pycon
>>> order_result, new_account_state3 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state2, 
...     size=5.0, 
...     price=100.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=2.0,
    price=100.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state3)
AccountState(
    cash=0.0,
    position=-3.0,
    debt=45.0,
    locked_cash=22.5,
    free_cash=-67.5
)
```

We were able to buy back only 2 shares out of the remaining 5. If we try the same operation again,
we will see the "Not enough cash" message because `debt + locked_cash + free_cash` is less than or
equal to zero. We also notice the regular cash balance drops to zero, indicating we have exhausted all
our capital; however, you should not rely on this balance alone when making trading decisions!
If another asset buys shares using leverage, the regular cash balance may become negative.
This does not necessarily mean we are out of cash—only free cash (along with debt and
locked cash when covering shorts) gives us the correct information.

### Leverage

Although VBT allows setting any cash amount (even infinite) and ordering as many shares as
the user wants, this approach comes with some drawbacks: infinite cash leads to an infinite portfolio value,
which makes certain operations on that value impossible, such as converting a target percentage into a target
number of shares. Also, the more cash we have, the smaller the potential contribution of positions to the
portfolio value, thus lowering the magnitude of portfolio returns. What we really want is to multiply those
contributions without inflating the cash balance, which can be effectively done using leverage.

Leverage involves borrowing additional funds to buy shares. In contrast to shorting, leverage is
applied to long positions and borrows cash rather than shares. However, the underlying mechanism is quite
similar. First, we multiply the available free cash by `leverage`. Next, we determine the order
value and the fraction to be borrowed. Finally, we move the borrowed cash to `debt`, declare a
part of the free cash as collateral, and move it to `locked_cash`.
Since locked cash must be spent to buy a portion of the shares, it changes the way we calculate effective
leverage: use `debt / locked_cash + 1` instead of `debt / locked_cash`.

Suppose we have $100 in our margin account and want to buy $200 worth of shares.
As we learned earlier, we can specify infinite leverage and VBT will calculate the effective
leverage for us:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=20, 
...     price=10.0,
...     leverage=np.inf
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=20,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=-100.0,
    position=20.0,
    debt=100.0,
    locked_cash=100.0,
    free_cash=0.0
)
```

As shown, $100 is deducted from our free cash balance, and an additional $100 is borrowed,
which brings the effective leverage to 2:

```pycon
>>> new_account_state.debt / new_account_state.locked_cash + 1
2.0
```

Buying 10 shares instead would use no leverage, since the transaction can be fully
covered by the free cash, even if leverage is set to infinity:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=10.0,
...     leverage=np.inf
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=0.0,
    position=10.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=0.0
)
```

Is it possible to use only a portion of our own free cash while borrowing the rest?
Yes! The command [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb) accepts
the argument `leverage_mode` of type [LeverageMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.LeverageMode),
which supports two modes: "lazy" and "eager" leveraging. The first mode is the default and
enables leverage only if the quantity to be bought cannot be fulfilled with your own resources.
The second mode enables leverage for any quantity and requires the leverage to be set explicitly.
Using infinite leverage in this mode will raise an error.

!!! note
    Shorting supports "lazy" leveraging only.

Let's buy 10 shares with 3x leverage:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=10.0,
...     leverage=3,
...     leverage_mode=vbt.pf_enums.LeverageMode.Eager
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10,
    price=10.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=0.0,
    position=10.0,
    debt=66.66666666666667,
    locked_cash=33.333333333333336,
    free_cash=66.66666666666666
)
```

We have used only $33.33 from our free cash balance as collateral to borrow an additional $66.67,
making a total of $100 spent to buy the desired quantity.

How do we repay the debt? When selling, the debt and locked cash balances decrease proportionally
to the number of shares sold. This is the same procedure used when (partially) closing
a short position. The main difference is in the calculation of P&L: we take the total of the released
debt and locked cash and subtract them from the cash received from the sale.

First, we will use 2x leverage to buy 10 shares at $20 per share:

```pycon
>>> order_result, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=20.0,
...     leverage=2
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10,
    price=20.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=-100.0,
    position=10.0,
    debt=100.0,
    locked_cash=100.0,
    free_cash=0.0
)
```

Now let's sell 5 shares at $5 per share (my condolences):

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=5.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=5.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=-75.0,
    position=5.0,
    debt=50.0,
    locked_cash=50.0,
    free_cash=-25.0
)
```

We have retrieved $25 from this operation, which was added to the regular cash balance.
The debt and locked cash have both been cut in half because half of the leveraged position
has been closed. The P&L for this operation is the cash received ($25) minus the released debt ($50)
and locked cash ($50), for a total loss of $75. By adding this number to the released locked cash,
we get a new free cash of -$25, a change that is applied to all other assets using the same
cash balance, preventing them from opening or increasing positions.

Another way to calculate the P&L is using equity:

```pycon
>>> st0 = account_state
>>> st1 = new_account_state2
>>> avg_entry_price = (st1.debt + st1.locked_cash) / abs(st1.position)  # (1)!
>>> new_equity = st1.cash + st1.position * avg_entry_price
>>> new_equity - st0.cash
-75.0
```

1. The remaining shares are valued at $20.

Now, let's say that instead of the price dipping, it jumps to $40 per share (my congratulations!):

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=40.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=40.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=100.0,
    position=5.0,
    debt=50.0,
    locked_cash=50.0,
    free_cash=150.0
)
```

We have received $200 from this operation, which has been added to the regular cash balance.
The debt and locked cash have both been cut in half because half of the leveraged position
has been closed. The P&L for this operation is the cash received ($200) minus the released debt ($50)
and locked cash ($50), making it a profit of $100. By adding this amount to the released locked cash,
we get a new free cash balance of $150, which can now be used by all other assets sharing the same balance.

Let's close out the remaining position at the same price:

```pycon
>>> order_result, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=5.0, 
...     price=40.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=5.0,
    price=40.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=300.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=300.0
)
```

We made a profit of $200, which is the same as if we had used only our own cash:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=200.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=200.0
... )
>>> _, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     size=10, 
...     price=20.0
... )
>>> _, new_account_state2 = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     size=10.0, 
...     price=40.0
... )
>>> new_account_state2.free_cash - account_state.free_cash
200.0
```

### Symmetry

Long and short positions behave symmetrically. For example, let's open two opposite positions using an
infinite size and 10x leverage, and close them with a $5 per share price difference
in favor of the current position:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0
... )

>>> _, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=account_state, 
...     direction=vbt.pf_enums.Direction.LongOnly,
...     size=np.inf, 
...     price=10.0,
...     leverage=10
... )
>>> _, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=new_account_state, 
...     direction=vbt.pf_enums.Direction.LongOnly,
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(new_account_state)
AccountState(
    cash=600.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=600.0
)

>>> _, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     direction=vbt.pf_enums.Direction.ShortOnly,
...     size=np.inf, 
...     price=10.0,
...     leverage=10
... )
>>> _, new_account_state = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     direction=vbt.pf_enums.Direction.ShortOnly,
...     size=np.inf, 
...     price=5.0
... )
>>> vbt.pprint(new_account_state)
AccountState(
    cash=600.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=600.0
)
```

### Reversing

Positions in VBT can be reversed with a single order. To reverse a position, the `direction`
argument should remain at its default value—`Direction.Both`. Let's start with a position of 10 shares,
reverse it to the maximum extent in the short direction, and then reverse it again
to the maximum extent in the opposite (long) direction:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=20.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=300.0,
    position=-10.0,
    debt=150.0,
    locked_cash=150.0,
    free_cash=0.0
)

>>> order_result, new_account_state2 = vbt.pf_nb.buy_nb(
...     account_state=new_account_state, 
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=20.0,
    price=15.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state2)
AccountState(
    cash=0.0,
    position=10.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=0.0
)
```

Both operations are symmetric and cancel each other out with repeated execution,
so we have ultimately returned to our initial account state.

### Closing

To close out a position and avoid a reversal, we can either specify the exact size
or an infinite size along with the current direction via the `direction` argument of type
[Direction](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Direction).
For example, if we are in a long position and specify the long-only direction, the position
will not be reversed:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.sell_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0, 
...     direction=vbt.pf_enums.Direction.LongOnly
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=150.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=150.0
)
```

!!! note
    Using the [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb)
    and [sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.sell_nb) commands guarantees
    execution of the order in the long and short direction, respectively.

We can also use commands that are guaranteed to execute within the current position
and not open an opposite one: [long_sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.long_sell_nb)
for long positions and [short_buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.short_buy_nb)
for short positions. These do not require the `direction` argument, just a size of infinity:

```pycon
>>> account_state = vbt.pf_enums.AccountState(
...     cash=0.0,
...     position=10.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=0.0
... )
>>> order_result, new_account_state = vbt.pf_nb.long_sell_nb(
...     account_state=account_state, 
...     size=np.inf, 
...     price=15.0
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=10.0,
    price=15.0,
    fees=0.0,
    side=1,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_account_state)
AccountState(
    cash=150.0,
    position=0.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=150.0
)
```

### Pipeline/1

Even with just these two essential commands, you can already build a backtesting pipeline of any
complexity or flexibility. As mentioned earlier, a simulation is simply a loop that iterates
over timestamps. Let's create a simplified pipeline that puts $1 into Bitcoin each time it detects
a [Golden Cross](https://www.investopedia.com/terms/g/goldencross.asp) entry signal, and sells $1
otherwise. Our goal is a single number: the final value of the portfolio.

```pycon
>>> @njit
... def pipeline_1_nb(close, entries, exits, init_cash=100):
...     account_state = vbt.pf_enums.AccountState(  # (1)!
...         cash=float(init_cash),
...         position=0.0,
...         debt=0.0,
...         locked_cash=0.0,
...         free_cash=float(init_cash)
...     )
...     for i in range(close.shape[0]):
...         if entries[i]:
...             _, account_state = vbt.pf_nb.buy_nb(  # (2)!
...                 account_state=account_state,
...                 size=1 / close[i],
...                 price=close[i]
...             )
...         if exits[i]:
...             _, account_state = vbt.pf_nb.sell_nb(
...                 account_state=account_state,
...                 size=1 / close[i],
...                 price=close[i]
...             )
...     return account_state.cash + account_state.position * close[-1]  # (3)!

>>> data = vbt.YFData.pull("BTC-USD", end="2022-01-01")
>>> sma_50 = vbt.talib("SMA").run(data.get("Close"), 50)
>>> sma_200 = vbt.talib("SMA").run(data.get("Close"), 200)
>>> entries = sma_50.real_crossed_above(sma_200)
>>> exits = sma_50.real_crossed_below(sma_200)

>>> pipeline_1_nb(
...     data.get("Close").values, 
...     entries.values, 
...     exits.values
... )
210.71073253390762
```

1. Initial account state.

2. Execute the order and return a new account state.

3. Calculate the final portfolio value.

!!! tip
    Adding the suffix `_nb` to indicate a Numba-compiled function is not required,
    but it remains a good convention in VBT.

We can validate this pipeline using one of the preset simulation methods:

```pycon
>>> vbt.Portfolio.from_orders(
...     data.get("Close"), 
...     size=entries.astype(int) - exits.astype(int), 
...     size_type="value"
... ).final_value
210.71073253390762
```

## Order execution

Using the primitive commands is convenient when we know the exact direction of the order and can be sure
that the provided arguments are appropriate. However, we often encounter more complex requirements, such 
as target percentages that may change the order direction based on the current value. In addition, these 
commands do not validate their arguments; for example, if a user accidentally passes a negative order 
price, no error will be thrown. Also, we need a better way to represent an order—it is not considered 
good practice to pass all parameters, such as slippage, as keyword arguments.

All checks and preprocessing steps are handled in the
[execute_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.execute_order_nb) function.
The first input to this function is an order execution state of type
[ExecState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ExecState).
This state contains the same information as the account state shown above but also includes additional
details about the current valuation. The second input is a named tuple of type
[Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order) that represents an order.
The third argument is the price area, which we have already discussed.

### Order

In VBT, an order is represented by a [named tuple](https://realpython.com/python-namedtuple/).
Named tuples offer an efficient and lightweight alternative to data classes in both the Python and
Numba environments. They can be easily created and processed. Let's create an instance of an order:

```pycon
>>> order = vbt.pf_enums.Order()
>>> vbt.pprint(order)
Order(
    size=np.inf,
    price=np.inf,
    size_type=0,
    direction=2,
    fees=0.0,
    fixed_fees=0.0,
    slippage=0.0,
    min_size=np.nan,
    max_size=np.nan,
    size_granularity=np.nan,
    leverage=1.0,
    leverage_mode=0,
    reject_prob=0.0,
    price_area_vio_mode=0,
    allow_partial=True,
    raise_reject=False,
    log=False
)
```

You can access the tuple's attributes using dot notation:

```pycon
>>> order.direction
2
```

Other than that, it behaves just like any other tuple in Python:

```pycon
>>> order[3]
2

>>> tuple(order)  # (1)!
(inf,
  inf,
  0,
  2,
  0.0,
  0.0,
  0.0,
  nan,
  nan,
  nan,
  1.0,
  0,
  0.0,
  0,
  True,
  False,
  False)
```

1. Convert to a regular tuple.

When working with Numba, there is still the issue of default arguments: even though you can
construct a new tuple in Numba using only default arguments as shown above, if you want to override
some values, the values to override must be strictly on the left side of the tuple's definition.
Otherwise, Numba requires you to explicitly provide all the default arguments that come before them:

```pycon
>>> @njit
... def create_order_nb():
...     return vbt.pf_enums.Order()  # (1)!

>>> create_order_nb()
Order(size=inf, price=inf, ...)

>>> @njit
... def create_order_nb(size, price):
...     return vbt.pf_enums.Order(size=size, price=price)  # (2)!

>>> create_order_nb(1, 15)
Order(size=1, price=15, ...)

>>> @njit
... def create_order_nb(size, price, direction):
...     return vbt.pf_enums.Order(size=size, price=price, direction=direction)  # (3)!

>>> create_order_nb(1, 15, 2)
Failed in nopython mode pipeline (step: nopython frontend)
```

1. Uses only the default values.

2. Overrides the default values of arguments on the left side.

3. Attempts to override default values at different positions.

Another issue involves data types. In the example above, where integer size and price are provided,
Numba handles them without problems. However, if you create such an order in a loop and provide a float
for one of the arguments instead of an integer as before, Numba will throw an error because it cannot
unify the data types. Therefore, you should cast all arguments to their target data types before
constructing an order.

Both issues can be resolved by using the
[order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.order_nb) function:

```pycon
>>> @njit
... def create_order_nb(size, price, direction):
...     return vbt.pf_nb.order_nb(size=size, price=price, direction=direction)

>>> create_order_nb(1, 15, 2)
Order(size=1.0, price=15.0, ..., direction=2, ...)
```

Notice that the size and price arguments are automatically cast to floats.

!!! tip
    Whenever possible, use [order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.order_nb)
    instead of [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order).

To create an order that closes the current position, you can use
[close_position_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.close_position_nb):

```pycon
>>> vbt.pf_nb.close_position_nb(15)  # (1)!
Order(size=0.0, price=15.0, size_type=6, ...)
```

1. Uses a size of zero and a size type of `TargetAmount`.

### Validation

After constructing the order, [execute_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.execute_order_nb)
will check whether the order's arguments have valid data types and values. For example, let's
try passing a negative price:

```pycon
>>> exec_state = vbt.pf_enums.ExecState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0,
...     val_price=15.0,
...     value=100.0
... )
>>> vbt.pf_nb.execute_order_nb(
...     exec_state,
...     vbt.pf_nb.order_nb(price=-15)
... )
ValueError: order.price must be finite and 0 or greater
```

### Price resolution

Once the inputs are validated, [execute_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.execute_order_nb)
uses them to determine whether to run the buy or sell command. Before that, it performs some preprocessing.

Vectorbt is not tied to any particular data schema and can work with tick data or bar data.
However, it allows you to provide the current candle (`price_area`) for validation and resolution.
If provided, VBT will consider the order price as a price point within four bounds:
the opening, high, low, and closing prices. Since order execution must occur strictly within these bounds,
setting the order price to `-np.inf` or `np.inf` will replace it with the opening or closing price,
respectively. So, when you see a default price set to `np.inf`, remember that it means the
close price :writing_hand:

```pycon
>>> price_area = vbt.pf_enums.PriceArea(
...     open=10,
...     high=14,
...     low=8,
...     close=12
... )
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(  # (1)!
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=-np.inf),
...     price_area=price_area
... )
>>> order_result.price
10.0

>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(  # (2)!
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=np.inf),
...     price_area=price_area
... )
>>> order_result.price
12.0

>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(  # (3)!
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=np.inf)
... )
>>> order_result.price
nan
```

1. Price is replaced by the open price. Order executed.

2. Price is replaced by the close price (default). Order executed.

3. Price becomes `np.nan` since the price area is not defined. Order ignored.

### Size type conversion

Primitive commands only accept size as a number of shares, so any size type defined in
[SizeType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType) must be converted to `Amount`.
Different size types require different information for conversion; for example,
`TargetAmount` needs to know the current position size, while `Value` also requires the current valuation
price.

Let's execute an order so that the new position has 3 shares:

```pycon
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(
...         size=3, 
...         size_type=vbt.pf_enums.SizeType.TargetAmount
...     ),
...     price_area=price_area
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=3.0,
    price=12.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_exec_state)
ExecState(
    cash=64.0,
    position=3.0,
    debt=0.0,
    locked_cash=0.0,
    free_cash=64.0,
    val_price=15.0,
    value=100.0
)
```

Since we are not in the market, VBT used [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb)
to buy 3 shares. If we were already in the market with 10 shares, it would have used
[sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.sell_nb) to sell 7 shares.

### Direction

The [order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.order_nb) function takes the
`direction` argument for two reasons: to resolve the direction of the order based on the sign of the
`size` argument, and to decide whether to reverse the position or simply close it out. When the direction
is `LongOnly` or `Both`, a positive size means buying, while a negative size means selling.
When the direction is `ShortOnly`, the opposite applies: a positive size means selling and a negative
size means buying. This is because a positive size means increasing a position, which corresponds to
buying to increase a long position and selling to increase a short position. For example, if the direction
is `ShortOnly` and the size is negative infinity, any short position will be closed out and any long
position will be enlarged.

### Valuation

For the valuation price, it is the latest available price at the time of decision-making,
or the price used to calculate the portfolio value. In many simulation methods, the valuation
price defaults to the order price, but sometimes it makes more sense to use the open or previous
close price for the conversion step. By separating the valuation and order price, we can introduce
a time gap between order placement and execution. This matters because, in reality, orders cannot
always be executed right away.

Let's place an order for 100% of the portfolio value:

```pycon
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(
...         size=1.0, 
...         size_type=vbt.pf_enums.SizeType.TargetPercent
...     ),
...     price_area=price_area
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.666666666666667,
    price=12.0,
    fees=0.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_exec_state)
ExecState(
    cash=20.0,
    position=6.666666666666667,
    debt=0.0,
    locked_cash=0.0,
    free_cash=20.0,
    val_price=15.0,
    value=100.0
)
```

Why didn't we spend the entire cash? To convert the target percentage to the target amount of shares,
VBT used the provided order execution state, where `val_price` is $15 and `value` is $100,
which resulted in `100 / 15 = 6.67`. The closer the valuation price is to the order price,
the closer the calculation result will match the target requirement.

By default, if you place multiple orders within the same bar (for example, in pairs trading),
VBT will not update the portfolio value after each order. This is because it assumes trading
decisions are made before execution, and updating the value would affect those decisions.
Additionally, an order typically causes only a marginal immediate change in the value,
such as through commissions. To force VBT to update the valuation price and value,
you can enable `update_value`:

```pycon
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(
...         size=1.0, 
...         size_type=vbt.pf_enums.SizeType.TargetPercent,
...         fixed_fees=10,
...         slippage=0.01
...     ),
...     price_area=price_area,
...     update_value=True
... )
>>> vbt.pprint(order_result)
OrderResult(
    size=6.666666666666667,
    price=12.120000000000001,
    fees=10.0,
    side=0,
    status=0,
    status_info=-1
)
>>> vbt.pprint(new_exec_state)
ExecState(
    cash=9.199999999999989,
    position=6.666666666666667,
    debt=0.0,
    locked_cash=0.0,
    free_cash=9.199999999999989,
    val_price=12.120000000000001,
    value=90.0
)
```

Notice how the new valuation price is set to the close price adjusted for slippage,
and the value is decreased by the fixed commission. Any additional orders placed after this one
will use the updated value and will likely result in a different outcome.

!!! note
    Use this feature only if you can control the order in which orders appear within a bar
    and when you have intra-bar data.

### Pipeline/2

Let's create another simplified pipeline that places orders based on a target percentage array.
In this case, we will keep 50% of the portfolio value in shares and rebalance monthly.
We will calculate the portfolio value based on the open price at the start of each bar and place orders
at the end of each bar (to make it realistic). Also, we will fill asset value and portfolio value arrays
to later plot the allocation at each bar.

```pycon
>>> @njit
... def pipeline_2_nb(open, close, target_pct, init_cash=100):
...     asset_value_out = np.empty(close.shape, dtype=float_)  # (1)!
...     value_out = np.empty(close.shape, dtype=float_)
...     exec_state = vbt.pf_enums.ExecState(  # (2)!
...         cash=float(init_cash),
...         position=0.0,
...         debt=0.0,
...         locked_cash=0.0,
...         free_cash=float(init_cash),
...         val_price=np.nan,
...         value=np.nan
...     )
...
...     for i in range(close.shape[0]):
...         if not np.isnan(target_pct[i]):  # (3)!
...             val_price = open[i]
...             value = exec_state.cash + val_price * exec_state.position  # (4)!
...
...             exec_state = vbt.pf_enums.ExecState(  # (5)!
...                 cash=exec_state.cash,
...                 position=exec_state.position,
...                 debt=exec_state.debt,
...                 locked_cash=exec_state.locked_cash,
...                 free_cash=exec_state.free_cash,
...                 val_price=val_price,
...                 value=value
...             )
...             order = vbt.pf_nb.order_nb(  # (6)!
...                 size=target_pct[i],
...                 price=close[i],
...                 size_type=vbt.pf_enums.SizeType.TargetPercent
...             )
...             _, exec_state = vbt.pf_nb.execute_order_nb(  # (7)!
...                 exec_state=exec_state,
...                 order=order
...             )
...
...         asset_value_out[i] = exec_state.position * close[i]  # (8)!
...         value_out[i] = exec_state.cash + exec_state.position * close[i]
...         
...     return asset_value_out, value_out
```

1. Create two empty arrays of floating-point type. Remember that using `np.empty` will produce arrays
with uninitialized (garbage) values, so you should overwrite them.

2. The initial execution state for orders.

3. No order is executed if the target percentage is `np.nan` (which means do not rebalance).

4. Calculate the portfolio value at the start of the bar (valuation).

5. Create a new execution state after recalculating the valuation.

6. Create a new order tuple using the close price.

7. Execute the order and update the execution state.

8. Fill in the arrays (each element should be set individually).

Now let's run the pipeline on our Bitcoin data:

```pycon
>>> symbol_wrapper = data.get_symbol_wrapper()  # (1)!
>>> target_pct = symbol_wrapper.fill()
>>> target_pct.vbt.set(0.5, every="M", inplace=True)  # (2)!

>>> asset_value, value = pipeline_2_nb(
...     data.get("Open").values, 
...     data.get("Close").values, 
...     target_pct.values
... )
>>> asset_value = symbol_wrapper.wrap(asset_value)  # (3)!
>>> value = symbol_wrapper.wrap(value)
>>> allocations = (asset_value / value).rename(None)  # (4)!
>>> allocations.vbt.scatterplot(trace_kwargs=dict(
...     marker=dict(
...         color=allocations, 
...         colorscale="Temps", 
...         size=3,
...         cmin=0.3,
...         cmid=0.5,
...         cmax=0.7
...     )
... )).show()
```

1. We cannot use `data.wrapper` because it contains OHLC as columns. Instead, we need a wrapper that has
symbols as columns, allowing us to fill the array with target percentages.

2. Fill the array with NaNs and set all data points at the beginning of each month to `0.5`.

3. Use the same wrapper to convert the NumPy array back to a Pandas Series.

4. Divide the asset value by the portfolio value to obtain the allocation.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_2_allocation1.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_2_allocation1.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    Each point represents a revaluation at the end of each bar.

As you can see, allocations are regularly pulled back to the target level of 50%.

Let's validate the pipeline using [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders):

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     data, 
...     size=target_pct, 
...     size_type="targetpercent"
... )
>>> pf.allocations.vbt.scatterplot(trace_kwargs=dict(
...     marker=dict(
...         color=allocations, 
...         colorscale="Temps", 
...         size=3,
...         cmin=0.3,
...         cmid=0.5,
...         cmax=0.7
...     )
... )).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_2_allocation2.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_2_allocation2.dark.svg#only-dark){: .iimg loading=lazy }

One of the greatest advantages of using VBT is that you can run your minimalistic trading
environment in any Python function, even inside the objective functions of machine learning models.
There is no need to trigger the entire backtesting pipeline as a script or use other complex
processes as most backtesting frameworks require :face_with_spiral_eyes:

## Order processing

Order execution takes an order instruction and translates it into a buy or sell operation.
It is the user's responsibility to handle the returned order execution state and result.

Typically, you will post-process and append each successful order to a list for later analysis—
this is where order and log records come into play. Additionally, you may want to raise an error if
an order has been rejected and a certain flag in the requirements is set. All of this is managed by
[process_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.process_order_nb).

### Order records

Order records are a [structured](https://numpy.org/doc/stable/user/basics.rec.html) NumPy array with
the data type [order_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.order_dt), which contains
information about each successful order. Each order in this array is considered completed; in VBT's
context, you should treat an order as a trade. Since Numba is used here, lists and other inefficient
data structures for storing such complex information cannot and should not be used. Because orders
contain fields with variable data types, a record array is the best data structure to use. A record
array is a standard NumPy array with a complex data type and behaves similarly to a Pandas DataFrame.

Because any NumPy array is not appendable, you must initialize an empty array of sufficient size and
fill it with new information as needed. For this, you need a counter—a simple integer that points to
the index of the next record to be written.

!!! info
    While you can append to a NumPy array, doing so creates a new array. Do not try this at home :smile:

Let's create an array with two order records and set up a counter:

```pycon
>>> order_records = np.empty(2, dtype=vbt.pf_enums.order_dt)
>>> order_count = 0
```

You should not access this array yet because it contains uninitialized memory, so you must manually set
all the values in the array and use it with caution.

```pycon
>>> order_records
array([(4585679916398730403, ..., 4583100142070297783),
        (4582795628349012822, ..., 4576866499094039639)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})
```

Let's execute an order using [execute_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.execute_order_nb)
at the 678th bar and fill the first record in the array:

```pycon
>>> exec_state = vbt.pf_enums.ExecState(
...     cash=100.0,
...     position=0.0,
...     debt=0.0,
...     locked_cash=0.0,
...     free_cash=100.0,
...     val_price=15.0,
...     value=100.0
... )
>>> order_result, new_exec_state = vbt.pf_nb.execute_order_nb(
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=15.0)
... )
>>> if order_result.status == vbt.pf_enums.OrderStatus.Filled:  # (1)!
...     order_records["id"][order_count] = order_count  # (2)!
...     order_records["col"][order_count] = 0
...     order_records["idx"][order_count] = 678  # (3)!
...     order_records["size"][order_count] = order_result.size
...     order_records["price"][order_count] = order_result.price
...     order_records["fees"][order_count] = order_result.fees
...     order_records["side"][order_count] = order_result.side
...     order_count += 1

>>> order_records[0]
(0, 0, 678, 6.66666667, 15., 0., 0)

>>> order_count
1
```

1. Check that the order has been filled.

2. Order IDs start at 0 and follow the counter.

3. The index of the current bar.

!!! note
    When writing to a record field element, first select the field and then the index.

At the next bar, we will reverse the position and fill the second record:

```pycon
>>> order_result, new_exec_state2 = vbt.pf_nb.execute_order_nb(
...     exec_state=new_exec_state,
...     order=vbt.pf_nb.order_nb(size=-np.inf, price=16.0)
... )
>>> if order_result.status == vbt.pf_enums.OrderStatus.Filled:
...     order_records["id"][order_count] = order_count  # (1)!
...     order_records["col"][order_count] = 0
...     order_records["idx"][order_count] = 679
...     order_records["size"][order_count] = order_result.size
...     order_records["price"][order_count] = order_result.price
...     order_records["fees"][order_count] = order_result.fees
...     order_records["side"][order_count] = order_result.side
...     order_count += 1

>>> order_records[1]
(1, 0, 679, 13.33333333, 16., 0., 1)

>>> order_count
2
```

1. Remember to increment the order ID.

Here are the order records that we have filled:

```pycon
>>> order_records
array([(0, 0, 678,  6.66666667, 15., 0., 0),
        (1, 0, 679, 13.33333333, 16., 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})
```

However, instead of setting each of these records manually, you can use
[process_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.process_order_nb) to do
this automatically! There is one small adjustment: both the order records and the counter must be set
per column, since VBT mainly works with multi-column data. This means the order records array must
be two-dimensional and the counter array must be one-dimensional (both with only one column in this example):

```pycon
>>> order_records = np.empty((2, 1), dtype=vbt.pf_enums.order_dt)
>>> order_counts = np.full(1, 0, dtype=int_)

>>> order_result1, new_exec_state1 = vbt.pf_nb.process_order_nb(
...     0, 0, 678,  # (1)!
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=15.0),
...     order_records=order_records,
...     order_counts=order_counts
... )
>>> order_result2, new_exec_state2 = vbt.pf_nb.process_order_nb(
...     0, 0, 679,
...     exec_state=new_exec_state,
...     order=vbt.pf_nb.order_nb(size=-np.inf, price=16.0),
...     order_records=order_records,
...     order_counts=order_counts
... )

>>> order_records
array([(0, 0, 678,  6.66666667, 15., 0., 0),
        (1, 0, 679, 13.33333333, 16., 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})
      
>>> order_counts
array([2])
```

1. Current group, column, and index (row).

These filled order records form the backbone of the post-analysis phase.

### Log records

Log records use the data type [log_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.log_dt) and
are similar to order records but have a few key differences: they are saved regardless of whether the
order has been filled, and they include information about the current execution state, the order request,
and the new execution state. This approach allows you to fully track and diagnose issues related to
order processing.

```pycon
>>> order_records = np.empty((2, 1), dtype=vbt.pf_enums.order_dt)
>>> order_counts = np.full(1, 0, dtype=int_)
>>> log_records = np.empty((2, 1), dtype=vbt.pf_enums.log_dt)
>>> log_counts = np.full(1, 0, dtype=int_)

>>> order_result1, new_exec_state1 = vbt.pf_nb.process_order_nb(
...     0, 0, 678,
...     exec_state=exec_state,
...     order=vbt.pf_nb.order_nb(size=np.inf, price=15.0, log=True),  # (1)!
...     order_records=order_records,
...     order_counts=order_counts,
...     log_records=log_records,
...     log_counts=log_counts
... )
>>> order_result2, new_exec_state2 = vbt.pf_nb.process_order_nb(
...     0, 0, 679,
...     exec_state=new_exec_state,
...     order=vbt.pf_nb.order_nb(size=-np.inf, price=16.0, log=True),
...     order_records=order_records,
...     order_counts=order_counts,
...     log_records=log_records,
...     log_counts=log_counts
... )

>>> log_records
array([[(0, 0, 0, 678, ..., 0., 15., 100., 0)],
        [(1, 0, 0, 679, ..., 0., 15., 100., 1)]],
      dtype={'names':['id','group',...,'res_status_info','order_id'], ...})
```

1. Logging for each order must be explicitly enabled.

!!! note
    Logging affects both performance and memory usage. Use it only when truly needed.

### Pipeline/3

Let's extend the [last pipeline](#pipeline2) to independently process any number of columns and
gradually fill order records. This approach allows us to backtest multiple parameter combinations
by taking advantage of array multidimensionality!

```pycon
>>> @njit
... def pipeline_3_nb(open, close, target_pct, init_cash=100):
...     order_records = np.empty(close.shape, dtype=vbt.pf_enums.order_dt)  # (1)!
...     order_counts = np.full(close.shape[1], 0, dtype=int_)
...
...     for col in range(close.shape[1]):  # (2)!
...         exec_state = vbt.pf_enums.ExecState(
...             cash=float(init_cash),
...             position=0.0,
...             debt=0.0,
...             locked_cash=0.0,
...             free_cash=float(init_cash),
...             val_price=np.nan,
...             value=np.nan
...         )
...
...         for i in range(close.shape[0]):
...             if not np.isnan(target_pct[i, col]):  # (3)!
...                 val_price = open[i, col]
...                 value = exec_state.cash + val_price * exec_state.position
...
...                 exec_state = vbt.pf_enums.ExecState(
...                     cash=exec_state.cash,
...                     position=exec_state.position,
...                     debt=exec_state.debt,
...                     locked_cash=exec_state.locked_cash,
...                     free_cash=exec_state.free_cash,
...                     val_price=val_price,
...                     value=value
...                 )
...                 order = vbt.pf_nb.order_nb(
...                     size=target_pct[i, col],
...                     price=close[i, col],
...                     size_type=vbt.pf_enums.SizeType.TargetPercent
...                 )
...                 _, exec_state = vbt.pf_nb.process_order_nb(
...                     col, col, i,  # (4)!
...                     exec_state=exec_state,
...                     order=order,
...                     order_records=order_records,
...                     order_counts=order_counts
...                 )
...         
...     return vbt.nb.repartition_nb(order_records, order_counts)  # (4)!
```

1. Since we cannot know the number of orders in advance, let's prepare for the worst-case scenario:
one record per bar. Remember, order records must be aligned column-wise.

2. Iterate over the columns in `close` and run our logic for each one.

3. Now that every array passed to the pipeline must be two-dimensional, remember to specify
the column when accessing an array element. In indexing, the row comes first, then the column :point_up:

4. Use [repartition_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.repartition_nb) to flatten
the final order records array (this concatenates records from all columns into a one-dimensional array).

5. Since each column represents an independent backtest, groups correspond to columns.

!!! info
    We flatten (repartition) order records because most records remain unfilled,
    which wastes memory. By flattening, we compress them effectively, without losing any information,
    since each record tracks the column it belongs to.

Our pipeline now expects all arrays to be two-dimensional. Let's test three different values
for the parameter `every`, which controls re-allocation periodicity. To do this, we need to
expand all arrays so they have the same number of columns as the number of parameter combinations.

```pycon
>>> every = pd.Index(["M", "Q", "Y"], name="every")

>>> open = data.get("Open").vbt.tile(3, keys=every)  # (1)!
>>> close = data.get("Close").vbt.tile(3, keys=every)
>>> close
every                                 M             Q             Y
Date                                                               
2014-09-17 00:00:00+00:00    457.334015    457.334015    457.334015
2014-09-18 00:00:00+00:00    424.440002    424.440002    424.440002
2014-09-19 00:00:00+00:00    394.795990    394.795990    394.795990
...                                 ...           ...           ...
2021-12-29 00:00:00+00:00  46444.710938  46444.710938  46444.710938
2021-12-30 00:00:00+00:00  47178.125000  47178.125000  47178.125000
2021-12-31 00:00:00+00:00  46306.445312  46306.445312  46306.445312

[2663 rows x 3 columns]

>>> target_pct = symbol_wrapper.fill().vbt.tile(3, keys=every)
>>> target_pct.vbt.set(0.5, every="M", columns=["M"], inplace=True)  # (2)!
>>> target_pct.vbt.set(0.5, every="Q", columns=["Q"], inplace=True)
>>> target_pct.vbt.set(0.5, every="Y", columns=["Y"], inplace=True)

>>> order_records = pipeline_3_nb(
...     open.values, 
...     close.values, 
...     target_pct.values
... )
>>> order_records
array([( 0, 0,   14, 1.29056570e-01,   383.61499023, 0., 0),  << first column
        ( 1, 0,   45, 1.00206092e-02,   325.74899292, 0., 0),
        ( 2, 0,   75, 7.10912824e-03,   379.24499512, 0., 1),
        ...
        (84, 0, 2571, 7.79003416e-04, 48116.94140625, 0., 0),
        (85, 0, 2602, 3.00678739e-03, 61004.40625   , 0., 1),
        (86, 0, 2632, 6.84410394e-04, 57229.828125  , 0., 0),
        ( 0, 1,   13, 1.32947604e-01,   386.94400024, 0., 0),  << second column
        ( 1, 1,  105, 1.16132613e-02,   320.19299316, 0., 0),
        ( 2, 1,  195, 1.83187063e-02,   244.22399902, 0., 0),
        ...
        (27, 1, 2478, 7.74416872e-03, 35040.8359375 , 0., 0),
        (28, 1, 2570, 2.08567037e-03, 43790.89453125, 0., 1),
        (29, 1, 2662, 1.72637091e-03, 46306.4453125 , 0., 1),
        ( 0, 2,  105, 1.60816173e-01,   320.19299316, 0., 0),  << third column
        ( 1, 2,  470, 2.34573523e-02,   430.56698608, 0., 1),
        ( 2, 2,  836, 3.81744650e-02,   963.74298096, 0., 1),
        ...
        ( 5, 2, 1931, 2.83026812e-02,  7193.59912109, 0., 1),
        ( 6, 2, 2297, 3.54188390e-02, 29001.72070312, 0., 1),
        ( 7, 2, 2662, 1.14541249e-02, 46306.4453125 , 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})
```

1. Use [BaseAccessor.tile](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.tile)
to populate columns and add a new column level for our parameter combinations.

2. Change only the corresponding column.

### Wrapping

This output is exactly what [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
expects as input: order records, along with a few other arguments, can be used to reconstruct
the simulation state, including the regular cash balance and the position size at each time step.
The reconstructed state allows you to model the equity curve, then calculate returns, and finally
derive related metrics such as the classic Sharpe ratio. So, how do we construct a portfolio?
Instead of using a class method, we will provide the data directly to the class. For this,
only three arguments are required: a wrapper, a close series, and order records. Ideally,
we should also provide arguments that were used during the simulation, such as the initial cash:

```pycon
>>> vbt.phelp(vbt.Portfolio)
Portfolio.__init__(
    self,
    wrapper,
    order_records,
    *,
    close,
    open=None,
    high=None,
    low=None,
    log_records=None,
    cash_sharing=False,
    init_cash='auto',
    ...
)
```

!!! tip
    See the `*`? This means any argument after `order_records` must be specified as a keyword argument.

Let's do the wrapping step:

```pycon
>>> pf = vbt.Portfolio(
...     close.vbt.wrapper,
...     order_records,
...     open=open,
...     close=close,
...     init_cash=100  # (1)!
... )
```

1. Make sure to use the same initial cash as during the simulation.

You can now interact with the portfolio the same way as if you simulated it with any preset method:

```pycon
>>> pf.sharpe_ratio
every
MS    1.267804
Q     1.309943
Y     1.393820
Name: sharpe_ratio, dtype: float64
```

## Flexible indexing

The issue with bringing all arrays to the same shape, as we did above, is that it unnecessarily
consumes memory. Even though the only array with different data in each column is `target_pct`, 
we have almost tripled memory consumption by having to expand other arrays like `close`. 
Imagine how expensive it would be to align dozens of such array-like arguments :face_exhaling:

Flexible indexing lets us avoid this alignment step and access each element of an array based
solely on its shape. For example, there is no need to tile `close` three times if each row remains
the same in every column. We can simply return the same row element, regardless of the column
being accessed. The same applies to a one-dimensional array with elements per column—return the
same column element for each row. The only requirement is that the array must have one dimension
if it should broadcast against rows or columns, and two dimensions if it should broadcast against
both rows and columns. Any scalars should be transformed into one of these formats; otherwise,
we will encounter an ugly Numba error.

For actual indexing, we can use the following Numba-compiled functions:

* One-dimensional array (generic): [flex_select_1d_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_1d_nb).

* One-dimensional array (per row): [flex_select_1d_pr_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_1d_pr_nb).

* One-dimensional array (per column): [flex_select_1d_pc_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_1d_pc_nb).

* Two-dimensional array: [flex_select_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_nb).

Let's see how to use them in different scenarios:

```pycon
>>> per_row_arr = np.array([1, 2, 3])
>>> per_col_arr = np.array([4, 5])
>>> per_elem_arr = np.array([
...     [6, 7],
...     [8, 9],
...     [10, 11]
... ])

>>> vbt.flex_select_1d_pr_nb(per_row_arr, 2)  # (1)!
3

>>> vbt.flex_select_1d_pc_nb(per_col_arr, 1)  # (2)!
5

>>> vbt.flex_select_nb(per_elem_arr, 2, 1)  # (3)!
11
```

1. Get the value under the third row.

2. Get the value under the second column.

3. Get the value under the third row and second column.

One-dimensional indexing functions work only for arguments that are one-dimensional by design,
such as initial capital (which only makes sense provided per column, not per element).
But what if the user should also be able to pass `per_row_arr` or `per_col_arr` as fully-broadcast arrays?
In this case, the user needs to expand both arrays to two dimensions, following [NumPy's broadcasting
rules](https://numpy.org/doc/stable/user/basics.broadcasting.html), and use exclusively 
[flex_select_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_nb). The reason
for this is that Numba is not flexible enough to allow operations on both one-dimensional
and two-dimensional arrays, so we must choose the indexing function in advance.

```pycon
>>> per_row_arr_2d = per_row_arr[:, None]  # (1)!
>>> per_row_arr_2d
array([[1],
        [2],
        [3]])
        
>>> vbt.flex_select_nb(per_row_arr_2d, 2, 1)
3

>>> per_col_arr_2d = per_col_arr[None]  # (2)!
>>> per_col_arr_2d
array([[4, 5]])

>>> vbt.flex_select_nb(per_col_arr_2d, 2, 1)
5
```

1. Create the second axis of length one: from `(3,)` to `(3, 1)`.

2. Create the first axis of length one: from `(2,)` to `(1, 2)`.

This provides the same results as if we had aligned the arrays before indexing, which is memory-intensive:

```pycon
>>> target_shape = (3, 2)

>>> vbt.broadcast_array_to(per_row_arr, target_shape[0])[2]
3

>>> vbt.broadcast_array_to(per_col_arr, target_shape[1])[1]
5

>>> vbt.broadcast_array_to(per_row_arr_2d, target_shape)[2, 1]
3

>>> vbt.broadcast_array_to(per_col_arr_2d, target_shape)[2, 1]
5
```

!!! tip
    If you are not sure whether a flexible array will be indexed correctly, try broadcasting it with NumPy!

### Rotational indexing

But what happens if the index is out of bounds? For example, suppose we are iterating over 6 columns, but 
an array holds data for only 3. In this situation, VBT can rotate the index and return the first element 
in the array for the fourth column, the second element for the fifth column, and so on:

```pycon
>>> vbt.flex_select_1d_pr_nb(per_row_arr, 100, rotate_rows=True)  # (1)!
2

>>> vbt.flex_select_1d_pc_nb(per_col_arr, 100, rotate_cols=True)  # (2)!
4

>>> vbt.flex_select_nb(per_elem_arr, 100, 100, rotate_rows=True, rotate_cols=True)
8
```

1. Resolves to index 100 % 3 == 1 and element 2.

2. Resolves to index 100 % 2 == 0 and element 4.

If you find this behavior odd and would prefer an error instead: rotational indexing is actually very helpful 
when testing multiple assets and parameter combinations. Without it (the default), we would need to tile 
the asset DataFrame by the number of parameter combinations. With it, we can pass the data 
without tiling and avoid wasting memory. Still, in many cases, VBT ensures that all arrays 
can broadcast against each other as needed.

### Pipeline/4

Let's adapt the previous pipeline for flexible indexing. Usually, we do not know which of the 
provided arrays has the full shape, or there may not be any array with the full shape at all. 
We need to introduce another argument—`target_shape`—to specify the full shape for our loops 
to iterate over. We will also try out rotational indexing, which is not supported 
by any preset simulation methods.

```pycon
>>> @njit
... def pipeline_4_nb(
...     target_shape, 
...     open, 
...     close, 
...     target_pct, 
...     init_cash=100,
...     rotate_cols=False
... ):
...     init_cash_ = vbt.to_1d_array_nb(np.asarray(init_cash))  # (1)!
...     open_ = vbt.to_2d_array_nb(np.asarray(open))  # (2)!
...     close_ = vbt.to_2d_array_nb(np.asarray(close))
...     target_pct_ = vbt.to_2d_array_nb(np.asarray(target_pct))
...     order_records = np.empty(target_shape, dtype=vbt.pf_enums.order_dt)
...     order_counts = np.full(target_shape[1], 0, dtype=int_)
...
...     for col in range(target_shape[1]):
...         init_cash_elem = vbt.flex_select_1d_pc_nb(
...             init_cash_, col, rotate_cols=rotate_cols)  # (3)!
...
...         exec_state = vbt.pf_enums.ExecState(
...             cash=float(init_cash_elem),
...             position=0.0,
...             debt=0.0,
...             locked_cash=0.0,
...             free_cash=float(init_cash_elem),
...             val_price=np.nan,
...             value=np.nan
...         )
...
...         for i in range(target_shape[0]):
...             open_elem = vbt.flex_select_nb(
...                 open_, i, col, rotate_cols=rotate_cols)  # (4)!
...             close_elem = vbt.flex_select_nb(
...                 close_, i, col, rotate_cols=rotate_cols)
...             target_pct_elem = vbt.flex_select_nb(
...                 target_pct_, i, col, rotate_cols=rotate_cols)
...
...             if not np.isnan(target_pct_elem):
...                 value = exec_state.cash + open_elem * exec_state.position
...
...                 exec_state = vbt.pf_enums.ExecState(
...                     cash=exec_state.cash,
...                     position=exec_state.position,
...                     debt=exec_state.debt,
...                     locked_cash=exec_state.locked_cash,
...                     free_cash=exec_state.free_cash,
...                     val_price=open_elem,
...                     value=value
...                 )
...                 order = vbt.pf_nb.order_nb(
...                     size=target_pct_elem,
...                     price=close_elem,
...                     size_type=vbt.pf_enums.SizeType.TargetPercent
...                 )
...                 _, exec_state = vbt.pf_nb.process_order_nb(
...                     col, col, i,
...                     exec_state=exec_state,
...                     order=order,
...                     order_records=order_records,
...                     order_counts=order_counts
...                 )
...         
...     return vbt.nb.repartition_nb(order_records, order_counts)
```

1. This line allows us to pass a one-dimensional flexible array or a scalar. Notice how we
assign the result to a new variable (with a trailing underscore) and then use it for indexing.

2. Same concept for two-dimensional flexible arrays.

3. Select the current element of the initial cash array. Remember, indexing functions
with the `1d` suffix require one-dimensional arrays.

4. Since none of the three arrays are guaranteed to have the full shape now,
we must use flexible indexing instead of `open[i, col]`. Indexing functions
without the `1d` suffix expect two-dimensional arrays.

Thanks to flexible indexing, we can now use all arrays without tiling:

```pycon
>>> target_shape = vbt.broadcast_shapes(  # (1)!
...     data.get("Open").values.shape,
...     data.get("Close").values.shape,
...     target_pct.values.shape
... )
>>> target_shape
(2663, 3)

>>> order_records = pipeline_4_nb(
...     target_shape,
...     data.get("Open").values,
...     data.get("Close").values,
...     target_pct.values
... )
>>> len(order_records)
125
```

1. We need to build the target shape for iteration. This also acts as a broadcasting check.

This approach also lets us provide target percentages as a constant to re-allocate at each bar!
Since constants do not affect the target shape, we only need to broadcast the price shapes:

```pycon
>>> target_shape = vbt.broadcast_shapes(
...     data.get("Open").values.shape,
...     data.get("Close").values.shape
... )
>>> target_shape
(2663,)

>>> target_shape = vbt.to_2d_shape(target_shape)  # (1)!
>>> target_shape
(2663, 1)

>>> order_records = pipeline_4_nb(
...     target_shape,
...     data.get("Open").values,
...     data.get("Close").values,
...     0.5
... )
len(order_records)
2663
```

1. Target shape must always be two-dimensional.

This operation has produced the same number of orders as there are elements in the data:

```pycon
>>> np.prod(symbol_wrapper.shape_2d)
2663
```

To demonstrate rotational indexing, let's pull multiple symbols and run the simulation 
without having to tile or otherwise modify them:

```pycon
>>> mult_data = vbt.YFData.pull(
...     ["BTC-USD", "ETH-USD"], 
...     end="2022-01-01",
...     missing_index="drop"
... )
```

[=100% "Symbol 2/2"]{: .candystripe .candystripe-animate }

```pycon
>>> mult_symbol_wrapper = mult_data.get_symbol_wrapper()
>>> mult_target_pct = pd.concat([
...     mult_symbol_wrapper.fill().vbt.set(0.5, every=every[i])
...     for i in range(len(every))
... ], axis=1, keys=every)  # (1)!

>>> target_shape = vbt.broadcast_shapes(
...     vbt.tile_shape(mult_data.get("Open").values.shape, len(every)),  # (2)!
...     vbt.tile_shape(mult_data.get("Close").values.shape, len(every)), 
...     mult_target_pct.values.shape
... )
>>> target_shape
(1514, 6)

>>> order_records = pipeline_4_nb(
...     target_shape,
...     mult_data.get("Open").values,  # (3)!
...     mult_data.get("Close").values,
...     mult_target_pct.values,
...     rotate_cols=True
... )
>>> len(order_records)
142
```

1. This is another way to construct the target percentage array.

2. Since broadcasting does not support rotations, use [tile_shape](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.tile_shape)
to tile the shape of `open` and `close` manually. Do not tile the actual array!

3. There is no need to tile any array thanks to rotational indexing.

Without rotation, we would get an _"IndexError: index out of bounds"_ error, as the 
number of columns in the target shape is greater than that in the price arrays.

## Grouping

Using groups, we can combine multiple columns into the same backtesting basket :basket:

Typically, a group consists of several columns that belong to a single portfolio entity 
and should be backtested as a single unit. Most often, we use groups to share capital among 
multiple columns, but groups can also bind columns on a logical level. During a simulation, 
it is up to us to use grouping as needed. For example, while 
[process_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.process_order_nb) requires 
a group index, it only uses it for filling log records and nothing else. After the simulation, 
VBT provides many tools to help us aggregate and analyze different types of information 
by group, such as portfolio value.

Groups can be built and supplied in two ways: as group lengths or as a group map. 
Group lengths are easier to manage, marginally faster, and require columns to be organized into 
monolithic groups. A group map, on the other hand, allows the columns of a group to be distributed 
arbitrarily and offers greater flexibility overall. Simulation methods mainly use group lengths (since 
asset columns, unlike parameter columns, are generally located together), 
while group maps are mostly used by generic functions for pre- and post-analysis. 
Both formats can be easily generated by a
[Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper) instance.

### Group lengths

Let's create a custom column index with 5 assets and assign them to 2 groups. Because group lengths 
only support monolithic groups, the assets in each group must be next to each other:

```pycon
>>> columns = pd.Index(["BTC-USD", "ETH-USD", "BNB-USD", "SOL-USD", "XRP-USD"])
>>> mono_grouper = vbt.Grouper(columns, group_by=[0, 0, 0, 1, 1])
>>> mono_grouper.get_group_lens()  # (1)!
array([3, 2])
```

1. Using [Grouper.get_group_lens](https://vectorbt.pro/pvt_41531c19/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper.get_group_lens)

The first element in the returned array is the number of columns with the label `0`, 
and the second element is the number of columns with the label `1`.

!!! tip
    [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper) accepts either 
    a list of integers or a sequence of strings as input and will convert everything into a 
    Pandas Index to treat as group labels. The labels **do not** need to be alphanumerically sorted.

If we use distributed groups, group length generation will fail:

```pycon
>>> dist_grouper = vbt.Grouper(columns, group_by=[0, 1, 0, 1, 1])
>>> dist_grouper.get_group_lens()
ValueError: group_by must form monolithic groups
```

So, how do we define logic per group? Here is a template:

```pycon
>>> group_lens = mono_grouper.get_group_lens()

>>> group_end_idxs = np.cumsum(group_lens)  # (1)!
>>> group_start_idxs = group_end_idxs - group_lens  # (2)!

>>> for group in range(len(group_lens)):  # (3)!
...     from_col = group_start_idxs[group]
...     to_col = group_end_idxs[group]
...     # (4)!
...
...     for col in range(from_col, to_col):  # (5)!
...         pass  # (6)!
```

1. Get the end column index of each group (excluding).

2. Get the start column index of each group (including).

3. Iterate over all groups.

4. Define your logic for each group here.

5. Iterate over all columns in the group.

6. Define your logic for each column in the group here.

### Group map

A group map consists of a tuple with two arrays:

1. A one-dimensional array containing column indices sorted by group.

2. A one-dimensional array specifying the length of each group in the first array.

This means a group map turns distributed groups into monolithic ones, allowing you to work with 
any group arrangement:

```pycon
>>> mono_grouper.get_group_map()
(array([0, 1, 2, 3, 4]), array([3, 2]))

>>> dist_grouper.get_group_map()
(array([0, 2, 1, 3, 4]), array([2, 3]))
```

In the second example, the first two (`2`) column indices in the first array belong to the first group, 
while the remaining three (`3`) column indices belong to the second group.

Here is a template for working with a group map:

```pycon
>>> group_map = dist_grouper.get_group_map()

>>> group_idxs, group_lens = group_map
>>> group_start_idxs = np.cumsum(group_lens) - group_lens  # (1)!

>>> for group in range(len(group_lens)):
...     group_len = group_lens[group]
...     start_idx = group_start_idxs[group]
...     col_idxs = group_idxs[start_idx : start_idx + group_len]  # (2)!
...     # (3)!
... 
...     for k in range(len(col_idxs)):  # (4)!
...         col = col_idxs[k]
...         # (5)!
```

1. Get the start index of each group in the first array.

2. Get the column indices for the group in the first array.

3. Define your logic for each group here.

4. Iterate over all column indices in the group.

5. Define your logic for each column in the group here.

### Call sequence

When sharing capital between multiple assets, we may want to process one column before the others. 
This is useful, for example, when closing positions before opening new ones to free up capital. 
If we look at the templates for both grouping formats above, the place to change 
the processing order is in the for-loop over the columns. How can we adjust this order programmatically? 
This is where the call sequence comes into play.

A call sequence is an array of column indices representing the order in which columns should be processed. 
For example, if you want to process the third column first, the first column second, and the second column 
third, your call sequence would be `[2, 0, 1]`. You always move from left to right in the call sequence, 
selecting the current column index. This approach has one major advantage: you can use another array, 
such as an array of order values, to (arg-)sort the call sequence.

Sorting is handled by the function [insert_argsort_nb](https://vectorbt.pro/pvt_41531c19/api/utils/array_/#vectorbtpro.utils.array_.insert_argsort_nb), 
which takes an array of values to sort by and an array of indices, sorting the indices in-place using 
[insertion sort](https://en.wikipedia.org/wiki/Insertion_sort) according to the order of 
the values in the first array. This algorithm works best for small arrays and requires no additional 
memory—perfect for asset groups!

Suppose you have three assets: one with no position, one with a short position, and one with a long position. 
If you want to close all positions, you should process the assets that will free up funds first, 
so you have enough cash to close other positions. To do this, start by using 
[approx_order_value_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.approx_order_value_nb) 
to estimate the order value for each operation:

```pycon
>>> position = np.array([0.0, -10.0, 10.0])
>>> val_price = np.array([10.0, 25.0, 15.0])
>>> debt = np.array([0.0, 100.0, 0.0])
>>> locked_cash = np.array([0.0, 100.0, 0.0])
>>> order_value = np.empty(3, dtype=float_)

>>> for col in range(len(position)):
...     exec_state = vbt.pf_enums.ExecState(
...         cash=200.0,  # (1)!
...         position=position[col],  # (2)!
...         debt=debt[col],
...         locked_cash=locked_cash[col],
...         free_cash=0.0,
...         val_price=val_price[col],
...         value=100.0
...     )
...     order_value[col] = vbt.pf_nb.approx_order_value_nb(
...         exec_state=exec_state,
...         size=0.,
...         size_type=vbt.pf_enums.SizeType.TargetAmount,
...         direction=vbt.pf_enums.Direction.Both
...     )
    
>>> order_value  # (3)!
array([0., 50., -150.])
```

1. Cash-related information is defined per group using a constant.

2. Position-related information is defined per column using an array.

3. A positive number means outbound cash flow; a negative number means inbound cash flow.

In this case, closing the second column requires about $50, while the third column would 
bring in about $150 by closing out the position. Let's create a call sequence and sort 
it by order value:

```pycon
>>> from vectorbtpro.utils.array_ import insert_argsort_nb

>>> call_seq = np.array([0, 1, 2])  # (1)!
>>> insert_argsort_nb(order_value, call_seq)
>>> call_seq
array([2, 0, 1])
```

1. Always begin with a simple range.

!!! note
    Both the order value and the call sequence are sorted in-place!

Now we can modify the for-loop to iterate over the call sequence instead:

```pycon
>>> for k in range(len(call_seq)):  # (1)!
...     c = call_seq[k]
...     col = from_col + c

>>> for k in range(len(call_seq)):  # (2)!
...     c = call_seq[k]
...     col = col_idxs[c]
```

1. For group lengths.

2. For a group map.

!!! tip
    It is good practice to use consistent variable names. Here, `k` is the index in the call sequence, 
    `c` is the column index within a group, and `col` is the global column index.

### Pipeline/5

Let's upgrade our previous pipeline to rebalance groups of assets. To better illustrate how 
important sorting by order value is when rebalancing multi-asset portfolios, we will introduce another 
argument, `auto_call_seq`, to switch between sorting and not sorting. We will use group lengths 
as the grouping format because of its simplicity. Also, now we need to keep 
a lot of position-related information in arrays rather than constants, since this information 
corresponds to columns instead of groups. Additionally, since we already know how to fill order records, 
let's track the allocation at each bar instead.

```pycon
>>> @njit
... def pipeline_5_nb(
...     target_shape,  # (1)!
...     group_lens,  # (2)!
...     open,
...     close, 
...     target_pct,  # (3)!
...     init_cash=100,
...     auto_call_seq=True,
...     rotate_cols=False
... ):
...     init_cash_ = vbt.to_1d_array_nb(np.asarray(init_cash))
...     open_ = vbt.to_2d_array_nb(np.asarray(open))
...     close_ = vbt.to_2d_array_nb(np.asarray(close))
...     target_pct_ = vbt.to_2d_array_nb(np.asarray(target_pct))
...     alloc = np.empty(target_shape, dtype=float_)  # (4)!
...
...     group_end_idxs = np.cumsum(group_lens)
...     group_start_idxs = group_end_idxs - group_lens
...
...     for group in range(len(group_lens)):  # (5)!
...         group_len = group_lens[group]
...         from_col = group_start_idxs[group]
...         to_col = group_end_idxs[group]
...
...         # (6)!
...         init_cash_elem = vbt.flex_select_1d_pc_nb(
...             init_cash_, group, rotate_cols=rotate_cols)
...     
...         last_position = np.full(group_len, 0.0, dtype=float_)  # (7)!
...         last_debt = np.full(group_len, 0.0, dtype=float_)
...         last_locked_cash = np.full(group_len, 0.0, dtype=float_)
...         cash_now = float(init_cash_elem)
...         free_cash_now = float(init_cash_elem)
...
...         order_value = np.empty(group_len, dtype=float_)  # (8)!
...         call_seq = np.empty(group_len, dtype=int_)
... 
...         for i in range(target_shape[0]):  # (9)!
...             # (10)!
...             value_now = cash_now
...             for c in range(group_len):
...                 col = from_col + c
...                 open_elem = vbt.flex_select_nb(
...                     open_, i, col, rotate_cols=rotate_cols)
...                 value_now += last_position[c] * open_elem  # (11)!
...         
...             # (12)!
...             for c in range(group_len):
...                 col = from_col + c
...                 open_elem = vbt.flex_select_nb(
...                     open_, i, col, rotate_cols=rotate_cols)
...                 target_pct_elem = vbt.flex_select_nb(
...                     target_pct_, i, col, rotate_cols=rotate_cols)
...                 exec_state = vbt.pf_enums.ExecState(
...                     cash=cash_now,
...                     position=last_position[c],
...                     locked_cash=last_locked_cash[c],
...                     debt=last_debt[c],
...                     free_cash=free_cash_now,
...                     val_price=open_elem,
...                     value=value_now,
...                 )
...                 order_value[c] = vbt.pf_nb.approx_order_value_nb(  # (13)!
...                     exec_state=exec_state,
...                     size=target_pct_elem,
...                     size_type=vbt.pf_enums.SizeType.TargetPercent,
...                     direction=vbt.pf_enums.Direction.Both
...                 )
...                 call_seq[c] = c  # (14)!
... 
...             if auto_call_seq:
...                 vbt.pf_nb.insert_argsort_nb(order_value, call_seq)  # (15)!
... 
...             for k in range(len(call_seq)):  # (16)!
...                 c = call_seq[k]  # (17)!
...                 col = from_col + c  # (18)!
...
...                 open_elem = vbt.flex_select_nb(
...                     open_, i, col, rotate_cols=rotate_cols)
...                 close_elem = vbt.flex_select_nb(
...                     close_, i, col, rotate_cols=rotate_cols)
...                 target_pct_elem = vbt.flex_select_nb(
...                     target_pct_, i, col, rotate_cols=rotate_cols)
...
...                 if not np.isnan(target_pct_elem):  # (19)!
...                     order = vbt.pf_nb.order_nb(
...                         size=target_pct_elem,
...                         price=close_elem,
...                         size_type=vbt.pf_enums.SizeType.TargetPercent,
...                         direction=vbt.pf_enums.Direction.Both
...                     )
...                     exec_state = vbt.pf_enums.ExecState(
...                         cash=cash_now,
...                         position=last_position[c],
...                         debt=last_debt[c],
...                         locked_cash=last_locked_cash[c],
...                         free_cash=free_cash_now,
...                         val_price=open_elem,
...                         value=value_now,
...                     )
...                     _, new_exec_state = vbt.pf_nb.process_order_nb(
...                         group=group,
...                         col=col,
...                         i=i,
...                         exec_state=exec_state,
...                         order=order
...                     )
...                     cash_now = new_exec_state.cash
...                     free_cash_now = new_exec_state.free_cash
...                     value_now = new_exec_state.value
...                     last_position[c] = new_exec_state.position
...                     last_debt[c] = new_exec_state.debt
...                     last_locked_cash[c] = new_exec_state.locked_cash
...
...             # (20)!
...             value_now = cash_now
...             for c in range(group_len):
...                 col = from_col + c
...                 close_elem = vbt.flex_select_nb(
...                     close_, i, col, rotate_cols=rotate_cols)
...                 value_now += last_position[c] * close_elem
...
...             # (21)!
...             for c in range(group_len):
...                 col = from_col + c
...                 close_elem = vbt.flex_select_nb(
...                     close_, i, col, rotate_cols=rotate_cols)
...                 alloc[i, col] = last_position[c] * close_elem / value_now
... 
...     return alloc
```

1. The second number in the target shape tracks assets (across parameter combinations). It does not track groups.

2. The group lengths array must contain as many elements as there are groups, and the sum of this array 
must equal the number of columns in `target_shape`.

3. Target allocations must be provided per asset, so the array should broadcast against `target_shape`.

4. Allocations must be filled per asset, so use the same shape as `target_shape`.

5. Iterate over groups.

6. Here, create various arrays that should exist only per group, such as the cash balance, 
position size per asset, and other state information. Remember, different groups represent independent, 
isolated tests and should not be connected in any way!

7. We cannot create a single instance of [ExecState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ExecState) 
as before, because the order execution state contains information for each asset. Therefore, 
we need to keep track of its fields using separate variables (constants for data per group, 
arrays for data per asset).

8. We could also create these two arrays at each bar, but frequent creation of arrays 
slows down the simulation. It is better to create an array just once and re-fill it as needed.

9. Iterate over time steps (bars), as in previous pipelines.

10. Calculate the value of each group (= portfolio value) by iterating over the assets in the group 
and adding their position value to the current cash balance.

11. The last position array is only for this group, so we use `c`, not `col`.

12. Prepare the order value and call sequence arrays.

13. Approximate the value of a potential order. At the beginning of the bar, use the open price.

14. Store the current column index within this group. This will relate order values to column indices.

15. Sort both arrays in-place so column indices with lower order values appear first 
in the call sequence.

16. Next, execute orders by iterating over the newly sorted call sequence.

17. Get the associated column index within the group.

18. Get the associated global column index.

19. Perform the same logic as in the previous pipeline.

20. Calculate the group value again, now using the updated position and the close price. 
This is done outside the `np.nan` check to track the allocation at each bar.

21. Calculate the current allocation of each asset and write it to the output array.

Wow, this became complex really fast! :dizzy_face:

But it is not as complex as it may seem. We simply take a set of columns and split them into 
groups. Then, for each group, we define a mini-pipeline that applies our logic only to the columns 
within that group, treating it as a single portfolio unit. At the beginning of each bar, we calculate 
the portfolio value and build a call sequence to rearrange the columns by their order value. We then 
iterate over this sequence and execute an order in each column. Finally, at the end of each bar, 
we recalculate the portfolio value and store the real allocation of each asset in the output array.

The best part of this pipeline is that it closely mimics how preset simulation methods work in VBT, 
and it is one of the most flexible pieces of code you can actually write!

Let's allocate 70% to BTC and 30% to ETH, and rebalance on a monthly basis:

```pycon
>>> mult_target_pct = mult_symbol_wrapper.fill()
>>> mult_target_pct.vbt.set([[0.7, 0.3]], every="M", inplace=True)
>>> grouper = vbt.Grouper(mult_symbol_wrapper.columns, group_by=True)
>>> group_lens = grouper.get_group_lens()

>>> target_shape = vbt.broadcast_shapes(
...     mult_data.get("Open").values.shape,
...     mult_data.get("Close").values.shape,
...     mult_target_pct.values.shape
... )
>>> target_shape
(1514, 2)

>>> alloc = pipeline_5_nb(
...     target_shape,
...     group_lens,
...     mult_data.get("Open").values,
...     mult_data.get("Close").values,
...     mult_target_pct.values
... )
>>> alloc = mult_symbol_wrapper.wrap(alloc)
>>> alloc.vbt.plot(
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_5_auto_call_seq.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_5_auto_call_seq.dark.svg#only-dark){: .iimg loading=lazy }

!!! info
    As you might have noticed, some allocations do not add up exactly to 100%. This is because we used the 
    open price for group valuation and decision-making, while the actual orders were executed using 
    the close price. By the way, it is a bad sign when everything aligns perfectly—this could mean 
    your simulation is too ideal for the real world.

And here is the same procedure but without sorting the call sequence array:

```pycon
>>> alloc = pipeline_5_nb(
...     target_shape,
...     group_lens,
...     mult_data.get("Open").values,
...     mult_data.get("Close").values,
...     mult_target_pct.values,
...     auto_call_seq=False
... )
>>> alloc = mult_symbol_wrapper.wrap(alloc)
>>> alloc.vbt.plot(
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_5_wo_auto_call_seq.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/pipeline_5_wo_auto_call_seq.dark.svg#only-dark){: .iimg loading=lazy }

As we can see, some rebalancing steps could not be completed at all because long operations 
were executed before short operations, leaving them without the required funds.

The biggest advantage of this pipeline is its flexibility: we can turn off grouping via `group_by=False` 
to run the entire logic per column (each group will then contain only one column). We can also test 
multiple weight combinations with multiple groups, without having to tile the pricing data 
thanks to rotational indexing. This, for example, cannot be done even with 
[Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders) :wink:

```pycon
>>> groups = pd.Index([0, 0, 1, 1], name="group")
>>> target_alloc = pd.Index([0.7, 0.3, 0.5, 0.5], name="target_alloc")

>>> final_columns = vbt.stack_indexes((  # (1)!
...     groups,
...     target_alloc, 
...     vbt.tile_index(mult_symbol_wrapper.columns, 2)
... ))
>>> final_wrapper = mult_symbol_wrapper.replace(  # (2)!
...     columns=final_columns,
...     group_by="group"
... )
>>> mult_target_pct = final_wrapper.fill(group_by=False)  # (3)!
>>> mult_target_pct.vbt.set(target_alloc.values[None], every="M", inplace=True)
>>> group_lens = final_wrapper.grouper.get_group_lens()

>>> n_groups = final_wrapper.grouper.get_group_count()
>>> target_shape = vbt.broadcast_shapes(
...     vbt.tile_shape(mult_data.get("Open").values.shape, n_groups),  # (4)!
...     vbt.tile_shape(mult_data.get("Close").values.shape, n_groups), 
...     mult_target_pct.values.shape
... )
>>> target_shape
(1514, 4)

>>> alloc = pipeline_5_nb(
...     target_shape,
...     group_lens,
...     mult_data.get("Open").values,
...     mult_data.get("Close").values, 
...     mult_target_pct.values,
...     rotate_cols=True
... )
>>> alloc = mult_target_pct.vbt.wrapper.wrap(alloc)
>>> alloc
group                                       0                   1          
target_alloc                    0.7       0.3       0.5       0.5
symbol                      BTC-USD   ETH-USD   BTC-USD   ETH-USD
Date                                                             
2017-11-09 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-10 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-11 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-12 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
2017-11-13 00:00:00+00:00  0.000000  0.000000  0.000000  0.000000
...                             ...       ...       ...       ...
2021-12-27 00:00:00+00:00  0.703817  0.296183  0.504464  0.495536
2021-12-28 00:00:00+00:00  0.703452  0.296548  0.504026  0.495974
2021-12-29 00:00:00+00:00  0.708035  0.291965  0.509543  0.490457
2021-12-30 00:00:00+00:00  0.706467  0.293533  0.507650  0.492350
2021-12-31 00:00:00+00:00  0.704346  0.295654  0.505099  0.494901

[1514 rows x 4 columns]
```

1. Build a new column hierarchy with three levels: groups, weights, and assets. 
Each level must have the same length.

2. Create a new (grouped) wrapper with the new column hierarchy.

3. Create the target percentage array with the same shape as the new wrapper and fill it with NaN.

4. We are using rotational indexing—tile the shapes of `open` and `close` by the number of groups 
for the shapes to broadcast correctly, and build the target shape.

## Contexts

Sometimes you may want to create a simulation method that takes a user-defined function (UDF) and 
calls it to make a trading decision. Such a UDF would need access to the simulation's state—like the 
current position size and direction—as well as other relevant information, which could quickly involve 
dozens of variables. Remember, since full-scale OOP is not possible in Numba, we need to pass data 
using primitive containers such as tuples. However, using variable positional arguments or a regular tuple 
would be cumbersome for the user because each field can only be accessed by integer index or tuple 
unpacking. To make this easier, we usually pass such information as a named tuple, often called a 
(simulation) "context".

### Pipeline/6

Let's build a simple pipeline that iterates over rows and columns and, at each element, 
calls a UDF to get an order and execute it!

First, we need to answer the question, "What information does a UDF need?" 
In most cases, we just include everything available:

```pycon
>>> SimContext = namedtuple("SimContext", [
...     "open",  # (1)!
...     "high",
...     "low",
...     "close",
...     "init_cash",
...     "col",  # (2)!
...     "i",
...     "price_area",  # (3)!
...     "exec_state"
... ])
```

1. Arguments passed to the simulator.

2. Loop variables.

3. State information, either unpacked (for marginal speed) or as named tuples (for convenience).

Here is our pipeline that accepts and calls an order function:

```pycon
>>> @njit
... def pipeline_6_nb(
...     open, high, low, close, 
...     order_func_nb, order_args=(), 
...     init_cash=100
... ):
...     order_records = np.empty(close.shape, dtype=vbt.pf_enums.order_dt)
...     order_counts = np.full(close.shape[1], 0, dtype=int_)
...
...     for col in range(close.shape[1]):
...         exec_state = vbt.pf_enums.ExecState(
...             cash=float(init_cash),
...             position=0.0,
...             debt=0.0,
...             locked_cash=0.0,
...             free_cash=float(init_cash),
...             val_price=np.nan,
...             value=np.nan
...         )
...
...         for i in range(close.shape[0]):
...             val_price = open[i, col]
...             value = exec_state.cash + val_price * exec_state.position
...
...             price_area = vbt.pf_enums.PriceArea(
...                 open[i, col],
...                 high[i, col],
...                 low[i, col],
...                 close[i, col]
...             )
...             exec_state = vbt.pf_enums.ExecState(
...                 cash=exec_state.cash,
...                 position=exec_state.position,
...                 debt=exec_state.debt,
...                 locked_cash=exec_state.locked_cash,
...                 free_cash=exec_state.free_cash,
...                 val_price=val_price,
...                 value=value
...             )
...             sim_ctx = SimContext(  # (1)!
...                 open=open,
...                 high=high,
...                 low=low,
...                 close=close,
...                 init_cash=init_cash,
...                 col=col,
...                 i=i,
...                 price_area=price_area,
...                 exec_state=exec_state
...             )
...             order = order_func_nb(sim_ctx, *order_args)  # (2)!
...             _, exec_state = vbt.pf_nb.process_order_nb(
...                 col, col, i,
...                 exec_state=exec_state,
...                 order=order,
...                 price_area=price_area,
...                 order_records=order_records,
...                 order_counts=order_counts
...             )
...         
...     return vbt.nb.repartition_nb(order_records, order_counts)
```

1. Initialize the simulation context (creates a named tuple).

2. Call the UDF by first passing the context, then any user-defined arguments.

Now let's write our own order function that generates orders based on signals:

```pycon
>>> @njit  # (1)!
... def signal_order_func_nb(c, entries, exits):
...     if entries[c.i, c.col] and c.exec_state.position == 0:  # (2)!
...         return vbt.pf_nb.order_nb()
...     if exits[c.i, c.col] and c.exec_state.position > 0:
...         return vbt.pf_nb.close_position_nb()
...     return vbt.pf_nb.order_nothing_nb()

>>> broadcasted_args = vbt.broadcast(  # (3)!
...     dict(
...         open=data.get("Open").values,
...         high=data.get("High").values,
...         low=data.get("Low").values,
...         close=data.get("Close").values,
...         entries=entries.values,  # (4)!
...         exits=exits.values
...     ),
...     min_ndim=2
... )

>>> pipeline_6_nb(
...     broadcasted_args["open"],
...     broadcasted_args["high"], 
...     broadcasted_args["low"], 
...     broadcasted_args["close"], 
...     signal_order_func_nb,
...     order_args=(
...         broadcasted_args["entries"],
...         broadcasted_args["exits"]
...     )
... )
array([( 0, 0,  300, 0.34786966,   287.46398926, 0., 0),
        ( 1, 0,  362, 0.34786966,   230.64399719, 0., 1),
        ( 2, 0,  406, 0.26339233,   304.61801147, 0., 0),
        ( 3, 0, 1290, 0.26339233,  6890.52001953, 0., 1),
        ( 4, 0, 1680, 0.33210511,  5464.86669922, 0., 0),
        ( 5, 0, 1865, 0.33210511,  9244.97265625, 0., 1),
        ( 6, 0, 1981, 0.31871477,  9633.38671875, 0., 0),
        ( 7, 0, 2016, 0.31871477,  6681.06298828, 0., 1),
        ( 8, 0, 2073, 0.2344648 ,  9081.76171875, 0., 0),
        ( 9, 0, 2467, 0.2344648 , 35615.87109375, 0., 1),
        (10, 0, 2555, 0.17333543, 48176.34765625, 0., 0)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})
```

1. Remember to decorate your order function with `@njit` as well!

2. All information can be accessed just like attributes on any Python object.

3. Prices and signals do not use flexible indexing, so we need to broadcast them to the full shape.

4. These two arrays were generated for the [first pipeline](#pipeline1).

We have just created our own shallow version of [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func)
functionality—neat! :boom:

Your homework is to extend this pipeline to use flexible indexing :wink:

## Performance

When it comes to performance, Numba code can be a real roller coaster :roller_coaster:

Numba is a just-in-time (JIT) compiler that analyzes and optimizes your code and ultimately uses the 
[LLVM compiler library](https://github.com/numba/llvmlite) to generate machine code for your Python 
functions. However, even if a function looks efficient on paper, Numba may sometimes generate 
suboptimal machine code because some variables or types do not interact optimally. In such cases, 
the code may still run much faster than native Python (or even another JIT compiler), but there 
can still be significant room for improvement—which may be hard to find, even for experienced users. 
For example, even switching the lines where variables are defined can unexpectedly have a 
positive or negative impact on performance.

Aside from following [official tips](https://numba.pydata.org/numba-doc/latest/user/performance-tips.html), 
there are also best practices you should always keep in mind when designing and optimizing 
Numba-compiled functions:

1. Numba works very well with loops and is often even more efficient with them than with vectorized operations.
That's why 90% of VBT's functionality relies on loops.

2. Numba performs poorly when new arrays are repeatedly created and small chunks of memory are allocated
inside loops. It is much better to create larger arrays before the iteration begins and use them as
buffers to store temporary information. Keep in mind that NumPy operations that produce a new array,
such as `np.cumsum`, create a new array each time.

3. Reading and writing one array element at a time is more efficient than processing elements in chunks.

4. Basic math operations such as `-`, `+`, `*`, and `/` should be preferred over their NumPy equivalents.

5. When passing a function as an argument to another function, arguments to that function should be accepted
in a packed format (`args`) instead of an unpacked format (`*args`). While this rule is occasionally broken
by VBT itself, these cases are typically benchmarked to ensure acceptable performance.

6. Packing named tuples inside other named tuples (as shown above) is not recommended,
but sometimes it does not have any negative effect.

7. NumPy arrays are almost always a better option than lists and dictionaries.

8. Even if enabling the `fastmath=True` option improves performance, be aware that it comes with various
[compromises](https://llvm.org/docs/LangRef.html#fast-math-flags) related to numeric operations.

9. Avoid iterating over elements of an array directly. Instead, iterate over a range of the same length
and use the loop variable to select the corresponding element.

10. When overwriting a variable, ensure it remains the same type.

!!! tip
    As a rule of thumb: the simpler the code, the easier it will be for Numba to analyze and optimize it.

### Benchmarking

To benchmark a simulator, you can use the [timeit](https://docs.python.org/3/library/timeit.html) module.
If possible, generate sample data of sufficient size and prepare for the worst-case scenario, where orders
are issued and executed at every time step to test the simulator under full load. Also, be sure to run
tests throughout the simulator's development to monitor how its execution time and stability change over time.

!!! note
    Generation of sample data and preparation of other inputs must be done before benchmarking.

Let's generate 1-minute random OHLC data for one year using 
[RandomOHLCData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random_ohlc/#vectorbtpro.data.custom.random_ohlc.RandomOHLCData):

```pycon
>>> test_data = vbt.RandomOHLCData.pull(
...     start="2020-01-01", 
...     end="2021-01-01",
...     timeframe="1min",  # (1)!
...     std=0.0001,  # (2)!
...     symmetric=True  # (3)!
... )
>>> test_data.resample("1d").plot().show()  # (4)!
```

1. Set the tick frequency to 1 minute.

2. Set the tick volatility to 0.01%.

3. Use symmetric returns (50% negative return is equal to 100% positive return).

4. Plot to verify that the generated data is realistic. Here, we resample to daily
frequency for faster plotting.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/simulation_random_ohlc_data.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/simulation_random_ohlc_data.dark.svg#only-dark){: .iimg loading=lazy }

Next, prepare all the data. This includes filling signals so that there is at least one 
order at each bar, which represents the worst-case scenario for performance and memory:

```pycon
>>> test_open = test_data.get("Open").values[:, None]  # (2)!
>>> test_high = test_data.get("High").values[:, None]
>>> test_low = test_data.get("Low").values[:, None]
>>> test_close = test_data.get("Close").values[:, None]
>>> test_entries = np.full(test_data.get_symbol_wrapper().shape, False)[:, None]
>>> test_exits = np.full(test_data.get_symbol_wrapper().shape, False)[:, None]
>>> test_entries[0::2] = True  # (3)!
>>> test_exits[1::2] = True  # (4)!
>>> del test_data  # (5)!

>>> test_entries.shape
(527041, 1)
```

1. Generate random OHLC data with symmetric returns.

2. Get a column, extract the NumPy array, and expand the array to two dimensions (skip this step if
your data is already two-dimensional).

3. Place an entry at every second bar starting from the first bar.

4. Place an exit at every second bar starting from the second bar.

5. Delete the data object to free up memory.

Each array contains 527,041 data points.

Now, let's check how our simulator performs with this data:

```pycon
>>> %%timeit  # (1)!
>>> pipeline_6_nb(
...     test_open, 
...     test_high, 
...     test_low, 
...     test_close, 
...     signal_order_func_nb,
...     order_args=(
...         test_entries, 
...         test_exits
...     )
... )
79.4 ms ± 290 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

1. This magic command works only in a Jupyter environment and only if placed at the beginning of the cell.
If you are running the code as a script, use the `timeit` module instead.

Generating half a million orders in 80 milliseconds on an Apple M1 is impressive! :fire:

To better demonstrate how even a small change can impact performance, let's create a new
order function that also creates a zero-sized empty array:

```pycon
>>> @njit
... def subopt_signal_order_func_nb(c, entries, exits):
...     _ = np.empty(0)  # (1)!
...
...     if entries[c.i, c.col] and c.exec_state.position == 0:
...         return vbt.pf_nb.order_nb()
...     if exits[c.i, c.col] and c.exec_state.position > 0:
...         return vbt.pf_nb.close_position_nb()
...     return vbt.pf_nb.order_nothing_nb()

>>> %%timeit
>>> pipeline_6_nb(
...     test_open, 
...     test_high, 
...     test_low, 
...     test_close, 
...     subopt_signal_order_func_nb,
...     order_args=(
...         test_entries, 
...         test_exits
...     )
... )
130 ms ± 675 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

1. Here

As you can see, creating an empty array at each bar increased execution time by more than 50%.
This demonstrates an important lesson: always create arrays outside of loops, and create them only once!

### Auto-parallelization

Because of path dependencies (where the current state depends on the previous state), we cannot parallelize 
the loop that iterates over rows (time). However, since VBT lets us
define multi-column backtesting logic, we can parallelize the loop that iterates over 
columns or groups of columns, as long as those columns or groups are independent
of each other, using only Numba. This is one of the main reasons VBT 
favors two-dimensional data layouts.

Automatic parallelization with Numba is straightforward: simply replace the `range` you wish to 
parallelize with `numba.prange`, and instruct Numba to parallelize the function by passing 
`parallel=True` to the `@njit` decorator. This will attempt to execute the code in the loop simultaneously 
using multiple parallel threads. You can read more about automatic parallelization with Numba 
[here](https://numba.pydata.org/numba-doc/latest/user/parallel.html) and about the
available threading layers [here](https://numba.pydata.org/numba-doc/latest/user/threading-layer.html).
On a MacBook Air (M1, 2020), enabling parallelization reduces processing time by 2–3 times on average.
Typically, simple arithmetic-heavy code without array creation can be parallelized more effectively
than more complex, vectorization-heavy code.

!!! important
    You can modify the same array from multiple threads, as done by many functions in VBT.
    Just make sure that multiple threads (columns, in this case) are not modifying the same elements
    or the same data in general!

Here is a simple example of a function that computes the expanding maximum on two-dimensional data,
with and without automatic parallelization:

```pycon
>>> arr = np.random.uniform(size=(1000000, 10))

>>> @njit
... def expanding_max_nb(arr):
...     out = np.empty_like(arr, dtype=float_)
...     for col in range(arr.shape[1]):
...         maxv = -np.inf
...         for i in range(arr.shape[0]):
...             if arr[i, col] > maxv:
...                 maxv = arr[i, col]
...             out[i, col] = maxv
...     return out

>>> %timeit expanding_max_nb(arr)
40.7 ms ± 558 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

>>> @njit(parallel=True)  # (1)!
... def parallel_expanding_max_nb(arr):
...     out = np.empty_like(arr, dtype=float_)
...     for col in prange(arr.shape[1]):  # (2)!
...         maxv = -np.inf
...         for i in range(arr.shape[0]):
...             if arr[i, col] > maxv:
...                 maxv = arr[i, col]
...             out[i, col] = maxv
...     return out

>>> %timeit parallel_expanding_max_nb(arr)
26.6 ms ± 437 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

1. This is the first change.

2. This is the second change.

Now it is your turn: enable automatic parallelization of columns in the [sixth pipeline](#pipeline6) 
and benchmark it. Just remember to reduce the number of rows and increase the number of columns.

### Caching

Even if you have optimized the simulation pipeline for the best possible performance,
the actual compilation step can greatly reduce those time savings. However, the good news is that
Numba does not need to recompile the function on subsequent executions, provided you pass the same
argument **types** (not data!). This means you only have to wait once if you want to test the 
same function across many parameter combinations within the same Python runtime. Unfortunately,
if even one argument changes type or you restart the Python runtime, Numba will need to recompile. 

Fortunately, Numba offers a solution to avoid recompilation even after a runtime restart, known as
[caching](https://numba.pydata.org/numba-doc/latest/developer/caching.html).
To enable caching, simply pass `cache=True` to the `@njit` decorator.

!!! important
    Avoid enabling caching for functions that take complex, user-defined data, such as
    (named) tuples and other functions. This can sometimes lead to hidden bugs and kernel crashes if the
    data changes in a later runtime. Also, make sure your function does not use global variables.
    For example, the [fifth pipeline](#pipeline5) is perfectly cacheable, while the [sixth pipeline](#pipeline6)
    is not cacheable unless `order_func_nb` is cacheable as well.

Make sure to define any cached function inside a Python file rather than in a notebook cell, since
Numba needs a clear filepath to introspect the function. To invalidate the cache, navigate to the
directory where the function resides and remove the `__pycache__` directory. You can use
the command `rm -rf __pycache__` from your terminal to do this.

!!! tip
    A good practice is to invalidate the cache every time you change the code of a cached function,
    to avoid any potential side effects. Also, keep caching disabled while developing a function
    and only enable it once the function is fully implemented.

### AOT compilation

With [ahead-of-time compilation](https://numba.pydata.org/numba-doc/dev/user/pycc.html), you can compile
a function once and eliminate compilation overhead at runtime. Although this Numba feature
is not widely used in VBT because it would limit flexible input data, it can be helpful
when you know the argument types in advance. Let's pre-compile our [fifth pipeline](#pipeline5)!

To do this, you must specify the explicit signature of the function. You can read more about this in the
[types](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types) reference.

```pycon
>>> from numba.pycc import CC
>>> cc = CC('pipeline_5')  # (1)!

>>> sig = "f8[:, :](" \ # (2)!
...       "UniTuple(i8, 2), " \ # (3)!
...       "i8[:], " \ # (4)!
...       "f8[:, :], " \ # (5)!
...       "f8[:, :], " \
...       "f8[:, :], " \
...       "f8[:], " \ # (6)!
...       "b1, " \ # (7)!
...       "b1" \
...       ")"

>>> cc.export('pipeline_5_nb', sig)(pipeline_5_nb)   # (8)!
>>> cc.compile() # (9)!
```

1. Initialize a new module.

2. The function should return a two-dimensional array of 64-bit floating-point type (allocations).

3. Tuple with two 64-bit integers (`target_shape`).

4. One-dimensional array of 64-bit integer type (`group_lens`).

5. Two-dimensional array of 64-bit floating-point type (`open`, `close`, and `target_pct`).

6. One-dimensional array of 64-bit floating-point type (`init_cash`).

7. Boolean value (`auto_call_seq` and `rotate_cols`).

8. Register the function with the provided signature. You can bind multiple signatures
to the same function.

9. Compile the module.

This generates an extension module named `pipeline_5`. On macOS, the actual filename will be
`pipeline_5.cpython-37m-darwin.so`. You can import this module like any regular Python module
and run the function `pipeline_5_nb` from that module:

```pycon
>>> import pipeline_5

>>> mult_alloc = pipeline_5.pipeline_5_nb(
...     target_shape,
...     group_lens,
...     vbt.to_2d_array(mult_data.get("Open")).astype("f8", copy=False),  # (1)!
...     vbt.to_2d_array(mult_data.get("Close")).astype("f8", copy=False),
...     vbt.to_2d_array(mult_target_pct).astype("f8", copy=False),
...     vbt.to_1d_array(100).astype("f8", copy=False),
...     auto_call_seq=True,  # (2)!
...     rotate_cols=True
... )
```

1. This ensures the array has the correct dimensionality and data type.

2. Keyword arguments must be passed as positional arguments. Default values must also be specified explicitly.

That was lightning fast! :zap:

!!! important
    Make sure that the provided arguments exactly match the registered signature,
    otherwise you may encounter errors that are very difficult to debug. For example,
    setting `init_cash` to `100` would result in an "index is out of bounds" error,
    while casting the array to integer would cause all allocations to be zero!

## Summary

We have explored in detail many components of a typical simulator in VBT. Simulation
is the main step in backtesting a trading strategy, and by mastering it you will
gain advanced skills that can be applied throughout VBT's rich
Numba ecosystem.

One of the key takeaways from this documentation is that implementing a custom
simulator is as easy (or as complex) as any other Numba-compiled function. There is little reason
to use the preset simulation methods such as [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
if you can achieve the same results, boost performance, utilize rotational
indexing, caching, and AOT compilation by designing your own pipeline from scratch. After all, it is
simply a set of loops that move over the shape of a matrix, execute orders, update the simulation state,
and store output data. Everything else is up to your imagination :mage:

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/portfolio/index.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/
parent: https://vectorbt.pro/pvt_41531c19/documentation/portfolio/
name: From orders
type: page
icon: material-checkerboard
---

# From orders

Instead of building a custom simulator from scratch, you can use one of the preset simulation methods
provided by VBT. There are three main methods:
[Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders),
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
and [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).
Each method has its own advantages and disadvantages, but all follow the same iteration scheme discussed
earlier: they iterate over the rows and columns, and at each step, convert the current element of all
user-provided input data into an order request. They then process the request by updating the current
simulation state and appending the filled order record to an array. This array, along with other
information, can later be used during the reconstruction phase to analyze the simulation.

[Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
is the simplest of the three methods: it does not accept any UDFs and allows you to provide every detail
about orders as separate, broadcastable arrays. Every element across all provided arrays is converted into
an instance of [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order) and processed as usual.
Since the number of orders is limited by the number of elements in the arrays, you can issue and execute
only one order per timestamp and asset.

For example, passing `[0.1, -0.1, np.nan]` as the order size array and `[11, 10, 12]` as the order price
array will generate three orders at three consecutive timestamps:

1. Buy 0.1 shares for $11.

2. Sell 0.1 shares for $10.

3. Do nothing (a size of NaN is ignored).

!!! tip
    This method should be used when you know exactly what to order at each timestamp.
    In practice, many types of signals and other inputs can be successfully converted
    to an order format for a significant speedup. For example, if your entries and exits are
    cleaned (meaning one exit comes exactly after one entry and vice versa), you can convert
    them to order size using `entries.astype(int) - exits.astype(int)`. This will order 1 share
    whenever a signal is encountered, in either direction.

## Numba

The core of this method is the Numba-compiled function
[from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb),
which is the fastest of the three simulation functions. It is also the only one that can be (and is)
safely cached in Numba, since it does not depend on any complex or global data. It uses flexible indexing,
is registered as a chunkable function, and can be parallelized by Numba with a single command.

This simulation function's arguments include some that we have already discussed in the documentation
on simulation: target shape, group lengths, initial cash, and others. Some of the new arguments here include
the call sequence array, initial state arrays (such as `init_position`), continuous state change arrays
(such as `cash_deposits`), order information arrays (such as `size`), and various flags for controlling the
simulation process (such as `save_returns`). Most arguments also have carefully chosen default values that
are consistent across most Numba-compiled functions defined in [portfolio.nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/).

### Order fields

Each field in the named tuple [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order)
is available as an argument in this simulation function's signature and can be provided in a format
suitable for flexible indexing.

Let's simulate the three orders mentioned above:

```pycon
>>> from vectorbtpro import *

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(3, 1),  # (1)!
...     group_lens=np.array([1]),  # (2)!
...     size=np.array([[0.1], [-0.1], [np.nan]]),  # (3)!
...     price=np.array([[11], [10], [12]])
... )
>>> sim_out.order_records
array([(0, 0, 0, 0.1, 11., 0., 0), (1, 0, 1, 0.1, 10., 0., 1)],
      dtype={'names':['id','col','idx','size','price','fees','side'], ...})
```

1. Target shape must be two-dimensional. Values such as `3` and `(3,)` are not valid.

2. A one-dimensional group lengths array is required. Without column grouping, this is just an array of
ones (one group per column).

3. For now, set the size and price arrays to have the same shape as `target_shape`.

The simulation returns an instance of the named tuple
[SimulationOutput](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput), which contains
the filled order and log records, along with other supporting information for post-analysis:

```pycon
>>> print(vbt.prettify(sim_out))
SimulationOutput(
    order_records=<numpy.ndarray object at 0x7f88606d5710 of shape (2,)>,
    log_records=<numpy.ndarray object at 0x7f8860907fa8 of shape (0,)>,
    cash_deposits=<numpy.ndarray object at 0x7f8860907f50 of shape (1, 1)>,
    cash_earnings=<numpy.ndarray object at 0x7f8860a355b0 of shape (1, 1)>,
    call_seq=None,
    in_outputs=FSInOutputs(
        returns=<numpy.ndarray object at 0x7f88a1976fa8 of shape (0, 0)>
    )
)
```

!!! info
    Even though we did not instruct VBT to create arrays for log records, cash deposits,
    or cash earnings, these arrays still appear in the simulation output. Because Numba has
    difficulty processing optional writable arrays, these outputs cannot be set to `None`.
    Instead, VBT creates empty arrays with ultra-small shapes to indicate that they should
    be ignored during post-analysis.

Here is a basic helper function to pretty-print order records:

```pycon
>>> def print_orders(target_shape, order_records):
...     wrapper = vbt.ArrayWrapper.from_shape(target_shape)
...     print(vbt.Orders(wrapper, order_records).readable)

>>> print_orders((3, 1), sim_out.order_records)
    Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   0.1   11.0   0.0   Buy
1         1       0          1   0.1   10.0   0.0  Sell
```

To apply information to __every__ element, you can provide a scalar. This works because most
simulation methods automatically convert any scalar into a two-dimensional array suitable for
[flexible indexing](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/#flexible-indexing). You can also specify
information per row, such as price, using a one-dimensional array, like this:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(3, 1),
...     group_lens=np.array([1]),
...     size=np.array([0.1, -0.1, np.nan]),
...     price=np.array([11, 10, 12]),
...     fees=0.01
... )
>>> print_orders((3, 1), sim_out.order_records)
    Order Id  Column  Timestamp  Size  Price   Fees  Side
0         0       0          0   0.1   11.0  0.011   Buy
1         1       0          1   0.1   10.0  0.010  Sell
```

1. Apply 1% commission to all orders.

!!! important
    The broadcasting rules in VBT differ slightly from NumPy's broadcasting rules. In NumPy,
    `(3,)` arrays are treated as specified per column (and broadcast along rows) in the shape `(3, 1)`,
    but VBT assumes that flexible two-dimensional arrays are always time series and are
    specified per row (and broadcast along columns).

To provide information per column, always wrap it in a two-dimensional array. You can also
make this array have only one row to apply information to all rows.
Let's test multiple size values specified per column, while the price is given per row:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(3, 3),  # (1)!
...     group_lens=np.array([1, 1, 1]),  # (2)!
...     size=np.array([[np.inf, np.nan, -np.inf]]),  # (3)!
...     price=np.array([11, 10, 12]),  # (4)!
...     fees=0.01  # (5)!
... )
>>> print_orders((3, 3), sim_out.order_records)
    Order Id  Column  Timestamp    Size  Price      Fees  Side
0         0       0          0  9.0009   11.0  0.990099   Buy
1         0       2          0  9.0009   11.0  0.990099  Sell
```

1. Shape `(n, m)` results from imaginary broadcasting of all the passed arrays.

2. Three groups, one for each column.

3. Arrays with shape `(1, m)` are specified per column. A size of `np.inf` will buy as much as
possible at every timestamp, a size of `np.nan` will be ignored at every timestamp, and a size of `-np.inf`
will short sell as much as possible at every timestamp.

4. Arrays with shape `(n,)` or `(n, 1)` are specified per row.

5. Scalars and arrays with shape `(1,)` or `(1, 1)` are specified for the entire array.

This approach works the same way as if you manually broadcasted all arrays before passing them.

```pycon
>>> size, price, fees = vbt.broadcast_arrays(  # (1)!
...     np.array([[np.inf, np.nan, -np.inf]]),
...     np.array([11, 10, 12]),
...     0.01
... )
>>> size
array([[ inf,  nan, -inf],
        [ inf,  nan, -inf],
        [ inf,  nan, -inf]])

>>> price
array([[11, 11, 11],
        [10, 10, 10],
        [12, 12, 12]])

>>> fees
array([[0.01, 0.01, 0.01],
        [0.01, 0.01, 0.01],
        [0.01, 0.01, 0.01]])

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=size.shape,  # (1)!
...     group_lens=np.full(size.shape[1], 1),
...     size=size,
...     price=price,
...     fees=fees
... )
>>> print_orders(size.shape, sim_out.order_records)
    Order Id  Column  Timestamp    Size  Price      Fees  Side
0         0       0          0  9.0009   11.0  0.990099   Buy
1         0       2          0  9.0009   11.0  0.990099  Sell
```

1. Use [broadcast_arrays](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast_arrays).

2. You can use the shape of any broadcasted array since they all now share the same size.

Notice that all arrays have the same shape `(3, 3)`, which becomes the target shape.

!!! important
    Do not use `np.broadcast_arrays` here. In NumPy, one-dimensional arrays are assumed
    to be specified per column.

!!! info
    To learn more about the different arguments and their meanings, visit the API documentation
    for [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order).

### Grouping

If any element in the array `group_lens` is greater than 1, VBT assumes that the columns are
grouped and enables cash sharing by default. Let's simulate a portfolio with two assets where we try
to order as many shares as possible in both assets:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(1, 2),  # (1)!
...     group_lens=np.array([2]),  # (2)!
...     size=np.array([[np.inf, np.inf]]),  # (3)!
...     price=np.array([[10, 5]])
... )
>>> print_orders((1, 2), sim_out.order_records)
    Order Id  Column  Timestamp  Size  Price  Fees Side
0         0       0          0  10.0   10.0   0.0  Buy
```

1. One row (bar) and two columns (assets).

2. Create a group with cash sharing consisting of two columns.

3. Provide order information per element by making an array the same shape as `target_shape`.

We see that the first asset used all the available cash, leaving the second asset without funds.

### Call sequence

One useful feature of [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb)
is that you can supply your own call sequence to change the order in which columns are processed within
their groups. Without a call sequence, the default order is _from left to right_.
For example, let's perform two rebalancing steps using the default call sequence: allocate 100% to the
second asset at the first timestamp, then close out the position and allocate 100% to the first asset
at the second timestamp.

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]])
... )
>>> print_orders((2, 2), sim_out.order_records)
    Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       1          0  20.0    5.0   0.0   Buy
1         1       1          1  20.0    4.0   0.0  Sell
```

We see that only column `1` has been processed. This is because we tried to allocate 100% to
the first asset without first closing out the second asset at the second timestamp.
To control this order, we can pass our own call sequence:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]]),
...     call_seq=np.array([[0, 1], [1, 0]])  # (1)!
... )
>>> print_orders((2, 2), sim_out.order_records)
    Order Id  Column  Timestamp       Size  Price  Fees  Side
0         0       0          1   7.272727   11.0   0.0   Buy
1         0       1          0  20.000000    5.0   0.0   Buy
2         1       1          1  20.000000    4.0   0.0  Sell
```

1. At the first timestamp: process the first asset, then the second. At the second timestamp:
process the second asset first, then the first. Note that a call sequence must provide exactly one
position per timestamp and asset, so its shape must match `target_shape`.

!!! info
    Order records are partitioned by column and always appear in the order they
    were filled within each column. Also, order ids (the first field) are generated per column, not globally.

At the second timestamp, the first asset now has the funds needed to go long.

To avoid manually providing the call sequence, you can leave it as `None` and enable `auto_call_seq`
instead. In this case, VBT will automatically sort the columns by their approximated order value:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]]),
...     auto_call_seq=True  # (1)!
... )
>>> print_orders((2, 2), sim_out.order_records)
    Order Id  Column  Timestamp       Size  Price  Fees  Side
0         0       0          1   7.272727   11.0   0.0   Buy
1         0       1          0  20.000000    5.0   0.0   Buy
2         1       1          1  20.000000    4.0   0.0  Sell
```

1. Here

Sometimes, you may want to view the sequence in which orders were processed, which is not shown
in the order records. For this, you can provide your own call sequence and let VBT modify it in
place, then return it as a field in the [SimulationOutput](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput)
named tuple. The call sequence array must be pre-filled with indices in strict order from 0 to `n`,
where `n` is the length of the respective group of columns. You can easily build such an array using
the function [build_call_seq](https://vectorbt.pro/pvt_41531c19/api/portfolio/call_seq/#vectorbtpro.portfolio.call_seq.build_call_seq),
which takes the target shape, group lengths, and the call sequence type from
[CallSeqType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.CallSeqType):

```pycon
>>> from vectorbtpro.portfolio.call_seq import build_call_seq

>>> call_seq = build_call_seq(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     call_seq_type=vbt.pf_enums.CallSeqType.Default  # (1)!
... )

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(2, 2),
...     group_lens=np.array([2]),
...     size=np.array([[0, 1], [1, 0]]),
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 5], [11, 4]]),
...     call_seq=call_seq,
...     auto_call_seq=True
... )
>>> sim_out.call_seq
array([[0, 1],
        [1, 0]])
```

1. If you want VBT to sort the call sequence using `auto_call_seq`, it must be of the type `Default`.

The generated and then sorted call sequence exactly matches the one we constructed manually earlier.

!!! tip
    There is usually little reason to provide your own call sequence. Keeping it as `None` (default)
    and enabling the `auto_call_seq` flag will find the best call sequence in the most
    resource-efficient way.

### Filling returns

In addition to returning order records, you can instruct VBT to also fill the return based
on portfolio value at the end of each bar. The resulting series will be available
under the `returns` field of [SimulationOutput.in_outputs](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput.in_outputs),
and can be used to calculate various metrics, such as the Sharpe ratio. Let's do the following:
fetch the BTC price and calculate the return of a simple buy-and-hold strategy, and see that the
filled returns closely match the returns calculated directly from the price.

```pycon
>>> data = vbt.YFData.pull("BTC-USD", end="2022-01-01")
>>> symbol_wrapper = data.get_symbol_wrapper()

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=symbol_wrapper.shape_2d,  # (1)!
...     group_lens=np.array([1]),
...     open=data.get("Open").values,  # (2)!
...     high=data.get("High").values,
...     low=data.get("Low").values,
...     close=data.get("Close").values,
...     save_returns=True  # (3)!
... )
>>> returns = symbol_wrapper.wrap(sim_out.in_outputs.returns)  # (4)!
>>> returns
Date
2014-09-17 00:00:00+00:00    0.000000
2014-09-18 00:00:00+00:00   -0.071926
2014-09-19 00:00:00+00:00   -0.069843
...                               ...
2021-12-29 00:00:00+00:00   -0.024042
2021-12-30 00:00:00+00:00    0.015791
2021-12-31 00:00:00+00:00   -0.018476
Freq: D, Name: BTC-USD, Length: 2663, dtype: float64

>>> data.get("Close").vbt.to_returns()  # (5)!
Date
2014-09-17 00:00:00+00:00    0.000000
2014-09-18 00:00:00+00:00   -0.071926
2014-09-19 00:00:00+00:00   -0.069843
...                               ...
2021-12-29 00:00:00+00:00   -0.024042
2021-12-30 00:00:00+00:00    0.015791
2021-12-31 00:00:00+00:00   -0.018476
Freq: D, Name: Close, Length: 2663, dtype: float64
```

1. Use the data's shape as the target shape. Be sure that columns represent symbols and the shape is two-dimensional.

2. Pass OHLC data as separate arrays. Including the open, high, and low prices is optional.

3. Instruct VBT to calculate the portfolio value and return at each timestamp.

4. Extract the return series and convert it to a Pandas object.

5. Buy-and-hold produces the same returns as the close price itself.

Returns are calculated from the value of each group, so the number of columns in the returned time series
matches the number of groups in `group_lens`:

```pycon
>>> mult_data = vbt.YFData.pull(  # (1)!
...     ["BTC-USD", "ETH-USD"],
...     end="2022-01-01",
...     missing_index="drop"
... )
>>> mult_symbol_wrapper = mult_data.get_symbol_wrapper()

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([2]),  # (2)!
...     close=mult_data.get("Close").values,
...     size=np.array([[0.5, 0.5]]),  # (3)!
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     save_returns=True
... )
>>> returns = mult_symbol_wrapper\
...     .replace(columns=["group"], ndim=1)\
...     .wrap(sim_out.in_outputs.returns)  # (4)!
>>> returns
Date
2017-11-09 00:00:00+00:00    0.000000
2017-11-10 00:00:00+00:00   -0.070482
2017-11-11 00:00:00+00:00    0.006159
...                               ...
2021-12-29 00:00:00+00:00   -0.034684
2021-12-30 00:00:00+00:00    0.019652
2021-12-31 00:00:00+00:00   -0.013406
Freq: D, Name: group, Length: 1514, dtype: float64
```

1. Fetch two columns (assets) of data.

2. Create a single group with two columns.

3. Define an allocation for each column, allowing the array to broadcast smoothly against the target shape.

4. The wrapper holds metadata per column, but the returned array is per group. Therefore, replace the columns
with the single group we created above.

### Initial state

The initial state defines the starting conditions for the entire trading environment.
It mainly consists of three variables: initial cash, initial position, and the average
entry price of the initial position. Each variable is at most one-dimensional and is defined
either per column or per group.

Of these, initial cash is the most important, and can be set as a flexible array
`init_cash` with values per column or per group if cash sharing is enabled. As a rule of thumb,
it should be either a scalar or a one-dimensional array with the same number of elements
as in `group_lens`.

Let's create a group of two columns and allocate 50% to each column, along with two additional groups
with one column and 100% allocation each. This lets us perform two independent backtests:
one with grouping and one without. If you supply the initial cash as a single number ($100 by default),
the first group will split it among two assets, making the starting condition of the grouped
columns different from that of the columns without grouping. This is not ideal if you
want fair statistical experiments.

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(1, 4),
...     group_lens=np.array([2, 1, 1]),
...     init_cash=100,  # (1)!
...     size=np.array([[0.5, 0.5, 1.0, 1.0]]),  # (2)!
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 11, 10, 11]]),
... )
>>> print_orders((1, 4), sim_out.order_records)
    Order Id  Column  Timestamp       Size  Price  Fees Side
0         0       0          0   5.000000   10.0   0.0  Buy
1         0       1          0   4.545455   11.0   0.0  Buy
2         0       2          0  10.000000   10.0   0.0  Buy
3         0       3          0   9.090909   11.0   0.0  Buy
```

1. Set $100 as the starting cash for all three groups.

2. Evenly split the cash among the two columns in the first group. Remember that size and other order
information must be provided per column (asset), not per group.

Let's fix this by providing the first group with twice as much capital:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=(1, 4),
...     group_lens=np.array([2, 1, 1]),
...     init_cash=np.array([200, 100, 100]),  # (1)!
...     size=np.array([[0.5, 0.5, 1.0, 1.0]]),
...     size_type=vbt.pf_enums.SizeType.TargetPercent,
...     price=np.array([[10, 11, 10, 11]]),
... )
>>> print_orders((1, 4), sim_out.order_records)
    Order Id  Column  Timestamp       Size  Price  Fees Side
0         0       0          0  10.000000   10.0   0.0  Buy
1         0       1          0   9.090909   11.0   0.0  Buy
2         0       2          0  10.000000   10.0   0.0  Buy
3         0       3          0   9.090909   11.0   0.0  Buy
```

1. Provide the initial cash as an array with values per group.

Besides initial cash, you can also set the initial position of each asset at the start of simulation.
Let's start the simulation with 1 BTC and 1 ETH, and calculate the returns:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([1, 1]),
...     init_position=np.array([1, 1]),  # (1)!
...     close=mult_data.get("Close").values,
...     save_returns=True
... )
>>> returns = mult_symbol_wrapper.wrap(sim_out.in_outputs.returns)
>>> returns
symbol                      BTC-USD   ETH-USD
Date                                         
2017-11-09 00:00:00+00:00       NaN       NaN
2017-11-10 00:00:00+00:00 -0.073554 -0.067411
2017-11-11 00:00:00+00:00 -0.039368  0.051555
...                             ...       ...
2021-12-29 00:00:00+00:00 -0.024042 -0.045348
2021-12-30 00:00:00+00:00  0.015791  0.023514
2021-12-31 00:00:00+00:00 -0.018476 -0.008406

[1514 rows x 2 columns]
```

1. Initial position must be provided per column (asset).

The first data point is NaN because VBT cannot calculate the initial value of each portfolio
instance without the entry price for each initial position. To fix this, set the entry
price to the first open price:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([1, 1]),
...     init_position=np.array([1, 1]),
...     init_price=mult_data.get("Open").values[0],  # (1)!
...     close=mult_data.get("Close").values,
...     save_returns=True
... )
>>> returns = mult_symbol_wrapper.wrap(sim_out.in_outputs.returns)
>>> returns
symbol                      BTC-USD   ETH-USD
Date                                         
2017-11-09 00:00:00+00:00 -0.040182  0.029950
2017-11-10 00:00:00+00:00 -0.073554 -0.067411
2017-11-11 00:00:00+00:00 -0.039368  0.051555
...                             ...       ...
2021-12-29 00:00:00+00:00 -0.024042 -0.045348
2021-12-30 00:00:00+00:00  0.015791  0.023514
2021-12-31 00:00:00+00:00 -0.018476 -0.008406

[1514 rows x 2 columns]
```

1. Entry price of the initial position must also be provided per column (asset).

!!! important
    Make sure to distinguish between a column and a group.

    Columns represent individual assets, and most information in VBT must be given at this
    (lowest) level of detail. These arrays must broadcast against `target_shape`. Groups, on
    the other hand, represent collections of assets that share the same cash, and only some variables
    related to cash must be provided per group. Such arrays must broadcast against `group_lens.shape`.

!!! tip
    If you are unsure whether an argument should be a one-dimensional or two-dimensional array,
    check the function's source code. One-dimensional arrays are annotated with
    `FlexArray1dLike`, and two-dimensional arrays with `FlexArray2dLike`. If an argument is required to be
    strictly one-dimensional or two-dimensional (that is, a scalar is not allowed), it is annotated
    with `FlexArray1d` and `FlexArray2d` respectively. If an argument is not flexible at all (meaning
    it must match `target_shape`), it will be just `Array1d` or `Array2d` accordingly.

### Cash deposits

In addition to providing the initial cash, you can also deposit or withdraw arbitrary cash amounts
throughout the simulation. Like `init_cash`, the argument `cash_deposits` must be
specified per group. The difference is that this array can now be specified per row __and__ per column,
allowing you to specify the deposited or withdrawn amount at each timestamp. Thanks to flexible indexing,
you can apply the amount to each element, each row, each group, or the entire frame.
The actual operation takes place at the beginning of each bar.

Let's simulate a simple [DCA](https://www.investopedia.com/terms/d/dollarcostaveraging.asp) strategy
where we deposit $100 at the beginning of each year and invest it right away:

```pycon
>>> cash_deposits = symbol_wrapper.fill(0)  # (1)!
>>> cash_deposits.vbt.set(100, every="Y", inplace=True)

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=symbol_wrapper.shape_2d,
...     group_lens=np.array([1]),
...     cash_deposits=cash_deposits.values,
...     close=data.get("Close").values
... )
>>> print_orders(symbol_wrapper.shape_2d, sim_out.order_records)
    Order Id  Column  Timestamp      Size         Price  Fees Side
0         0       0          0  0.218659    457.334015   0.0  Buy
1         1       0        106  0.318219    314.248993   0.0  Buy
2         2       0        471  0.230238    434.334015   0.0  Buy
3         3       0        837  0.100168    998.325012   0.0  Buy
4         4       0       1202  0.007322  13657.200195   0.0  Buy
5         5       0       1567  0.026018   3843.520020   0.0  Buy
6         6       0       1932  0.013889   7200.174316   0.0  Buy
7         7       0       2298  0.003404  29374.152344   0.0  Buy
```

1. Create an array with the same shape as your data, fill it with zeros,
and set elements at an annual frequency to 100.

Below, we do the same but for a group with two assets and equal allocations. Since the array
`cash_deposits` changes the cash balance, it must have the same number of columns as the number of groups
(with cash sharing).

```pycon
>>> cash_deposits = mult_symbol_wrapper\
...     .replace(columns=["group"], ndim=1)\
...     .fill(0)  # (1)!
>>> cash_deposits.vbt.set(100, every="Y", inplace=True)
>>> size = mult_symbol_wrapper.fill(np.nan)  # (2)!
>>> size.vbt.set(0.5, every="Y", inplace=True)
>>> size.iloc[0] = 0.5  # (3)!

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([2]),
...     cash_deposits=cash_deposits.values,
...     close=mult_data.get("Close").values,
...     size=size.values,
...     size_type=vbt.pf_enums.SizeType.TargetPercent
... )
>>> print_orders(mult_symbol_wrapper.shape_2d, sim_out.order_records)
    Order Id  Column  Timestamp      Size         Price  Fees  Side
0         0       0          0  0.006999   7143.580078   0.0   Buy
1         1       0         53  0.004569  13657.200195   0.0   Buy
2         2       0        418  0.010971   3843.520020   0.0   Buy
3         3       0        783  0.001263   7200.174316   0.0   Buy
4         4       0       1149  0.003404  29374.152344   0.0   Buy
5         0       1          0  0.155820    320.884003   0.0   Buy
6         1       1         53  0.048663    772.640991   0.0   Buy
7         2       1        418  0.410697    140.819412   0.0   Buy
8         3       1        783  0.695013    130.802002   0.0   Buy
9         4       1       1149  0.108007    730.367554   0.0  Sell
```

1. Create an array with one column for our group.

2. Create a size array to allocate 50% of the deposited cash to each asset.

3. Since we have initial cash, allocate at the first timestamp as well.

To withdraw cash, provide a negative amount. If the amount of cash to be withdrawn
exceeds the available cash in your account, only the available cash will be withdrawn.
Let's start with 1 BTC, sell 10% each year, and continuously withdraw the entire cash balance:

```pycon
>>> size = symbol_wrapper.fill(np.nan)  # (1)!
>>> size.vbt.set(-0.1, every="Y", inplace=True)

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=symbol_wrapper.shape_2d,
...     group_lens=np.array([1]),
...     init_position=1,
...     cash_deposits=-np.inf,  # (2)!
...     close=data.get("Close").values,
...     size=size.values,
...     size_type=vbt.pf_enums.SizeType.Percent,  # (3)!
...     direction=vbt.pf_enums.Direction.LongOnly  # (4)!
... )
>>> print_orders(symbol_wrapper.shape_2d, sim_out.order_records)
    Order Id  Column  Timestamp      Size         Price  Fees  Side
0         0       0        106  0.100000    314.248993   0.0  Sell
1         1       0        471  0.090000    434.334015   0.0  Sell
2         2       0        837  0.081000    998.325012   0.0  Sell
3         3       0       1202  0.072900  13657.200195   0.0  Sell
4         4       0       1567  0.065610   3843.520020   0.0  Sell
5         5       0       1932  0.059049   7200.174316   0.0  Sell
6         6       0       2298  0.053144  29374.152344   0.0  Sell

>>> cash_deposits = symbol_wrapper.wrap(sim_out.cash_deposits)  # (5)!
>>> print(cash_deposits[cash_deposits != 0])  # (6)!
Date
2014-09-17 00:00:00+00:00    -100.000000
2015-01-02 00:00:00+00:00     -31.424899
2016-01-02 00:00:00+00:00     -39.090061
2017-01-02 00:00:00+00:00     -80.864326
2018-01-02 00:00:00+00:00    -995.609894
2019-01-02 00:00:00+00:00    -252.173348
2020-01-02 00:00:00+00:00    -425.163093
2021-01-02 00:00:00+00:00   -1561.062890
Name: BTC-USD, dtype: float64
```

1. Create a size array to sell 10% of the current position at an annual frequency.

2. Withdraw as much as possible at each timestamp. Remember that most arrays can be provided
as a scalar to be applied to each element using flexible indexing.

3. The size value indicates the percentage of the current position.

4. Make each order long-only to avoid moving into the short direction.

5. Access and wrap the cash deposits.

6. Positive cash deposits mean cash inflow. Negative cash deposits mean cash outflow.

As you can see, on the first date of each year, we sold 10% of our position. Since any changes
related to cash are applied only at the beginning of each bar, the cash generated from the
transaction can only be withdrawn on the following date. Also, whenever you specify cash deposits,
VBT will create a full-scale array [SimulationOutput.cash_deposits](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput.cash_deposits)
to record values that could not be met, such as `-np.inf`, which was replaced by the
cash balance at each timestamp. Without this array, we would be unable to properly
reconstruct the simulation during the post-analysis phase.

### Cash earnings

Unlike cash deposits, cash earnings (`cash_earnings`) refer to cash that is either inflowing to
or outflowing from the user and thus has a direct effect on profitability. Cash earnings are also
applied at the end of each bar. For example, you can use (negative) cash earnings to charge a
fixed commission during a set period of time, or to simulate profit-taking from staking
cryptocurrency. One of the most useful applications of cash earnings is for cash dividends, which are
handled with a separate argument, `cash_dividends`. Cash dividends are multiplied by the current position
size and added to cash earnings. Just like with cash deposits, VBT creates a separate array for
cash earnings whenever it finds any non-zero value in either cash earnings or cash dividends provided
by the user, and writes final operations to this array, which is available under
[SimulationOutput.cash_earnings](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput.cash_earnings).

Let's simulate investing in Apple and keeping the dividends:

```pycon
>>> aapl_data = vbt.YFData.pull("AAPL", end="2022-01-01")
>>> aapl_wrapper = aapl_data.get_symbol_wrapper()
>>> size = aapl_wrapper.fill()  # (1)!
>>> size.iloc[0] = np.inf

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=aapl_wrapper.shape_2d,
...     group_lens=np.array([1]),
...     close=aapl_data.get("Close").values,
...     cash_dividends=aapl_data.get("Dividends").values,
...     size=size.values
... )
>>> print_orders(aapl_wrapper.shape_2d, sim_out.order_records)
    Order Id  Column  Timestamp        Size     Price  Fees Side
0         0       0          0  996.754204  0.100326   0.0  Buy

>>> cash_earnings = aapl_wrapper.wrap(sim_out.cash_earnings)
>>> print(cash_earnings[cash_earnings != 0])  # (2)!
Date
1987-05-11 00:00:00+00:00      0.534260
1987-08-10 00:00:00+00:00      0.534260
1987-11-17 00:00:00+00:00      0.711683
...                                 ...
2021-05-07 00:00:00+00:00    219.285925
2021-08-06 00:00:00+00:00    219.285925
2021-11-05 00:00:00+00:00    219.285925
Name: AAPL, Length: 73, dtype: float64
```

1. Create a size array to buy the maximum amount at the first bar (`np.inf`), then hold (`np.nan`).

2. Positive cash earnings indicate money inflow, negative cash earnings indicate money outflow.

### Max record count

By default, since VBT does not know how many order records will be needed in advance,
it creates an empty array with the same shape as `target_shape` and gradually "appends" new records.
If there are hundreds of thousands or even millions of elements in the target shape,
you may run out of memory trying to create such a large empty array with a complex data type.
To avoid putting too much stress on your RAM, you can specify the maximum number of potential records
for each column using `max_order_records` for order records and `max_log_records` for log records.

For example, if you have tick data with one million data points and want to simulate
a simple strategy where you buy at the first timestamp and sell at the last timestamp,
it makes sense to restrict the number of possible orders in each column to just 2:

```pycon
>>> target_shape = (1000000, 1)
>>> np.random.seed(42)  # (1)!
>>> rand_price = np.random.randint(8, 12, size=target_shape)  # (2)!
>>> size = np.full(target_shape, np.nan)
>>> size[0] = np.inf
>>> size[-1] = -np.inf

>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=2  # (3)!
... )
>>> print_orders(target_shape, sim_out.order_records)
    Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0  10.0   10.0   0.0   Buy
1         1       0     999999  20.0    8.0   0.0  Sell
```

1. Set a seed to make the results reproducible.

2. Generate a random time series to use as a price.

3. We expect to create just two orders.

Exceeding this limit will cause an error:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=1  # (1)!
... )
IndexError: order_records index out of range. Set a higher max_order_records.
```

1. Reduce the maximum number of order records to 1.

You can also completely disable filling order records by setting `max_order_records` to zero:

```pycon
>>> sim_out = vbt.pf_nb.from_orders_nb(
...     target_shape=target_shape,
...     group_lens=np.array([1]),
...     price=rand_price,
...     size=size,
...     max_order_records=0
... )
>>> print_orders(target_shape, sim_out.order_records)
Empty DataFrame
Columns: [Order Id, Column, Timestamp, Size, Price, Fees, Side]
Index: []
```

!!! note
    `max_order_records` and `max_log_records` apply to each column. If one column needs to generate 2 records
    and another needs 1000 records, use the value 1000. Also, do not reduce the maximum
    number of log records (except for setting it to zero) since logs are generated at every timestamp.

### Jitting

Every simulation function, including [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb),
is registered as a jittable function in the [JITRegistry](https://vectorbt.pro/pvt_41531c19/api/registries/jit_registry/#vectorbtpro.registries.jit_registry.JITRegistry)
once VBT is imported. The so-called "jittable setup" resulting from the registration includes various
details about compilation and decoration, such as which arguments were passed to Numba's `@njit` decorator.
At any time, you can instruct the registry to redecorate the function while keeping the other decoration
arguments at their defaults. For example, to disable Numba and run the simulator as a normal Python function:

```pycon
>>> f_py = vbt.jit_reg.resolve_option(
...     task_id=vbt.pf_nb.from_orders_nb, 
...     option=False  # (1)!
... )
```

1. The option is resolved using [resolve_jitted_option](https://vectorbt.pro/pvt_41531c19/api/utils/jitting/#vectorbtpro.utils.jitting.resolve_jitted_option).

To disable caching:

```pycon
>>> f_no_cache = vbt.jit_reg.resolve_option(
...     task_id=vbt.pf_nb.from_orders_nb, 
...     option=dict(cache=False)
... )
```

To enable automatic parallelization:

```pycon
>>> f_parallel = vbt.jit_reg.resolve_option(
...     task_id=vbt.pf_nb.from_orders_nb, 
...     option=dict(parallel=True)
... )
```

!!! tip
    All returned functions can be used exactly the same way as `from_orders_nb`.
    The two latter functions can also be called from within Numba.

### Chunking

The same principle applies to chunking: each simulation function is registered as a chunkable function in
the [ChunkableRegistry](https://vectorbt.pro/pvt_41531c19/api/registries/ch_registry/#vectorbtpro.registries.ch_registry.ChunkableRegistry),
and all arguments in [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb)
are perfectly chunkable across __groups__ (not rows or columns!). However, since Numba-compiled functions
are intended to be used by other Numba-compiled functions, and Numba cannot call regular Python functions,
they have not yet been decorated with the [chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked)
decorator. To decorate the function, you need to explicitly tell the registry:

```pycon
>>> f_chunked = vbt.ch_reg.resolve_option(
...     setup_id_or_func=vbt.pf_nb.from_orders_nb, 
...     option=True  # (1)!
... )
>>> print(vbt.prettify(f_chunked.options))
Config(
    n_chunks=None,
    size=ArraySizer(
        arg_query='group_lens',
        single_type=None,
        axis=0
    ),
    min_size=None,
    chunk_len=None,
    chunk_meta=None,
    skip_single_chunk=None,
    arg_take_spec={...},
    template_context=None,
    engine=None,
    engine_config={},
    merge_func=<function merge_sim_outs at 0x7f88873f1ea0>,
    merge_kwargs={...},
    return_raw_chunks=False,
    silence_warnings=None,
    disable=None
)
```

1. The option is resolved using [resolve_chunked_option](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.resolve_chunked_option).

!!! tip
    The returned function can be used just like `from_orders_nb`, but not within Numba,
    since it is now wrapped with a regular Python function for chunking.

Let's go all in on BTC and ETH while using chunking. This approach will produce two fully isolated
simulations. Internally, the process splits `group_lens` into two arrays (`[1]` and `[1]`), then separates
each argument value into chunks so that each chunk contains information only for one group. It then runs
the same function on different chunks with [Dask](https://dask.org/), and finally merges the results
from both simulations as if they were generated by a single, monolithic simulation :magic_wand:

```pycon
>>> sim_out = f_chunked(
...     target_shape=mult_symbol_wrapper.shape_2d,
...     group_lens=np.array([1, 1]),
...     close=mult_data.get("Close").values,
...     _n_chunks=2,  # (1)!
...     _execute_kwargs=dict(engine="dask")
... )
>>> print_orders(mult_symbol_wrapper.shape_2d, sim_out.order_records)
    Order Id  Column  Timestamp      Size        Price  Fees Side
0         0       0          0  0.013999  7143.580078   0.0  Buy
1         0       1          0  0.311639   320.884003   0.0  Buy
```

1. Add an underscore before an argument to override a setting with the same name in
[chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked).

Chunking can even split flexible arrays!

## Class method

Often, using the Numba-compiled simulation function directly can be cumbersome because many inputs are
scalars or Pandas objects that need to be converted to NumPy arrays. However, understanding how inputs
are handled at the most fundamental level is essential for learning how VBT works behind the
scenes. To make this process easier, VBT wraps each simulation function with a class method, such
as [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
for [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb).
This wrapper automates some input pre-processing and output post-processing.

For example, here is how simple it is to test the three orders mentioned at the beginning:

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=[11, 10, 12],
...     size=[0.1, -0.1, np.nan]
... )
>>> pf.orders.readable
    Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   0.1   11.0   0.0   Buy
1         1       0          1   0.1   10.0   0.0  Sell
```

But the simplest example is just a single order (for $10, buy 1 share):

```pycon
>>> pf = vbt.Portfolio.from_orders(10, 1)  # (1)!
>>> pf.orders.readable
    Order Id  Column  Timestamp  Size  Price  Fees Side
0         0       0          0   1.0   10.0   0.0  Buy
```

1. Any array-like argument will be converted into a NumPy array.

Each class method acts as a small pipeline that retrieves default argument values from global settings,
broadcasts arguments, checks argument data types, redecorates the simulation function for jitting and
chunking, runs the simulation, and finally creates a new
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
instance based on the simulation result. This instance is ready for analysis.
Most importantly, it leverages Pandas, including datetime indexes and column hierarchies—because nobody
wants to work with only NumPy arrays!

### Close price

Unlike their Numba-compiled counterparts, the class methods require you to provide the close price.
This requirement is tied to the post-analysis stage: many metrics and time series, such as the equity
curve, can only be reliably calculated using the latest price at each bar. These metrics rely on
information available during the bar, and we want to avoid referencing future data, such as the open
price or other intermediate points.

### Defaults

If you examine the signature of [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders),
you will notice many arguments have a default value of `None`. The value `None` has a specific
meaning and typically instructs VBT to replace it with the appropriate default from the global 
settings.

```pycon
>>> vbt.phelp(vbt.Portfolio.from_orders, incl_doc=False)
Portfolio.from_orders(
    close,
    size=None,
    size_type=None,
    direction=None,
    price=None,
    fees=None,
    ...
    jitted=None,
    chunked=None,
    wrapper_kwargs=None,
    freq=None,
    bm_close=None,
    **kwargs
)
```

The global settings for [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) are found in
the config [settings.portfolio](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.portfolio). For example,
by default, VBT uses the close price as the order price (remember that negative infinity means
the open price and positive infinity means the close price?):

```pycon
>>> vbt.settings.portfolio["price"]
inf
```

To change a default, you can override it directly in the settings. For example, let's introduce a
fixed commission of $1 to every order:

```pycon
>>> vbt.settings.portfolio["fixed_fees"] = 1

>>> pf = vbt.Portfolio.from_orders(
...     close=pd.Series([11, 10, 12]),
...     size=pd.Series([0.1, -0.1, np.nan])
... )
>>> pf.orders.readable
    Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   0.1   11.0   1.0   Buy
1         1       0          1   0.1   10.0   1.0  Sell
                                                ^
                                              here
```

Settings can be reset just like any other [config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config):

```pycon
>>> vbt.settings.portfolio.reset()

>>> vbt.settings.portfolio["fixed_fees"]
0.0
```

In general, global defaults follow the same pattern as the keyword arguments across most simulation
functions. For example, `price` defaults to `np.array(np.inf)` almost everywhere in Numba. If you
cannot find an argument in the global settings, it means there is no default for that argument,
and `None` is a valid value.

### Enums

Within the VBT ecosystem, many arguments are of an enumerated type. Enums are regular integers that
act as categorical variables. Like contexts, they are also represented by named tuples, but these tuples
are already initialized with values, usually ranging from 0 to the total number of categories in the enum.
As an example, consider [SizeType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType):

```pycon
>>> print(vbt.prettify(vbt.pf_enums.SizeType))
SizeTypeT(
    Amount=0,
    Value=1,
    Percent=2,
    TargetAmount=3,
    TargetValue=4,
    TargetPercent=5
)
```

Instead of requiring users to provide each value as an integer, you can pass the name of the field and
convert it to an integer using [map_enum_fields](https://vectorbt.pro/pvt_41531c19/api/utils/enum_/#vectorbtpro.utils.enum_.map_enum_fields).
This function takes the field name and the enumerated type and returns the corresponding value.
Additionally, it lets you convert entire collections of fields, such as lists or Pandas objects.

```pycon
>>> vbt.map_enum_fields("targetamount", vbt.pf_enums.SizeType)
3

>>> vbt.map_enum_fields([
...     "amount",
...     "targetamount",
...     "targetpercent"
... ], vbt.pf_enums.SizeType)
[0, 3, 5]
```

Internally, this function uses [apply_mapping](https://vectorbt.pro/pvt_41531c19/api/utils/mapping/#vectorbtpro.utils.mapping.apply_mapping),
which provides options like ignoring the input if it is already an integer:

```pycon
>>> vbt.map_enum_fields(3, vbt.pf_enums.SizeType)
3
```

By default, it also ignores case and removes all non-alphanumeric characters:

```pycon
>>> vbt.map_enum_fields("Target Amount", vbt.pf_enums.SizeType)
3
```

That's the magic behind [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and other simulation methods knowing how to handle string options! For example, let's enter a position of
one share at one bar and then sell 50 percent of it at the next bar:

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=pd.Series([10, 11]),
...     size=pd.Series([1, -0.5]),
...     size_type=pd.Series(["amount", "percent"]),
...     direction="longonly"
... )
>>> pf.orders.readable
    Order Id  Column  Timestamp  Size  Price  Fees  Side
0         0       0          0   1.0   10.0   0.0   Buy
1         1       0          1   0.5   11.0   0.0  Sell
```

!!! note
    Conversion is not vectorized. For large arrays, it is best to use integers directly to avoid
    performance penalties.

### Broadcasting

Once argument values are resolved, VBT identifies all arguments that should broadcast against the
target shape and passes them to the function [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast).

Broadcasting is one of the most important features in VBT because it allows you to provide arrays
with various shapes and types, including NumPy arrays, Pandas Series and DataFrames, and even regular
lists. Whenever we iterate over rows (timestamps) and columns (assets), the simulator needs to know
which element to pick from each array. Instead of providing large arrays with every element set,
you can pass some arrays with elements per row, some per column, and some as scalars, and the
broadcaster will ensure they align perfectly with the target shape over which the simulator iterates.

Let's manually broadcast some arrays. In this example, we have one column in the price array and want
to test multiple combinations of order size by making `size` a DataFrame with one row and multiple columns:

```pycon
>>> close = pd.Series(
...     [11, 10, 12], 
...     index=vbt.date_range("2020-01-01", periods=3)
... )
>>> size = pd.DataFrame(
...     [[-np.inf, np.nan, np.inf]],
...     columns=pd.Index(["short", "nan", "long"], name="size")
... )
>>> fees = 0.01

>>> broadcasted = vbt.broadcast(dict(
...     close=close,
...     size=size,
...     fees=0.01
... ))
>>> broadcasted["close"]
size        short  nan  long
2020-01-01     11   11    11
2020-01-02     10   10    10
2020-01-03     12   12    12

>>> broadcasted["size"]
size        short  nan  long
2020-01-01   -inf  NaN   inf
2020-01-02   -inf  NaN   inf
2020-01-03   -inf  NaN   inf

>>> broadcasted["fees"]
size        short   nan  long
2020-01-01   0.01  0.01  0.01
2020-01-02   0.01  0.01  0.01
2020-01-03   0.01  0.01  0.01
```

Thanks to flexible indexing, you do not have to expand each argument to the full shape or materialize it.
To avoid high memory usage, each simulation method passes `keep_flex=True` to the broadcaster, keeping
all arguments in their original form for flexible indexing. For these arguments, the broadcaster
only checks whether they __can broadcast__ to the desired shape. Since you can broadcast not only
NumPy arrays but also Pandas objects, the broadcaster returns the wrapper resulting from the broadcasting
operation, which contains the final shape and Pandas metadata, including the index and columns:

```pycon
>>> broadcasted, wrapper = vbt.broadcast(dict(
...     close=close,
...     size=size,
...     fees=0.01
... ), keep_flex=True, return_wrapper=True)
>>> broadcasted["close"]
[[11]
  [10]
  [12]]

>>> broadcasted["size"]
[[-inf  nan  inf]]

>>> broadcasted["fees"]
[0.01]

>>> wrapper.fill()  # (1)!
size        short  nan  long
2020-01-01    NaN  NaN   NaN
2020-01-02    NaN  NaN   NaN
2020-01-03    NaN  NaN   NaN
```

1. Create a dummy array to reveal the broadcasted Pandas metadata.

!!! tip
    Even though we passed `fees` as a scalar, the broadcaster automatically wrapped it in a NumPy 
    array and expanded it to two dimensions for Numba. For the same reason, it also converted Pandas to NumPy.

Some arrays, such as `init_cash` and `cash_deposits`, cannot be broadcasted together with `close`
because their final shape depends on the number of groups or because they are inherently one-dimensional.
Therefore, after all regular arrays have been aligned, the wrapper has been created, and the target shape
has been established, VBT will first create the group lengths array, and then individually broadcast
all arrays that cannot be broadcasted against `target_shape`. For example, to broadcast
the initial position array:

```pycon
>>> init_position = 1
>>> new_init_position = vbt.broadcast_array_to(init_position, wrapper.shape_2d[1])
>>> new_init_position
array([1, 1, 1])
```

All of this is handled automatically by [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)!

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=close,
...     size=size,
...     fees=fees,
...     init_position=init_position
... )
>>> pf.orders.readable
    Order Id Column  Timestamp       Size  Price      Fees  Side
0         0  short 2020-01-01  10.981098   11.0  1.207921  Sell
1         0   long 2020-01-01   9.000900   11.0  0.990099   Buy

>>> pf.value
size             short    nan        long
2020-01-01  109.792079  111.0  110.009901
2020-01-02  119.773177  110.0  100.009001
2020-01-03   99.810981  112.0  120.010801
```

To control the broadcasting process, you can pass additional arguments to
[broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast) using `broadcast_kwargs`.
For example, let's set custom final columns:

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=close,
...     size=size,
...     fees=fees,
...     init_position=init_position,
...     broadcast_kwargs=dict(columns_from=["a", "b", "c"])
... )
>>> pf.value
                      a      b           c
2020-01-01  109.792079  111.0  110.009901
2020-01-02  119.773177  110.0  100.009001
2020-01-03   99.810981  112.0  120.010801
```

You can also wrap any argument with the class [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO)
to provide broadcasting-related keyword arguments specifically for that object, or with the class
[Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) to mark the object as a parameter.
Below, we are testing the Cartesian product of two parameters: size and fees.

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=close,
...     size=vbt.Param([-np.inf, np.inf]),  # (1)!
...     fees=vbt.Param([0, 0.01]),
...     init_position=init_position
... )
>>> pf.value
size                          -inf                     inf            
fees              0.00        0.01        0.00        0.01
2020-01-01  111.000000  109.792079  111.000000  110.009901
2020-01-02  121.090909  119.773177  100.909091  100.009001
2020-01-03  100.909091   99.810981  121.090909  120.010801
```

1. You could also pass a regular `pd.Index` for the same effect.

Furthermore, you can broadcast differently shaped Pandas DataFrames if their column levels overlap.
Suppose you have two assets and want to test using both the open and close price as the order price.
For this, create a Pandas DataFrame for the order price with four columns, one for each price type
per asset:

```pycon
>>> mult_close = mult_data.get("Close")
>>> mult_close
symbol                          BTC-USD      ETH-USD
Date                                                
2017-11-09 00:00:00+00:00   7143.580078   320.884003
2017-11-10 00:00:00+00:00   6618.140137   299.252991
2017-11-11 00:00:00+00:00   6357.600098   314.681000
...                                 ...          ...
2021-12-29 00:00:00+00:00  46444.710938  3628.531738
2021-12-30 00:00:00+00:00  47178.125000  3713.852051
2021-12-31 00:00:00+00:00  46306.445312  3682.632812

[1514 rows x 2 columns]

>>> mult_price = pd.concat((
...     mult_data.get("Open"),
...     mult_data.get("Close")
... ), axis=1, keys=pd.Index(["open", "close"], name="price"))
>>> mult_price
price                              open                      close  \
symbol                          BTC-USD      ETH-USD       BTC-USD   
Date                                                                 
2017-11-09 00:00:00+00:00   7446.830078   308.644989   7143.580078   
2017-11-10 00:00:00+00:00   7173.729980   320.670990   6618.140137   
2017-11-11 00:00:00+00:00   6618.609863   298.585999   6357.600098   
...                                 ...          ...           ...   
2021-12-29 00:00:00+00:00  47623.871094  3797.436279  46444.710938   
2021-12-30 00:00:00+00:00  46490.605469  3632.219727  47178.125000   
2021-12-31 00:00:00+00:00  47169.371094  3713.430176  46306.445312   

price                                   
symbol                         ETH-USD  
Date                                    
2017-11-09 00:00:00+00:00   320.884003  
2017-11-10 00:00:00+00:00   299.252991  
2017-11-11 00:00:00+00:00   314.681000  
...                                ...  
2021-12-29 00:00:00+00:00  3628.531738  
2021-12-30 00:00:00+00:00  3713.852051  
2021-12-31 00:00:00+00:00  3682.632812  

[1514 rows x 4 columns]
```

Even though both shapes (1524, 2) and (1524, 4) are [not broadcastable](https://numpy.org/doc/stable/user/basics.broadcasting.html)
(!) in NumPy, VBT recognizes that both DataFrames share the same column level `symbol` and aligns
them based on that level using [align_pd_arrays](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.align_pd_arrays),
which results in a successful simulation:

```pycon
>>> pf = vbt.Portfolio.from_orders(close=mult_close, price=mult_price)
>>> pf.value
price                            open                    close             
symbol                        BTC-USD      ETH-USD     BTC-USD      ETH-USD
Date                                                                       
2017-11-09 00:00:00+00:00   95.927798   103.965402  100.000000   100.000000
2017-11-10 00:00:00+00:00   88.871910    96.957022   92.644585    93.258931
2017-11-11 00:00:00+00:00   85.373240   101.955648   88.997394    98.066902
...                               ...          ...         ...          ...
2021-12-29 00:00:00+00:00  623.684312  1175.632804  650.160150  1130.792345
2021-12-30 00:00:00+00:00  633.532987  1203.276315  660.426908  1157.381490
2021-12-31 00:00:00+00:00  621.827608  1193.161381  648.224627  1147.652355

[1514 rows x 4 columns]
```

!!! info
    See the API documentation for [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast)
    for more examples on broadcasting.

Even though [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
is the most basic simulation method, it already accepts dozens of broadcastable array-like arguments.
So, how do you know exactly which argument is broadcastable?

To determine which arguments can be broadcasted, check the API documentation for the argument,
or look at the annotation of the argument in the source code. If its type is `ArrayLike`, it can be
provided both as a scalar and as an array. You can also examine the argument annotations of the
Numba-compiled simulation function (in this case,
[from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb))
and look for the `FlexArray` prefix. Finally, the last and probably most underrated method
is to check the argument-taking specification of the chunking decorator: arguments that are chunked
using [FlexArraySlicer](https://vectorbt.pro/pvt_41531c19/api/base/chunking/#vectorbtpro.base.chunking.FlexArraySlicer) or have `flex`
in the specification name are broadcastable by nature. The specification also reveals the shape
against which the argument should broadcast.

```pycon
>>> print(vbt.prettify(f_chunked.options["arg_take_spec"]["close"]))
FlexArraySlicer(
    single_type=None,
    ignore_none=True,
    mapper={
        'should_cache': True,
        'chunk_meta_cache': {...},
        'arg_query': {
            'pattern': '(group_lens|group_map)',
            'flags': 0
        }
    },
    axis=1
)
```

Here, the argument `close` is expected as a flexible array that is chunked along the column axis
using the group lengths mapper. Since the simulation function is always chunked by its groups,
and this argument's columns are mapped to those groups, it should be specified per column
in `target_shape` instead of per group in `group_lens`. Arguments that have no mapper, such as
`cash_deposits`, are always specified per group:

```pycon
>>> print(vbt.prettify(f_chunked.options["arg_take_spec"]["cash_deposits"]))
FlexArraySlicer(
    single_type=None,
    ignore_none=True,
    mapper=None,
    axis=1
)
```

!!! tip
    As a rule of thumb: if you examine the source code of
    [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb),
    only the arguments with `portfolio_ch.flex_array_gl_slicer` as their specification 
    are broadcasted together and build the target shape. All other flexible arguments broadcast 
    individually once the target shape has been established.

### Grouping

Since the target shape is now generated from broadcasting instead of being passed manually by the user,
the user cannot provide the group lengths directly. Instead, VBT uses a grouping instruction to
create this array for you. Grouping is performed by constructing a
[Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper) instance, which
takes the broadcasted columns and a group-by object (`group_by`). It then uses the group-by object
to assign the columns to groups and generate the group lengths array. See the API documentation
for [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/base/#vectorbtpro.base.grouping.base.Grouper) to learn about the different
group-by options. For example, you can pass `group_by=True` to put all columns into a single group,
or specify the column level by which the columns should be grouped.

And although [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb)
automatically enables cash sharing whenever it detects multiple columns in a group, you must explicitly 
enable cash sharing with `cash_sharing` in the class method, or grouping will not be performed 
during the simulation! This is because a [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
instance can also group its columns during the post-analysis phase, and `cash_sharing`
is a special flag that tells VBT to group during the simulation as well.

Let's demonstrate how investing all the initial cash into two assets behaves with no grouping, with grouping,
and with grouping plus cash sharing:

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close")
... )
>>> pf.value  # (1)!
symbol                        BTC-USD      ETH-USD
Date                                              
2017-11-09 00:00:00+00:00  100.000000   100.000000
2017-11-10 00:00:00+00:00   92.644585    93.258931
2017-11-11 00:00:00+00:00   88.997394    98.066902
...                               ...          ...
2021-12-29 00:00:00+00:00  650.160150  1130.792345
2021-12-30 00:00:00+00:00  660.426908  1157.381490
2021-12-31 00:00:00+00:00  648.224627  1147.652355

[1514 rows x 2 columns]

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"),
...     group_by=True
... )
>>> pf.value  # (2)!
Date
2017-11-09 00:00:00+00:00     200.000000
2017-11-10 00:00:00+00:00     185.903516
2017-11-11 00:00:00+00:00     187.064296
...                                  ...
2021-12-29 00:00:00+00:00    1780.952495
2021-12-30 00:00:00+00:00    1817.808397
2021-12-31 00:00:00+00:00    1795.876982
Freq: D, Name: group, Length: 1514, dtype: float64

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"),
...     group_by=True,
...     cash_sharing=True
... )
>>> pf.value  # (3)!
Date
2017-11-09 00:00:00+00:00    100.000000
2017-11-10 00:00:00+00:00     92.644585
2017-11-11 00:00:00+00:00     88.997394
...                                 ...
2021-12-29 00:00:00+00:00    650.160150
2021-12-30 00:00:00+00:00    660.426908
2021-12-31 00:00:00+00:00    648.224627
Freq: D, Name: group, Length: 1514, dtype: float64
```

1. Without grouping, each column manages its own initial cash.

2. With grouping but without cash sharing, each column still manages its own initial cash.
However, during the post-analysis phase, VBT concatenates both columns to analyze them as a single group,
so the first value appears as $200 instead of $100.

3. With grouping and cash sharing, both columns share the same initial cash. The first column 
invested the entire amount, leaving the second column with insufficient funds, so the value effectively 
reflects only the first asset.

Passing `group_by=True` is only suitable when all columns represent different assets and there are no parameter
combinations. So, what happens when there are multiple assets and parameter combinations?

Let's experiment with different group-by instructions on the `mult_close` and `mult_price` arrays 
we created earlier. The resulting broadcasted shape has 4 columns: each price type for each asset.
Our goal is to create two groups, each containing the assets for one parameter combination.
We cannot use `group_by=True` because it would combine all 4 columns. We also cannot
group by the column level `symbol`, as this would group by asset and place all columns with `BTC-USD` (such as `(Open, BTC-USD)` and `(Close, BTC-USD)`) together, and all `ETH-USD` columns together.
Instead, we need to group `(Open, BTC-USD)` and `(Open, ETH-USD)` together, and `(Close, BTC-USD)` 
and `(Close, ETH-USD)` together; meaning, we need to use all column levels __except for symbols__ as `group_by`.
This can be done in multiple ways:

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=mult_close, 
...     price=mult_price,
...     group_by=pd.Index(["group1", "group1", "group2", "group2"])  # (1)!
... )
>>> pf.value
                                group1       group2
Date                                               
2017-11-09 00:00:00+00:00   199.893199   200.000000
2017-11-10 00:00:00+00:00   185.828932   185.903516
2017-11-11 00:00:00+00:00   187.328888   187.064296
...                                ...          ...
2021-12-29 00:00:00+00:00  1799.317116  1780.952495
2021-12-30 00:00:00+00:00  1836.809302  1817.808397
2021-12-31 00:00:00+00:00  1814.988988  1795.876982

[1514 rows x 2 columns]

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_close, 
...     price=mult_price,
...     group_by=["price"]  # (2)!
... )
>>> pf.value
price                             open        close
Date                                               
2017-11-09 00:00:00+00:00   199.893199   200.000000
2017-11-10 00:00:00+00:00   185.828932   185.903516
2017-11-11 00:00:00+00:00   187.328888   187.064296
...                                ...          ...
2021-12-29 00:00:00+00:00  1799.317116  1780.952495
2021-12-30 00:00:00+00:00  1836.809302  1817.808397
2021-12-31 00:00:00+00:00  1814.988988  1795.876982

[1514 rows x 2 columns]

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_close, 
...     price=mult_price,
...     group_by=vbt.ExceptLevel("symbol")  # (3)!
... )
>>> pf.value
price                             open        close
Date                                               
2017-11-09 00:00:00+00:00   199.893199   200.000000
2017-11-10 00:00:00+00:00   185.828932   185.903516
2017-11-11 00:00:00+00:00   187.328888   187.064296
...                                ...          ...
2021-12-29 00:00:00+00:00  1799.317116  1780.952495
2021-12-30 00:00:00+00:00  1836.809302  1817.808397
2021-12-31 00:00:00+00:00  1814.988988  1795.876982
```

1. Manually pass any collection of group labels, ideally a Pandas Index. This collection
will be converted into a column level. It must have the same length as the number of columns 
in the target shape.

2. Pass the names of all column levels except for the one with the assets. This works only 
when there are multiple levels, meaning the column index is a `pd.MultiIndex`. The grouped column levels 
will be shown in the final column hierarchy.

3. Pass the asset-level column to [ExceptLevel](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.base.indexes.ExceptLevel)
to group all column levels except that one. This approach works with any column index and 
is the most flexible option.

!!! important
    To ensure the grouping operation on assets worked as expected, the final column
    hierarchy should include all column levels except the one for asset symbols. For example,
    passing `group_by=True` will hide all column levels, while passing `group_by='symbol'`
    will display only the asset symbol column level :x:

### Call sequence

Similar to grouping, the class method makes handling call sequences easier. There is an
argument `call_seq` that not only accepts a (broadcastable) array, but can also take
a value from the enum [CallSeqType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.CallSeqType).
For example, you can set it to "auto" to automatically sort the call sequence so assets that should be 
sold are executed before assets that should be bought. This is an important requirement for 
rebalancing. Let's create an equally-weighted portfolio that is rebalanced each month:

```pycon
>>> size = mult_symbol_wrapper.fill(np.nan)
>>> size.vbt.set(0.5, every="M", inplace=True)
>>> size.iloc[0] = 0.5

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     size_type="targetpercent",
...     group_by=vbt.ExceptLevel("symbol"),
...     cash_sharing=True,
...     call_seq="auto"
... )
>>> allocations = pf.get_asset_value(group_by=False).vbt / pf.value
>>> allocations.vbt.plot(  # (1)!
...    trace_kwargs=dict(stackgroup="one"),
...    use_gl=False
... ).show()
```

1. Plot how much each asset contributes to the portfolio value.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_orders_call_seq.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_orders_call_seq.dark.svg#only-dark){: .iimg loading=lazy }

!!! tip
    By the way, this is exactly how 
    [Portfolio.from_optimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_optimizer)
    uses [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders) 
    for rebalancing!

To access the sorted call sequence after the simulation, you can pass `attach_call_seq` and
then use the property [Portfolio.call_seq](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.call_seq):

```pycon
>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     size_type="targetpercent",
...     group_by=vbt.ExceptLevel("symbol"),
...     cash_sharing=True,
...     call_seq="auto",
...     attach_call_seq=True  # (1)!
... )
>>> pf.call_seq
symbol                     BTC-USD  ETH-USD
Date                                       
2017-11-09 00:00:00+00:00        0        1
2017-11-10 00:00:00+00:00        1        0
2017-11-11 00:00:00+00:00        1        0
...                            ...      ...
2021-12-29 00:00:00+00:00        1        0
2021-12-30 00:00:00+00:00        1        0
2021-12-31 00:00:00+00:00        1        0

[1514 rows x 2 columns]
```

1. Access the stored call sequence after the simulation.

### Unlimited cash

When backtesting many trading strategies and parameter combinations, cash can quickly become a limiting factor. 
Choosing the right amount of starting capital can also be a challenge on its own. Fortunately, there are 
several options that let's backtest a trading strategy without worrying about cash constraints. One common 
approach is to pass `np.inf` as `init_cash` to simulate an unlimited cash balance. Welcome to the billionaires 
club :money_with_wings:

However, this approach would cause issues during post-analysis because the portfolio value at each timestamp 
would be infinite. To address this, we can tell VBT to simulate unlimited cash during the backtest, 
and then analyze the actual expenditures afterward to determine the optimal starting capital needed. 
If desired, we can then rerun the simulation with the calculated amount. To facilitate this, we can pass
an option from [InitCashMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.InitCashMode)
as `init_cash`. Unlike other enums, this one contains only negative values, so it cannot be confused 
with zero or positive amounts.

!!! important
    During the simulation, each group value will be infinity. Therefore, we cannot use sizes of (+/-) 
    infinity, or use percentages, target percentages, or any size types that depend on the group value.
    Returns also cannot be calculated during the simulation.

Let's [DCA](https://www.investopedia.com/terms/d/dollarcostaveraging.asp) into BTC and ETH by buying one unit 
each year. Afterward, we can see how much capital would have been required for this activity:

```pycon
>>> size = mult_symbol_wrapper.fill(np.nan)
>>> size.vbt.set(1, every="Y", inplace=True)
>>> size.iloc[0] = 1

>>> pf = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     init_cash="auto"  # (1)!
... )
>>> pf.init_cash  # (2)!
symbol
BTC-USD    61218.626953
ETH-USD     2095.513962
Name: init_cash, dtype: float64
```

1. Use [InitCashMode.Auto](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.InitCashMode.Auto).

2. Calculated after the simulation.

We can then use these amounts in a new simulation if desired:

```pycon
>>> pf2 = vbt.Portfolio.from_orders(
...     close=mult_data.get("Close"), 
...     size=size,
...     init_cash=pf.init_cash
... )
>>> pf2.cash.loc[~size.isnull().all(axis=1)]  # (1)!
symbol                          BTC-USD      ETH-USD
Date                                                
2017-11-09 00:00:00+00:00  54075.046875  1774.629959
2018-01-01 00:00:00+00:00  40417.846680  1001.988968
2019-01-01 00:00:00+00:00  36574.326660   861.169556
2020-01-01 00:00:00+00:00  29374.152344   730.367554
2021-01-01 00:00:00+00:00      0.000000     0.000000
```

1. Display the cash balance after each investment.

We can observe how each investment reduces the cash balance, and how the final investment depletes
it completely, while still allowing us to purchase exactly one unit of each cryptocurrency:

```pycon
>>> pf2.orders.readable
    Order Id   Column                 Timestamp  Size         Price  Fees Side
0         0  BTC-USD 2017-11-09 00:00:00+00:00   1.0   7143.580078   0.0  Buy
1         1  BTC-USD 2018-01-01 00:00:00+00:00   1.0  13657.200195   0.0  Buy
2         2  BTC-USD 2019-01-01 00:00:00+00:00   1.0   3843.520020   0.0  Buy
3         3  BTC-USD 2020-01-01 00:00:00+00:00   1.0   7200.174316   0.0  Buy
4         4  BTC-USD 2021-01-01 00:00:00+00:00   1.0  29374.152344   0.0  Buy
5         0  ETH-USD 2017-11-09 00:00:00+00:00   1.0    320.884003   0.0  Buy
6         1  ETH-USD 2018-01-01 00:00:00+00:00   1.0    772.640991   0.0  Buy
7         2  ETH-USD 2019-01-01 00:00:00+00:00   1.0    140.819412   0.0  Buy
8         3  ETH-USD 2020-01-01 00:00:00+00:00   1.0    130.802002   0.0  Buy
9         4  ETH-USD 2021-01-01 00:00:00+00:00   1.0    730.367554   0.0  Buy
```

### Output arrays

All arrays returned as part of [SimulationOutput](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput),
such as `cash_deposits`, `cash_earnings`, and `in_outputs.returns`, can be accessed as attributes 
with the same name on a [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) instance.
Let's repeat the same example as in [Cash deposits](#cash-deposits):

```pycon
>>> size = symbol_wrapper.fill(np.nan)
>>> size.vbt.set(-0.1, every="Y", inplace=True)

>>> pf = vbt.Portfolio.from_orders(
...     close=data.get("Close"),
...     size=size,
...     size_type="percent",
...     direction="longonly",
...     init_position=1,
...     cash_deposits=-np.inf
... )
>>> pf.cash_deposits[pf.cash_deposits != 0]
Date
2014-09-17 00:00:00+00:00    -100.000000
2015-01-02 00:00:00+00:00     -31.424899
2016-01-02 00:00:00+00:00     -39.090061
2017-01-02 00:00:00+00:00     -80.864326
2018-01-02 00:00:00+00:00    -995.609894
2019-01-02 00:00:00+00:00    -252.173348
2020-01-02 00:00:00+00:00    -425.163093
2021-01-02 00:00:00+00:00   -1561.062890
dtype: float64
```

### Max record count

Another automation provided by [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
is managing the maximum number of order and log records. Whenever we pass `None`, 
[from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb)
will choose the maximum possible number of records. In the class method, VBT
counts the number of non-NaN values in the size array and selects the highest count
across all columns. The same process applies to logs, where it checks the number of `True` values.
This automation has almost no impact on performance because VBT does not need to fully broadcast
both arrays to get these numbers. Therefore, we do not need to provide `max_order_records` or `max_log_records`
if we choose to represent inactive data points with NaN in a large close array.

### Data type checks

Numba brings static typing to VBT, and passing incorrect
data types to Numba usually results in a poorly formatted exception. To make debugging easier,
VBT validates all data types before calling 
[from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb).

```pycon
>>> vbt.Portfolio.from_orders(True)
AssertionError: Data type of 'close' must be <class 'numpy.number'>, not bool
```

In this case, the argument `close` must be a numeric data type :relieved:

!!! note
    In VBT, almost no function will change an array's data type silently,
    since casting can be expensive in terms of performance, especially for large arrays.
    It is each user's responsibility to supply properly typed and sized data!

### Jitting

Recall how we handled various jitting options to redecorate 
[from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb)?
Just as elsewhere in VBT, you can pass a jitting option to the class method using the 
argument `jitted`. Let's run a random simulation both without and with automatic Numba parallelization:

```pycon
>>> big_target_shape = (1000, 1000)
>>> big_rand_price = np.random.randint(8, 12, size=big_target_shape)
>>> big_size = np.full(big_target_shape, 1)
>>> big_size[1::2] = -1
```

```pycon
>>> %%timeit
>>> vbt.Portfolio.from_orders(
...     close=big_rand_price, 
...     size=big_size
... )
113 ms ± 2.34 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

```pycon
>>> %%timeit
>>> vbt.Portfolio.from_orders(
...     close=big_rand_price, 
...     size=big_size,
...     jitted=dict(parallel=True)
... )
97 ms ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
```

!!! info
    Preset simulators like [from_orders_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_orders/#vectorbtpro.portfolio.nb.from_orders.from_orders_nb)
    can only be parallelized along groups and columns. Therefore, enabling parallel mode will not
    help when there is only one group or one column present. Also, it is often better to use
    chunking instead, because Numba parallelization may provide little or no performance benefit.
    Only custom user pipelines with heavy math can be well parallelized with Numba.

### Chunking

Just like jitting, chunking can be enabled by passing an option to the `chunked` argument. 
Below, we repeat the above benchmark using Dask:

```pycon
>>> %%timeit
>>> vbt.Portfolio.from_orders(
...     close=big_rand_price, 
...     size=big_size,
...     chunked=dict(engine="dask", n_chunks=4)
... )
67.2 ms ± 3.63 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)
```

!!! info
    Multithreading with Dask is often better suited than multiprocessing with Ray,
    because by default, all Numba-compiled functions in VBT release the [GIL](https://realpython.com/python-gil/).
    There is much less overhead when starting multiple threads compared to processes.
    Consider using multiprocessing only when the function takes a significant amount of time to run.

### Use cases

This method is ideal for backtesting when order information is known in advance,
and no order changes its parameters in response to changing market conditions.
That means we cannot use it to implement SL, TP, limit, or any other advanced order types.
It is simply a smart way to represent multiple instances of 
[Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order) in a highly vectorized and efficient way.
Imagine how difficult it would be to provide a list of named tuples instead of arrays! There
are two main use cases where [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
really shines: portfolio rebalancing with predefined weights, and "what-if" analysis.
We have already discussed rebalancing in detail. The "what-if" use case is 
about simulating and analyzing various hypothetical scenarios of real-world trading activity.

For example, suppose you made 3 trades on SOL/BTC on Binance and want to analyze them deeply.
Even though Binance now has improved trade analysis features, doing it with VBT
opens up a completely new dimension. First, you need to obtain the close price at the desired granularity 
for analysis. Then, you need to convert the trade information into orders.
Finally, you can use [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) to explore 
how your portfolio evolved over time!

```pycon
>>> trade1 = dict(
...     timestamp="2022-01-22 12:39:26",
...     price=0.0027702,
...     size=4.99,
...     fixed_fees=1.01571e-05  # (1)!
... )
>>> trade2 = dict(
...     timestamp="2022-01-29 02:12:50",
...     price=0.00243,
...     size=-1.72,
...     fixed_fees=3.0549e-06
... )
>>> trade3 = dict(
...     timestamp="2022-01-29 02:52:54",
...     price=0.0024299,
...     size=-3.27,
...     fixed_fees=5.8102e-06
... )

>>> trades = pd.DataFrame([trade1, trade2, trade3])  # (2)!
>>> trades["timestamp"] = pd.to_datetime(trades["timestamp"], utc=True)  # (3)!
>>> trades.set_index("timestamp", inplace=True)
>>> trades
                              price  size  fixed_fees
timestamp                                           
2022-01-22 12:39:26+00:00  0.00277  4.99    0.000010
2022-01-29 02:52:54+00:00  0.00243 -1.72    0.000003
2022-01-29 02:52:54+00:00  0.00243 -3.27    0.000006

>>> solbtc_data = vbt.BinanceData.pull(  # (4)!
...     "SOLBTC", 
...     start=trades.index[0] - pd.Timedelta(days=1), 
...     end=trades.index[-1] + pd.Timedelta(days=1),
...     timeframe="1h"
... )

>>> resampler = vbt.Resampler(  # (5)!
...     source_index=trades.index, 
...     target_index=solbtc_data.wrapper.index,
...     source_freq=None,
...     target_freq="1h"
... )

>>> # (6)!

>>> @njit
... def avg_price_reduce_meta_nb(from_i, to_i, col, size, price):  # (7)!
...     _size = size[from_i:to_i, col]
...     _price = price[from_i:to_i, col]
...     return np.sum(_price * _size) / np.sum(_size)

>>> price = pd.Series.vbt.resample_to_index(  # (8)!
...     resampler, 
...     avg_price_reduce_meta_nb,
...     vbt.to_2d_array(trades["size"]),
...     vbt.to_2d_array(trades["price"]),
...     wrapper=trades["price"].vbt.wrapper,
... )
>>> price.loc[~price.isnull()]  # (9)!
Open time
2022-01-22 12:00:00+00:00    0.00277
2022-01-29 02:00:00+00:00    0.00243
Freq: 158H, Name: price, dtype: float64

>>> size = trades["size"].vbt.resample_to_index(  # (10)!
...     resampler, 
...     vbt.nb.sum_reduce_nb
... )
>>> size.loc[~size.isnull()]
Open time
2022-01-22 12:00:00+00:00    4.99
2022-01-29 02:00:00+00:00   -4.99
Freq: 158H, Name: size, dtype: float64

>>> fixed_fees = trades["fixed_fees"].vbt.resample_to_index(
...     resampler, 
...     vbt.nb.sum_reduce_nb
... )
>>> fixed_fees.loc[~fixed_fees.isnull()]
Open time
2022-01-22 12:00:00+00:00    0.000010
2022-01-29 02:00:00+00:00    0.000009
Freq: 158H, Name: fixed_fees, dtype: float64

>>> pf = vbt.Portfolio.from_orders(  # (11)!
...     open=solbtc_data.get("Open"),
...     high=solbtc_data.get("High"),
...     low=solbtc_data.get("Low"),
...     close=solbtc_data.get("Close"),
...     price=price,
...     size=size,
...     fixed_fees=fixed_fees,
...     init_cash=0.1,
...     ffill_val_price=False,
...     skipna=True
... )
>>> pf.orders.readable  # (12)!
    Order Id  Column                 Timestamp  Size    Price      Fees  Side
0         0       0 2022-01-22 12:00:00+00:00  4.99  0.00277  0.000010   Buy
1         1       0 2022-01-29 02:00:00+00:00  4.99  0.00243  0.000009  Sell

>>> pf.plot().show()  # (13)!
```

1. Commission for each trade is given in absolute terms, so we call it "fixed fees"
to avoid confusion with "fees" that are usually expressed as a percentage. Also, make sure this amount 
is specified in the quote currency (BTC in this example).

2. Store the trades in a DataFrame for easier handling.

3. The timezone must match the one used by Binance, which is UTC.

4. Pull hourly SOL/BTC data starting from the day before the first trade and ending the day after the last trade 
(this time buffer is optional).

5. The datetime index of the pulled data acts as the backbone for our time-series analysis, 
so we need to align our trades with timestamps present in this index. To do this, create an instance of 
[Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler) to map
trade timestamps to the pulled bar times.

6. Since some trades (such as the second and third in our example) may fall within the same hour and 
[Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
cannot execute multiple orders at the same bar, we need to aggregate their information.

7. Implement a meta function to compute the size-weighted average price for all trades that fall within the same bar.

8. Pass the resampler and meta function to [GenericAccessor.resample_to_index](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_to_index)
to resample the trade price.

9. Display the aggregated price points for validation.

10. Size and fixed fees can be easily aggregated using the sum operation.

11. Backtest these trades by setting the initial "cash" to 0.1 BTC and skipping missing data points.

12. Filled orders now have the same information as the original trades.

13. Plot the portfolio.

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_orders_pf_plot.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_orders_pf_plot.dark.svg#only-dark){: .iimg loading=lazy }

Now you can change the `size`, `price`, and `fixed_fees` arrays as you wish and re-run the simulation to 
observe how your trading strategy's performance changes :dna:

## Summary

As we have seen, VBT offers a variety of preset simulators, each built around a Numba-compiled
core, and a class method on top of [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
that wraps the core and provides enhancements for a more user-friendly experience. Specifically,
we examined the most basic simulator, "from orders," which takes an input shape 
(timestamps x assets + parameter combinations) and, for each single element, combines different pieces of 
information like puzzle pieces to create an order instance. To visualize this: imagine taking all the arrays, 
broadcasting them on the fly to a common shape, and overlaying them to form a cube. Each element, when 
viewed from above, is a vector with order information from [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order).

Thanks to flexible indexing, there is no need to broadcast and materialize all these arrays.
VBT is smart enough to project (that is, extrapolate) smaller arrays to larger shapes,
allowing you to provide incomplete information per timestamp, per asset, or for the entire matrix,
as if you had supplied the information for each element. This minimizes memory usage,
enabling you to work with large datasets and perform hyperparameter optimization within Numba,
as long as all input arrays fit into RAM, of course :wink:

Lastly, this documentation has given you insight into how VBT builds layers of abstraction
to automate tasks. We started with simple buy and sell commands, added many features along the way,
and arrived at a Python method that makes backtesting almost unbelievably easy.
Still, this method sits at a lower level: it cannot backtest trading strategies where orders depend
on the current simulation state, meaning all order information must be known before starting the simulation.
Get ready, because this is where signals and order functions come into play!

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/portfolio/from-orders.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-signals/
parent: https://vectorbt.pro/pvt_41531c19/documentation/portfolio/
name: From signals
type: page
icon: material-broadcast
---

# From signals

The [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
(FO) method, discussed earlier, is the most basic simulation approach. It accepts order information as
multiple array-like arguments and broadcasts them to a single shape, allowing us to know exactly what
should be ordered for each asset at each bar. This method requires having all this information upfront,
regardless of what happens during the simulation. But what if we want to create an order only when we
are not currently in the market, or generally, make an order based on the current simulation state?
Such conditional logic cannot be represented using orders alone—we would need to use either a callback or
define more arrays. Both approaches are implemented in
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals) (FS).

Before diving into this method, make sure to learn more about signals
[here](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development). In short, signals are an abstraction layer over orders.
Each signal consists of four boolean values: :one: long entry, :two: long exit, :three: short entry,
and :four: short exit. The combination of these values allows us to control the direction of an
order relative to the current position. For example, a short entry flag will reverse the current
long position or open a new short one if we are not in the market. This way, position management can be
separated from order management, letting us focus on deciding whether we are bullish or bearish at any
given time—a perfect playground for machine learning models.

There is another reason to appreciate signals: statistically, within the entire universe of signal
[permutations](https://en.wikipedia.org/wiki/Permutation), there is at least one permutation that
always outperforms the market. This means we could design the perfect trading algorithm using only
the above signal schema—we just need to guess the right timing and direction for each signal. This
reduces the number of factors to just two (in an ideal scenario, since in the real world we must also
consider risk, execution constraints, and more). For example, if the price of a security is $21
on day 1, $20 on day 2, and $22 on day 3, we could enter a short position on day 1 and a long
position on day 2 to achieve positive returns. That's why trading systems and their backtesting
components do not have to be complex to be profitable—they just need a robust signal generator as
their algorithmic backbone, along with trading infrastructure that closely matches the backtesting system.

!!! example
    Here are all the signal permutations for a price series with four points and their total return.

    ```pycon
    >>> from vectorbtpro import *
    
    >>> price = np.array([21, 20, 22, 21])
    >>> returns = (price[1:] - price[:-1]) / price[:-1]
    >>> permutations = list(product([False, True], repeat=len(returns)))
    >>> total_return = np.prod(1 + np.where(permutations, returns, -returns), axis=1) - 1
    >>> pd.Series(total_return, index=permutations).sort_values(ascending=False)
    (False, True, False)     0.204762
    (False, True, True)      0.100000
    (True, True, False)      0.095238
    (True, True, True)       0.000000
    (False, False, False)   -0.014286
    (False, False, True)    -0.100000
    (True, False, False)    -0.103896
    (True, False, True)     -0.181818
    dtype: float64
    ```

    Do not run this on longer price series, since the number of permutations grows exponentially with
    the number of data points—`2^n`. For example, a year of daily history would require checking
    `2^365` or `7.515336e+109` permutations.

## Mechanics

Similar to FO, this method is also a class method of [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
and includes two Numba-compiled core functions: [from_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signals_nb)
and [from_signal_func_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signal_func_nb).
FS shares many arguments with FO, especially those that set up the simulation, such as `init_cash`, as well as
those that contain order information, like `size`. For example, if you check the API documentation for the
`size` argument under [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
you will see "See `Portfolio.from_orders`." The simulation procedure of FS is also very similar to that of
FO: as it loops over all columns and rows, at each iteration it resolves the current order and executes it
by appending information about the filled order to the order records and updating the current simulation state.
However, that is where the similarities end.

### Framework

Below is an abstract visualization of the FS framework running on three rows and two groups, with two columns
and one column, respectively:

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_signals_framework.svg){: loading=lazy style="width:800px;" }

If you have worked with VBT for a while, you have likely noticed that the framework of
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
follows that of [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
and [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).
Like most things in the VBT universe, simulation with FS is performed by iterating over a
"target shape." This shape has two dimensions: rows representing the time axis and columns representing
the asset axis (or, more generally, configurations). Columns are further divided into groups:
if multiple columns share the same cash, they are placed into the same group (as shown in the blue rectangle
on the left above), while columns without cash sharing or grouping are treated as isolated and appear as a
group with exactly one column (blue rectangle on the right above). Groups are treated as separate, atomic
backtesting instances that are not connected in any way; splitting the shape by target groups should not
affect the final result. This is also why chunking is generally performed on groups rather than columns :bulb:

The actual iteration over rows and groups takes place in [column-major order](https://en.wikipedia.org/wiki/Row-_and_column-major_order):
the simulator starts by moving over the rows in the first group, and once finished, continues with
the second group. Each time it processes a new row within a group, all the assets at that row
are considered a "segment" because they compete for the same resources at the same time
or are connected by any user-defined methods. For example, an account with `BTC-USD` and `ETH-USD` on
the date `2020-01-01` forms a segment since the value of both assets contributes to the total value
of the group at this date. Each asset within a segment is called an "element," which is the smallest
simulation unit. An element in FS can hold only one order, so the number of filled orders is
capped by the number of rows times the number of columns. For example, a year's worth of daily
`BTC-USD` and `ETH-USD` history can generate at most `365 * 2 = 730` orders, or one order per bar and asset.

#### Segment workflow

The segment is where the main part of the simulation occurs:

```mermaid
flowchart TD;
    id0["Update state at open"]
    
    id4["Get signals (0)"]
    id5["Get signals (1)"]
    id6["Get signals (2)"]
    
    id7["Generate order"]
    id8["Generate order"]
    id9["Generate order"]
    
    id10["Sort orders"]
    
    id11["Execute order (2)"]
    id12["Execute order (0)"]
    id13["Execute order (1)"]
    
    id14["Update state at close"]
    
    id15["Post-segment processing (optional)"]
    
    id0 --> id4;
    id0 --> id5;
    id0 --> id6;
    id4 --> id7;
    id5 --> id8;
    id6 --> id9;
    id7 --> id10;
    id8 --> id10;
    id9 --> id10;
    id10 --> id11;
    id10 --> id12;
    id10 --> id13;
    id11 --> id14;
    id12 --> id14;
    id13 --> id14;
    id14 --> id15;
```

FS first updates the current simulation state using the opening price. This step is needed to obtain
the group value in case an order has a size defined as a (target) percentage, allowing the size to be
converted into an absolute number of units. Then, it iterates over the columns in the
current group and determines the four signals for each column. These signals are then
converted into an order specification similar to that used by FO.

After resolving all order specifications, and if the automatic call sequence is enabled, the
simulator attempts to sort the orders by their potential value so that sell orders are executed first.
This is necessary for rebalancing. This sorting is only possible if all orders within the current group
(limit, order, and user-defined) happen either all at the beginning or all at the end of the current bar.
If some orders occur in the middle of the bar, or if some happen at the beginning and others at the end,
an error will be thrown because orders can only be sorted if they are guaranteed to occur simultaneously.
If the dynamic call sequence is disabled and there are orders in multiple bar zones, the simulator
will sort them by bar zone.

Finally, FS processes the columns in the newly established call sequence to execute the orders,
and updates the simulation state again using the closing price. This final update is necessary to
optionally generate the portfolio returns and for the segment post-processing function.

### Signal generation

FS supports two signal generation modes: fixed (cached) and dynamic (non-cached). The first
mode is implemented by the function [from_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signals_nb),
which uses four pre-defined arrays as signals and does not allow defining callbacks. Because of this,
it is fully cacheable and does not need to be recompiled for each new runtime, unless a new set of data types
is detected. The second mode is implemented by the function
[from_signal_func_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signal_func_nb),
which does not accept signal arrays but defines a signal function. This is a special callback meant to
generate the four signals for each asset and at each bar dynamically. This mode is especially suited
for use cases where the signal depends on the current simulation state. Additionally, it defines a
callback that is called after processing the current segment, which can be used to pre-compute various metrics,
such as the Sharpe ratio. The main drawback is that it cannot be cached (yet), so it must
be recompiled in each new runtime (:coffin: to those running VBT as a script).

The convenience of the [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
method, which wraps both of these modes, is its ability to automatically choose the appropriate mode:
whenever you override any default callback, it will run the second mode instead of the first.

#### Signal function

Recall that in FO, all data had to be provided as arrays, and it was not possible to dynamically change
the information or influence execution in any way. FS is much more flexible: while most information is
still expected to be defined beforehand (acting as a facade), signals can be generated both
statically and dynamically. Let's experiment with dynamic signal generation.

The second mode is implemented by accepting a user-defined callback function, `signal_func_nb`. 
Whenever the main simulation loop processes a new row (bar), it asks each asset in the current group to 
generate signals using this callback function. To do so, it packs all potentially useful information, 
such as the current cash balance and group value, into a named tuple of 
type [SignalContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext). In return, 
the function must provide four signals, which will then be used to create an order for that asset.

Here is an example of a very simple signal function that does not generate any orders:

```pycon
>>> from vectorbtpro import *

>>> @njit
... def signal_func_nb(c):
...     return False, False, False, False

>>> close = pd.DataFrame({
...     "BTC-USD": [20594.29, 20719.41, 19986.60, 21084.64],
...     "ETH-USD": [1127.51, 1125.37, 1051.32, 1143.20],
...     "DOT-USD": [7.88, 7.74, 7.41, 7.78],
...     "BNB-USD": [216.90, 219.67, 214.23, 228.92]
... })

>>> pf = vbt.Portfolio.from_signals(
...     close=close,
...     signal_func_nb=signal_func_nb
... )
>>> pf.order_records
array([], dtype={...})
```

!!! tip
    To avoid waiting for the function to compile, remove the `@njit` decorator from `signal_func_nb`
    and pass `jitted=False` to `from_signals` to fully disable Numba for this method. Do this only if the
    amount of input data is small (< 1000).

To understand when the function is called, let's narrow the data to two assets and print out 
the current column and row:

```pycon
>>> @njit
... def signal_func_nb(c):
...     print(c.col, c.i)
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     close=close[["BTC-USD", "ETH-USD"]],
...     signal_func_nb=signal_func_nb
... )
0 0
0 1
0 2
0 3
1 0
1 1
1 2
1 3
```

You can see that the function was called at each row, first for column `BTC-USD` and then for column `ETH-USD`.
In this scenario, both assets are isolated tests, so the simulator processes one column after the other.
However, once we introduce grouping with or without cash sharing, which binds columns together,
the simulator processes the columns group-wise: it iterates through groups, then rows, and finally
through columns within the current group and row. Let's demonstrate this by defining two groups,
with two assets sharing the same cash:

```pycon
>>> @njit
... def signal_func_nb(c):
...     print(c.group, c.col, c.i)
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     close=close,
...     signal_func_nb=signal_func_nb,
...     group_by=[0, 0, 1, 1],
...     cash_sharing=True
... )
0 0 0
0 1 0
0 0 1
0 1 1
0 0 2
0 1 2
0 0 3
0 1 3
1 2 0
1 3 0
1 2 1
1 3 1
1 2 2
1 3 2
1 2 3
1 3 3
```

The context tuple passed to the signal function contains all the necessary information to
identify the position of the call in the simulation. For example, `c.index[c.i]` can be used with
[SignalContext.index](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.index)
and [SignalContext.i](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.i)
to get the timestamp of the current bar. You can also change the state of any pending limit
or stop order before it is processed, since the signal function is conceptually executed
just before the beginning of the bar.

Because the groups are processed from left to right and each group's state is stored globally,
you can access the order records and, in general, the latest simulation state of all groups
that have been processed so far. For example, [SignalContext.last_cash](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.last_cash)
contains as many elements as there are groups. This is both powerful and potentially risky:
you can create complex intergroup relationships if you wish, or accidentally access the wrong group
if you are not paying close attention.

### Signal resolution

Signals add an extra layer of abstraction over orders, so logic is needed to translate signals into
order specifications. Whenever the simulator receives a new set of four signals at each row and column,
it first consolidates them into a single signal, which is then converted into an order. The resolution step
checks for conflicting signals. Usually, you would expect only one `True` signal and three `False` signals,
but sometimes multiple signals can be `True`, especially when the signal function is merging data from
different boolean arrays. In such cases, the simulator uses the following multi-step procedure to resolve
conflicts:

```mermaid
flowchart TD;
    id1["Long entry and long exit?"]
    id2["Short entry and short exit?"]
    id3["Long entry and short entry?"]
    id4["Long/short position and short/long entry?"]
    id5["Final signal"]
    
    id1 -->|"fix"| id2;
    id2 -->|"fix"| id3;
    id3 -->|"fix"| id4;
    id4 -->|"fix"| id5;
```

First, the simulator checks if there are multiple `True` signals within the same direction, such as both
long entry and long exit being set. To choose between these, the simulator uses the argument
`upon_long_conflict` of type [ConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ConflictMode).
For example, the "adjacent" option will pick the signal adjacent to your current position,
so only the long entry remains active if you are in a long position. This step uses the function
[resolve_signal_conflict_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.resolve_signal_conflict_nb):

```pycon
>>> vbt.pf_nb.resolve_signal_conflict_nb(
...     position_now=20,
...     is_entry=True,
...     is_exit=True,
...     direction=vbt.pf_enums.Direction.LongOnly,
...     conflict_mode=vbt.pf_enums.ConflictMode.Adjacent
... )
(True, False)
```

After at most one signal is selected in each direction, the simulator checks if both long entry and
short entry are active. It then calls [resolve_dir_conflict_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.resolve_dir_conflict_nb),
using the argument `upon_dir_conflict` of type [DirectionConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.DirectionConflictMode)
to determine which direction wins. For example, you can choose to always go short when there is uncertainty:

```pycon
>>> vbt.pf_nb.resolve_dir_conflict_nb(
...     position_now=20,
...     is_long_entry=True,
...     is_short_entry=True,
...     upon_dir_conflict=vbt.pf_enums.DirectionConflictMode.Short,
... )
(False, True)
```

Finally, the function [resolve_opposite_entry_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.resolve_opposite_entry_nb)
handles cases when an entry signal is opposite to the current position's direction. For example,
if you are in a long position and the short entry signal is given, the simulator uses the
argument `upon_opposite_entry` of type [OppositeEntryMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OppositeEntryMode)
to decide whether to reduce, close, or fully reverse the long position.
Here is how you could make the short entry signal behave like the long exit signal:

```pycon
>>> vbt.pf_nb.resolve_opposite_entry_nb(
...     position_now=20,
...     is_long_entry=False,
...     is_long_exit=False,
...     is_short_entry=True,
...     is_short_exit=False,
...     upon_opposite_entry=vbt.pf_enums.OppositeEntryMode.Close,
...     accumulate=False,  # (1)!
... )
(False, True, False, False, 0)
```

1. This argument can be `True`, `False`, or any of [AccumulationMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccumulationMode).

At the end of these steps, there will be only one active signal out of four :railway_track:

### Signal conversion

Now that we have identified the single signal, what comes next? It is time to convert it into an order!
This step is straightforward and is performed by [signal_to_size_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.signal_to_size_nb),
which takes the four signals (with three now deactivated) and the size requirement for this row and column,
and returns the order size, size type, and direction to use.
For example, if you are in a position of 20 shares and receive a long exit signal, the
size becomes minus 20 shares, the size type is [SizeType.Amount](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType.Amount),
and the direction is [Direction.LongOnly](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Direction.LongOnly):

```pycon
>>> vbt.pf_nb.signal_to_size_nb(
...     position_now=20,
...     val_price_now=20594.29,  # (1)!
...     value_now=411885.80,  # (2)!
...     is_long_entry=False,
...     is_long_exit=True,
...     is_short_entry=False,
...     is_short_exit=False,
...     size=0.1,  # (3)!
...     size_type=vbt.pf_enums.SizeType.ValuePercent,  # (4)!
...     accumulate=False  # (5)!
... )
(-20.0, 0, 0)
```

1. Latest known asset price.

2. Latest known group value.

3. Default value for this element.

4. Default value for this element.

5. Default value for this element.

Even though we provided the default order specification for the current element,
such as `size`, the function ignored it here because it is not needed for closing the current position.
However, if you wanted to reverse the current position (close it and then place a new order
using the default specification), those inputs would now be used:

```pycon
>>> vbt.pf_nb.signal_to_size_nb(
...     position_now=20,
...     val_price_now=20594.29,
...     value_now=411885.80,
...     is_long_entry=False,
...     is_long_exit=False,
...     is_short_entry=True,  # (1)!
...     is_short_exit=False,
...     size=0.1,
...     size_type=vbt.pf_enums.SizeType.ValuePercent,
...     accumulate=False
... )
(-22.0, 0, 2)
```

1. Reverses the long position.

Here, the size is calculated as follows: decrease the position by 20 shares to close out the long position,
and, since we are working with a percentage of the current group value, open a new short position
of `size * value_now / val_price_now = 2.0` shares. The size type is [SizeType.Amount](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType.Amount)
and the direction is [Direction.Both](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Direction.Both)
because this operation goes in both directions.

### Main order resolution

The simulator calls the signal function, resolves the signals, and converts them into
an order specification. However, this is not the only order that may compete for the current bar:
there may also be pending limit and stop orders. Since the FS simulation function can process at most
one order per bar, it must pick a winner, which should always be the order that executes first.
But how do we determine which order comes first without any intra-bar data?
We can divide each bar into three "zones": opening (the first rectangle below), somewhere in the middle
(the second rectangle), and closing (the third rectangle). For example, if a stop order is triggered at or
before the opening of the current bar and a user order is set to execute at the closing price,
the stop order should go first. Here is the full decision chain:

```mermaid
flowchart TD;
    subgraph " "
    id1["Pending limit price hit at/before open?"]
    id2["Pending stop price hit at/before open?"]
    id3["User price is open?"]
    
    id1 -->|"no"| id2;
    id2 -->|"no"| id3;
    end
    
    subgraph " "
    id4["Pending limit price hit?"]
    id5["Pending stop price hit before close?"]
    id6["User price isn't close?"]
    
    id3 -->|"no"| id4;
    id4 -->|"no"| id5;
    id5 -->|"no"| id6;
    end
    
    subgraph " "
    id7["Pending stop price hit at close?"]
    id8["User price is close?"]
    
    id6 -->|"no"| id7;
    id7 -->|"no"| id8;
    end
```

As shown, limit orders have precedence over stop orders, and stop orders have priority over user orders,
but only if they are triggered within the same zone of a bar.

### Limit management

A market order is a transaction designed to execute as quickly as possible at the current market price.
A limit order, by contrast, is an order to buy or sell an asset with a restriction on the maximum price to
be paid or the minimum price to be received (the "limit price"). The price of a limit order is compared
against a pre-defined price level. If this level is not reached, the order remains pending and will not be
filled unless the price reaches the specified limit.

#### Creation

When a stop or user-defined order is created and its order type is provided via `order_type` as
[OrderType.Limit](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderType.Limit), the simulator first
determines the limit price type at which the order should execute: open, close, or something else.
This is an important concept: the `price` argument guides VBT on where in the bar the operation should
occur. If the limit price is the open price (provided as either
[PriceType.Open](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceType.Open) or `-np.inf`), the
simulator can use the entire candle for its checks and execute the order as soon as the price is hit within
the same bar. If the limit price is not the close price, but falls somewhere in between, the simulator can use
only the close price. If the limit price is the close price (provided as either
[PriceType.Close](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceType.Close) or `np.inf`), the
simulator cannot execute the limit order immediately and must delay its first check to the next bar.

If the limit order is not executed in the same bar where it was created, it is marked as pending, and all
relevant information is stored in a record array of type
[limit_info_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.limit_info_dt), structured by asset.
This array can hold only one instance per asset, so FS allows only one limit order to be active at a time.
In a signal function, you can access this array through `c.limit_info_dt`, allowing you to change any
information before the new bar. For example, to change the price: `c.limit_info_dt["init_price"][c.col] =
new_price`.

#### Expiration

When the simulator arrives at the next bar, it first calls
[check_limit_expired_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.check_limit_expired_nb)
to check if the pending limit order has expired at the start or somewhere in the middle of the bar.
If it has expired at the start, the order is discarded. If it expires during the bar, the simulator also checks
whether the order was hit at the bar's open; if it was not, the order is discarded since there is no guarantee
that the order was hit before the deadline. For example, suppose the order can be in force for at most 36
hours, it was issued on `2020-01-01`, and now it is `2020-01-02`:

```pycon
>>> vbt.pf_nb.check_limit_expired_nb(
...     creation_idx=0,
...     i=1,
...     tif=vbt.dt.to_ns(vbt.timedelta("36h")),  # (1)!
...     index=vbt.dt.to_ns(vbt.date_range("2020-01-01", periods=3)),
...     freq=vbt.dt.to_ns(vbt.timedelta("1d"))
... )
(False, True)
```

1. Index and time deltas must be converted into nanoseconds.

We see that the function marks the order as expired, but not at the bar's start, so it can still be executed
using the open price. If the order's lifespan was 24 hours, the function would also raise the first flag and
prevent any execution:

```pycon
>>> vbt.pf_nb.check_limit_expired_nb(
...     creation_idx=0,
...     i=1,
...     tif=vbt.dt.to_ns(vbt.timedelta("24h")),
...     index=vbt.dt.to_ns(vbt.date_range("2020-01-01", periods=3)),
...     freq=vbt.dt.to_ns(vbt.timedelta("1d"))
... )
(True, True)
```

!!! info
    The lifespan is calculated by subtracting any time from the opening time of the creation bar,
    even if the order was placed at the very end of the creation bar.

#### Activation

Once it is confirmed that the order **can** be executed at the current bar (meaning it will not expire), the
simulator uses the function
[check_limit_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.check_limit_hit_nb)
to determine whether the order **should** be executed by checking if its target price has been hit.
This check compares the price against the current candle. For example, with a pending buy limit order and
a target price of `9.5`, the function will check if the low price went below this target:

```pycon
>>> vbt.pf_nb.check_limit_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     price=9.5,
...     size=2.0
... )
(9.5, False, False)
```

If the target price were `11`, the function would indicate that the price was hit at the bar's start, allowing
the order to be executed right away at the open price:

```pycon
>>> vbt.pf_nb.check_limit_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     price=11.0,
...     size=2.0
... )
(10.0, True, False)
```

If the target price were `8`, the function would indicate that the price was not hit at all:

```pycon
>>> vbt.pf_nb.check_limit_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     price=8.0,
...     size=2.0
... )
(nan, False, False)
```

#### Cancellation

If the target price is not hit, the limit order remains pending. The order can still be canceled manually
within the signal function, which is called before all the checks above, or within the post-segment function,
which is called after processing the entire segment. The pending order will also be canceled automatically
once a stop order is executed, since executing the stop order may change the simulation state and may
consume resources required to execute the limit order in the future.

Finally, the four signals returned by the signal function and resolved into a single signal can also affect the
pending order, regardless of whether the final signal is executed. For example, if there is a pending buy
limit order and the user decides to issue a long exit or short entry signal, the most intuitive action is to
cancel the pending order, since the user's intent has changed. This is the default behavior. Such
"pending conflicts" are resolved using the function
[resolve_pending_conflict_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.resolve_pending_conflict_nb),
which uses the arguments `upon_adj_limit_conflict` and `upon_opp_limit_conflict`, both of type
[PendingConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PendingConflictMode), to decide
what to do if the direction of the pending order is adjacent or opposite to the direction of the resolved
user-defined signal.

```pycon
>>> vbt.pf_nb.resolve_pending_conflict_nb(
...     is_pending_long=True,
...     is_user_long=False,
...     upon_adj_conflict=vbt.pf_enums.PendingConflictMode.KeepIgnore,
...     upon_opp_conflict=vbt.pf_enums.PendingConflictMode.CancelIgnore,
... )
(False, False)
```

In this example, the function decides to cancel the limit order and to ignore the user-defined signal.

### Stop management

Stop orders are used to increase the likelihood of reaching a set entry or exit price, limit potential
losses, or lock in profits. These orders stay inactive until a specific price is reached, at which point they
are activated as a market or limit order. When a stop order is executed, it typically closes the position.

#### Types

There are four types of stop orders:

1. Stop loss (SL).

2. Trailing stop loss (TSL).

3. Trailing take profit (TTP).

4. Take profit (TP).

A stop-loss order limits our risk in a trade to a set amount if the market moves against us. For example,
if a stop-loss sell order is placed at $45 per unit, the order remains inactive until the price reaches or falls
below $45. At that point, the order is converted into a market or limit order, and the units are sold at the
best available price. A take profit order works in the opposite way. It defines the amount of profit we are 
willing to make on a trade and closes the position once that amount is reached. Combining an SL and TP order
creates a specific risk-to-reward ratio, which can be further adjusted to match the probabilities of
reaching each breakout scenario.

Trailing orders behave differently. As the price rises, the trailing stop follows it higher. When the
price stops rising, the new stop-loss price stays at its last level, automatically protecting our downside
while locking in profits as the price sets new highs. TTP is a variation of TSL that becomes active only
after a specific threshold is reached. These two orders are often viewed and displayed as a single order.

#### Creation

Unlike limit orders, stop orders are created after an entry order has been filled and act the same as
user-defined exit signals that are triggered once a stop condition is met. An entry order is any successfully
filled order that has opened a new position or increased an existing one. When the simulator identifies
such an order, it first determines the stop entry price provided through the `stop_entry_price` argument
of type [StopEntryPrice](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopEntryPrice).
This price serves as the starting point for all stop values and thresholds.

!!! note
    By default, the stop entry price is the closing price, not the order price.
    This prevents the situation where the stop is hit on the very first bar and cannot be
    executed, as we lack intra-bar data and cannot execute two orders within the same bar.
    If the order price is used, the soonest the stop can execute is at the open of the next bar.

Based on this price, the simulator can also determine the exact timing within the bar when the stop order
should be triggered. Why does this matter? The simulator needs to know if it can use the current candle
to update the price of any TSL or TTP order. Internally, stop order data is stored in three arrays by asset:
[sl_info_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.sl_info_dt),
[tsl_info_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.tsl_info_dt), and
[tp_info_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.tp_info_dt). Each data type follows a similar
structure: the initial row and price, the current stop value (in absolute or percentage terms), the limit delta
and its format if the stop should eventually trigger a limit order. For trailing stops, the schema also
includes the updated price and the row where the update occurred.

#### Activation

Stop orders cannot be activated on the same bar in which they are issued, even if the entry price is the opening
price. This is because FS cannot handle two orders within the same bar. If you need this functionality, use a
flexible order function with [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).
Upon reaching a new bar, the price of any pending SL and TP orders is then checked against the low and high
prices of the current candle, respectively (the reverse for short positions). This uses the
[check_stop_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.check_stop_hit_nb)
function, which returns the stop price, whether it was hit on open, and whether it was hit on close.
For example, if the initial price is $10 per unit and the stop loss is set at 10%, the stop is marked as hit
if the lowest price of the candle is below `10 * (1 - 0.1) = 9` per unit:

```pycon
>>> vbt.pf_nb.check_stop_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     is_position_long=True,
...     init_price=10.0,
...     stop=0.1
... )
(9.0, False, False)
```

If the initial price was $12 per unit, the stop would trigger immediately on open:

```pycon
>>> vbt.pf_nb.check_stop_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     is_position_long=True,
...     init_price=12.0,
...     stop=0.1
... )
(10.0, True, False)
```

If the stop was not hit (here the initial price was $9), the returned stop price is NaN:

```pycon
>>> vbt.pf_nb.check_stop_hit_nb(
...     open=10.0,
...     high=11.0,
...     low=9.0,
...     close=10.5,
...     is_position_long=True,
...     init_price=9.0,
...     stop=0.1
... )
(nan, False, False)
```

For a TP order, set the argument `hit_below` to `False`.

Unlike fixed stop orders, TSL and TTP orders must also track the peak price on which the stop price is
based. Since we do not know if the highest price of a candle comes before the lowest price or the other way
around, the candle must be split into distinct zones, updating the peak price in the zone that comes before
the stop check. First, the simulator uses the opening price to update the peak price. Then, it checks if
the stop was hit during the entire bar. If not, it proceeds to update the peak price with the highest (for
long positions) or lowest (for short positions) price of the candle, and then checks the stop again using
only the closing price to avoid this ambiguity. This way, the simulator always assumes the pessimistic
scenario that the worst event (the stop being hit) happens before the best event (updating the peak price).

TTP orders add another layer of complexity, as they must also check if their activation threshold has been
reached. This check is performed using the
[check_tsl_th_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.check_tsl_th_hit_nb)
function, which takes the initial and the peak price and tests whether the difference between them is
greater than or equal to the threshold. If it is, the order is converted into a regular TSL order. If not, the
simulator updates the peak price using the current candle and tries again to check for threshold crossover.
If the difference now meets the threshold, it uses
[check_stop_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.check_stop_hit_nb),
but can no longer use the current candle. Since it is unclear if the stop was hit before or after the threshold
was crossed, only the closing price is used, and the argument `can_use_ohlc` is disabled. The following
diagram shows this process:

```mermaid
flowchart TD;
    subgraph " "
    id0["Update peak price using open"]
    id1["Has a threshold?"]
    id2["Threshold crossed at/before open?"]
    id3["Stop hit at/after open?"]
    
    id0 --> id1;
    id1 -->|"yes - TTP"| id2;
    id1 -->|"no - TSL"| id3;
    id2 -->|"yes"| id3;
    end
    
    subgraph " "
    id4["Update peak price using high/low"]
    id5["Has a threshold?"]
    id6["Threshold crossed before close?"]
    id7["Stop hit before close?"]
    
    id2 -->|"no"| id4;
    id3 -->|"no"| id4;
    id4 --> id5;
    id5 -->|"yes - TTP"| id6;
    id5 -->|"no - TSL"| id7;
    id6 -->|"yes"| id7;
    end
    
    id3 -->|"yes"| id9;
    id6 -->|"no"| id8;
    id7 -->|"yes"| id9;
    id7 -->|"no"| id8;
    
    id8["Keep pending"]
    id9["Execute"]
```

If multiple stops are hit, the simulator takes a pessimistic approach: SL is checked first, TSL and TTP are
checked second, and TP is checked last. The first pending stop is executed and all other pending stops
are canceled.

#### Resolution

After that, the winning stop is converted into four signals by the function
[generate_stop_signal_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.generate_stop_signal_nb),
which uses the current position and the default stop exit behavior defined by the
`stop_exit_type` argument of type [StopExitType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopExitType).
For example, instead of closing the position, you can have the function reverse it by using
`StopExitType.Reverse`:

```pycon
>>> vbt.pf_nb.generate_stop_signal_nb(
...     position_now=20,
...     stop_exit_type=vbt.pf_enums.StopExitType.Reverse
... )
(False, False, True, False, 0)
```

As shown, the short entry signal is `True` while the other signals are `False`. The number following the
signals represents the selected accumulation mode of type
[AccumulationMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccumulationMode), which is used
in situations where you want to reduce the position instead of closing or reversing it. Next, the stop exit
price is resolved using the function [resolve_stop_exit_price_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.resolve_stop_exit_price_nb).
The logic is simple: if the `stop_exit_price` argument of type [StopExitPrice](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopExitPrice)
is `StopExitPrice.Close`, then the closing price is used; otherwise, the stop price that was hit is used.
You can also provide an actual price as the argument.

```pycon
>>> vbt.pf_nb.resolve_stop_exit_price_nb(
...     stop_price=9.0,
...     close=10.5,
...     stop_exit_price=vbt.pf_enums.StopExitPrice.Stop
... )
9.0

>>> vbt.pf_nb.resolve_stop_exit_price_nb(
...     stop_price=9.0,
...     close=10.5,
...     stop_exit_price=9.5
... )
9.5
```

Finally, the order signal is converted into a market or limit order specification just like a
user-defined signal. See [Signal conversion](#signal-conversion).

#### Updating

In addition to updating any stop within a callback, you can also update the stop automatically when the
current position increases. In this case, the `upon_stop_update` argument of type
[StopUpdateMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopUpdateMode) controls whether the
current stop should stay the same or be reset. This decision is made by the function
[should_update_stop_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.should_update_stop_nb).
For example, if your position has increased and you want to know whether the current stop should be
updated with `StopUpdateMode.Override`, you would do the following:

```pycon
>>> vbt.pf_nb.should_update_stop_nb(
...     new_stop=0.1,
...     upon_stop_update=vbt.pf_enums.StopUpdateMode.Override
... )
True
```

If the new stop value is NaN (i.e., no stop), you should not update:

```pycon
>>> vbt.pf_nb.should_update_stop_nb(
...     new_stop=np.nan,
...     upon_stop_update=vbt.pf_enums.StopUpdateMode.Override
... )
False
```

Unless you choose the option `StopUpdateMode.OverrideNaN`, which effectively disables all stops:

```pycon
>>> vbt.pf_nb.should_update_stop_nb(
...     new_stop=np.nan,
...     upon_stop_update=vbt.pf_enums.StopUpdateMode.OverrideNaN
... )
True
```

This does not apply when the current position decreases. But why is it important to consider updates
if signals normally just open or close positions, rather than increase or decrease them?
With accumulation, signals can add to or remove from the position. In such cases, you should consider:
should this change to the position invalidate previously defined stops, or should new stops be created?
The `upon_stop_update` argument controls this behavior.

#### Cancellation

Like updating, cancellation of currently pending stop orders occurs when the position is closed.
This clears all stops automatically. But similar to limit orders, there may be a conflict with an active
user-defined signal. This is resolved by the function
[resolve_pending_conflict_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.resolve_pending_conflict_nb).
The arguments used to resolve pending conflicts for stop orders are `upon_adj_stop_conflict` and
`upon_opp_stop_conflict`, both of type [PendingConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PendingConflictMode).
For example, if a user decides to reduce the position and clear all pending stops at the same time, they can set
the `upon_adj_stop_conflict` argument (reducing or attempting to close the position are considered adjacent
signals) to the option `PendingConflictMode.CancelExecute`.

## Signals

We have covered some theory on how this simulation method works. Now, let's take a break from reading
and focus on signal arrays, which—together with a signal function—are the main input to this method.
As you already know, signals come in two types:

1. Direction-unaware signals: entries and exits enhanced by direction.

2. Direction-aware signals: long entries, long exits, short entries, short exits.

The first type is a compressed form of the second. You can always convert direction-unaware signals
into direction-aware signals, but not the other way around, since the first format covers a total of
`2 * 2 * 3 = 12` combinations, while the second format covers
`2 * 2 * 2 * 2 = 16` combinations. On the other hand, the first format is easier to use
because you can set the direction globally and work with two arrays instead of four.

First, let's fetch the entire history of `BTCUSDT` and `ETHUSDT` for our examples below:

```pycon
>>> data = vbt.BinanceData.pull(["BTCUSDT", "ETHUSDT"])
```

Since we will not need the entire history to illustrate most concepts, let's select the week of data
between February 18 and February 24, 2021, when there was a substantial price change in both directions:

```pycon
>>> sub_data = data.loc["2021-02-18":"2021-02-24"]

>>> sub_data.plot(symbol="BTCUSDT").show()
```

![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_signals_sub_data.light.svg#only-light){: .iimg loading=lazy }
![](https://vectorbt.pro/pvt_41531c19/assets/images/documentation/pf/from_signals_sub_data.dark.svg#only-dark){: .iimg loading=lazy }

Let's try passing the data without any signals:

```pycon
>>> pf = vbt.Portfolio.from_signals(sub_data)  # (1)!
>>> pf.orders.count()
symbol
BTCUSDT    0
ETHUSDT    0
Name: count, dtype: int64
```

1. Even though the first argument expects the closing price (`close`), you can pass the entire data
instance, from which the OHLC features will be extracted automatically.

By default, all signals are set to `False`, so no orders were generated.

Now, let's say our ML model correctly predicted the peak on February 21 and signaled us to enter a
position on February 18 and close it on February 21. To do this, we need to build our entry and exit
arrays with the same shape as our data. Instead of specifying the same signals for each asset
repeatedly, you can provide a Series instead of a DataFrame. The Series will be applied to each asset
thanks to [broadcasting](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/#broadcasting):

```pycon
>>> entries = pd.Series([X, O, O, O, O, O, O])
>>> exits   = pd.Series([O, O, O, X, O, O, O])
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,  # (1)!
...     exits=exits
... )
>>> pf.orders.readable
    Order Id   Column              Signal Index            Creation Index  \
0         0  BTCUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
1         1  BTCUSDT 2021-02-21 00:00:00+00:00 2021-02-21 00:00:00+00:00   
2         0  ETHUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
3         1  ETHUSDT 2021-02-21 00:00:00+00:00 2021-02-21 00:00:00+00:00   

                  Fill Index      Size     Price  Fees  Side    Type Stop Type  
0 2021-02-18 00:00:00+00:00  0.001940  51552.60   0.0   Buy  Market      None  
1 2021-02-21 00:00:00+00:00  0.001940  57408.57   0.0  Sell  Market      None  
2 2021-02-18 00:00:00+00:00  0.051557   1939.61   0.0   Buy  Market      None  
3 2021-02-21 00:00:00+00:00  0.051557   1933.53   0.0  Sell  Market      None  
```

1. You could also specify this as a two-dimensional NumPy array with one column.

You can see that the first order in the `BTCUSDT` column is a buy market order that opened a new long
position. The second order is the same size but on the opposite side, so it was used to close the long
position. Reading orders is not always straightforward, especially when you want to determine when
positions are opened or closed. To get a better overview, let's calculate and print the position for each
symbol at each bar:

```pycon
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194  0.051557
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

The returned array represents the position at the end of each bar, so we are still in the market on
February 20 but out of the market on February 21.

We provided the same array for each symbol, but what if our ML model indicated that the peak for
`ETHUSDT` was one day ahead of `BTCUSDT`? As soon as your signal specification varies by columns, you
need to build the signal array as a DataFrame with values defined per element. Let's keep the entry
array the same for both symbols (since the entry signals do not vary by columns in this case) and
expand only the exit array:

```pycon
>>> entries = pd.Series([X, O, O, O, O, O, O])
>>> exits = pd.DataFrame({
...     0: [O, O, O, X, O, O, O],
...     1: [O, O, X, O, O, O, O],
... })  # (1)!
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

1. You could also use symbol names as column names. Defining rows and columns as a simple range of
values (from `0` to `n`) will ignore the labels and broadcast only by shapes.

We can now see that the long position in the `ETHUSDT` column was closed one day before the position
in `BTCUSDT`, just as our hypothetical model intended. To simplify array creation and avoid setting
each element manually, you can use the symbol wrapper of the data instance to create empty boolean
arrays matching the shape of your data, and fill them on specific dates:

```pycon
>>> exits = sub_data.symbol_wrapper.fill(False)  # (1)!
>>> exits.loc["2021-02-21", "BTCUSDT"] = True
>>> exits.loc["2021-02-20", "ETHUSDT"] = True
>>> exits
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00    False    False
2021-02-20 00:00:00+00:00    False     True
2021-02-21 00:00:00+00:00     True    False
2021-02-22 00:00:00+00:00    False    False
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False
```

1. Create an array with the same number of columns as your symbols, and fill it with `False`.

For those who enjoy using advanced features, here is how to let VBT's broadcaster create both
arrays and fill them dynamically with [index dictionaries](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.fill_and_set):

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=vbt.index_dict({0: True}),
...     exits=vbt.index_dict({
...         vbt.idx("2021-02-21", "BTCUSDT"): True,
...         vbt.idx("2021-02-20", "ETHUSDT"): True
...     })
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

The best part of the approach above is that the broadcaster will not create arrays larger than
necessary. It will detect that the entry specification is the same for both symbols and create
an array with one column instead of two, saving memory.

### Direction-unaware

In all the examples above, we provided only two arrays: `entries` and `exits`. When you do this,
the method treats the provided signals as direction-unaware, meaning an additional argument,
`direction`, is used to control the signal direction. By default, the direction is `Direction.LongOnly`
(see `signal_direction` in [portfolio settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.portfolio)).
To change the direction, you can override `direction` with any option available in
[Direction](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Direction). You can provide
an option either as an integer or a field name. For example, let's allow *both* directions so the
system reverses the position on exit instead of just closing it:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits,
...     direction="both"  # (1)!
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00194  0.051557
2021-02-20 00:00:00+00:00  0.00194 -0.051557
2021-02-21 00:00:00+00:00 -0.00194 -0.051557
2021-02-22 00:00:00+00:00 -0.00194 -0.051557
2021-02-23 00:00:00+00:00 -0.00194 -0.051557
2021-02-24 00:00:00+00:00 -0.00194 -0.051557
```

1. You can also provide this in numeric format as `Direction.Both`.

Zero values have now turned negative, which means positions are being reversed.
Just like with the signal arguments, the direction argument can also be provided as an array.
This allows you to define different directions for different dates and symbols. For example,
let's long `BTCUSDT` and short `ETHUSDT`. Thanks to broadcasting, you can provide per-column
information as a two-dimensional array with just one row:

```pycon
>>> direction = pd.DataFrame([["longonly", "shortonly"]])
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits,
...     direction=direction
... )
>>> pf.assets
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194 -0.051557
2021-02-19 00:00:00+00:00  0.00194 -0.051557
2021-02-20 00:00:00+00:00  0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

Position values under the `ETHUSDT` column are negative, indicating a short position.
The following example shows how to use per-element directions by entering a long position
at the first bar, exiting at the peak, then entering a short position at the next bar,
and finally exiting at the last bar:

```pycon
>>> L = vbt.pf_enums.Direction.LongOnly  # (1)!
>>> S = vbt.pf_enums.Direction.ShortOnly

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),  # (2)!
...     entries=  pd.Series([X, O, O, O, X, O, O]),
...     exits=    pd.Series([O, O, O, X, O, O, X]),
...     direction=pd.Series([L, L, L, L, S, S, S])
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64
```

1. Shortcuts for better readability, numeric format.

2. Select one symbol of data.

!!! note
    For larger arrays, prefer the numeric format over the string format, since strings
    must be converted to integers before simulation, which is a slower operation.

### Direction-aware

Direction-aware signals are a more flexible type of signal, allowing for a greater variety of signal
combinations. To use this mode, provide the arguments `short_entries` and `short_exits`
as short signals, along with the `entries` and `exits` arguments for long signals. This disables
the `direction` argument entirely, since the signals themselves now control the direction.
Let's adapt the example above:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64
```

So, when should you use each signal mode? Use direction-unaware signals when you have a single
direction throughout an entire column, and direction-aware signals when you want more granular control,
especially when positions must be closed under both directions. For example,
to close out any position at the end of a day.

### Signal function

Providing signals as pre-defined arrays offers one main advantage: caching. Even after you restart
the runtime, there will not be any recompilation if you pass signal arrays of the same format again.
Sometimes, however, you may want to trade some performance for added flexibility. This includes
path-dependent cases, where signals depend on previous or current simulation state,
making them impossible to generate in advance. Another use case is reducing RAM usage: 
by putting all indicator and signal generation logic into a single signal function,
you avoid the need for any intermediate arrays. This is useful when you need to test a large
number of parameters, or when you want to select which assets to trade from a large universe.
These situations often require very wide arrays in RAM, but a signal function makes such arrays
unnecessary.

Let's implement the last example above, but without using any arrays!

```pycon
>>> @njit
... def signal_func_nb(c):
...     ts = c.index[c.i]  # (1)!
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 18):  # (2)!
...         return True, False, False, False  # (3)!
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 21):
...         return False, True, False, False
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 22):
...         return False, False, True, False
...     if vbt.dt_nb.matches_date_nb(ts, 2021, 2, 24):
...         return False, False, False, True
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     signal_func_nb=signal_func_nb
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64
```

1. `c.index` returns an array with timestamps in nanosecond format, and `c.i`
returns the current row. By applying the latter to the former, we get the current timestamp.

2. Use [matches_date_nb](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_nb/#vectorbtpro.utils.datetime_nb.matches_date_nb)
to check if the current day matches a specific date.

3. Signal functions must return a set of direction-aware signals.

We have replaced vectorized logic with iterative logic, which is usually more verbose
but offers greater flexibility and resembles the format used in most open-source backtesting frameworks.
However, this does not mean you must define everything iteratively. You can still
pass one or more arrays and make decisions based on them. To do this, have the signal
function accept arrays as positional arguments and select one element from each at each time step
to generate the four signals. Then, pass the actual arrays to FS as a tuple using `signal_args`.
Keep in mind that any array-like object must be a NumPy array, since Numba does not support Pandas.

```pycon
>>> @njit
... def signal_func_nb(c, entries, exits, short_entries, short_exits):
...     long_entry = entries[c.i]
...     long_exit = exits[c.i]
...     short_entry = short_entries[c.i]
...     short_exit = short_exits[c.i]
...     return long_entry, long_exit, short_entry, short_exit

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         pd.Series([X, O, O, O, O, O, O]).values,
...         pd.Series([O, O, O, X, O, O, O]).values,
...         pd.Series([O, O, O, O, X, O, O]).values,
...         pd.Series([O, O, O, O, O, O, X]).values
...     )
... )
>>> pf.assets
Open time
2021-02-18 00:00:00+00:00    0.001940
2021-02-19 00:00:00+00:00    0.001940
2021-02-20 00:00:00+00:00    0.001940
2021-02-21 00:00:00+00:00    0.000000
2021-02-22 00:00:00+00:00   -0.002059
2021-02-23 00:00:00+00:00   -0.002059
2021-02-24 00:00:00+00:00    0.000000
Freq: D, dtype: float64
```

But what if we want to expand our data to multiple assets? The example above works only
if each array remains one-dimensional, since only rows are selected in the signal function.
To create shape-agnostic logic, you should use two-dimensional arrays for each input
and select the current column in the signal function as well. However, there is another issue:
you must handle broadcasting, which can be handled flexibly in the signal function,
either manually with [flex_select_nb](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/#vectorbtpro.base.flex_indexing.flex_select_nb)
or automatically with [select_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/iter_/#vectorbtpro.portfolio.nb.iter_.select_nb):

```pycon
>>> @njit
... def signal_func_nb(c, entries, exits, short_entries, short_exits):
...     long_entry = vbt.pf_nb.select_nb(c, entries)
...     long_exit = vbt.pf_nb.select_nb(c, exits)
...     short_entry = vbt.pf_nb.select_nb(c, short_entries)
...     short_exit = vbt.pf_nb.select_nb(c, short_exits)
...     return long_entry, long_exit, short_entry, short_exit

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.to_2d_array(pd.Series([X, O, O, O, O, O, O])),
...         vbt.to_2d_array(pd.Series([O, O, O, X, O, O, O])),
...         vbt.to_2d_array(pd.Series([O, O, O, O, X, O, O])),
...         vbt.to_2d_array(pd.Series([O, O, O, O, O, O, X]))
...     )
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.001940  0.051557
2021-02-20 00:00:00+00:00  0.001940  0.051557
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.002059 -0.056080
2021-02-23 00:00:00+00:00 -0.002059 -0.056080
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

Now our strategy can be applied to any number of columns. Great! But even this is not
the most flexible design :face_exhaling: What if the user provides a signal array that does not
have the same number of rows as the data? If bound checking is enabled, you would get an "index is
out of bounds" error because the signal function would try to select an element that does not exist.
To ensure an array broadcasts against the data automatically before simulation, define it in the
`broadcast_named_args` dictionary, and then use a template to substitute its name with the
broadcasted array in `signal_args`:

```pycon
>>> entries = pd.Series({vbt.utc_timestamp("2021-02-18"): True})
>>> exits = pd.Series({vbt.utc_timestamp("2021-02-21"): True})
>>> short_entries = pd.Series({vbt.utc_timestamp("2021-02-22"): True})
>>> short_exits = pd.Series({vbt.utc_timestamp("2021-02-24"): True})

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("entries"),  # (1)!
...         vbt.Rep("exits"),
...         vbt.Rep("short_entries"),
...         vbt.Rep("short_exits")
...     ),
...     broadcast_named_args=dict(
...         entries=entries,
...         exits=exits,
...         short_entries=short_entries,
...         short_exits=short_exits
...     )
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.001940  0.051557
2021-02-20 00:00:00+00:00  0.001940  0.051557
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.002059 -0.056080
2021-02-23 00:00:00+00:00 -0.002059 -0.056080
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

1. `entries` is recognized as a key in `broadcast_named_args`.

Our setup now works just like the built-in arguments `entries`, `exits`, `short_entries`,
and `short_exits` :magic_wand: You no longer need to convert them to NumPy arrays,
as the broadcaster takes care of this automatically. This also allows you to use index
dictionaries and other advanced broadcasting features:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("entries"),
...         vbt.Rep("exits"),
...         vbt.Rep("short_entries"),
...         vbt.Rep("short_exits")
...     ),
...     broadcast_named_args=dict(
...         entries=      vbt.index_dict({"2021-02-18": True, "_def": False}),
...         exits=        vbt.index_dict({"2021-02-21": True, "_def": False}),
...         short_entries=vbt.index_dict({"2021-02-22": True, "_def": False}),
...         short_exits=  vbt.index_dict({"2021-02-24": True, "_def": False})
...     )
... )
```

However, backtesting signals on fixed dates is rarely our main interest. Let's create a signal
function that produces a long entry signal when there is an above-crossover and a short entry signal
when there is a below-crossover of two moving average arrays. We will also parameterize this strategy
by introducing a flexible parameter, `wait`, that controls the number of bars to wait after a crossover
has been detected before placing a signal. If an opposite crossover occurs during this time,
the signal is canceled, so `wait` works as a confirmation period. This parameter will broadcast
with the data, letting you define it per row, column, or element.

```pycon
>>> @njit
... def signal_func_nb(c, fast_sma, slow_sma, wait):
...     curr_wait = vbt.pf_nb.select_nb(c, wait)  # (1)!
...     i_wait = c.i - curr_wait  # (2)!
...     if i_wait < 0:  # (3)!
...         return False, False, False, False
...
...     if vbt.nb.iter_crossed_above_nb(fast_sma, slow_sma, i_wait, c.col):  # (4)!
...         cross_confirmed = True
...         for j in range(i_wait + 1, c.i + 1):  # (5)!
...             if not vbt.nb.iter_above_nb(fast_sma, slow_sma, j, c.col):
...                 cross_confirmed = False
...                 break
...         if cross_confirmed:
...             return True, False, False, False
...
...     if vbt.nb.iter_crossed_below_nb(fast_sma, slow_sma, i_wait, c.col):  # (6)!
...         cross_confirmed = True
...         for j in range(i_wait + 1, c.i + 1):
...             if not vbt.nb.iter_below_nb(fast_sma, slow_sma, j, c.col):
...                 cross_confirmed = False
...                 break
...         if cross_confirmed:
...             return False, False, True, False
...
...     return False, False, False, False

>>> fast_sma = data.run("sma", 20, short_name="fast_sma").real
>>> slow_sma = data.run("sma", 50, short_name="slow_sma").real
>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("fast_sma"),
...         vbt.Rep("slow_sma"),
...         vbt.Rep("wait")
...     ),
...     broadcast_named_args=dict(
...         fast_sma=fast_sma,
...         slow_sma=slow_sma,
...         wait=0  # (7)!
...     )
... )
>>> pf.orders.count()
fast_sma_timeperiod  slow_sma_timeperiod  symbol 
20                   50                   BTCUSDT    40
                                          ETHUSDT    32
Name: count, dtype: int64
```

1. Since the confirmation period is an array-like parameter, get the value defined for the current row and column.

2. Calculate the row where the crossover should have occurred.

3. If the current period is less than the confirmation period, return no signal.

4. Check whether the fast SMA has crossed above the slow SMA at the current bar.

5. If true, loop over the confirmation period (including the current bar) and verify that the fast
SMA has always stayed above the slow SMA for confirmation.

6. Apply the same logic for the below-crossover.

7. The waiting period is a flexible parameter that broadcasts with other arrays.

To confirm that our strategy has produced the correct number of orders, let's
manually count the total number of crossover signals:

```pycon
>>> n_crossed_above = fast_sma.vbt.crossed_above(slow_sma).sum()
>>> n_crossed_below = fast_sma.vbt.crossed_below(slow_sma).sum()
>>> n_crossed_above + n_crossed_below
fast_sma_timeperiod  slow_sma_timeperiod  symbol 
20                   50                   BTCUSDT    40
                                          ETHUSDT    32
Name: count, dtype: int64
```

To demonstrate the full power of VBT's broadcaster, let's test the confirmation period
with 0, 1, 7, and 30 bars by wrapping the parameter using [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param):

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("fast_sma"),
...         vbt.Rep("slow_sma"),
...         vbt.Rep("wait")
...     ),
...     broadcast_named_args=dict(
...         fast_sma=fast_sma,
...         slow_sma=slow_sma,
...         wait=vbt.Param([0, 1, 7, 30])
...     )
... )
>>> pf.orders.count()
wait  fast_sma_timeperiod  slow_sma_timeperiod  symbol 
0     20                   50                   BTCUSDT    40
                                                ETHUSDT    32
1     20                   50                   BTCUSDT    38
                                                ETHUSDT    32
7     20                   50                   BTCUSDT    36
                                                ETHUSDT    30
30    20                   50                   BTCUSDT    14
                                                ETHUSDT    16
Name: count, dtype: int64
```

We can see that the number of orders gradually decreases as the confirmation period increases.
If you appreciate VBT for its performance, you might notice that although the number of crossovers
is low, having a second loop is not ideal for performance. We can rewrite the logic to
iterate over the data only once. To do this, we need to introduce a temporary array that stores
the index of the latest crossover confirmed so far, and once the confirmation period is complete,
we can issue a signal. This is a perfect example of how to temporarily store and then share data
between multiple calls to the signal function!

The temporary array we create will be a one-dimensional NumPy array where
the latest crossover index is stored for each column. While a regular typed list could work,
remember that NumPy arrays have advantages when used with Numba. Why store data per column and
not just one value? Using a single value works for ungrouped portfolios, where columns are processed
one after another. However, if the portfolio is grouped, columns are processed in a zigzag manner
within their groups, so you should always structure your temporary data per column to be safe.
Another challenge is creating such an array: how do we know the number of columns in advance?
Fortunately, we can use a template!

```pycon
>>> @njit
... def signal_func_nb(c, fast_sma, slow_sma, wait, temp_coi):  # (1)!
...     if temp_coi[c.col] != -1:  # (2)!
...         crossed_above = vbt.nb.iter_crossed_above_nb(
...             fast_sma, slow_sma, temp_coi[c.col], c.col
...         )
...         crossed_below = vbt.nb.iter_crossed_below_nb(
...             fast_sma, slow_sma, temp_coi[c.col], c.col
...         )
...         if crossed_above:  # (3)!
...             if not vbt.pf_nb.iter_above_nb(c, fast_sma, slow_sma):  # (4)!
...                 temp_coi[c.col] = -1
...         if crossed_below:
...             if not vbt.pf_nb.iter_below_nb(c, fast_sma, slow_sma):
...                 temp_coi[c.col] = -1
...
...     curr_wait = vbt.pf_nb.select_nb(c, wait)
...     if temp_coi[c.col] != -1:  # (5)!
...         if c.i - temp_coi[c.col] == curr_wait:
...             if crossed_above:
...                 temp_coi[c.col] = -1
...                 return True, False, False, False
...             if crossed_below:
...                 temp_coi[c.col] = -1
...                 return False, False, True, False
...     else:  # (6)!
...         if vbt.pf_nb.iter_crossed_above_nb(c, fast_sma, slow_sma):
...             if curr_wait == 0:
...                 return True, False, False, False
...             temp_coi[c.col] = c.i
...         if vbt.pf_nb.iter_crossed_below_nb(c, fast_sma, slow_sma):
...             if curr_wait == 0:
...                 return False, False, True, False
...             temp_coi[c.col] = c.i
...
...     return False, False, False, False

>>> pf = vbt.Portfolio.from_signals(
...     data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("fast_sma"),
...         vbt.Rep("slow_sma"),
...         vbt.Rep("wait"),
...         vbt.RepEval("np.full(wrapper.shape_2d[1], -1)")  # (7)!
...     ),
...     broadcast_named_args=dict(
...         fast_sma=fast_sma,
...         slow_sma=slow_sma,
...         wait=vbt.Param([0, 1, 7, 30])
...     )
... )
>>> pf.orders.count()
wait  fast_sma_timeperiod  slow_sma_timeperiod  symbol 
0     20                   50                   BTCUSDT    40
                                                ETHUSDT    32
1     20                   50                   BTCUSDT    38
                                                ETHUSDT    32
7     20                   50                   BTCUSDT    36
                                                ETHUSDT    30
30    20                   50                   BTCUSDT    14
                                                ETHUSDT    16
Name: count, dtype: int64
```

1. The temporary array is taken as a regular argument.

2. Check whether there is a crossover index stored for this column.

3. If true, check the type of the crossover and whether it can still be confirmed.
If it cannot be confirmed at this bar, remove the index.

4. Use generic iterative functions (those starting with `vbt.nb.iter_`; see [generic.iter_](https://vectorbt.pro/pvt_41531c19/api/generic/nb/iter_))
when you need to operate on a custom row or column, and portfolio iterative functions
(those starting with `vbt.pf_nb.iter_`; see [portfolio.iter_](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/iter_))
to operate on the current row or column from the context.

5. If there is still an index in the temporary array, the crossover has been confirmed so far,
so check if the confirmation period is over, and if so, return the signal.

6. If there is no crossover index, check for a crossover at this bar.
If true, store it in the temporary array and continue. If the confirmation period
is zero, return the signal immediately.

7. Create the temporary array `temp_coi` using an evaluation template that runs a code expression
after all arrays have been broadcast and the final shape is established.
Use `wrapper` to access the shape information.

This code is not even complicated: you would need about the same number of lines to implement this logic
in traditional backtesting software. The main difference is that VBT relies on functional
programming, while other frameworks are object-oriented, where functions
like `crossed_above_nb` are methods of the backtesting instance (`self.crossed_above()`)
and variables like `temp_coi` are instance attributes (`self.temp_coi`).

### Conflicts

When signals are generated automatically, it is common for multiple signals of the same type
to occur one after another, or for multiple signals of different types to appear at the same bar.
The first case is handled by the FS method, which considers only the first signal
and ignores the rest (unless [accumulation](#accumulation) is enabled), as shown in this
example where we issue a long entry signal multiple times:

```pycon
>>> pf = vbt.Portfolio.from_signals(sub_data, entries=True)  # (1)!
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00  0.00000  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

1. When signals are supplied as a single value, that value is broadcast across the entire data shape,
so it appears in every row and column.

But what happens if we start issuing a long exit signal at the same time?

```pycon
>>> pf = vbt.Portfolio.from_signals(sub_data, entries=True, exits=True)
>>> pf.asset_flow
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      0.0      0.0
2021-02-19 00:00:00+00:00      0.0      0.0
2021-02-20 00:00:00+00:00      0.0      0.0
2021-02-21 00:00:00+00:00      0.0      0.0
2021-02-22 00:00:00+00:00      0.0      0.0
2021-02-23 00:00:00+00:00      0.0      0.0
2021-02-24 00:00:00+00:00      0.0      0.0
```

We can see that the simulator simply ignored conflicting signals. However, there are times when you may
want to give preference to one signal type over another. In the previous example, we encountered a
"long signal conflict," which you can resolve using the `upon_long_conflict` argument of type
[ConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ConflictMode).
For example, if long entries are more important to you than long exits:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data, 
...     entries=True, 
...     exits=True,
...     upon_long_conflict="entry"
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00  0.00000  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

What if we want to allow both directions, so that an exit signal can become a short entry signal?
In this case, we have a "signal direction conflict," which is controlled by the argument
`upon_dir_conflict` of type [DirectionConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.DirectionConflictMode).
Let's choose to prefer short signals over long signals in any direction:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data, 
...     entries=True, 
...     exits=True,
...     direction="both",
...     upon_dir_conflict="short"
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00 -0.00194 -0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00  0.00000  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

We can see that both orders became sell orders. Now, let's combine both cases
and apply this knowledge to a scenario where all four signals are provided! We will open a long position
at the first bar, and on each subsequent bar, only the signal opposite to the current
position will win and reverse the position:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data, 
...     entries=True, 
...     exits=vbt.index_dict({0: False, "_def": True}),
...     short_entries=vbt.index_dict({0: False, "_def": True}),
...     short_exits=vbt.index_dict({0: False, "_def": True}),
...     upon_long_conflict="entry",
...     upon_short_conflict="entry",
...     upon_dir_conflict="opposite",
...     upon_opposite_entry="reverse"
... )
>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00 -0.003880 -0.103114
2021-02-20 00:00:00+00:00  0.003884  0.105377
2021-02-21 00:00:00+00:00 -0.003889 -0.107641
2021-02-22 00:00:00+00:00  0.004127  0.117085
2021-02-23 00:00:00+00:00 -0.004366 -0.126528
2021-02-24 00:00:00+00:00  0.004297  0.122982
```

Great, we have forced VBT to reverse the position at each bar :cold_face:

## Orders

As we have learned, signals are simply another abstraction layer over orders; they control the timing
and direction of orders. But how do you specify the parameters of a typical order that a signal should be
converted into? Just like [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders),
the class method [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
accepts a variety of order-related arguments. In fact, it takes all the arguments that can be
found as fields in the class [Order](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.Order).
If any argument is set to `None`, the method will use the default value defined in the
[portfolio settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.portfolio). For example, the default
size is `np.inf`, which means that each signal instructs the simulator to use the entire available capital.
Also, each order-related argument is array-like and broadcasts together with the signals. This allows you to
set an argument to a single value and have it apply to each signal. Let's make each
entry signal order $1 worth of each asset by adjusting the size and size type arguments:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size=1,
...     size_type="value"
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000019  0.000516
2021-02-19 00:00:00+00:00  0.000019  0.000516
2021-02-20 00:00:00+00:00  0.000019  0.000516
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

As we can see, the simulator ordered `1 / 51552.60 = 0.000019` units of `BTCUSDT` and then closed the
position. For more granular control over any order parameter, you can specify this information
as an array. For example, let's enter a position with 50% of the available cash, close the position,
then open a new position in the opposite direction with 25% of the available cash, and close that position
again:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X,   O, O, O, O,    O, O]),
...     exits=        pd.Series([O,   O, O, X, O,    O, O]),
...     short_entries=pd.Series([O,   O, O, O, X,    O, O]),
...     short_exits=  pd.Series([O,   O, O, O, O,    O, X]),
...     size=         pd.Series([0.5, 0, 0, 0, 0.25, 0, 0]),
...     size_type="valuepercent",  # (1)!
... )
>>> pf.asset_value / pf.value  # (2)!
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.500000  0.500000  << long entry of 50%
2021-02-19 00:00:00+00:00  0.520256  0.501976
2021-02-20 00:00:00+00:00  0.519967  0.496546
2021-02-21 00:00:00+00:00  0.000000  0.000000  << long exit
2021-02-22 00:00:00+00:00 -0.250000 -0.250000  << short entry of 25%
2021-02-23 00:00:00+00:00 -0.220680 -0.215853
2021-02-24 00:00:00+00:00  0.000000  0.000000  << short exit
```

1. Treat the size as a percentage of the total portfolio value.

2. Get the allocation at each bar, that is, the ratio of the asset value to the
total portfolio value.

Thanks to VBT's powerful broadcasting mechanism, you can backtest arbitrary
configurations with just a couple of lines of code. Below, we are testing three different
mutual configurations of the size and size type arguments:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size=     vbt.Param([np.inf,   1,       0.5], level=0),  # (1)!
...     size_type=vbt.Param(["amount", "value", "valuepercent"], level=0)
... )
>>> pf.total_return
size  size_type     symbol 
inf   amount        BTCUSDT    0.113592
                    ETHUSDT   -0.003135
1.0   value         BTCUSDT    0.001136
                    ETHUSDT   -0.000031
0.5   valuepercent  BTCUSDT    0.056796
                    ETHUSDT   -0.001567
Name: total_return, dtype: float64
```

1. Without sharing the same `level`, both parameters would build a Cartesian product.

Each configuration is applied to the entire set of signal arrays.

### Accumulation

If you want to sell $1 worth of each asset instead of closing the entire position whenever
an exit signal is encountered, you need to enable accumulation:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size=1,
...     size_type="value",
...     accumulate=True  # (1)!
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000019  0.000516
2021-02-19 00:00:00+00:00  0.000019  0.000516
2021-02-20 00:00:00+00:00  0.000019  0.000516
2021-02-21 00:00:00+00:00  0.000002  0.000000
2021-02-22 00:00:00+00:00  0.000002  0.000000
2021-02-23 00:00:00+00:00  0.000002  0.000000
2021-02-24 00:00:00+00:00  0.000002  0.000000
```

1. See [AccumulationMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccumulationMode).

There is a leftover in the `BTCUSDT` column since we made a profit, while the position
has been closed entirely in the `ETHUSDT` column since we made a loss ($1 is worth less than
at the beginning of the simulation). There is also another implication: whenever accumulation is enabled,
the method starts working similarly to [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders).
For instance, it treats each signal as an order, regardless of whether we are already in the market.
This is best illustrated by the following example, where we issue a long entry signal
at every single bar, both without and with accumulation:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=True,
...     size=1,
...     size_type="value",
...     accumulate=vbt.Param([False, True])
... )
>>> pf.asset_flow  # (1)!
accumulate                    False                True          
symbol                      BTCUSDT   ETHUSDT   BTCUSDT   ETHUSDT
Open time                                                        
2021-02-18 00:00:00+00:00  0.000019  0.000516  0.000019  0.000516
2021-02-19 00:00:00+00:00  0.000000  0.000000  0.000018  0.000512
2021-02-20 00:00:00+00:00  0.000000  0.000000  0.000018  0.000523
2021-02-21 00:00:00+00:00  0.000000  0.000000  0.000017  0.000517
2021-02-22 00:00:00+00:00  0.000000  0.000000  0.000018  0.000563
2021-02-23 00:00:00+00:00  0.000000  0.000000  0.000020  0.000634
2021-02-24 00:00:00+00:00  0.000000  0.000000  0.000020  0.000616
```

1. Calculate and print the asset flow at each bar, that is, the absolute number of units
bought (positive) or sold (negative).

As you can see, without accumulation, only the first signal is executed and all following
signals are ignored since we are already in a position. With accumulation enabled,
each signal is executed regardless of the current position. This allows for pyramiding
and other trading strategies that do not require binary position restrictions.

### Size types

A variety of size types are supported. To see the full list, refer to the enumerated type
[SizeType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType). Each size type can be specified
either as a (case-insensitive) string representing the field name or as an integer representing the value.
For example, both `size_type="value"` and `size_type=SizeType.Value` behave identically.
Most size types will be internally converted to the size type `Amount`, which represents the absolute
number of units to order. This conversion is mainly done using the valuation price `val_price`,
which defaults to the order price and is intended to represent the latest price at the time of decision-making.

!!! note
    When working directly with Numba-compiled functions, only the integer format is supported.

However, not all size types are supported in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
Any target size type that defines a target, such as `TargetAmount`, `TargetValue`, and `TargetPercent(100)`,
cannot be safely used since the final order size might contradict the signal. For example, if you are in a
position of 10 units and issue an entry signal with a size of 3 units and a size type of `TargetAmount`,
the actual order will be a sell order of size 7, which is opposite to the direction of the signal issued.
Additionally, the size type `Percent` cannot be used in certain situations when both directions are allowed,
such as when reversing a position, because such a percentage cannot simply be "flipped".

### Size granularity

Size is always represented as a (usually 64-bit) floating-point number, and the entire simulation
logic of VBT is also built upon this number format. However, as you might know, floating
point numbers are not ideal for monetary calculations because their arithmetic
often leads to a loss of precision. Since Numba does not allow the use of fixed-point numbers,
VBT is forced to use floating-point numbers and applies several techniques to compare
them reliably, such as using
[numpy.isclose](https://numpy.org/doc/stable/reference/generated/numpy.isclose.html) and
[numpy.round_](https://numpy.org/doc/stable/reference/generated/numpy.round_.html).

But what if the size needs to be an integer, such as when trading stocks?
For this, you can use the `size_granularity` argument, which will round the final absolute
size to a certain number of decimal places. As a rule of thumb: use `1` for whole shares, `0.001` for
fractional shares, or a custom value for crypto. For example, Binance provides the step size
for each trading pair, which can be directly used as `size_granularity`.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     size_granularity=vbt.Param([1, 0.001]),
...     init_cash=1000_000
... )
>>> pf.asset_flow
size_granularity            1.000           0.001         
symbol                    BTCUSDT ETHUSDT BTCUSDT  ETHUSDT
Open time                                                 
2021-02-18 00:00:00+00:00    19.0   515.0  19.397  515.567
2021-02-19 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-20 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-21 00:00:00+00:00   -19.0  -515.0 -19.397 -515.566
2021-02-22 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-23 00:00:00+00:00     0.0     0.0   0.000    0.000
2021-02-24 00:00:00+00:00     0.0     0.0   0.000    0.000
```

!!! info
    Even though the traded size appears as an integer, it is still represented as a float.

### Price

By default, VBT executes an order immediately using the current closing price.
This behavior can be changed by adjusting the `price` argument, which can accept
either a price array or a price option of type [PriceType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceType).
If you look into the portfolio settings, you will notice that `price` is set to the string `"close"`,
which is translated at runtime into the option `PriceType.Close`. This option
is simply an alias for the value `np.inf` (positive infinity). What does infinity do here?
Since an order price must fall within the price bounds of a bar, negative and positive
infinity represent the opening and closing prices, respectively (see [Price resolution](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/#price-resolution));
both can be used within arrays. The other two options, `NextOpen` and `NextClose`,
are standalone options and cannot be used within arrays because they require other arguments.

!!! important
    Which option to choose depends on the price you used to generate your signals. Most of the time,
    signals are both generated and executed using the same closing price. To account for potential
    time gaps between signal generation and execution, you can use the next open or close, or shift
    your signals manually. If you generated signals using the opening price or another price that
    comes before it, you can also use the option `"open"` along with a bit of slippage.

Let's execute entries using the open and exits using the close. Remember that each order-related argument
is defined for all signal types: long entries and exits, and short entries and exits. This means
there is no argument that defines the price specifically for exits; otherwise, the number of
arguments would become too large. To make any argument value apply only to a subset of signal types,
you should set it using the signal types as a mask:

```pycon
>>> price = sub_data.symbol_wrapper.fill()  # (1)!
>>> entries = pd.Series([X, O, O, O, O, O, O]).vbt.broadcast_to(price)  # (2)!
>>> exits =   pd.Series([O, O, O, X, O, O, O]).vbt.broadcast_to(price)
>>> price[entries] = sub_data.open  # (3)!
>>> price[exits] = sub_data.close

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=entries,
...     exits=exits,
...     price=price
... )

>>> pf.orders.price.to_pd() == sub_data.open  # (4)!
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     True     True
2021-02-19 00:00:00+00:00    False    False
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00    False    False
2021-02-22 00:00:00+00:00    False    False
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False

>>> pf.orders.price.to_pd() == sub_data.close  # (5)!
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00    False    False
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00     True     True
2021-02-22 00:00:00+00:00    False    False
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False
```

1. Create a new price array with the same shape as the data.

2. Define signals and broadcast them to the price array so they can be used as a mask.

3. Set any element in the price array that corresponds to an entry signal to the opening price.

4. Generate a mask of filled orders whose price is open (this comparison works only if slippage is zero!).

5. Generate a mask of filled orders whose price is close.

Now, let's execute orders using the opening price of the next bar instead:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     price="nextopen"
... )
>>> pf.orders.price.to_pd() == sub_data.open
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00     True     True
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00    False    False
2021-02-22 00:00:00+00:00     True     True
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False
```

As we can see, the simulator waited for one bar and then executed each signal using the opening price.
This is one of the safest approaches for backtesting because it allows you the freedom
to run indicators on any price without worrying about look-ahead bias during execution.
The most bulletproof approach is to use the next close, as the difference between
the previous close and the next open is usually negligible.

### Shifting

We could have achieved the same result as above by manually shifting the signal arrays by one bar:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]).vbt.signals.fshift(),
...     exits=  pd.Series([O, O, O, X, O, O, O]).vbt.signals.fshift(),
...     price="open"  # (1)!
... )
>>> pf.orders.price.to_pd() == sub_data.open
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    False    False
2021-02-19 00:00:00+00:00     True     True
2021-02-20 00:00:00+00:00    False    False
2021-02-21 00:00:00+00:00    False    False
2021-02-22 00:00:00+00:00     True     True
2021-02-23 00:00:00+00:00    False    False
2021-02-24 00:00:00+00:00    False    False
```

1. When shifting manually, use the open or close of the current bar.

This is one of the most underrated features of VBT: since you are working with array
data, you can shift the data so that any current bar uses information from the past. In the example
above, after forward-shifting the signal arrays, the long entry signal on February 18
moves to February 19, but the index remains the same. Thus, the surrounding price
information in the form of OHLC (the `open`, `high`, `low`, and `close` arguments) must be left unchanged
and should never be shifted. This also means you not only have to shift the signals
but also any information linked to those signals, such as the order direction and price.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=  pd.Series([X, O, O, O, X, O, O]).vbt.fshift(1, False),
...     exits=    pd.Series([O, O, O, X, O, O, X]).vbt.fshift(1, False),
...     direction=pd.Series([L, L, L, L, S, S, S]).vbt.fshift(1, -1)
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.001789  0.051151
2021-02-20 00:00:00+00:00  0.001789  0.051151
2021-02-21 00:00:00+00:00  0.001789  0.051151
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00 -0.001979 -0.057624
2021-02-24 00:00:00+00:00 -0.001979 -0.057624
```

!!! tip
    As a rule of thumb: if an argument is signal-anchored, it should be shifted as well.
    If an argument is date-anchored, it should remain unchanged.

To reduce the need for manual shifting, VBT can handle it for us automatically!
To enable this, provide the `from_ago` argument, which represents how many bars ago
all the signal and order information should be taken from. For example, when `from_ago=1`,
the related information is taken from the previous bar:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=  pd.Series([X, O, O, O, X, O, O]),
...     exits=    pd.Series([O, O, O, X, O, O, X]),
...     direction=pd.Series([L, L, L, L, S, S, S]),
...     from_ago=1
... )
>>> pf.assets
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.001789  0.051151
2021-02-20 00:00:00+00:00  0.001789  0.051151
2021-02-21 00:00:00+00:00  0.001789  0.051151
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00 -0.001979 -0.057624
2021-02-24 00:00:00+00:00 -0.001979 -0.057624
```

This argument can also be supplied as an array.

### Slippage

By default, an order is executed as a market order. You can see this in the portfolio settings
under the key `order_type`. Market orders are transactions meant to execute as quickly as possible
at the current market price, which is always `price` (by default, the closing price). In reality,
however, the price at which an order is executed often does not match the price at which it was requested.
To account for this difference, we need to introduce slippage. Assuming the trading volume
is high, we should see less slippage. When the trading volume is low, we should expect
to see more slippage. An optimal slippage value can be calculated from order book data
(see [this blog](https://www.hodlbot.io/blog/an-analysis-of-slippage-on-the-binance-exchange)).

But let's assume for a moment that the average slippage is 0.5%. Using this together with the default
price is generally not recommended, since the closing price is meant to be the latest price seen at each bar.
To make the simulation more realistic, we should apply slippage to the next open instead:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     price="nextopen",
...     slippage=0.005  # (1)!
... )
>>> pf.orders.price.to_pd()  # (2)!
symbol                         BTCUSDT     ETHUSDT
Open time                                         
2021-02-18 00:00:00+00:00          NaN         NaN
2021-02-19 00:00:00+00:00  51810.37305  1948.60455
2021-02-20 00:00:00+00:00          NaN         NaN
2021-02-21 00:00:00+00:00          NaN         NaN
2021-02-22 00:00:00+00:00  57125.28825  1923.87230
2021-02-23 00:00:00+00:00          NaN         NaN
2021-02-24 00:00:00+00:00          NaN         NaN

>>> pf.orders.price.to_pd() / sub_data.open  # (3)!
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00    1.005    1.005
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00    0.995    0.995
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. 0.01 is 1%.

2. Get the filled order price.

3. Get the filled order price relative to the opening price.

We can see that slippage increased the price by 0.5% when buying and decreased
the price by 0.5% when selling. Adding slippage will always result in a fixed price penalty,
so the slippage value should always reflect the average penalty recorded in the market for
transactions of this size. Since slippage is not static and depends on many factors,
it can be provided as an array.

### Limit orders

To help reduce or eliminate slippage, traders use limit orders instead of market orders. A limit order
only fills at the price we want, or better. Unlike a market order, it will never fill at a worse price.
However, there is a catch: while the price is guaranteed, the filling of the order is not, and limit orders
will not be executed unless the asset price meets the order requirements. If the asset does not reach
the specified price, the order does not get filled, and we may miss out on the trading opportunity. So,
what happens when we execute our signals using limit orders? Let's change the default order type
by setting the `order_type` argument to `"limit"`:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     exits=  pd.Series([O, O, O, X, O, O, O]),
...     order_type="limit"
... )
>>> pf.orders.price.to_pd()
symbol                      BTCUSDT  ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00       NaN      NaN
2021-02-19 00:00:00+00:00  51552.60  1938.91
2021-02-20 00:00:00+00:00       NaN      NaN
2021-02-21 00:00:00+00:00       NaN      NaN
2021-02-22 00:00:00+00:00  57412.35  1933.54
2021-02-23 00:00:00+00:00       NaN      NaN
2021-02-24 00:00:00+00:00       NaN      NaN
```

Since the default order price is the closing price, the simulator used it as the target limit
price and skipped the entry bar. On the next bar, the simulator checked if the limit price
was reached by comparing it against the full candle. As shown above, each of the target prices
could be met as early as the next bar, which is similar to using the next opening price
as the order execution price. However, such a rapid match does not always happen: what if we want
to enter a short trade on February 22 using the previous high as the limit price?

!!! tip
    To skip the entry bar regardless of the price, you can set the `limit_delay` argument to `True`.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, O, O, X, O, O]),
...     direction="shortonly",
...     price=sub_data.high.vbt.fshift(),  # (1)!
...     order_type="limit"
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. Shift the high price by one bar to use the previous high price.

In this case, no limit order could be filled since the same price or higher (we are selling) could not be
found at any time during or after February 22. Now we have an order that could potentially
remain pending forever. How can we limit its lifetime? There are several possibilities.

#### Time in force

Time-in-force orders can be created using the `limit_tif` argument, which expects a time delta in any
format that can be converted to `np.timedelta64` and then to a 64-bit integer representing nanoseconds.
Accepted formats include a string, `pd.Timedelta`, `datetime.timedelta`, `np.timedelta64`, or an integer.
The time-in-force period begins counting at the **start** of the entry bar (even if the limit order
was placed at the end of the bar) and will be checked at the beginning of each bar, starting with the
second one. Let's create a buy limit order on February 20 using the previous low price:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit"
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00  50710.2      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

We can see that the limit order in the `BTCUSDT` column was executed in two bars,
while the limit order in the `ETHUSDT` column was executed in just one bar.
Below, we test a TIF option passed as a frequency string, which is supported only outside of arrays:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_tif="2d"  # (1)!
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. Gets translated into a time delta using [to_timedelta64](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_timedelta64).

Why is there no order in `BTCUSDT`? Because 2 days from `2021-02-20 00:00:00` is `2021-02-22 00:00:00`,
which is when the order is canceled. Since each timestamp represents the
opening time of a bar, the pending order no longer exists in the fifth bar. To specify
TIF inside an array or to test multiple configurations, use a Pandas or NumPy format,
or specify the total duration in nanoseconds instead of a string.
Let's test all formats at once!

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_tif=vbt.Param([
...         -1,  # (1)!
...         pd.Timedelta(days=1),  # (2)!
...         2 * np.timedelta64(86400000000000),  # (3)!
...         3 * vbt.dt_nb.d_ns  # (4)!
...     ], keys=["none", "1 days", "2 days", "3 days"])
... )
>>> pf.orders.price.to_pd()
limit_tif                     none          1 days          2 days          \
symbol                     BTCUSDT ETHUSDT BTCUSDT ETHUSDT BTCUSDT ETHUSDT   
Open time                                                                    
2021-02-18 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-19 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-20 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-21 00:00:00+00:00      NaN  1891.0     NaN     NaN     NaN  1891.0   
2021-02-22 00:00:00+00:00  50710.2     NaN     NaN     NaN     NaN     NaN   
2021-02-23 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   
2021-02-24 00:00:00+00:00      NaN     NaN     NaN     NaN     NaN     NaN   

limit_tif                   3 days          
symbol                     BTCUSDT ETHUSDT  
Open time                                   
2021-02-18 00:00:00+00:00      NaN     NaN  
2021-02-19 00:00:00+00:00      NaN     NaN  
2021-02-20 00:00:00+00:00      NaN     NaN  
2021-02-21 00:00:00+00:00      NaN  1891.0  
2021-02-22 00:00:00+00:00  50710.2     NaN  
2021-02-23 00:00:00+00:00      NaN     NaN  
2021-02-24 00:00:00+00:00      NaN     NaN  
```

1. Do not use TIF.

2. TIF of 1 day in Pandas format.

3. TIF of 2 days in NumPy format.

4. TIF of 3 days in integer format (nanoseconds).

!!! note
    NumPy and integer formats are preferred when building large arrays.

You can also specify the number of rows instead of a time delta by adjusting the time delta
format (`time_delta_format`), which uses the enumerated type
[TimeDeltaFormat](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.TimeDeltaFormat). This is
needed when the index is not datetime-like, such as after splitting the data
for cross-validation. Let's switch the time delta format to `Rows` and do the same as above,
but now wait for a specific number of rows to pass:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_tif=vbt.Param(
...         [-1, 1, 2, 3], 
...         keys=["none", "1 rows", "2 rows", "3 rows"]
...     ),
...     time_delta_format="rows"
... )
```

If you are confident that your index has a fixed frequency (for example, days instead of business days)
and does not have gaps, you can also calculate the number of rows by dividing time deltas:

```pycon
>>> index_td = vbt.timedelta("1 minute")
>>> tif_td = vbt.timedelta("1 day")
>>> int(tif_td / index_td)
1440
```

#### Expiration date

Another way to let limit orders expire based on dates and times is by setting an expiration date
with `limit_expiry`. There are two main options: set a frequency at which limit orders should expire,
or provide a datetime-like array that can include `pd.Timestamp`, `datetime.datetime`,
and `np.datetime64` objects. The argument will be converted into the 64-bit integer format
representing Unix time (total nanoseconds since 1970). Let's start with the first option
by making limit orders behave like day orders. When passed a frequency, the simulation method
will determine the right bound of each timestamp by using
[ArrayWrapper.get_period_ns_index](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_period_ns_index):

```pycon
>>> sub_data.symbol_wrapper.get_period_ns_index("1d")
array([1613692800000000000, 1613779200000000000, 1613865600000000000,
        1613952000000000000, 1614038400000000000, 1614124800000000000,
        1614211200000000000])
```

Since our data is already daily, let's make a pending limit order expire at
the end of the week in which it was created:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_expiry="monday"  # (1)!
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00  50710.2      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> sub_data.symbol_wrapper.index.to_period(vbt.offset("monday"))  # (2)!
PeriodIndex(['2021-02-16/2021-02-22', '2021-02-16/2021-02-22',
              '2021-02-16/2021-02-22', '2021-02-16/2021-02-22',
              '2021-02-16/2021-02-22', '2021-02-23/2021-03-01',
              '2021-02-23/2021-03-01'],
            dtype='period[W-MON]', name='Open time')
```

1. See [Anchored offsets](https://pandas.pydata.org/docs/user_guide/timeseries.html#anchored-offsets).

2. The user warning "Converting to PeriodArray/Index representation will drop timezone information" can be ignored.

Both orders could be executed because the dates `2021-02-20`, `2021-02-21`, and `2021-02-22`
are part of the same week, `2021-02-16/2021-02-22`. If we change the week layout to start
on a Sunday, only the first two dates belong to the same week, resulting in the
expiration of the pending order in the `BTCUSDT` column:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_expiry="sunday"
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN

>>> sub_data.symbol_wrapper.index.to_period(vbt.offset("sunday"))
PeriodIndex(['2021-02-15/2021-02-21', '2021-02-15/2021-02-21',
              '2021-02-15/2021-02-21', '2021-02-15/2021-02-21',
              '2021-02-22/2021-02-28', '2021-02-22/2021-02-28',
              '2021-02-22/2021-02-28'],
            dtype='period[W-SUN]', name='Open time')
```

We can also build our own `limit_expiry` array. For example, let's simulate a TIF
of 2 days by relying only on the expiration dates, just as if we used `limit_tif="2d"`:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([O, O, X, O, O, O, O]),
...     price=sub_data.low.vbt.fshift(),
...     order_type="limit",
...     limit_expiry=vbt.RepEval("""
...     expiry_index = wrapper.index + pd.Timedelta(days=2)
...     expiry_arr = vbt.to_2d_array(vbt.dt.to_ns(expiry_index))
...     expiry_arr
...     """)  # (1)!
... )
>>> pf.orders.price.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN   1891.0
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. After broadcasting, the final index may be different from the original one,
so we need to create a template that adds 2 days to the final index and converts it
into a two-dimensional array. Leaving it as one-dimensional will broadcast values per column.

!!! note
    Do not attempt to pass a `pd.Index` directly, as it will be converted into parameters.
    Use `pd.Series` instead.

#### Conflicts

What happens if you issue a signal while a limit order is pending? Since FS can track
only one limit order per column at a time, it must choose a winner. Two arguments
are relevant here: `upon_adj_limit_conflict` and `upon_opp_limit_conflict`, both of type
[PendingConflictMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PendingConflictMode).
The first controls the decision if both orders attempt to go in the same direction,
and the second applies when their directions are opposite. Let's introduce a sell market order
on February 21 and test various conflict resolution options:

```pycon
>>> all_conflict_modes = vbt.pf_enums.PendingConflictMode._fields  # (1)!
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=      pd.Series([O, O, X, O, O, O, O]),
...     short_entries=pd.Series([O, O, O, X, O, O, O]),
...     price=sub_data.select("BTCUSDT").low.vbt.fshift(),
...     order_type=vbt.index_dict({2: "limit", "_def": "market"}),  # (2)!
...     upon_opp_limit_conflict=vbt.Param(all_conflict_modes)
... )
>>> pf.orders.price.to_pd()
upon_opp_limit_conflict    KeepIgnore  KeepExecute  CancelIgnore  \
Open time                                                          
2021-02-18 00:00:00+00:00         NaN          NaN           NaN   
2021-02-19 00:00:00+00:00         NaN          NaN           NaN   
2021-02-20 00:00:00+00:00         NaN          NaN           NaN   
2021-02-21 00:00:00+00:00         NaN     53863.93           NaN   
2021-02-22 00:00:00+00:00     50710.2     50710.20           NaN   
2021-02-23 00:00:00+00:00         NaN          NaN           NaN   
2021-02-24 00:00:00+00:00         NaN          NaN           NaN   

upon_opp_limit_conflict    CancelExecute  
Open time                                 
2021-02-18 00:00:00+00:00            NaN  
2021-02-19 00:00:00+00:00            NaN  
2021-02-20 00:00:00+00:00            NaN  
2021-02-21 00:00:00+00:00       53863.93  
2021-02-22 00:00:00+00:00            NaN  
2021-02-23 00:00:00+00:00            NaN  
2021-02-24 00:00:00+00:00            NaN  
```

1. Get the list of all available options.

2. Only the first order should be a limit order.

As you may have noticed, the first word in the option's name indicates what to do with the
pending limit order, and the second word indicates what to do with the user-defined order.
By default, the limit order wins when the user-defined signal is in the same direction (buy and buy,
or sell and sell; option `KeepIgnore`) to avoid repeated execution of similar signals.
The user-defined signal wins when the two orders are in opposite directions (buy and sell,
or sell and buy; option `CancelExecute`) to account for a change in market regime.

#### Delta

The target price of a limit order comes from the `price` argument. But what if you want
to place a limit order at a price a certain distance (also called "delta") above or below another price,
such as 10 percent away from the current closing price? For this, you need to know in advance
whether the signal is a buy or a sell: if it is a buy order, the price should be below the
closing price, and if it is a sell order, the price should be above the closing price.
To avoid figuring this out ahead of time and building a price array manually,
you can use the `limit_delta` argument, which specifies how far from `price` the target limit
price should be. This argument's format is controlled by another option, `delta_format`, of type
[DeltaFormat](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.DeltaFormat).
The default format is a percentage (`Percent`), so passing `limit_delta=0.1` is interpreted as 10 percent.
Let's try different deltas for a buy limit order on the very first bar:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     order_type="limit",
...     limit_delta=vbt.Param([0, 0.1, 0.5])
... )
>>> pf.orders.price.to_pd()
limit_delta                    0.0                0.1               0.5  \
symbol                     BTCUSDT  ETHUSDT   BTCUSDT   ETHUSDT BTCUSDT   
Open time                                                                 
2021-02-18 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   
2021-02-19 00:00:00+00:00  51552.6  1938.91       NaN       NaN     NaN   
2021-02-20 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   
2021-02-21 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   
2021-02-22 00:00:00+00:00      NaN      NaN       NaN  1745.649     NaN   
2021-02-23 00:00:00+00:00      NaN      NaN  46397.34       NaN     NaN   
2021-02-24 00:00:00+00:00      NaN      NaN       NaN       NaN     NaN   

limit_delta                        
symbol                    ETHUSDT  
Open time                          
2021-02-18 00:00:00+00:00     NaN  
2021-02-19 00:00:00+00:00     NaN  
2021-02-20 00:00:00+00:00     NaN  
2021-02-21 00:00:00+00:00     NaN  
2021-02-22 00:00:00+00:00     NaN  
2021-02-23 00:00:00+00:00     NaN  
2021-02-24 00:00:00+00:00     NaN  
```

As we can see, without a delta, the target limit price is just the current closing price.
With a limit delta of 10 percent, the target limit price becomes `close * (1 - limit_delta)`,
which in this example was only matched after a few bars. The final limit delta of 50 percent
was not matched at all, since the selected price window does not include dips of that magnitude.
If the limit order were a sell, the calculation would be `close * (1 + limit_delta)`.

#### Reversing

So far, we have looked at regular buy/sell limit orders that fill whenever the price
reaches or drops below/exceeds the target price. Now, you can reverse the matching
logic with the `limit_reverse` argument to simulate buy/sell **stop** orders. For example, a buy
stop order with an absolute delta of $100 searches for a price $100 higher (instead
of lower) than the current closing price and executes the limit order at that price:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     order_type="limit",
...     limit_delta=vbt.Param([0, 100, 5000, 10000]),
...     delta_format="absolute",
...     limit_reverse=True
... )
>>> pf.orders.price.to_pd()
limit_delta                   0        100      5000   10000
Open time                                                   
2021-02-18 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-19 00:00:00+00:00  51552.61  51652.6      NaN    NaN
2021-02-20 00:00:00+00:00       NaN      NaN  56552.6    NaN
2021-02-21 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-22 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-23 00:00:00+00:00       NaN      NaN      NaN    NaN
2021-02-24 00:00:00+00:00       NaN      NaN      NaN    NaN
```

!!! info
    Why does a delta of zero result in a different fill price with and without reversing?
    When reaching the second bar, the limit price is compared to the opening price.
    In the first example, the opening price is higher than the limit price, so the limit order
    is executed using the limit price. In the second example, because the opening price is higher,
    the limit order is executed using the opening price.

#### Adjustment

In the theoretical section, we discussed how you can modify any limit order from a callback.
There are four callbacks you can use: a signal function (`signal_func_nb`), an adjustment function
(`adjust_func_nb`), a post-signal function (`post_signal_func_nb`), and a post-segment function
(`post_segment_func_nb`). The signal function is called at the beginning of a bar and should only be
used if you need to define the four user-defined signals dynamically. The adjustment function is called
at the beginning of the default signal function [dir_signal_func_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.dir_signal_func_nb)
for direction-unaware and [ls_signal_func_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.ls_signal_func_nb)
for direction-aware signals, which are provided using boolean arrays (statically). The post-segment
function is called at the end of a bar, useful for tasks such as pre-calculating metrics like portfolio returns.

!!! note
    The adjustment function is called only if the signals are provided statically,
    meaning `signal_func_nb` has not been overridden. Also, note that overriding any of the
    above functions will disable caching of [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)!

Each of these functions accepts a named tuple representing the current simulation context, which includes
a one-dimensional record array [SignalContext.last_limit_info](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.last_limit_info)
with the latest limit order information defined per column. By changing this array, you can modify the
limit order that is yet to be processed at this bar (or the next one, for the post-segment function).
We will focus on the adjustment function since it is designed to work with signal arrays.
This function should be Numba-compiled, take the current context variable (usually named `c`)
and any custom arguments (`adjust_args`), and return nothing (`None`). Why nothing? Because
all relevant information can be changed directly in the context. Let's create a function
that cancels any pending limit order (that is, one that has not been executed at the entry bar).
Limit orders will be created using the opening price and a parameterized custom delta:

```pycon
>>> @njit
... def adjust_func_nb(c):  # (1)!
...     limit_info = c.last_limit_info[c.col]  # (2)!
...     if limit_info.creation_idx != -1:  # (3)!
...         if c.i - limit_info.creation_idx >= 1:  # (4)!
...             vbt.pf_nb.clear_limit_info_nb(limit_info)  # (5)!

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="open",
...     order_type="limit",
...     limit_delta=vbt.Param([0, 0.1]),
...     adjust_func_nb=adjust_func_nb
... )
>>> pf.orders.price.to_pd()
limit_delta                     0.0             0.1        
symbol                      BTCUSDT ETHUSDT BTCUSDT ETHUSDT
Open time                                                  
2021-02-18 00:00:00+00:00  52117.67  1849.7     NaN     NaN
2021-02-19 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-20 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-21 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-22 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-23 00:00:00+00:00       NaN     NaN     NaN     NaN
2021-02-24 00:00:00+00:00       NaN     NaN     NaN     NaN
```

1. Must take [SignalContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext).

2. Get the pending limit order information for the current column (asset).

3. Check whether there is a pending limit order.

4. Check whether the order has been pending for one bar or longer.

5. Use [clear_limit_info_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.clear_limit_info_nb) to cancel the order.

As we can see, without a delta, each order could be filled already at the entry bar. With a delta of 10
percent, the orders could not be filled at the entry bar and thus were canceled at the beginning
of the second bar (February 19). Now, let's do the opposite: if a limit order is still pending,
override its price with the valuation price and set the delta to 1 percent so it executes at this bar:

```pycon
>>> @njit
... def adjust_func_nb(c):
...     limit_info = c.last_limit_info[c.col]
...     if limit_info.creation_idx != -1:
...         if c.i - limit_info.creation_idx >= 1:
...             limit_info.init_price = -np.inf  # (1)!
...             limit_info.delta = 0.01

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="open",
...     order_type="limit",
...     limit_delta=vbt.Param([0.1, 0.2]),
...     adjust_func_nb=adjust_func_nb
... )
>>> pf.orders.price.to_pd()
limit_delta                       0.1                    0.2           
symbol                        BTCUSDT    ETHUSDT     BTCUSDT    ETHUSDT
Open time                                                              
2021-02-18 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-19 00:00:00+00:00  51037.0839  1919.5209  51037.0839  1919.5209
2021-02-20 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-21 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-22 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-23 00:00:00+00:00         NaN        NaN         NaN        NaN
2021-02-24 00:00:00+00:00         NaN        NaN         NaN        NaN
```

1. Negative infinity will be translated into the valuation price, which is the latest
known price at the time the callback is called: the opening price for adjustment
and signal callbacks, and the closing price for post-segment callbacks. Note that
you cannot use positive infinity for the closing price because it is unknown!

Just as we canceled an order, we can also create a new order without the need for
user-defined signals—essentially, out of nothing!

```pycon
>>> @njit
... def adjust_func_nb(c, custom_delta):
...     limit_info = c.last_limit_info[c.col]
...     if c.i == 0:  # (1)!
...         curr_delta = vbt.pf_nb.select_nb(c, custom_delta)
...         vbt.pf_nb.set_limit_info_nb(limit_info, c.i, delta=curr_delta)  # (2)!

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     broadcast_named_args=dict(custom_delta=vbt.Param([0, 0.1])),  # (3)!
...     adjust_func_nb=adjust_func_nb,
...     adjust_args=(vbt.Rep("custom_delta"),)  # (4)!
... )
>>> pf.orders.price.to_pd()
custom_delta                    0.0                0.1         
symbol                      BTCUSDT ETHUSDT    BTCUSDT  ETHUSDT
Open time                                                      
2021-02-18 00:00:00+00:00  52117.67  1849.7        NaN      NaN
2021-02-19 00:00:00+00:00       NaN     NaN        NaN      NaN
2021-02-20 00:00:00+00:00       NaN     NaN        NaN      NaN
2021-02-21 00:00:00+00:00       NaN     NaN        NaN      NaN
2021-02-22 00:00:00+00:00       NaN     NaN        NaN  1664.73
2021-02-23 00:00:00+00:00       NaN     NaN  46905.903      NaN
2021-02-24 00:00:00+00:00       NaN     NaN        NaN      NaN
```

1. Create a signal order only at the very first bar.

2. Use [set_limit_info_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.set_limit_info_nb).
Check its signature to learn more about the default values. For example, it uses the current
valuation price, infinite size, and both directions by default.

3. To make a custom argument parameterizable, define it as a flexible argument
in `broadcast_named_args`. This means it should be set up to be specified per element.

4. Remember to put a comma after a single element in a tuple.

We just created limit orders dynamically, without any signals :sparkles:

### Stop orders

Like limit orders, stop orders are executed only when a price condition is met. Their information is also
stored per column inside a one-dimensional record array. However, unlike limit orders, stop orders are
deferred user-defined signals that result in a market or limit order, mainly to close out an existing
position. They are created after a successful entry order that either opens, increases, or reverses a
position (that is, closes out and then opens a new position in the opposite direction). There are three
types of stop orders: SL (`sl_stop`), TSL and TTP (`tsl_stop` and `tsl_th`, which act as a single order),
and TP (`tp_stop`). Each type is stored and tracked separately, but all operate as a one-cancels-all
(OCA) order, meaning that triggering one order fully cancels the others automatically. Any pending stop
orders will also be canceled if the entered position is closed out by other means.

Let's place a stop loss order at 10%:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=0.1,
... )
>>> pf.orders.price.to_pd()
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  51552.60  1939.610
2021-02-19 00:00:00+00:00       NaN       NaN
2021-02-20 00:00:00+00:00       NaN       NaN
2021-02-21 00:00:00+00:00       NaN       NaN
2021-02-22 00:00:00+00:00       NaN  1745.649
2021-02-23 00:00:00+00:00  46397.34       NaN
2021-02-24 00:00:00+00:00       NaN       NaN
```

!!! important
    When passing an instance of [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) as the first argument
    (as shown with `sub_data`), the method will automatically extract OHLC from it. If you pass the
    closing price as the first argument instead, make sure to also provide the other price features (OHL)
    using the `open`, `high`, and `low` arguments. Without them, candles will be incomplete and VBT
    will make decisions based only on `close`!

We can see that the entered long position was exited at the price `51552.60 * 0.9 = 46397.34`.
To test multiple stop values, we can wrap them with the [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
class:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=vbt.Param([np.nan, 0.1, 0.2]),  # (1)!
... )
>>> pf.orders.price.to_pd()
sl_stop                        NaN                0.1                0.2  \
symbol                     BTCUSDT  ETHUSDT   BTCUSDT   ETHUSDT  BTCUSDT   
Open time                                                                  
2021-02-18 00:00:00+00:00  51552.6  1939.61  51552.60  1939.610  51552.6   
2021-02-19 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   
2021-02-20 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   
2021-02-21 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   
2021-02-22 00:00:00+00:00      NaN      NaN       NaN  1745.649      NaN   
2021-02-23 00:00:00+00:00      NaN      NaN  46397.34       NaN      NaN   
2021-02-24 00:00:00+00:00      NaN      NaN       NaN       NaN      NaN   

sl_stop                              
symbol                      ETHUSDT  
Open time                            
2021-02-18 00:00:00+00:00  1939.610  
2021-02-19 00:00:00+00:00       NaN  
2021-02-20 00:00:00+00:00       NaN  
2021-02-21 00:00:00+00:00       NaN  
2021-02-22 00:00:00+00:00  1551.688  
2021-02-23 00:00:00+00:00       NaN  
2021-02-24 00:00:00+00:00       NaN  
```

1. NaN means no stop order.

Since each stop argument and its accompanying information broadcast together with the data,
we can provide them as fully-fledged arrays. Below, we define a stop loss based on the ATR:

```pycon
>>> atr = data.run("atr").atr
>>> sub_atr = atr.loc["2021-02-18":"2021-02-24"]

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=sub_atr / sub_data.close
... )
>>> pf.orders.price.to_pd()
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2021-02-18 00:00:00+00:00  51552.600000  1939.610000
2021-02-19 00:00:00+00:00           NaN          NaN
2021-02-20 00:00:00+00:00           NaN  1805.283249
2021-02-21 00:00:00+00:00           NaN          NaN
2021-02-22 00:00:00+00:00  48335.716826          NaN
2021-02-23 00:00:00+00:00           NaN          NaN
2021-02-24 00:00:00+00:00           NaN          NaN
```

What about absolute stop values? They can be specified just like for limit orders:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=100,
...     delta_format="absolute"
... )
>>> pf.orders.price.to_pd()
Open time
2021-02-18 00:00:00+00:00    51552.6
2021-02-19 00:00:00+00:00    51452.6
2021-02-20 00:00:00+00:00        NaN
2021-02-21 00:00:00+00:00        NaN
2021-02-22 00:00:00+00:00        NaN
2021-02-23 00:00:00+00:00        NaN
2021-02-24 00:00:00+00:00        NaN
Freq: D, dtype: float64
```

You can also provide a target stop price directly:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=51452.6,
...     delta_format="target"
... )
>>> pf.orders.price.to_pd()
Open time
2021-02-18 00:00:00+00:00    51552.6
2021-02-19 00:00:00+00:00    51452.6
2021-02-20 00:00:00+00:00        NaN
2021-02-21 00:00:00+00:00        NaN
2021-02-22 00:00:00+00:00        NaN
2021-02-23 00:00:00+00:00        NaN
2021-02-24 00:00:00+00:00        NaN
Freq: D, dtype: float64
```

Since the simulator can track multiple stop types at the same time, we can simulate a specific
risk-to-reward (R/R) ratio by setting both a stop loss and a take profit. For example, let's
simulate a risk-to-reward ratio of 1/2 based on the ATR:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=1 * sub_atr / sub_data.close,
...     tp_stop=2 * sub_atr / sub_data.close
... )
>>> pf.orders.price.to_pd()
symbol                          BTCUSDT      ETHUSDT
Open time                                           
2021-02-18 00:00:00+00:00  51552.600000  1939.610000
2021-02-19 00:00:00+00:00           NaN          NaN
2021-02-20 00:00:00+00:00           NaN  1805.283249
2021-02-21 00:00:00+00:00  57986.366348          NaN
2021-02-22 00:00:00+00:00           NaN          NaN
2021-02-23 00:00:00+00:00           NaN          NaN
2021-02-24 00:00:00+00:00           NaN          NaN
```

The TP order won in the first column, while the SL order won in the second column. Is there a way to see
the order type, and ideally, the date of the signal that initiated it? Absolutely, VBT can handle
all of your wishes! :genie:

```pycon
>>> pf.orders.readable  # (1)!
    Order Id   Column              Signal Index            Creation Index  \
0         0  BTCUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
1         1  BTCUSDT 2021-02-18 00:00:00+00:00 2021-02-21 00:00:00+00:00   
2         0  ETHUSDT 2021-02-18 00:00:00+00:00 2021-02-18 00:00:00+00:00   
3         1  ETHUSDT 2021-02-18 00:00:00+00:00 2021-02-20 00:00:00+00:00   

                  Fill Index      Size         Price  Fees  Side    Type  \
0 2021-02-18 00:00:00+00:00  0.001940  51552.600000   0.0   Buy  Market   
1 2021-02-21 00:00:00+00:00  0.001940  57986.366348   0.0  Sell  Market   
2 2021-02-18 00:00:00+00:00  0.051557   1939.610000   0.0   Buy  Market   
3 2021-02-20 00:00:00+00:00  0.051557   1805.283249   0.0  Sell  Market   

  Stop Type  
0      None  
1        TP  
2      None  
3        SL  

>>> pf.orders.stop_type.to_pd(mapping=True)  # (2)!
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-18 00:00:00+00:00    None    None
2021-02-19 00:00:00+00:00    None    None
2021-02-20 00:00:00+00:00    None      SL
2021-02-21 00:00:00+00:00      TP    None
2021-02-22 00:00:00+00:00    None    None
2021-02-23 00:00:00+00:00    None    None
2021-02-24 00:00:00+00:00    None    None
```

1. Orders are represented as rows in a compressed record array. Specific information is included as a column.

2. Specific information as a time series.

From the first DataFrame, we can see that the first order is a buy market order (`Side` and `Type`) that
was filled on February 18th (`Fill Index`). The second order in the same column is a sell market order,
resulting from a TP order (`Stop Type`), that was issued on February 18th (`Signal Index`) but actually
filled on February 21st (`Fill Index`). The `Creation Index` column is the same as the `Fill Index` column
because all orders are market orders. The second DataFrame shows a typical representation of a single
piece of information by time and asset; it is best used when the information should be analyzed as a
time series with Pandas.

Now, let's discuss multiple stop configurations. Here is how to test each stop type independently:
that is, SL at 10%, TSL at 10%, TTP at 10% with a 10% threshold, and TP at 10%:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop= vbt.Param([0.1,    np.nan, np.nan, np.nan], level=0),
...     tsl_stop=vbt.Param([np.nan, 0.1,    0.1,    np.nan], level=0),
...     tsl_th=  vbt.Param([np.nan, np.nan, 0.1,    np.nan], level=0),
...     tp_stop= vbt.Param([np.nan, np.nan, np.nan, 0.1   ], level=0),
... )
>>> pf.total_return
sl_stop  tsl_stop  tsl_th  tp_stop  symbol 
0.1      NaN       NaN     NaN      BTCUSDT   -0.100000
                                    ETHUSDT   -0.100000
NaN      0.1       NaN     NaN      BTCUSDT    0.018717
                                    ETHUSDT   -0.052332
NaN      0.1       0.1     NaN      BTCUSDT    0.018717
                                    ETHUSDT   -0.163033
NaN      NaN       NaN     0.1      BTCUSDT    0.100000
                                    ETHUSDT   -0.163033
Name: total_return, dtype: float64
```

To build a product of various stop types, we can specify unique (non-repeating) values and omit
the `level` argument:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=vbt.Param([np.nan, 0.1]),
...     tsl_stop=vbt.Param([np.nan, 0.1]),
...     tsl_th=vbt.Param([np.nan, 0.1]),
...     tp_stop=vbt.Param([np.nan, 0.1]),
... )
```

!!! warning
    The number of columns generated from a Cartesian product of multiple parameter combinations
    grows exponentially with the number of parameter combinations. Please show mercy to your RAM!

However, this would include a combination where `tsl_th` is not NaN but `tsl_stop` is NaN, which does
not make sense. What we want are three combinations: no TSL/TTP stop, TSL stop, and TTP stop. Therefore,
we need to combine these two arguments manually and link them by the same `level`:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=vbt.Param([np.nan, 0.1], level=0),
...     tsl_stop=vbt.Param([np.nan, 0.1, 0.1], level=1),
...     tsl_th=vbt.Param([np.nan, np.nan, 0.1], level=1),
...     tp_stop=vbt.Param([np.nan, 0.1], level=2),
... )
```

!!! note
    If any arguments require a level, you must define a level for all arguments.

We can then call a metric and analyze it using Pandas. For example, let's calculate the average return
for 1) positions using either SL or TP, or 2) positions using both SL and TP together:

```pycon
>>> total_return = pf.total_return
>>> sl_stops = total_return.index.get_level_values("sl_stop")
>>> tp_stops = total_return.index.get_level_values("tp_stop")

>>> total_return[np.isnan(sl_stops) | np.isnan(tp_stops)].mean()
-0.045462468872515135

>>> total_return[~np.isnan(sl_stops) & ~np.isnan(tp_stops)].mean()
0.0
```

Seems like combining SL and TP works better on our "huge" time series with just 7 data points :upside_down_face:

The examples above only work if you are testing arguments provided as scalars (single values). But what
if you want to backtest different combinations of stop values given as arrays, such as those calculated
from the ATR? Doing this manually would be quite difficult, since you would need to tile each stop type
array by the number of stop values and then combine all stop type arrays using a Cartesian product.

A simpler solution is to build a basic indicator that prepares the arrays for us:

```pycon
>>> StopOrderPrep = vbt.IF.from_expr("""
...     sl_stop = @p_sl_mult * @in_atr / @in_close
...     tp_stop = @p_tp_mult * @in_atr / @in_close
...     sl_stop, tp_stop
... """)
>>> stop_order_prep = StopOrderPrep.run(
...     close=sub_data.close, 
...     atr=sub_atr,
...     sl_mult=[np.nan, 1],  # (1)!
...     tp_mult=[np.nan, 1],
...     param_product=True
... )
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     sl_stop=stop_order_prep.sl_stop,
...     tp_stop=stop_order_prep.tp_stop,
... )
>>> pf.total_return
custom_sl_mult  custom_tp_mult  symbol 
NaN             NaN             BTCUSDT   -0.036398
                                ETHUSDT   -0.163033
NaN             1.0             BTCUSDT    0.062400
                                ETHUSDT   -0.163033
1.0             NaN             BTCUSDT   -0.062400
                                ETHUSDT   -0.069255
1.0             1.0             BTCUSDT    0.062400
                                ETHUSDT   -0.069255
Name: total_return, dtype: float64
```

1. NaN means no order, while 1 means one ATR divided by the close.

#### Entry point

You can set the entry price of stop orders using the `stop_entry_price` argument. This argument
accepts either the price itself or any option from the enumerated type
[StopEntryPrice](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopEntryPrice). To separate real
prices from options, the options use a negative sign. By default, the entry price is the closing
price, even if the entry order was actually filled before the closing price, such as at the opening
price. This is because a stop order cannot be filled on the same bar as the entry order, so the
first check is postponed to the next bar. Let's see what happens if we define a stop order that can
be executed as early as the first bar:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="open",
...     tp_stop=0.005,
...     stop_entry_price="fillprice"
... )
>>> pf.orders.price.to_pd()
symbol                         BTCUSDT  ETHUSDT
Open time                                      
2021-02-18 00:00:00+00:00  52117.67000  1849.70
2021-02-19 00:00:00+00:00  52378.25835  1938.91
2021-02-20 00:00:00+00:00          NaN      NaN
2021-02-21 00:00:00+00:00          NaN      NaN
2021-02-22 00:00:00+00:00          NaN      NaN
2021-02-23 00:00:00+00:00          NaN      NaN
2021-02-24 00:00:00+00:00          NaN      NaN
```

As we can see, even though the target price `52378.26` could theoretically be filled at the first
bar, since it is lower than the highest price of `52530.00` on that bar, the check was still
postponed to the next bar. This happens because the FS only allows one order per bar and asset.

#### Exit point

There are only a few options for defining a stop **exit** price: the stop price and the closing
price. If the stop is triggered at the beginning of a bar, the stop price will become the opening
price, and the same applies for the closing price. By default, a stop order is executed using the
stop price, but if you want to run the entire simulation using only closing prices, the closing
price option may be more suitable.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.05,
...     stop_exit_price=vbt.Param(["stop", "close"])
... )
>>> pf.orders.price.to_pd()
stop_exit_price                stop               close         
symbol                      BTCUSDT    ETHUSDT  BTCUSDT  ETHUSDT
Open time                                                       
2021-02-18 00:00:00+00:00  51552.60  1939.6100  51552.6  1939.61
2021-02-19 00:00:00+00:00  54130.23        NaN  55906.0      NaN
2021-02-20 00:00:00+00:00       NaN  2036.5905      NaN  1913.00
2021-02-21 00:00:00+00:00       NaN        NaN      NaN      NaN
2021-02-22 00:00:00+00:00       NaN        NaN      NaN      NaN
2021-02-23 00:00:00+00:00       NaN        NaN      NaN      NaN
2021-02-24 00:00:00+00:00       NaN        NaN      NaN      NaN
```

By doing this, we effectively delay the execution of a stop order until the end of a bar.
Not only can we change the default stop exit price, but we can also change the default stop
exit behavior. Using the `stop_exit_type` argument of type
[StopExitType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopExitType), we can reduce,
reverse, or reverse and reduce the position, instead of just closing it. By default, the position
is closed by issuing an exit signal and disabling accumulation. Alternatively, we can reverse it by
issuing an opposite entry signal, and/or reduce it by keeping accumulation enabled.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.01,  # (1)!
...     stop_exit_type=vbt.Param(["close", "reverse"])
... )
>>> pf.orders.price.to_pd()
stop_exit_type                 close                reverse           
symbol                       BTCUSDT    ETHUSDT     BTCUSDT    ETHUSDT
Open time                                                             
2021-02-18 00:00:00+00:00  51552.600  1939.6100  51552.6000  1939.6100
2021-02-19 00:00:00+00:00  52068.126  1959.0061  52068.1260  1959.0061
2021-02-20 00:00:00+00:00        NaN        NaN  55346.9400  1935.4500
2021-02-21 00:00:00+00:00        NaN        NaN  56399.6019  1932.1300
2021-02-22 00:00:00+00:00        NaN        NaN  56834.4843  1914.1947
2021-02-23 00:00:00+00:00        NaN        NaN         NaN        NaN
2021-02-24 00:00:00+00:00        NaN        NaN         NaN        NaN
```

1. TP of 1%.

Look at the columns under the parameter value `reverse`: they now contain five orders instead of two.
Why is that? By reversing, we close the current position and open a new one in the opposite
direction, using a single order. Opening a new position also creates a new stop order, which
results in a never-ending cycle of conditional reversals :grimacing: If you do not want this cycle,
you can define `stop_exit_type` as an array where only a specific stop is closed or reversed. Each
value in this array should be linked to the entry point rather than the exit point, since this
information (along with `stop_exit_price`) is order-anchored, not date-anchored. For example, let's
reverse only the first order:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.01,
...     stop_exit_type=vbt.index_dict({0: "reverse", "_def": "close"})
... )
>>> pf.orders.price.to_pd()
symbol                       BTCUSDT    ETHUSDT
Open time                                      
2021-02-18 00:00:00+00:00  51552.600  1939.6100
2021-02-19 00:00:00+00:00  52068.126  1959.0061
2021-02-20 00:00:00+00:00  55346.940  1935.4500
2021-02-21 00:00:00+00:00        NaN        NaN
2021-02-22 00:00:00+00:00        NaN        NaN
2021-02-23 00:00:00+00:00        NaN        NaN
2021-02-24 00:00:00+00:00        NaN        NaN
```

The first position was reversed, while the second position was simply exited.

What about the order type? Upon exit, the order type becomes either a market or a limit order. This
behavior is controlled by the `stop_order_type` argument of type
[OrderType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.OrderType). There is also a
`stop_limit_delta` argument to specify a delta for the resulting limit order. Let's compare a 5% TP
order as a stop market order versus a stop limit order with a 1% delta:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     tp_stop=0.05,
...     stop_order_type=vbt.Param(["market", "limit"]),
...     stop_limit_delta=0.01
... )
>>> pf.orders.price.to_pd()
stop_order_type              market                  limit         
symbol                      BTCUSDT    ETHUSDT     BTCUSDT  ETHUSDT
Open time                                                          
2021-02-18 00:00:00+00:00  51552.60  1939.6100  51552.6000  1939.61
2021-02-19 00:00:00+00:00  54130.23        NaN  54671.5323      NaN
2021-02-20 00:00:00+00:00       NaN  2036.5905         NaN      NaN
2021-02-21 00:00:00+00:00       NaN        NaN         NaN      NaN
2021-02-22 00:00:00+00:00       NaN        NaN         NaN      NaN
2021-02-23 00:00:00+00:00       NaN        NaN         NaN      NaN
2021-02-24 00:00:00+00:00       NaN        NaN         NaN      NaN
```

The latest stop limit order could not be filled because the highest price in the entire `ETHUSDT`
column is `2042.34`, while the requested limit price is `1939.61 * 1.05 * 1.01 = 2056.96`. Even if a
higher price existed on February 20, unless the stop price was triggered at open, the simulator
would not be able to use the entire candle to match the limit price since there is no intra-bar data
to confirm that the limit price was reached strictly after the stop price. The only information the
simulator can check in this case is the closing price. Only if the closing price is higher than the
limit price can we be sure the limit price was actually hit.

#### Conflicts

Conflicts between stop orders and user-defined signals follow the same resolution logic as limit
orders, with the only difference being in the argument naming: `upon_adj_stop_conflict` and
`upon_opp_stop_conflict`. By default, when a user-defined signal in any direction is encountered,
nothing happens. Any stop order remains pending, and the signal is executed as usual. This is
because pending stop orders are automatically canceled whenever the position is closed out. Let's
now cancel any signal when there is a stop order pending:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, X, O, X, O, X]),
...     exits=  pd.Series([O, X, O, X, O, X, O]),
...     direction="both",
...     sl_stop=0.1,
...     upon_adj_stop_conflict="keepignore",
...     upon_opp_stop_conflict="keepignore",
... )
>>> pf.orders.price.to_pd()
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  51552.60  1939.610
2021-02-19 00:00:00+00:00       NaN       NaN
2021-02-20 00:00:00+00:00       NaN       NaN
2021-02-21 00:00:00+00:00       NaN       NaN
2021-02-22 00:00:00+00:00       NaN  1745.649
2021-02-23 00:00:00+00:00  46397.34  1577.890
2021-02-24 00:00:00+00:00  49676.20       NaN
```

#### Adjustment

Similar to limit orders, stop orders are stored in one-dimensional record arrays where information
is organized by column: [SignalContext.last_sl_info](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.last_sl_info)
for SL, [SignalContext.last_tsl_info](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.last_tsl_info)
for TSL and TTP, and [SignalContext.last_tp_info](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext.last_tp_info)
for TP. For example, you can create an adjustment function that creates both a TP and an SL order
based on the maximum allowed, absolute, parameterizable profit and loss (PnL):

```pycon
>>> @njit
... def adjust_func_nb(c, max_loss, max_profit):
...     position_now = c.last_position[c.col]  # (1)!
...     if position_now != 0:  # (2)!
...         sl_info = c.last_sl_info[c.col]
...         tp_info = c.last_tp_info[c.col]
...         ml = vbt.pf_nb.select_nb(c, max_loss)  # (3)!
...         mp = vbt.pf_nb.select_nb(c, max_profit)
...
...         if not vbt.pf_nb.is_stop_info_active_nb(sl_info):  # (4)!
...             sl_info.stop = ml / (sl_info.init_price * position_now)  # (5)!
...         if not vbt.pf_nb.is_stop_info_active_nb(tp_info):
...             tp_info.stop = mp / (sl_info.init_price * position_now)

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     adjust_func_nb=adjust_func_nb,
...     adjust_args=(vbt.Rep("max_loss"), vbt.Rep("max_profit")),
...     broadcast_named_args=dict(
...         max_loss=10,
...         max_profit=10
...     )
... )
>>> pf.exit_trades.pnl.to_pd()  # (6)!
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00      NaN      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00     10.0      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN    -10.0
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. Get the size of the current position. Remember that fields not associated with cash are 
accessible via column.

2. We are in the market when the position is not zero.

3. Our parameters broadcast against the data, so we need to select the current element flexibly.

4. There is a useful function [is_stop_info_active_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.is_stop_info_active_nb)
that checks whether the current stop information record is active (the stop value is not NaN).

5. Divide the maximum loss or profit by the notional value to get the optimal stop loss or take profit.

6. Get the PnL of each trade as a time series.

!!! tip
    To avoid recompiling the entire simulation function with each new run, define the adjustment
    function in a different cell or even a separate file. Caching does not help, because Numba does
    not allow caching callbacks.

Our losses and profits are now capped at $10! Here is what happened. Whenever you provide a custom
adjustment function or any other callback, the simulator switches to flexible mode and enables 
stop orders. Whenever a new position is opened or an existing one is increased, the simulator
initializes each stop type with the provided arguments. However, since we have not passed anything
related to SL and TP, their default stop value is NaN, so they are inactive by default. Regardless, 
they still contain valuable information, such as the initial price. Therefore, all that is left 
is to check whether the respective stop is inactive and override the stop value to activate it. 
You could also use the functions [set_sl_info_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.set_sl_info_nb)
and [set_tp_info_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.set_tp_info_nb)
to set the entire information, but in this case, it was not necessary.

#### Laddering :ladder:

??? youtube "Take Profit Ladders on YouTube"
    <iframe class="youtube-video" src="https://www.youtube.com/embed/u4RABJMXhXc?si=wkwT18xYQt4EUw0A" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

We know how to define a single stop to close out an entire position, but what if we want to exit
a position gradually? For this, we need a way to provide multiple stop values. However, there is a 
challenge: stop arguments should broadcast together with other broadcastable arguments,
so when providing (for example) `tp_stop` as an array, the first axis should represent time
and the second axis should represent columns. Thanks to a new single-axis broadcasting feature,
we can now notify the broadcaster that the array's rows do not represent time, but something different.

In practice, [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
has a `stop_ladder` argument. When enabled, it switches to single-axis broadcasting, so that
providing a list (or one-dimensional array) of values is treated as a single ladder. You can also
provide a two-dimensional array to specify the ladder for each column.

!!! note
    Enabling this argument causes __all__ stop arguments to behave like ladders. There is no way to
    make one argument a ladder series and another a time series. Also, the `stop_ladder`
    argument affects all columns and cannot be specified per column.

In a ladder, each value is a step with a stop value. Just like steps on a real ladder, steps must be
ordered from low to high, so that a step is executed only after the previous one has been executed.
There is no limit to the number of steps. Let's enable laddering using the `stop_ladder` argument and
create our first TP ladder with two values: 1% and 5%.

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder=True,
...     tp_stop=[0.01, 0.05]
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00 -0.00097 -0.025778
2021-02-20 00:00:00+00:00 -0.00097 -0.025778
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

Both ladder steps were successfully executed and removed equal portions of the position. By default,
the exit size distribution of the steps is uniform: the exit size depends only on the number of
steps. But what if you want to remove an amount from the position that is proportional to the step
size? In this context, step size refers to the difference between the current stop price and the
previous stop price (the step range), relative to the difference between the last stop price and
the entry price (the full range). To achieve this, you can provide the `stop_ladder` argument with
the value "weighted", which corresponds to the mode `StopLadderMode.Weighted` (for possible ladder
modes, see
[StopLadderMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopLadderMode)):

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder="weighted",
...     tp_stop=[0.01, 0.05]
... )
>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00 -0.000388 -0.010311
2021-02-20 00:00:00+00:00 -0.001552 -0.041245
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

The first step removed `(0.01 - 0) / (0.05 - 0) = 20%` of the initial position, while the second stop
removed the remaining `(0.05 - 0.01) / (0.05 - 0) = 80%`.

Suppose you want to have two parallel ladders: SL and TP. What happens to the exit size distribution
if both ladders are partially executed?

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder="uniform",
...     tp_stop=[0.1, 0.2],
...     sl_stop=[0.05, 0.1, 0.15]
... )

>>> pf.orders.stop_type.to_pd(mapping=True)
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-18 00:00:00+00:00    None    None
2021-02-19 00:00:00+00:00    None    None
2021-02-20 00:00:00+00:00      TP      SL
2021-02-21 00:00:00+00:00    None    None
2021-02-22 00:00:00+00:00      SL      SL
2021-02-23 00:00:00+00:00      SL      SL
2021-02-24 00:00:00+00:00    None    None

>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00 -0.000970 -0.017186
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.000647 -0.017186
2021-02-23 00:00:00+00:00 -0.000323 -0.017186
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

You can see that the first TP step in the `BTCUSDT` column removed half of the position because
there are only two steps in the TP ladder. Then, an SL step was hit and removed 33.3% of the
initial position, as there are three steps in the SL ladder. The final step then removed the rest of
the position. But what if your intention is to exit according to the current position size and not
the initial one? If the position changes suddenly, the exit sizes will be redistributed based on
this change. This can be achieved by providing the `StopLadderMode.AdaptUniform` or
`StopLadderMode.AdaptWeighted` mode:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder="adaptuniform",
...     tp_stop=[0.1, 0.2],
...     sl_stop=[0.05, 0.1, 0.15]
... )
>>> pf.orders.stop_type.to_pd(mapping=True)
symbol                    BTCUSDT ETHUSDT
Open time                                
2021-02-18 00:00:00+00:00    None    None
2021-02-19 00:00:00+00:00    None    None
2021-02-20 00:00:00+00:00      TP      SL
2021-02-21 00:00:00+00:00    None    None
2021-02-22 00:00:00+00:00      SL      SL
2021-02-23 00:00:00+00:00      SL      SL
2021-02-24 00:00:00+00:00    None    None

>>> pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00 -0.000970 -0.017186
2021-02-21 00:00:00+00:00  0.000000  0.000000
2021-02-22 00:00:00+00:00 -0.000323 -0.017186
2021-02-23 00:00:00+00:00 -0.000323 -0.017186
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

Both SL steps in the first column now remove exactly `1 / 3` of the new position and act as if they
were created right after the position changed due to the first TP step.

Now, let's illustrate how to specify a different ladder for a different column. To do this, we need
to construct a two-dimensional array. Since some ladders may have fewer steps than others, we need
to pad them with NaN:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder=True,
...     tp_stop=np.column_stack((
...         np.array([0.1, 0.2, np.nan]),  # (1)!
...         np.array([0.01, 0.02, 0.03])  # (2)!
...     ))
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.051557
2021-02-19 00:00:00+00:00  0.00000 -0.017186
2021-02-20 00:00:00+00:00 -0.00097 -0.017186
2021-02-21 00:00:00+00:00  0.00000  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000
```

1. Gets applied on `BTCUSDT`.

2. Gets applied on `ETHUSDT`.

Padding would not be necessary if we provided the ladders as parameters:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     stop_ladder=True,
...     tp_stop=vbt.Param([
...         np.array([0.1, 0.2]),  # (1)!
...         np.array([0.01, 0.02, 0.03])  # (2)!
...     ])
... )
>>> pf.asset_flow
tp_stop                    array_0             array_1          
symbol                     BTCUSDT   ETHUSDT   BTCUSDT   ETHUSDT
Open time                                                       
2021-02-18 00:00:00+00:00  0.00194  0.051557  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.00000  0.000000 -0.000647 -0.017186
2021-02-20 00:00:00+00:00 -0.00097  0.000000 -0.000647 -0.017186
2021-02-21 00:00:00+00:00  0.00000  0.000000 -0.000647  0.000000
2021-02-22 00:00:00+00:00  0.00000  0.000000  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.00000  0.000000  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.00000  0.000000  0.000000  0.000000
```

1. Gets applied on both columns.

2. Gets applied on both columns.

Finally, let's discuss how to specify your own exit size for each ladder step. Since there are
no arguments for specifying the exit size, you need to modify it inside an adjustment or signal
function. Remember to use an adjustment function if you already have signal arrays, and a signal
function if you generate your signals dynamically. Inside the callback, you need to 1) retrieve the
current information record of the stop type for which you have defined the ladder, 2) select the exit
size corresponding to the current step (available via the record field `step`), and 3) write the exit
size to the record. Once the step is hit, your user-defined exit size will be used instead of the
default.

!!! note
    The default size and size type in any record are NaN and -1, respectively. Only once the step is
    hit will these be internally replaced by the calculated values. Therefore, you can check whether the
    ladder uses the default exit size by testing these fields against these values.

In the following example, we enter a trade with 6 units and then test two TP ladders:

1. Remove 3 units at the 10% level and the remaining 3 units at the 20% level.

2. Remove 1 unit at the 1% level, another 2 units at the 2% level, and the remaining 3 units at the 3% level.

```pycon
>>> @njit
... def adjust_func_nb(c, exit_size, exit_size_type):
...     tp_info = c.last_tp_info[c.col]
...     if vbt.pf_nb.is_stop_info_ladder_active_nb(tp_info):  # (1)!
...         if np.isnan(tp_info.exit_size):  # (2)!
...             tp_info.exit_size = vbt.pf_nb.select_nb(c, exit_size, i=tp_info.step)  # (3)!
...             tp_info.exit_size_type = vbt.pf_nb.select_nb(c, exit_size_type, i=tp_info.step)

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     adjust_func_nb=adjust_func_nb,
...     adjust_args=(vbt.Rep("exit_size"), vbt.Rep("exit_size_type")),  # (4)!
...     stop_ladder=True,
...     tp_stop=vbt.Param([
...         np.array([0.1, 0.2]),
...         np.array([0.01, 0.02, 0.03])
...     ], level=0),  # (5)!
...     broadcast_named_args=dict(
...         exit_size=vbt.BCO(  # (6)!
...             vbt.Param([
...                 np.array([3, 3]),
...                 np.array([1, 2, 3])
...             ], level=0, hide=True),  # (7)!
...             axis=1,
...             merge_kwargs=dict(  # (8)!
...                 reset_index="from_start", 
...                 fill_value=np.nan,
...             )
...         ),
...         exit_size_type=vbt.BCO(
...             vbt.pf_enums.SizeType.Amount,  # (9)!
...             axis=1,
...             merge_kwargs=dict(
...                 reset_index="from_start", 
...                 fill_value=-1,
...             )
...         )
...     ),
...     size=6,
...     init_cash="auto"
... )
>>> pf.asset_flow
tp_stop                   array_0         array_1        
symbol                    BTCUSDT ETHUSDT BTCUSDT ETHUSDT
Open time                                                
2021-02-18 00:00:00+00:00     6.0     6.0     6.0     6.0
2021-02-19 00:00:00+00:00     0.0     0.0    -1.0    -1.0
2021-02-20 00:00:00+00:00    -3.0     0.0    -2.0    -2.0
2021-02-21 00:00:00+00:00     0.0     0.0    -3.0     0.0
2021-02-22 00:00:00+00:00     0.0     0.0     0.0     0.0
2021-02-23 00:00:00+00:00     0.0     0.0     0.0     0.0
2021-02-24 00:00:00+00:00     0.0     0.0     0.0     0.0
```

1. Check whether the TP ladder is active using
[is_stop_info_ladder_active_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.is_stop_info_ladder_active_nb).

2. We want to override the exit size and type only once per step. If the exit size is not NaN, then it has
already been overridden.

3. Because the exit size and type arrays at least partially broadcast to the target shape, we can use
[select_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/iter_/#vectorbtpro.portfolio.nb.iter_.select_nb) by providing the current
step as `i`.

4. Exit size and type are passed to `broadcast_named_args`. These templates are then replaced by the actual
arrays once broadcasting is complete.

5. Specify the same level to avoid building the product between `tp_stop`, `exit_size`, and `exit_size_type`.
This way, the three parameters are treated as a single parameter.

6. Exit size and type must broadcast in the same way as our ladders, so we use
[BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO) to change the default behavior.

7. The exit size should be provided in the same manner as `tp_stop`. Additionally, we hide the parameter
from the final columns since it is redundant.

8. These keyword arguments are needed if parameter arrays have different shapes. In this case, smaller
arrays are padded with NaN to match the length of the longest array.

9. In our example, the size type is the same for all sizes, so we can use a single value here.

### Dynamic

We have learned that order-related arguments such as `size` serve as a facade for the backtesting process
with FS. They are designed to be anchored by time rather than signals, so we cannot change
them dynamically. This is demonstrated by the fact that any signal function (`signal_func_nb`) returns
only signals and nothing else. But what if a signal needs to have a size or other order information
different from the default?

Remember that VBT takes order information as arrays and processes them iteratively. It goes
through each row and column, reads the current element, and uses this element along with the returned
signals. In theory, if we override the current element in the signal function, VBT should pick it up
and use it as the new order information. The only question is how to access the respective array if the
context (`c`) does not list it. To do this, use templates! For example, by passing `vbt.Rep("size")` in
`signal_args`, the method will look for a variable named `size` in its template context and pass it to
the signal function. Conveniently, the template context contains all the information passed
to the method, including the order-related arguments.

However, there is a catch: to avoid using too much memory, VBT keeps most array-like arguments
as two-dimensional arrays with only one element and uses flexible indexing (see
[this](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/#flexible-indexing) and [this](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/#flexible-indexing))
to select that one element for each row and column. That means we either have to tell the broadcaster
to build the full array and then override each element (`arr[c.i, c.col] = ...`), which is wasteful but
could be useful if you also need to keep track of all values used previously.
Or, we can create an array with only one element per column and override only that element
(`arr[0, c.col] = ...`) so VBT uses flexible indexing.

Let's demonstrate both approaches by buying $10 worth of units when a long signal occurs and
selling $5 worth of units when a short signal occurs. Here is the first approach using a full-sized array:

```pycon
>>> @njit
... def signal_func_nb(c, long_signals, short_signals, size):
...     long_signal = vbt.pf_nb.select_nb(c, long_signals)
...     short_signal = vbt.pf_nb.select_nb(c, short_signals)
...     if long_signal:
...         size[c.i, c.col] = 10  # (1)!
...     if short_signal:
...         size[c.i, c.col] = 5
...     return long_signal, False, short_signal, False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("long_signals"),
...         vbt.Rep("short_signals"),
...         vbt.Rep("size")  # (2)!
...     ),
...     size=vbt.RepEval("np.full(wrapper.shape_2d, np.nan)"),  # (3)!
...     size_type="value",
...     accumulate=True,
...     broadcast_named_args=dict(
...         long_signals= pd.Series([X, O, O, O, X, O, O]),
...         short_signals=pd.Series([O, O, X, O, O, O, X]),
...     )
... )
>>> pf.orders.value.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     10.0     10.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00     -5.0     -5.0
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00     10.0     10.0
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00     -5.0     -5.0
```

1. Set the value under the current row and column to be picked by VBT.

2. Forward the `size` array we built below to the signal function using templates.

3. With this template, we can delay the creation of the array until the target shape is known (that is,
after broadcasting all arrays), which is available via `wrapper.shape`.

!!! example
    Can we access the filled array after the simulation? Yes! The trick is to create an empty dictionary,
    pass it to the template, and save the created array to the dictionary inside the template expression:

    ```pycon
    >>> memory = {}
    >>> pf = vbt.Portfolio.from_signals(
    ...     # ...
    ...     size=vbt.RepEval("""
    ...         size = np.full(wrapper.shape_2d, np.nan)
    ...         memory["size"] = size
    ...         return size
    ...         """,
    ...         context=dict(memory=memory),  # (1)!
    ...         context_merge_kwargs=dict(nested=False)  # (2)!
    ...     ),
    ...     # ...
    ... )
    >>> memory["size"]
    [[10. 10.]
      [nan nan]
      [ 5.  5.]
      [nan nan]
      [10. 10.]
      [nan nan]
      [ 5.  5.]]
    ```

    1. Everything passed as `context` will be available as a variable in the template.

    2. To avoid losing the reference to the `memory` dictionary, disable
    nested dictionary merging when the local context is merged with the global context.

Here is the second approach using a one-element array:

```pycon
>>> @njit
... def signal_func_nb(c, long_signals, short_signals, size):
...     long_signal = vbt.pf_nb.select_nb(c, long_signals)
...     short_signal = vbt.pf_nb.select_nb(c, short_signals)
...     if long_signal:
...         size[0, c.col] = 10  # (1)!
...     if short_signal:
...         size[0, c.col] = 5
...     return long_signal, False, short_signal, False

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(
...         vbt.Rep("long_signals"),
...         vbt.Rep("short_signals"),
...         vbt.Rep("size")
...     ),
...     size=vbt.RepEval("np.full((1, wrapper.shape_2d[1]), np.nan)"),  # (2)!
...     size_type="value",
...     accumulate=True,
...     broadcast_named_args=dict(
...         long_signals= pd.Series([X, O, O, O, X, O, O]),
...         short_signals=pd.Series([O, O, X, O, O, O, X]),
...     )
... )
>>> pf.orders.value.to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     10.0     10.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00     -5.0     -5.0
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00     10.0     10.0
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00     -5.0     -5.0
```

1. Since our array has only one row, VBT will pick its value for any row and column.
Thus, we need to override this exact element.

2. Unlike in the previous example, here we only need one row.

#### Entry laddering

To showcase the full power of this trick, we will create a custom entry ladder!
Whenever a user-defined entry signal is detected, it will be split into multiple smaller
entry signals. Each of these signals will be executed after a predefined number of bars and will add a
predefined percentage of resources to the current position. We will also demonstrate how to group
arguments using named tuples and define custom record arrays to hold complex temporary information.
Finally, we will show how to parameterize the ladder and test two completely different ladders in a
single simulation.

```pycon
>>> Signals = namedtuple("Signals", [  # (1)!
...     "entries",
...     "exits"
... ])
>>> Order = namedtuple("Order", [  # (2)!
...     "size",
...     "size_type"
... ])
>>> ladder_dt = np.dtype([  # (3)!
...     ("after_n_bars", int_),
...     ("exit_pct", float_)
... ])
>>> ladder_info_dt = np.dtype([  # (4)!
...     ("init_idx", int_),
...     ("step", int_)
... ], align=True)

>>> @njit  # (5)!
... def signal_func_nb(c, signals, order, ladders, last_ladder_info):  # (6)!
...     is_entry = vbt.pf_nb.select_nb(c, signals.entries)
...     is_exit = vbt.pf_nb.select_nb(c, signals.exits)
...     position_now = c.last_position[c.col]
...     ladder_info = last_ladder_info[c.col]  # (7)!
...     
...     if position_now > 0 and is_exit:  # (8)!
...         ladder_info["init_idx"] = -1  # (9)!
...         ladder_info["step"] = -1
...         order.size[c.i, c.col] = np.inf  # (10)!
...         order.size_type[c.i, c.col] = vbt.pf_enums.SizeType.Amount
...         return False, True, False, False  # (11)!
...     
...     if is_entry:  # (12)!
...         ladder_info["init_idx"] = c.i
...         ladder_info["step"] = 0
...         return False, False, False, False
...         
...     step = ladder_info["step"]
...     if step != -1 and step < ladders.shape[0]:  # (13)!
...         after_n_bars = ladders.after_n_bars[step, c.col]  # (14)!
...         if after_n_bars != -1 and c.i >= ladder_info["init_idx"] + after_n_bars:  # (15)!
...             ladder_info["step"] = step + 1  # (16)!
...             order.size[c.i, c.col] = ladders.exit_pct[step, c.col]  # (17)!
...             order.size_type[c.i, c.col] = vbt.pf_enums.SizeType.Percent
...             return True, False, False, False  # (18)!
...     
...     return False, False, False, False  # (19)!

>>> pf = vbt.Portfolio.from_signals(
...     sub_data.select("BTCUSDT"),
...     signal_func_nb=signal_func_nb,
...     signal_args=(  # (20)!
...         vbt.RepEval(
...             "Signals(entries, exits)",
...             context=dict(Signals=Signals)  # (21)!
...         ),
...         vbt.RepEval(
...             "Order(size, size_type)",
...             context=dict(Order=Order)
...         ),
...         vbt.Rep("ladders"),  # (22)!
...         vbt.RepEval(  # (23)!
...             "np.full(wrapper.shape_2d[1], np.array((-1, -1), ladder_info_dt))",
...             context=dict(ladder_info_dt=ladder_info_dt)
...         )
...     ),
...     size=vbt.RepEval("np.full(wrapper.shape_2d, np.nan)"),  # (24)!
...     size_type=vbt.RepEval("np.full(wrapper.shape_2d, -1)"),
...     accumulate=True,  # (25)!
...     broadcast_named_args=dict(
...         entries=pd.Series([X, O, O, O, O, O, O]),
...         exits=  pd.Series([O, O, O, O, O, O, O]),
...         ladders=vbt.BCO(  # (26)!
...             vbt.Param([  # (27)!
...                 np.array([  # (28)!
...                     (2, 1/4),  # (29)!
...                     (3, 1/3),
...                     (4, 1/2),
...                     (5, 1/1)
...                 ], dtype=ladder_dt),
...                 np.array([
...                     (2, 1/2),
...                     (4, 1/1),
...                 ], dtype=ladder_dt),
...             ]),
...             axis=1,  # (30)!
...             merge_kwargs=dict(  # (31)!
...                 reset_index="from_start", 
...                 fill_value=np.array((-1, np.nan), dtype=ladder_dt),
...             )
...         )
...     ),
... )
>>> pf.asset_flow
ladder                      array_0   array_1
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00  0.000448  0.000895
2021-02-21 00:00:00+00:00  0.000435  0.000000
2021-02-22 00:00:00+00:00  0.000462  0.000924
2021-02-23 00:00:00+00:00  0.000511  0.000000
2021-02-24 00:00:00+00:00  0.000000  0.000000
```

1. Create a [namedtuple](https://realpython.com/python-namedtuple/) to store multiple arrays in a single
container. Passing several arrays to the signal function can take up a lot of space and make the code
harder to read. This named tuple holds two signal arrays: entries and exits.

2. The second named tuple holds order-related arrays, which are intended to be
overridden in the signal function: size and size type. You can add more fields if needed.

3. Ladders are stored in a two-dimensional array where each column represents a ladder and each
row corresponds to a step within that ladder. Each step specifies how many bars must pass and how much to
order (essentially as a tuple). Regular NumPy arrays cannot store this information, but structured arrays can.
Here, we define the data type for such an array.

4. We also need to track the index of the user-defined entry and the current step being processed.
We need this per ladder (per column), so we create a one-dimensional structured array.

5. If you have finished the function, remember to put it in a separate cell to avoid re-compilation.
Also, do not cache it because it might break if you modify any of the named tuples.

6. Our function takes the context, as well as the initialized named tuples and structured arrays we defined above.

7. Similar to built-in information arrays like `last_position` and `last_sl_info`, our latest ladder
information is also structured per column.

8. If we encounter a user-defined exit signal, we close out the position.

9. Reset the temporary information to deactivate our laddering logic in the upcoming bars.

10. Override the order-related information for this signal; here, we go all out.

11. Long exit.

12. If we encounter a user-defined entry signal, record the current bar index in the temporary information,
set the current step to zero to activate our laddering logic, and return no signal to skip to the next bar.

13. Check whether the ladder is active and the current step is less than the total number of steps.

14. Remember, ladders are stored in a two-dimensional array with one ladder per column. The first axis
represents steps and the second axis represents columns.

15. Next, check whether the condition for the current step is met,
meaning whether the current bar index is after the target bar index. But first, check
if the value is defined. This is important because ladders may have different numbers of steps.

16. Increment the current step.

17. Execute the signal by ordering the requested percentage of available resources.

18. Long entry.

19. If no action occurred during this bar, return no signal.

20. Use templates to construct named tuples and record arrays. The arrays defined in
`broadcast_named_args` will be automatically available to use in template expressions.

21. We need to provide the classes through the context, since they are not automatically available in expressions.

22. We define ladders in `broadcast_named_args` to broadcast it along with the data.

23. Create a structured NumPy array to hold temporary information per ladder (per column).
Here, `np.array((-1, -1), ladder_info_dt)` is a single value that instructs NumPy to use the data
type `ladder_info_dt` and fill both fields with -1.

24. Size and size type are built-in arguments that we want to override, so make them match the full shape.

25. Do not forget to enable accumulation to allow gradual increases in the position.

26. Use [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO) to control broadcasting
of a single argument.

27. Use [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) to test multiple values
(such as different ladder arrays).

28. Any structured array can be initialized using regular tuples.

29. For example, in the first step, once we are two bars away from the user-defined entry signal,
place an order for 25% of the available cash balance.

30. Remember, the first axis represents steps, not bars. Therefore, the array is not broadcastable along
the time axis (`axis=0`), only along the column axis (`axis=1`).

31. What if one array has fewer steps than another? Internally, both arrays will
be stacked along columns, but stacking will not work if they have different shapes. Here, we specify that
whenever an array has too few rows, it should be padded using `fill_value`.

The strategy has successfully split the entry signal into several smaller entry orders distributed over time.
The code above is a great template for defining dynamic signal strategies of any complexity.

## Grouping

Until now, we have processed the columns `BTCUSDT` and `ETHUSDT` separately, meaning the second
column is always processed after the first column. However, you may want to include both
columns in the same portfolio for analysis. By introducing grouping, you can combine multiple columns
to be treated as a single column (read more [here](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/#grouping_1)).
In this context, it is important to distinguish between grouping after the simulation and
grouping before the simulation.

### After simulation

When your main scenario is to simulate columns __separately__ and then analyze them together,
you can group the columns after the simulation. This can be done when querying a specific metric
of interest:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
... )
>>> pf.get_value(group_by=True)
Open time
2021-02-18 00:00:00+00:00    200.000000
2021-02-19 00:00:00+00:00    209.238037
2021-02-20 00:00:00+00:00    206.946937
2021-02-21 00:00:00+00:00    211.045749
2021-02-22 00:00:00+00:00    211.045749
2021-02-23 00:00:00+00:00    232.943589
2021-02-24 00:00:00+00:00    228.775332
Freq: D, Name: group, dtype: float64
```

Or by updating the entire portfolio:

```pycon
>>> grouped_pf = pf.replace(wrapper=pf.wrapper.replace(group_by=True))  # (1)!
>>> grouped_pf.value
Open time
2021-02-18 00:00:00+00:00    200.000000
2021-02-19 00:00:00+00:00    209.238037
2021-02-20 00:00:00+00:00    206.946937
2021-02-21 00:00:00+00:00    211.045749
2021-02-22 00:00:00+00:00    211.045749
2021-02-23 00:00:00+00:00    232.943589
2021-02-24 00:00:00+00:00    228.775332
Freq: D, Name: group, dtype: float64
```

1. The group-by instruction is part of the portfolio's wrapper.

In this case, the portfolio aggregates various metrics, such as the initial capital and time series
like cash flows, along the column axis. However, some time series cannot be aggregated,
such as asset flows:

```pycon
>>> grouped_pf.asset_flow
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.001940  0.051557
2021-02-19 00:00:00+00:00  0.000000  0.000000
2021-02-20 00:00:00+00:00  0.000000  0.000000
2021-02-21 00:00:00+00:00 -0.001940 -0.051557
2021-02-22 00:00:00+00:00 -0.002059 -0.056080
2021-02-23 00:00:00+00:00  0.000000  0.000000
2021-02-24 00:00:00+00:00  0.002059  0.056080
```

You can disable grouping for individual metrics by passing `group_by=False`. However, you cannot
introduce cash sharing after the simulation, as it must be applied during the simulation to
have any real impact on the trading logic.

### Before simulation

Adding a group-by instruction before the simulation may or may not affect the simulation results.
For example, grouping with cash sharing will always affect the outcome, while grouping without cash
sharing will only affect the simulation when FS is run in flexible mode (that is, when any default
callbacks are overridden). In this case, columns are grouped so users can leverage grouping in
the callbacks, while all calculations are still performed on a per-column basis as if there were no
grouping. Let's use this feature to limit the number of active positions to just one in each group:

```pycon
>>> @njit
... def signal_func_nb(c, entries, exits):
...     is_entry = vbt.pf_nb.select_nb(c, entries)
...     is_exit = vbt.pf_nb.select_nb(c, exits)
...     other_in_position = False
...     for col in range(c.from_col, c.to_col):  # (1)!
...         if col != c.col and c.last_position[col] != 0:
...             other_in_position = True
...             break
...     if other_in_position:  # (2)!
...         return False, False, False, False
...     return is_entry, is_exit, False, False  # (3)!

>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     signal_func_nb=signal_func_nb,
...     signal_args=(vbt.Rep("entries"), vbt.Rep("exits")),
...     broadcast_named_args=dict(
...         entries=pd.DataFrame({
...             0: [X, O, O, O, O, O, X], 
...             1: [O, X, O, X, O, O, O]
...         }), 
...         exits=pd.DataFrame({
...             0: [O, O, X, O, X, O, O], 
...             1: [O, O, O, O, O, X, O]
...         })
...     ),
...     group_by=True  # (4)!
... )
>>> pf.asset_flow
symbol                     BTCUSDT   ETHUSDT
Open time                                   
2021-02-18 00:00:00+00:00  0.00194  0.000000
2021-02-19 00:00:00+00:00  0.00000  0.000000
2021-02-20 00:00:00+00:00 -0.00194  0.000000
2021-02-21 00:00:00+00:00  0.00000  0.051719
2021-02-22 00:00:00+00:00  0.00000  0.000000
2021-02-23 00:00:00+00:00  0.00000 -0.051719
2021-02-24 00:00:00+00:00  0.00218  0.000000
```

1. Loop through the columns in this group and check whether there are open positions apart from this one.

2. If true, return no signal.

3. If false, return the signal.

4. Group all columns together. For multiple groups, use `vbt.ExceptLevel("symbol")`.

The first signal in the column `ETHUSDT` could not execute because there was already a position
open in column `BTCUSDT`. However, once the position was closed, the second signal was able to go through.

#### Sorting

Orders are only sorted in groups with cash sharing and in two situations: when orders in different
columns must be executed in different bar zones (that is, at different times) and need to be sorted
by time, and when orders are set to be sorted by order value as part of the automatic call sequence
enabled with `call_seq="auto"`. These two cases cannot be combined.

Let's look at the first case, where the first column is executed using the closing price and the
second column is executed using the opening price:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price=[["close", "open"]],  # (1)!
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True  # (2)!
... )
>>> pf.orders.get_value(group_by=False).to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     50.0     50.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. If you want to provide information per column, build a two-dimensional array with exactly one row.
(Any array-like object is converted to a NumPy array, so a list works as well.)

2. Enable cash sharing.

As you can see, the second column was executed first and ended up using half of the available capital.
If both assets were executed in the same bar zone, they would not be sorted and would be executed
in order from the left column to the right:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="close",
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True
... )
>>> pf.orders.get_value(group_by=False).to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00    100.0      NaN
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

Even though the second column requested $50 worth of assets, the simulator could not fulfill the
request because the default initial capital is $100, which was fully allocated to the first column.

To sort by order value, use the automatic call sequence:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price="close",
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True,
...     call_seq="auto"  # (1)!
... )
>>> pf.orders.get_value(group_by=False).to_pd()
symbol                     BTCUSDT  ETHUSDT
Open time                                  
2021-02-18 00:00:00+00:00     50.0     50.0
2021-02-19 00:00:00+00:00      NaN      NaN
2021-02-20 00:00:00+00:00      NaN      NaN
2021-02-21 00:00:00+00:00      NaN      NaN
2021-02-22 00:00:00+00:00      NaN      NaN
2021-02-23 00:00:00+00:00      NaN      NaN
2021-02-24 00:00:00+00:00      NaN      NaN
```

1. This enables the automatic call sequence.

With this, the second column was executed first. However, if you attempt to sort by order value when
columns need to execute orders at different times, the simulation will fail:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price=[["close", "open"]],
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True,
...     call_seq="auto"
... )
ValueError: Cannot sort orders by value if they are executed at different times
```

This makes sense if you think about it: if the simulator tries to sort both columns by value,
the second column would come first, but that is not possible if it should execute at the start of
the bar. Recall that each bar is divided into three zones: open, middle, and close. The only way
to sort orders by value is to use either the opening or the closing point of the bar. Sorting
cannot use the middle of the bar because its events are unknown. In VBT, you indicate where in
the bar an order should execute by setting the price to `-np.inf` (bar open) or `np.inf` (bar close).
Passing custom prices is considered to happen in the middle of the bar, which causes the operation to fail:

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=pd.Series([X, O, O, O, O, O, O]),
...     price=sub_data.close,
...     size=[[100, 50]],
...     size_type="value",
...     group_by=True,
...     cash_sharing=True,
...     call_seq="auto"
... )
ValueError: Cannot sort orders by value if they are executed at different times
```

Even if you know the provided array represents the closing price, VBT is not aware of this.
Therefore, always use an option from [PriceType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceType)
when the automatic call sequence should be enabled. If you are using stop orders, set both
`PriceType.Close` and `StopExitPrice.Close` to execute all orders at the closing price, such as when
rebalancing. Limit orders, however, are never executed at the closing price, so using them with
the automatic call sequence is generally not possible.

## Custom outputs

After simulating the portfolio and obtaining your new portfolio instance, the first thing you
usually do is run various statistics. Since most statistics rely on returns, the portfolio
instance may sometimes need to recalculate returns multiple times. This process can be slow,
as it requires translating order records into asset flows, cash flows, assets, cash, asset value,
portfolio value, and finally, returns. These operations are called "reconstructions" because they
derive a specific property of the simulation after it is complete. As a result, performance
analysis often becomes the main bottleneck in a typical backtesting workflow. Fortunately, there is
a `save_returns` argument that, when enabled, pre-calculates returns during simulation and makes
them available to all metrics that need them (see more [here](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/#filling-returns)):

```pycon
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
...     save_returns=True
... )
>>> pf.returns
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.084446  0.007935
2021-02-20 00:00:00+00:00 -0.001159 -0.021483
2021-02-21 00:00:00+00:00  0.028069  0.010732
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.096079  0.112338
2021-02-24 00:00:00+00:00 -0.013245 -0.023012
```

To verify these values, compare them to the reconstructed returns:

```pycon
>>> pf.get_returns()  # (1)!
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.084446  0.007935
2021-02-20 00:00:00+00:00 -0.001159 -0.021483
2021-02-21 00:00:00+00:00  0.028069  0.010732
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.096079  0.112338
2021-02-24 00:00:00+00:00 -0.013245 -0.023012
```

1. Every attribute with `get_` triggers a reconstruction.

Internally, FS creates a named tuple with an uninitialized returns array and gradually
fills it at the end of each bar. Once the simulation is finished, this tuple is attached
to the portfolio instance under the attribute
[Portfolio.in_outputs](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.in_outputs):

```pycon
>>> pf.in_outputs
FSInOutputs(returns=array([[ 0.        ,  0.        ],
                            [ 0.08444579,  0.00793458],
                            [-0.00115927, -0.02148338],
                            [ 0.02806853,  0.01073183],
                            [ 0.        ,  0.        ],
                            [ 0.09607864,  0.11233812],
                            [-0.01324464, -0.02301153]]))
```

Since most outputs from the simulator are in NumPy format, you can use the method
[Portfolio.get_in_output](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_in_output)
to wrap any array with Pandas:

```pycon
>>> pf.get_in_output("returns")
symbol                      BTCUSDT   ETHUSDT
Open time                                    
2021-02-18 00:00:00+00:00  0.000000  0.000000
2021-02-19 00:00:00+00:00  0.084446  0.007935
2021-02-20 00:00:00+00:00 -0.001159 -0.021483
2021-02-21 00:00:00+00:00  0.028069  0.010732
2021-02-22 00:00:00+00:00  0.000000  0.000000
2021-02-23 00:00:00+00:00  0.096079  0.112338
2021-02-24 00:00:00+00:00 -0.013245 -0.023012
```

There is one catch: the `save_returns` argument is available only in the fixed simulation mode,
which means that none of the default callbacks have been overridden. As soon as you switch to
flexible mode, you must define your own in-place output tuple using the `in_outputs` argument and fill
it with any information you want, usually in the segment post-processing function
`post_segment_func`. This callback receives the context
[SignalSegmentContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalSegmentContext),
which is similar to
[SignalContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext)
but does not include information on the current column, since a segment can include multiple columns
within a particular group at a given row. Therefore, you need to iterate over the columns in the
current segment manually.

For demonstration, here is a post-segment callback that fills the portfolio returns in the same
way as the fixed FS mode does. Additionally, it stores the total portfolio return per column to
avoid reconstructing it during post-analysis:

```pycon
>>> @njit
... def post_segment_func_nb(c):
...     returns = c.in_outputs.returns  # (1)!
...     total_return = c.in_outputs.total_return
...     i = c.i
...     g = c.group
...     if c.cash_sharing:  # (2)!
...         returns[i, g] = c.last_return[g]
...         total_return[g] = c.last_value[g] / c.init_cash[g] - 1
...     else:  # (3)!
...         for col in range(c.from_col, c.to_col):  # (4)!
...             returns[i, col] = c.last_return[col]
...             total_return[col] = c.last_value[col] / c.init_cash[col] - 1
            
>>> pf = vbt.Portfolio.from_signals(
...     sub_data,
...     entries=      pd.Series([X, O, O, O, O, O, O]),
...     exits=        pd.Series([O, O, O, X, O, O, O]),
...     short_entries=pd.Series([O, O, O, O, X, O, O]),
...     short_exits=  pd.Series([O, O, O, O, O, O, X]),
...     post_segment_func_nb=post_segment_func_nb,  # (5)!
...     in_outputs=dict(  # (6)!
...         returns=vbt.RepEval(  # (7)!
...             "np.full((target_shape[0], len(cs_group_lens)), np.nan)"
...         ),
...         total_return=vbt.RepEval(
...             "np.full(len(cs_group_lens), np.nan)"
...         )
...     )
... )
>>> pd.testing.assert_frame_equal(  # (8)!
...     pf.get_in_output("returns"),
...     pf.get_returns()
... )
>>> pd.testing.assert_series_equal(
...     pf.get_in_output("total_return"),
...     pf.get_total_return()
... )
```

1. [SignalSegmentContext.in_outputs](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalSegmentContext.in_outputs)
contains exactly the in-place output tuple that we provided.

2. If cash sharing is enabled, returns must be computed per group.

3. If cash sharing is disabled, returns must be computed per column.

4. Iterate over each column in the current group.

5. Overriding this callback is enough to switch to flexible mode.

6. The in-place output tuple can be provided as either a named tuple or a dict. If provided as a dict,
it will be automatically transformed into a named tuple by the method.

7. Use expression evaluation templates to build arrays only after the target shape is established.
Use `len(cs_group_lens)` to get the number of groups when cash sharing is enabled, and the number
of columns when cash sharing is disabled.

8. Check the pre-computed and reconstructed arrays for equality.

The most impressive aspect of the approach above is that any pre-computed metric found
among the attributes of the [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) class
is automatically used by any built-in property or method that requires it, such as
[Portfolio.stats](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.stats). The only limitation
is that you cannot change the grouping after the array has been created; when that happens,
the reconstructed version of the metric will be returned instead.

## Summary

Backtesting should not be complicated! One of the most crucial factors in successful trading is having
proper entry and exit timing. By parametrizing both the timing and direction of our trades, we can
theoretically achieve the worst and best possible returns in any market. Thus, representing
a trading strategy as a permutation of buy, sell, short-sell, and short-cover signals is
almost "[Turing-complete](https://en.wikipedia.org/wiki/Turing_completeness)" in this sense.
Reducing complex strategies to such a simple signal set has another advantage: it enables and
standardizes backtesting for many trading strategies using the same code base. The method
[Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
not only makes this possible, but also encourages you to adjust the simulation to define
your own trading logic or to customize the default simulation behavior, making it a combination
of both vectorized and event-driven approaches. However, not everything should be represented
using signals: strategies that heavily rely on order details, such as rebalancing
strategies, are best implemented using other methods. Fortunately, VBT provides solutions for
these problems as well :ninja:

*[FO]: From-orders simulation method
*[FS]: From-signals simulation method
*[SL]: Stop loss
*[TSL]: Trailing stop loss
*[TTP]: Trailing take profit
*[TP]: Take profit

[:material-language-python: Python code](https://vectorbt.pro/pvt_41531c19/assets/jupytext/documentation/portfolio/from-signals.py.txt){ .md-button target="blank_" }

---
link: https://vectorbt.pro/pvt_41531c19/documentation/to-be-continued/
parent: https://vectorbt.pro/pvt_41531c19/documentation/
children:
  - https://vectorbt.pro/pvt_41531c19/documentation/to-be-continued/#to-be-continued
name: To be continued...
type: page
---

Documentation is a work in progress

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/overview/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Overview
type: page
---

# Overview

This repository contains concise examples and links to useful VBT recipes.
These examples are streamlined, condensed, and crafted to be friendly for new users.
In-line examples are included wherever possible to enhance tutorials, documentation,
and the API. We encourage users to contribute to this documentation.

<div class="grid cards" markdown>

- [:material-table-multiple:{ .overview }__Arrays__ :material-alert-decagram:{ .mdx-pulse title="Recently added" }](https://vectorbt.pro/pvt_41531c19/cookbook/arrays/)

- [:material-tape-measure:{ .overview }__Benchmarking__](https://vectorbt.pro/pvt_41531c19/cookbook/benchmarking/)

- [:material-cached:{ .overview }__Caching__](https://vectorbt.pro/pvt_41531c19/cookbook/caching/)

- [:material-speedometer:{ .overview }__Compilation__](https://vectorbt.pro/pvt_41531c19/cookbook/compilation/)

- [:material-cog:{ .overview }__Configuration__](https://vectorbt.pro/pvt_41531c19/cookbook/configuration/)

- [:material-check-all:{ .overview }__Cross-validation__](https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/)

- [:material-database:{ .overview }__Data__](https://vectorbt.pro/pvt_41531c19/cookbook/data/)

- [:material-calendar:{ .overview }__Datetime__](https://vectorbt.pro/pvt_41531c19/cookbook/datetime/)

- [:material-file-tree:{ .overview }__Discovery__](https://vectorbt.pro/pvt_41531c19/cookbook/discovery/)

- [:material-button-pointer:{ .overview }__Indexing__](https://vectorbt.pro/pvt_41531c19/cookbook/indexing/)

- [:material-chart-timeline-variant:{ .overview }__Indicators__](https://vectorbt.pro/pvt_41531c19/cookbook/indicators/)

- [:material-message-text-outline:{ .overview }__Knowledge__ :material-alert-decagram:{ .mdx-pulse title="Recently added" }](https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/)

- [:material-magnify-expand:{ .overview }__Optimization__](https://vectorbt.pro/pvt_41531c19/cookbook/optimization/)

- [:material-tray-arrow-down:{ .overview }__Persistence__](https://vectorbt.pro/pvt_41531c19/cookbook/persistence/)

- [:material-chart-bubble:{ .overview }__Plotting__](https://vectorbt.pro/pvt_41531c19/cookbook/plotting/)

- [:material-chart-areaspline:{ .overview }__Portfolio__](https://vectorbt.pro/pvt_41531c19/cookbook/portfolio/)

- [:material-broadcast:{ .overview }__Signals__](https://vectorbt.pro/pvt_41531c19/cookbook/signals/)

</div>

!!! info
    ```python title="Import required by code examples"
    from vectorbtpro import *  # (1)!
    ```

    1. To see what is imported, call `whats_imported()`.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
children:
  - https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#arrays
name: Arrays
type: page
icon: material-table-multiple
---

Recipes for working with arrays in VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#displaying
parent: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#arrays
name: Displaying
type: heading 2
---

## Displaying

Any array, whether it is a NumPy array, Pandas object, or regular list, can be displayed as a table
using [ptable](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.ptable), regardless of its size.
When called in an IPython environment such as Jupyter Lab, the table becomes interactive.

**Print out an array in various ways**

```python
vbt.ptable(df)  # (1)!
vbt.ptable(df, ipython=False)  # (2)!
vbt.ptable(df, ipython=False, tabulate=False)  # (3)!
```

1. Selects a table format based on the current environment.

2. Prints the table as a string formatted by [`tabulate`](https://github.com/astanin/python-tabulate).

3. Prints the table as a string formatted by Pandas.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#wrapper
parent: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#arrays
name: Wrapper
type: heading 2
---

## Wrapper

A wrapper can be extracted from any array-like object using
[ArrayWrapper.from_obj](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.from_obj).

**Extract the wrapper from various objects**

```python
wrapper = data.symbol_wrapper  # (1)!
wrapper = pf.wrapper  # (2)!
wrapper = df.vbt.wrapper  # (3)!

wrapper = vbt.ArrayWrapper.from_obj(sr)  # (4)!
```

1. `data.wrapper` returns an OHLC wrapper.

2. Most VBT objects have a `wrapper` attribute.

3. The wrapper for a Pandas Series or DataFrame can be accessed through the accessor.

4. Lets VBT extract the wrapper.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

An empty Pandas array can be created with
[ArrayWrapper.fill](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.fill).

**Create an empty array with the same shape, index, and columns as in another array**

```python
new_float_df = wrapper.fill(np.nan)  # (1)!
new_bool_df = wrapper.fill(False)  # (2)!
new_int_df = wrapper.fill(-1)  # (3)!
```

1. Creates a floating-point array.

2. Creates a boolean array.

3. Creates an integer or categorical array.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

A NumPy array can be wrapped with a Pandas Series or DataFrame using
[ArrayWrapper.wrap](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap).

**Convert NumPy array to Pandas**

```python
df = wrapper.wrap(arr)
```

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#product
parent: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#arrays
name: Product
type: heading 2
---

## Product

The product of multiple DataFrames can be achieved using the accessor method
[BaseAccessor.x](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.x).
This method can be called either as an instance method or as a class method.

**Cross-join columns of multiple DataFrames**

```python
new_df1, new_df2 = df1.vbt.x(df2)  # (1)!
new_df1, new_df2, new_df3 = df1.vbt.x(df2, df3)  # (2)!
new_dfs = vbt.pd_acc.x(*dfs)  # (3)!
```

1. For two DataFrames.

2. For three DataFrames.

3. For a variable number of DataFrames.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#splitting
parent: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#arrays
name: Splitting
type: heading 2
tags:
  - recently_added
---

## Splitting

You can build and use a [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter)
instance to split arrays into smaller arrays by time (not to be confused with the [chunking](#chunking) functionality).
This allows for more granular data manipulation and analysis. To automate this process, you can fire
the process directly from the array using [BaseAccessor.split](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.split),
which distributes arguments between splitting and [taking](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take).
When passing arguments, these methods will determine the split strategy based on the input.

**Split an array**

```python
splits = df.vbt.split(length=30)  # (1)!
splits = df.vbt.split(n=5, length="optimize")  # (2)!
splits = df.vbt.split(min_length=30, offset=30)  # (3)!
splits = df.vbt.split(min_length=30, n=5)  # (4)!
splits = df.vbt.split(every="W", lookback_period="2W")  # (5)!
splits = df.vbt.split(by="Y")  # (6)!
splits = df.vbt.split(min_length=30, n=5, splitter="from_n_random")  # (7)!

results = your_func(splits)  # (8)!

# ______________________________________________________________

splits = df.vbt.split(every="W", fixed_start=True, into=None)  # (9)!
splits = df.vbt.split(  # (10)!
    splitter="from_ranges",
    splitter_kwargs=dict(every="W", fixed_start=True),
    take_kwargs=dict(into=None),
)

results = splits.apply(your_func)  # (11)!
```

1. Uses [Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling).

2. Uses [Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling).

3. Uses [Splitter.from_expanding](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_expanding).

4. Uses [Splitter.from_n_expanding](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_expanding).

5. Uses [Splitter.from_ranges](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_ranges).

6. Uses [Splitter.from_grouper](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_grouper).

7. Specify the splitting strategy manually.

8. Since the call returned a single DataFrame, you can apply your function directly to it.

9. Arguments related to [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
are recognized automatically. Here, return the splits as a Series of DataFrames rather than a stacked DataFrame using `into`.

10. Same as above but annotates each argument to avoid guessing.

11. Since the call returned a Series of DataFrames, you can use [pd.Series.apply](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.apply.html) to apply your function to each DataFrame.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Instead of using two operations separately (i.e., taking and applying), you can use the
[BaseAccessor.split_apply](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.split_apply)
method to achieve the same result in one go, which distributes arguments between splitting and
[applying](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply).

**Split an array and apply a function to each split**

```python
results = df.vbt.split_apply(your_func, length=30)  # (1)!
results = df.vbt.split_apply(your_func, n=5, length="optimize")  # (2)!

# (3)!

# ______________________________________________________________

results = df.vbt.split_apply(your_func, every="W", fixed_start=True, merge_func="column_stack")  # (4)!
results = df.vbt.split_apply(  # (5)!
    your_func,
    splitter="from_ranges",
    splitter_kwargs=dict(every="W", fixed_start=True),
    apply_kwargs=dict(merge_func="column_stack"),
)
```

1. Uses [Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling).

2. Uses [Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling).

3. See examples above for other strategies.

4. Arguments related to [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply)
are recognized automatically. Here, stack the results as columns into a single DataFrame.

5. Same as above but annotates each argument to avoid guessing.

!!! tip
    Both methods can be accessed from most VBT objects, such as 
    [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#chunking
parent: https://vectorbt.pro/pvt_41531c19/cookbook/arrays/#arrays
name: Chunking
type: heading 2
tags:
  - recently_added
---

## Chunking

Chunking is a specific type of splitting that divides the data into fixed-size chunks.
This is useful when you want to process the data in smaller, manageable pieces.
You can use the [BaseAccessor.chunk](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.chunk)
method to achieve this.

**Chunk an array**

```python
chunks = df.vbt.chunk()  # (1)!
chunks = sr.vbt.chunk(n_chunks=10)  # (2)!
chunks = df.vbt.chunk(n_chunks=10)  # (3)!
chunks = df.vbt.chunk(n_chunks=10, axis=0)  # (4)!
chunks = df.vbt.chunk(chunk_len=50)  # (5)!
chunk_meta, chunks = df.vbt.chunk(chunk_len=50, return_chunk_meta=True)  # (6)!

chunk_meta = [vbt.ChunkMeta(start=0, end=10), vbt.ChunkMeta(start=20, end=30), ...]
chunks = df.vbt.chunk(chunk_meta=chunk_meta)  # (7)!

tasks = []
for chunk in chunks:
    tasks.append(vbt.Task(your_func, chunk))
results = vbt.execute(tasks, engine="threadpool")  # (8)!
```

1. Uses default parameters to create chunks of the same size as the number of CPU cores.

2. Splits a Series into 10 chunks along rows.

3. Splits a DataFrame into 10 chunks along columns (`axis=1` is default for DataFrames).

4. Splits a DataFrame into 10 chunks along rows.

5. Splits a DataFrame into chunks of length 50.

6. Returns chunk metadata alongside the chunks.

7. Uses custom chunk metadata to create chunks.

8. Uses parallel execution to apply a function to each chunk.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To chunk an array and apply a function to each chunk in one go, you can use the
[BaseAccessor.chunk_apply](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.chunk_apply)
method; this method distributes arguments between chunking and applying, but the function isn't
applied right away. Instead, a list of tasks is created, which is then executed using
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute).

**Chunk an array and apply a function to each chunk**

```python
results = df.vbt.chunk(your_func)  # (1)!
results = sr.vbt.chunk(your_func, n_chunks=10)  # (2)!

# (3)!

results = df.vbt.chunk(your_func, n_chunks=10, *args, **kwargs)  # (4)!
results = df.vbt.chunk(your_func, chunk_kwargs=dict(n_chunks=10), *args, **kwargs)  # (5)!
results = sr.vbt.chunk(your_func, n_chunks=10, execute_kwargs=dict(engine="threadpool"))  # (6)!
```

1. Uses default parameters to create chunks of the same size as the number of CPU cores,
and applies a function to each chunk.

2. Splits a Series into 10 chunks along rows and applies a function to each chunk.

3. See examples above for other strategies.

4. Applies a function to each chunk, passing additional arguments and keyword arguments to the function.

5. Same as above but annotate each argument to avoid guessing.

6. Uses parallel execution to apply a function to each chunk.

!!! tip
    Both methods can be accessed from most VBT objects, such as 
    [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/benchmarking/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Benchmarking
type: page
icon: material-tape-measure
---

# Benchmarking

To measure the execution time of a code block by running it __only once__, use the
[Timer](https://vectorbt.pro/pvt_41531c19/api/utils/profiling/#vectorbtpro.utils.profiling.Timer).

**Measure execution time by running once**

```python
with vbt.Timer() as timer:
    my_pipeline()
  
print(timer.elapsed())
```

!!! note
    The code block may depend on Numba functions that need to be compiled first. To exclude
    any compilation time from the estimate (recommended since compilation may take up to a minute
    while the code block may execute in milliseconds), dry-run the code block.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Another way is to run a code block multiple times and evaluate a statistic, such as the shortest 
average execution time. This is easy to do using the [timeit](https://docs.python.org/3/library/timeit.html)
module and the corresponding vectorbtpro function, which returns the result in a human-readable format.
The benefit of this approach is that it effectively ignores any compilation overhead.

**Measure execution time by running multiple times**

```python
print(vbt.timeit(my_pipeline))
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

There is also a profiling tool for measuring peak memory usage:
[MemTracer](https://vectorbt.pro/pvt_41531c19/api/utils/profiling/#vectorbtpro.utils.profiling.MemTracer), which helps determine
the approximate size of all objects generated when running a code block.

**Measure peak memory usage by running once**

```python
with vbt.MemTracer() as tracer:
    my_pipeline()
  
print(tracer.peak_usage())
```

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/caching/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Caching
type: page
icon: material-cached
---

# Caching

When performing a high-level task repeatedly (such as during parameter optimization), it is recommended
to occasionally clear the cache using [clear_cache](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.clear_cache)
and collect memory garbage. This helps prevent RAM consumption from increasing due to cached and dead
objects.

**Clear cache and collect garbage once every 1000 iterations**

```python
for i in range(1_000_000):
    ...  # (1)!
    
    if i != 0 and i % 1000 == 0:
        vbt.flush()  # (2)!
```

1. Place your code here.

2. Equivalent to calling `vbt.clear_cache()` and `vbt.collect_garbage()`.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To clear the cache for a specific class, method, or instance, pass it directly to the function.

**Clear the cache associated with various objects**

```python
vbt.clear_cache(vbt.PF)  # (1)!
vbt.clear_cache(vbt.PF.total_return)  # (2)!
vbt.clear_cache(pf)  # (3)!
```

1. Applies to all instances and attributes of a specific class.

2. Applies to a specific attribute of all instances of a particular class.

3. Applies to all attributes of a specific class instance.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To display various statistics about the current cache, use [print_cache_stats](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.print_cache_stats).

**Various way to print cache statistics**

```python
vbt.print_cache_stats()  # (1)!
vbt.print_cache_stats(vbt.PF)  # (2)!
```

1. Displays statistics for the global cache.

2. Displays statistics for a specific object only.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To disable or enable caching globally, use
[disable_caching](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.disable_caching)
and [enable_caching](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.enable_caching),
respectively.

**Disable caching globally**

```python
vbt.disable_caching()
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To disable or enable caching within a code block, use the context managers
[CachingDisabled](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CachingDisabled)
and [CachingEnabled](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CachingEnabled),
respectively.

**How to disable caching within a code block**

```python
with vbt.CachingDisabled():  # (1)!
    ...  # (2)!

...  # (3)!

with vbt.CachingDisabled(vbt.PF):  # (4)!
    ...
```

1. Disables caching globally.

2. Place your code here that should not use caching.

3. Place your code here that should use the usual behavior.

4. Disables caching for a specific object only.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/compilation/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Compilation
type: page
icon: material-speedometer
---

# Compilation

You can disable Numba globally by setting an environment variable or by changing the config 
(see [Environment variables](https://numba.readthedocs.io/en/stable/reference/envvars.html)).

!!! note
    Make sure to do this *before* importing VBT.

**Disable Numba via an environment variable**

```python
import os

os.environ["NUMBA_DISABLE_JIT"] = "1"
```

**Disable Numba via the config**

```python
from numba import config

config.DISABLE_JIT = True
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can also achieve this by creating a [configuration](https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#settings)
file with the following content:

=== "INI"

    **File vbt.cfg**

```ini
    [numba]
    disable = True
    ```

=== "YAML"

    ```yaml title="File vbt.yml"
    numba:
      disable: true
    ```

=== "TOML"

    ```toml title="File vbt.toml"
    [numba]
    disable = true
    ```

!!! note
    All commands above must be executed before importing VBT.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To check whether Numba is enabled, use [is_numba_enabled](https://vectorbt.pro/pvt_41531c19/api/utils/checks/#vectorbtpro.utils.checks.is_numba_enabled).

```python title="Check whether Numba is enabled"
print(vbt.is_numba_enabled())
```

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/configuration/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
children:
  - https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#configuration
name: Configuration
type: page
icon: material-cog
---

Recipes for configuring VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#objects
parent: https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#configuration
name: Objects
type: heading 2
---

## Objects

!!! question
    Learn more in [Building blocks - Configuring documentation](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/#configuring).

VBT objects that subclass [Configured](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Configured)
(which represent most of the implemented classes) store the keyword arguments passed 
to their initializer, available under `config`. Copying an object simply involves passing the same
config to the class to create a new instance, which can be done automatically using the `copy()` method.

**Copy a Portfolio instance**

```python
new_pf = pf.copy()
new_pf = vbt.PF(**pf.config)  # (1)!
```

1. Manually.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Since changing any information in-place is strongly discouraged due to caching reasons, to replace
something, copy the config, modify it, and pass it to the class. This can be done automatically with the
`replace()` method.

**Replace initial capital in a Portfolio instance**

```python
new_pf = pf.replace(init_cash=1_000_000)
new_pf = vbt.PF(**vbt.merge_dicts(pf.config, dict(init_cash=1_000_000)))  # (1)!
```

1. Manually.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

In many cases, a VBT object contains other VBT objects. To make changes to a deep
vectorbtpro object, you can enable the `nested_` flag and pass the instruction as a nested dict.

**Enable grouping in the wrapper of a Portfolio instance**

```python
new_pf = pf.replace(wrapper=dict(group_by=True), nested_=True)
new_pf = pf.replace(wrapper=pf.wrapper.replace(group_by=True))  # (1)!
```

1. Manually.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The same VBT objects can be saved as config files for easy editing.
Such a config file has a format that is very similar to the [INI format](https://en.wikipedia.org/wiki/INI_file)
but includes various extensions such as code expressions and nested dictionaries.
This allows representing objects of any complexity. The same goes for VBT-enhanced YAML files.

=== "INI"

    ```python title="Save a Portfolio instance to a config file and load it back"
    pf.save(file_format="config")

    # (1)!

    pf = vbt.PF.load()
    ```

    1. ...make changes to the file...

=== "YAML"

    ```python title="Save a Portfolio instance to a YAML file and load it back"
    pf.save(file_format="yaml")

    # (1)!

    pf = vbt.PF.load()
    ```

    1. ...make changes to the file...

=== "TOML"

    ```python title="Save a Portfolio instance to a TOML file and load it back"
    pf.save(file_format="toml")

    # (1)!

    pf = vbt.PF.load()
    ```

    1. ...make changes to the file...

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#settings
parent: https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#configuration
name: Settings
type: heading 2
---

## Settings

Settings that control the default behavior of most functionalities in VBT are located
under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings). Each functionality has its own config;
for example, [settings.portfolio](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.portfolio) defines the defaults
for the [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) class. All configs
are combined into a single config that you can access via `vbt.settings`.

**Set the default initial cash**

```python
vbt.settings.portfolio.init_cash = 1_000_000
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The initial state of any config can be accessed through `options_["reset_dct"]`.

**Get the default initial cash before any changes**

```python
print(vbt.settings.portfolio.options_["reset_dct"]["init_cash"])  # (1)!
```

1. The reset dict is a regular dict, so dot notation does not work here.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Any config can be reset to its initial state using the `reset()` method.

**Reset the Portfolio config**

```python
vbt.settings.portfolio.reset()
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

For convenience, settings can be defined in a text file that will be loaded automatically the next time
VBT is imported. The file should be placed in the directory of the script importing the package and be
named `vbt.cfg`, `vbt.yml`, or `vbt.toml` (or other extension variants). Alternatively, you can set the
path to the settings file by setting the environment variable `VBT_SETTINGS_PATH`. The file must use either
the [INI format](https://en.wikipedia.org/wiki/INI_file#Format) with extensions provided by vectorbtpro
(see [Pickleable.decode_config](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.decode_config)
for examples), the [YAML format](https://yaml.org/)
(see [Pickleable.decode_yaml](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.decode_yaml) for examples),
or the [TOML format](https://toml.io/en/)
(see [Pickleable.decode_toml](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.decode_toml) for examples).

=== "INI"

    **Configuration file vbt.cfg that defines the default initial cash**

```ini
    # (1)!
    [portfolio]
    init_cash = 1_000_000
    ```

    1. The name of the config under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings).

=== "YAML"

    ```yaml title="Configuration file vbt.yml that defines the default initial cash"
    # (1)!
    portfolio:
      init_cash: 1_000_000
    ```

    1. The name of the config under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings).

=== "TOML"

    ```toml title="Configuration file vbt.toml that defines the default initial cash"
    # (1)!
    [portfolio]
    init_cash = 1_000_000
    ```

    1. The name of the config under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings).

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

This is especially useful for changing settings that only take effect on import,
such as various Numba-related settings, caching, and chunking machinery.

=== "INI"

    ```ini title="Configuration file vbt.cfg that disables Numba"
    # (1)!
    [numba]
    disable = True
    ```

    1. The name of the config under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings).

=== "YAML"

    ```yaml title="Configuration file vbt.yml that disables Numba"
    # (1)!
    numba:
      disable: true
    ```

    1. The name of the config under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings).

=== "TOML"

    ```toml title="Configuration file vbt.toml that disables Numba"
    # (1)!
    [numba]
    disable = true
    ```

    1. The name of the config under [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings).

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To save all settings or a specific config to a text file, modify it, and let VBT
load it on import (or manually), use the `save()` method with `file_format="config"` or `file_format="yaml"`.

=== "INI"

    ```python title="Save Portfolio config and import it automatically"
    vbt.settings.portfolio.save("vbt.cfg", top_name="portfolio")
    ```

    ```python title="Save Portfolio config and import it manually"
    vbt.settings.portfolio.save("portfolio.cfg")

    # (1)!

    vbt.settings.portfolio.load_update("portfolio.cfg")
    ```

    1. ...make changes to the file and restart the kernel...

=== "YAML"

    ```python title="Save Portfolio config and import it automatically"
    vbt.settings.portfolio.save("vbt.yml", root_key="portfolio")
    ```

    ```python title="Save Portfolio config and import it manually"
    vbt.settings.portfolio.save("portfolio.yml")

    # (1)!

    vbt.settings.portfolio.load_update("portfolio.yml")
    ```

    1. ...make changes to the file and restart the kernel...

=== "TOML"

    ```python title="Save Portfolio config and import it automatically"
    vbt.settings.portfolio.save("vbt.toml", root_key="portfolio")
    ```

    ```python title="Save Portfolio config and import it manually"
    vbt.settings.portfolio.save("portfolio.toml")

    # (1)!

    vbt.settings.portfolio.load_update("portfolio.toml")
    ```

    1. ...make changes to the file and restart the kernel...

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If the readability of the file does not matter, you can modify the settings in place and then save 
them to a Pickle file in one Python session, allowing them to be automatically imported in the next session.

```python title="Disable Numba in the next Python session"
vbt.settings.numba.disable = True
vbt.settings.save("vbt")
```

!!! warning
    This approach is discouraged if you plan to upgrade VBT frequently, as new releases
    may introduce changes to the settings.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
children:
  - https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/#cross-validation
name: Cross-validation
type: page
icon: material-check-all
---

Recipes for cross-validation in VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/#splitting
parent: https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/#cross-validation
name: Splitting
type: heading 2
---

## Splitting

!!! question
    Learn more in the [Cross-validation tutorial](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation/).

To select a fixed number of windows and optimize the window length so that they collectively cover the
maximum area of the index while keeping the train or test set non-overlapping, use
[Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling)
with `length="optimize"`. Under the hood, SciPy is used to minimize any empty space.

**Pick longest 20 windows for WFA such that test ranges do not overlap**

```python
splitter = vbt.Splitter.from_n_rolling(
    data.index,
    n=20,
    length="optimize",
    split=0.7,  # (1)!
    optimize_anchor_set=1,  # (2)!
    set_labels=["train", "test"]
)
```

1. 70% for training, 30% for testing.

2. Make the test set non-overlapping. Change to 0 for the train set.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

When using [Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling)
and the last window does not fit, it will be removed, causing a gap on the right-hand side.
To remove the oldest window instead, use `backwards="sorted"`.

**Roll a window that fills more recent data and with no gaps between test sets**

```python
length = 1000
ratio = 0.95
train_length = round(length * ratio)
test_length = length - train_length

splitter = vbt.Splitter.from_rolling(
    data.index,
    length=length,
    split=train_length,
    offset_anchor_set=None,
    offset=-test_length,
    backwards="sorted"
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To create a gap between the train set and the test set, use
[RelRange](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.RelRange) with `is_gap=True`.

**Roll an expanding window with a variable train set, a gap of 10 rows, and a test set of 20 rows**

```python
splitter = vbt.Splitter.from_expanding(
    data.index,
    min_length=130,
    offset=10,  # (1)!
    split=(1.0, vbt.RelRange(length=10, is_gap=True), 20),
    split_range_kwargs=dict(backwards=True)  # (2)!
)
```

1. Shift each split by the same number of rows as in the gap.

2. Split each range by first calculating the test set and then the train set.
Otherwise, `1.0` (100%) will be calculated first and will take up the entire split.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To roll a time-periodic window, use [Splitter.from_ranges](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_ranges)
with the `every` and `lookback_period` arguments as date offsets.

**Reserve 3 years for training and 1 year for testing**

```python
splitter = vbt.Splitter.from_ranges(
    data.index,
    every="Y",
    lookback_period="4Y",
    split=(
        vbt.RepEval("index.year != index.year[-1]"),  # (1)!
        vbt.RepEval("index.year == index.year[-1]")  # (2)!
    )
)
```

1. The training set should include all years in the split except for the last year.

2. The test set should include only the last year in the split.

### Taking

To split an object along the index (time) axis, first create a
[Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter)
instance and then "take" chunks from that object.

**How to split an object in two lines**

```python
splitter = vbt.Splitter.from_n_rolling(data.index, n=10)
data_chunks = splitter.take(data)  # (1)!

# ______________________________________________________________

splitter = vbt.Splitter.from_ranges(df.index, every="W")
new_df = splitter.take(df, into="reset_stacked")  # (2)!
```

1. VBT object.

2. Regular array.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Most VBT objects also have a `split` method that can combine both operations into a single step.
This method will automatically determine the correct splitting operation based on the supplied arguments.

**How to split an object in one line**

```python
data_chunks = data.split(n=10)  # (1)!

# ______________________________________________________________

new_df = df.vbt.split(every="W")  # (2)!
```

1. VBT object. The method `from_n_rolling` is selected based on `n=10`.

2. Regular array. The method `from_ranges` is selected based on `every="W"`.
The option `into="reset_stacked"` is enabled automatically.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/#testing
parent: https://vectorbt.pro/pvt_41531c19/cookbook/cross-validation/#cross-validation
name: Testing
type: heading 2
---

## Testing

To cross-validate a function that takes only one parameter combination at a time across a grid of parameter
combinations, use [`@vbt.cv_split`](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.cv_split).
This decorator combines [`@vbt.parameterized`](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized)
(which applies a function to each combination of parameters from a grid and merges the results),
and [`@vbt.split`](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split)
(which applies a function to each split and set combination).

**Cross-validate a function to maximize the Sharpe ratio**

```python
def selection(grid_results):  # (1)!
    return vbt.LabelSel([grid_results.idxmax()])  # (2)!
    
@vbt.cv_split(
    splitter="from_n_rolling",  # (3)!
    splitter_kwargs=dict(n=10, split=0.5, set_labels=["train", "test"]),  # (4)!
    takeable_args=["data"],  # (5)!
    execute_kwargs=dict(),  # (6)!
    parameterized_kwargs=dict(merge_func="concat"),  # (7)!
    merge_func="concat",  # (8)!
    selection=vbt.RepFunc(selection),  # (9)!
    return_grid=False  # (10)!
)
def my_pipeline(data, param1_value, param2_value):  # (11)!
    ...
    return pf.sharpe_ratio

cv_sharpe_ratios = my_pipeline(  # (12)!
    data,
    vbt.Param(param1_values),
    vbt.Param(param2_values)
)

# ______________________________________________________________

@vbt.cv_split(..., takeable_args=None)  # (13)!
def my_pipeline(range_, data, param1_value, param2_value):
    data_range = data.iloc[range_]
    ...
    return pf.sharpe_ratio

cv_sharpe_ratios = my_pipeline(
    vbt.Rep("range_"),
    data,
    vbt.Param([1, 2, 3]),
    vbt.Param([1, 2, 3]),
    _index=data.index  # (14)!
)
```

1. Function that returns the index of the best parameter combination.

2. Find the parameter combination with the highest Sharpe ratio. Wrap with `LabelSel` to let
vectorbtpro know that the returned value is a label, not a position, in case it is an integer.
Also, wrap the value with a list to display the parameter combination in the final index.

3. Name of the splitting method, such as [Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling).

4. Keyword arguments passed to the splitting method.

5. Names of the arrays that should be split. You can also pass `vbt.Takeable(data)` directly to the function instead.

6. Keyword arguments passed to [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute)
to control the execution of split and set combinations.

7. Keyword arguments passed to `@vbt.parameterized`. Here, we want to concatenate
the results of all parameter combinations into a single Pandas Series.

8. Function used to merge all split and set combinations. Here, we want to concatenate
all of the Pandas Series into a single Pandas Series.

9. Either a specific index or a template used to select the best parameter combination.

10. Whether to return the full grid of parameter combinations, not just the best ones.

11. Similar to `@vbt.parameterized`, VBT will provide only one parameter combination at a time
as single values. Any takeable argument (here, `data`) will include only values that correspond to the
current split and set combination.

12. The returned Pandas Series will contain cross-validation results by split and set combination.

13. Same as above, but select the date range manually within the function.

14. Any argument meant to be passed to the decorator can also be passed directly
to the function by prepending the underscore.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To skip a parameter combination, return [NoResult](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.NoResult).
This helps exclude parameter combinations that raise an error.
`NoResult` can also be returned by the selection function to skip an entire split and set combination.
Once excluded, the combination will not appear in the final index.

**Skip split and set combinations where there are no satisfactory parameters**

```python
# (1)!

def selection(grid_results):
    sharpe_ratio = grid_results.xs("Sharpe Ratio", level=-1).astype(float)
    return vbt.LabelSel([sharpe_ratio.idxmax()])

@vbt.cv_split(...)
def my_pipeline(...):
    ...
    stats_sr = pf.stats(agg_func=None)
    if stats_sr["Min Value"] > 0 and stats_sr["Total Trades"] >= 20:  # (2)!
        return stats_sr
    return vbt.NoResult

# ______________________________________________________________

# (3)!

def selection(grid_results):
    sharpe_ratio = grid_results.xs("Sharpe Ratio", level=-1).astype(float)
    min_value = grid_results.xs("Min Value", level=-1).astype(float)
    total_trades = grid_results.xs("Total Trades", level=-1).astype(int)
    sharpe_ratio = sharpe_ratio[(min_value > 0) & (total_trades >= 20)]
    if len(sharpe_ratio) == 0:
        return vbt.NoResult
    return vbt.LabelSel([sharpe_ratio.idxmax()])
    
@vbt.cv_split(...)
def my_pipeline(...):
    ...
    return pf.stats(agg_func=None)
```

1. Filter parameter combinations on the fly, then select the best one from those remaining (if any).

2. Keep a parameter combination only if the minimum portfolio value is greater than 0
(that is, the position was not liquidated) and the number of trades is 20 or higher.

3. Same as above, but return all parameter combinations and filter them in the selection function.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To warm up one or more indicators, instruct VBT to pass a date range instead of selecting it from
the data, and prepend a buffer to this date range. Then, manually select this extended date range from the data
and run your indicators on that date range. Finally, remove the buffer from the indicator(s).

**Warm up a SMA crossover**

```python
@vbt.cv_split(..., index_from="data")
def buffered_sma_pipeline(data, range_, fast_period, slow_period, ...):
    buffer_len = max(fast_period, slow_period)  # (1)!
    buffered_range = slice(range_.start - buffer_len, range_.stop)  # (2)!
    data_buffered = data.iloc[buffered_range]  # (3)!
    
    fast_sma_buffered = data_buffered.run("sma", fast_period, hide_params=True)
    slow_sma_buffered = data_buffered.run("sma", slow_period, hide_params=True)
    entries_buffered = fast_sma_buffered.real_crossed_above(slow_sma_buffered)
    exits_buffered = fast_sma_buffered.real_crossed_below(slow_sma_buffered)
    
    data = data_buffered.iloc[buffer_len:]  # (4)!
    entries = entries_buffered.iloc[buffer_len:]
    exits = exits_buffered.iloc[buffer_len:]
    ...

buffered_sma_pipeline(
    data,  # (5)!
    vbt.Rep("range_"),  # (6)!
    vbt.Param(fast_periods, condition="x < slow_period"),
    vbt.Param(slow_periods),
    ...
)
```

1. Determine the length of the buffer.

2. Extend the date range using the buffer.

3. Select the data corresponding to the extended date range.

4. After running all indicators, remove the buffer.

5. Pass the data as it is (without selection).

6. Instruct VBT to pass the date range as a slice.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/data/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Data
type: page
icon: material-database
---

# Data

!!! question
    Learn more in the [Data documentation](https://vectorbt.pro/pvt_41531c19/documentation/data/).

There are many supported data sources available for OHLC and indicator data. For the complete list,
see the [custom](https://vectorbt.pro/pvt_41531c19/api/data/custom/) module.

## Listing

Many data classes offer a class method to list all symbols that can be fetched. Typically, this method
starts with `list_`, such as
[TVData.list_symbols](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData.list_symbols),
[SQLData.list_tables](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData.list_tables), or
[CSVData.list_paths](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData.list_paths).
Most of these methods also support client-side filtering of symbols using glob-style or regex-style patterns.

**How to list symbols**

```python
all_symbols = vbt.BinanceData.list_symbols()  # (1)!
usdt_symbols = vbt.BinanceData.list_symbols("*USDT")  # (2)!
usdt_symbols = vbt.BinanceData.list_symbols(r"^.+USDT$", use_regex=True)

all_symbols = vbt.TVData.list_symbols()  # (3)!
nasdaq_symbols = vbt.TVData.list_symbols(exchange_pattern="NASDAQ")  # (4)!
btc_symbols = vbt.TVData.list_symbols(symbol_pattern="BTC*")  # (5)!
pl_symbols = vbt.TVData.list_symbols(market="poland")  # (6)!
usdt_symbols = vbt.TVData.list_symbols(fields=["currency"], filter_by=["USDT"])  # (7)!

def filter_by(market_cap_basic):
    if market_cap_basic is None:
        return False
    return market_cap_basic >= 1_000_000_000_000

trillion_symbols = vbt.TVData.list_symbols(  # (8)!
    fields=["market_cap_basic"], 
    filter_by=vbt.RepFunc(filter_by)
)

all_paths = vbt.FileData.list_paths()  # (9)!
csv_paths = vbt.CSVData.list_paths()  # (10)!
all_csv_paths = vbt.CSVData.list_paths("**/*.csv")  # (11)!
all_data_paths = vbt.HDFData.list_paths("data.h5")  # (12)!
all_paths = vbt.HDFData.list_paths()  # (13)!

all_schemas = vbt.SQLData.list_schemas(engine=engine)  # (14)!
all_tables = vbt.SQLData.list_tables(engine=engine)  # (15)!
```

1. List all Binance symbols.

2. List Binance symbols ending with "USDT".

3. List all TradingView symbols.

4. List TradingView symbols traded on NASDAQ.

5. List TradingView symbols starting with "BTC".

6. List TradingView symbols traded in Poland.

7. List TradingView symbols traded in USD currency.

8. List TradingView symbols with a market capitalization of 1 trillion or more.

9. List all files in the current directory.

10. List CSV files in the current directory.

11. List CSV files in the current directory and all subdirectories.

12. List all keys in the HDF file "data.h5".

13. List all keys in all HDF files in the current directory.

14. List all schemas in a SQL database.

15. List all tables in a SQL database.

## Pulling

Each data class provides a `fetch_symbol()` method to fetch a single symbol and return raw data, typically
as a DataFrame. To return a data instance, use the `pull()` method, which accepts one or more symbols,
calls `fetch_symbol()` for each, and aligns all DataFrames. For testing purposes, use
[YFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/yf/#vectorbtpro.data.custom.yf.YFData), which is easy to use but lower in quality.
For production, use more reliable data sources like
[CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData) for crypto and
[AlpacaData](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData) for stocks.
For technical analysis using the latest data, use
[TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData) (TradingView).

!!! tip
    To check which arguments a data class like `YFData` accepts, use `vbt.phelp(vbt.YFData.fetch_symbol)`.

**How to fetch data**

```python
data = vbt.YFData.pull("AAPL")  # (1)!
data = vbt.YFData.pull(["AAPL", "MSFT"])  # (2)!
data = vbt.YFData.pull("AAPL", start="2020")  # (3)!
data = vbt.YFData.pull("AAPL", start="2020", end="2021")  # (4)!
data = vbt.YFData.pull("AAPL", start="1 month ago")  # (5)!
data = vbt.YFData.pull("AAPL", start="1 month ago", timeframe="hourly")  # (6)!
data = vbt.YFData.pull("AAPL", tz="UTC")  # (7)!
data = vbt.YFData.pull(symbols, execute_kwargs=dict(engine="threadpool"))  # (8)!

data = vbt.YFData.pull("AAPL", auto_adjust=False)  # (9)!
data = vbt.BinanceData.pull("BTCUSDT", klines_type="futures")  # (10)!
data = vbt.CCXTData.pull("BTCUSDT", exchange="binanceusdm")  # (11)!
data = vbt.BinanceData.pull("BTCUSDT", tld="us")  # (12)!
data = vbt.TVData.pull("CRYPTOCAP:TOTAL")  # (13)!
```

1. Pull all data for one symbol. Note that some data classes will only return a subset of data by default.

2. Pull all data for multiple symbols. Symbols will be fetched sequentially (one after another).

3. Pull data starting from 2020-01-01 (inclusive). Dates can be strings, `datetime` objects,
or `pd.Timestamp` objects. Dates are assigned the same timezone as the ticker, unless specified otherwise.

4. Pull data between 2020-01-01 (inclusive) and 2021-01-01 (exclusive).

5. Pull data starting 1 month ago. Both `start` and `end` support human-readable string inputs.

6. Pull hourly data starting 1 month ago. Timeframes also support human-readable strings.

7. Pull all data and convert to the UTC timezone. Use this when tickers have different timezones.

8. Pull multiple symbols in parallel. Note that many data providers enforce strict API rate limits,
which could lead to a ban if exceeded.

9. Disable auto-adjustment.

10. Pull BTC/USDT futures data from Binance.

11. Same as above, but using CCXT.

12. Pass `tld` if using an exchange from the US, Japan, or other TLDs.

13. Pull [Crypto Total Market Cap](https://www.tradingview.com/chart/r7sfuYga/?symbol=CRYPTOCAP%3ATOTAL).

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To provide different keyword arguments for different symbols, either pass an argument as a
[symbol_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.symbol_dict), or pass a dictionary with keyword
arguments keyed by symbol as the first argument.

**How to provide keyword arguments by symbol**

```python
data = vbt.TVData.pull(
    ["SPX", "NDX", "VIX"],
    exchange=vbt.symbol_dict({"SPX": "SP", "NDX": "NASDAQ", "VIX": "CBOE"})
)
data = vbt.TVData.pull({  # (1)!
    "SPX": dict(exchange="SP"),
    "NDX": dict(exchange="NASDAQ"),
    "VIX": dict(exchange="CBOE")
})
data = vbt.TVData.pull(["SP:SPX", "NASDAQ:NDX", "CBOE:VIX"])  # (2)!
```

1. Same as above.

2. Same as above, but now symbols include their exchange as a prefix.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If your data provider requires credentials and you want to fetch multiple symbols, the client will be
created for each symbol, leading to multiple authentications and slower execution.
To avoid this, create the client beforehand and pass it to the `fetch()` method.

**Create a Data client in advance**

```python
client = vbt.TVData.resolve_client(username="YOUR_USERNAME", password="YOUR_PASSWORD")
```

**Use the Data client**

```python
data = vbt.TVData.pull(["NASDAQ:AAPL", "NASDAQ:MSFT"], client=client)

# ______________________________________________________________

vbt.TVData.set_custom_settings(client=client)
data = vbt.TVData.pull(["NASDAQ:AAPL", "NASDAQ:MSFT"])
```

## Persisting

Once fetched, data can be saved in several ways. The most common and recommended approach is to
pickle the data, which saves the entire object, including the arguments used during fetching.
Other options include CSV files ([Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv)),
HDF files ([Data.to_hdf](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_hdf)), and more.
These methods save only the data itself, not the associated metadata, such as the timeframe.

**How to save data**

```python
data.save()  # (1)!
data.save(compression="blosc")  # (2)!

data.to_csv("data", mkdir_kwargs=dict(mkdir=True))  # (3)!
data.to_csv("AAPL.csv")  # (4)!
data.to_csv("AAPL.tsv", sep="\t")  # (5)!
data.to_csv(vbt.symbol_dict(AAPL="AAPL.csv", MSFT="MSFT.csv"))  # (6)!
data.to_csv(vbt.RepEval("symbol + '.csv'"))  # (7)!

data.to_hdf("data")  # (8)!
data.to_hdf("data.h5")  # (9)!
data.to_hdf("data.h5", key=vbt.RepFunc(lambda symbol: symbol.replace(" ", "_")))  # (10)!
data.to_hdf("data.h5", key=vbt.RepFunc(lambda symbol: "stocks/" + symbol))  # (11)!
data.to_hdf(vbt.RepEval("symbol + '.h5'"), key="df")  # (12)!

data.to_parquet("data")  # (13)!
data.to_parquet(vbt.symbol_dict(
    AAPL="data/AAPL.parquet", 
    MSFT="data/MSFT.parquet"
))  # (14)!
data.to_parquet("data", partition_by="Y")  # (15)!
data.to_parquet(vbt.symbol_dict(
    AAPL="data/AAPL", 
    MSFT="data/MSFT"
), partition_by="Y")  # (16)!

data.to_sql(engine="sqlite:///data.db")  # (17)!
data.to_sql(engine="postgresql+psycopg2://postgres:admin@localhost:5432/data")  # (18)!
data.to_sql(engine=engine, schema="yahoo")  # (19)!
data.to_sql(engine=engine, table=vbt.symbol_dict(AAPL="AAPL", MSFT="MSFT"))  # (20)!
data.to_sql(engine=engine, if_exists="replace")  # (21)!
data.to_sql(engine=engine, attach_row_number=True)  # (22)!
data.to_sql(
    engine=engine, 
    attach_row_number=True, 
    row_number_column="RN",
    from_row_number=vbt.symbol_dict(AAPL=100, MSFT=200), 
    if_exists="append"
)  # (23)!
```

1. Serialize and save to a pickle file (recommended). The filename will be
`{class_name}.pickle`, such as "YFData.pickle".

2. Serialize, compress, and save to a pickle file using the specified compression algorithm.

3. Save one CSV file per symbol in the "data" directory. If the directory does not exist, it will be created.
Each filename will be `{symbol}.csv`, such as "AAPL.csv".

4. If there is only one symbol, save it to a comma-delimited file named "AAPL.csv".

5. Same as above, but save to a tab-delimited file.

6. Specify the file path for each symbol.

7. Use a template to determine the file path based on the symbol.

8. Save all symbols to a single HDF file in the "data" directory. The filename will be `{class_name}.h5`,
such as "YFData.h5". Each symbol will be saved as a separate key, such as "AAPL".

9. Specify the path to the HDF file.

10. Use a template to choose the key based on the symbol. In this example, spaces in symbols
are replaced with underscores.

11. Use a template to save all symbols under the group "stocks" in the HDF file.

12. Save each symbol to a separate HDF file with the key "df".

13. Save one Parquet file per symbol in the "data" directory. Each filename will be
`{symbol}.parquet`, such as "AAPL.parquet".

14. Same as above, but specify the path for each file.

15. Partition each symbol by year start and save it to a separate subdirectory within the "data" directory.
Each subdirectory will be named after the symbol.

16. Same as above, but specify the path for each subdirectory.

17. Save each DataFrame as a table in a SQLite database.

18. Same as above, but save to a PostgreSQL database.

19. Specify a schema (if supported by the database).

20. Specify each table name explicitly.

21. Drop the table if it already exists.

22. Attach a column with row numbers to each DataFrame for easier querying later.

23. Same as above, but generate row numbers starting from a specific value for each symbol,
label the column as "RN", and append to an existing table.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

After saving, data can be loaded using the corresponding class method.

**How to load data**

```python
data = vbt.YFData.load()  # (1)!

data = vbt.Data.from_csv("data")  # (2)!
data = vbt.Data.from_csv("data/*.csv")  # (3)!
data = vbt.Data.from_csv("data/*/**.csv")  # (4)!
data = vbt.Data.from_csv(symbols=["BTC-USD.csv", "ETH-USD.csv"])  # (5)!
data = vbt.Data.from_csv(features=["High.csv", "Low.csv"])  # (6)!
data = vbt.Data.from_csv("BTC-USD", paths="polygon_btc_1hour.csv")  # (7)!
data = vbt.Data.from_csv("AAPL.tsv", sep="\t")  # (8)!
data = vbt.Data.from_csv(["MSFT.csv", "AAPL.tsv"], sep=vbt.symbol_dict(MSFT=",", AAPL="\t"))  # (9)!
data = vbt.Data.from_csv("https://datahub.io/core/s-and-p-500/r/data.csv", match_paths=False)  # (10)!

data = vbt.Data.from_hdf("data")  # (11)!
data = vbt.Data.from_hdf("data.h5")  # (12)!
data = vbt.Data.from_hdf("data.h5/AAPL")  # (13)!
data = vbt.Data.from_hdf(["data.h5/AAPL", "data.h5/MSFT"])  # (14)!
data = vbt.Data.from_hdf(["AAPL", "MSFT"], paths="data.h5", match_paths=False)
data = vbt.Data.from_hdf("data.h5/stocks/*")  # (15)!

data = vbt.Data.from_parquet("data")  # (16)!
data = vbt.Data.from_parquet("AAPL.parquet")  # (17)!
data = vbt.Data.from_parquet("AAPL")  # (18)!

data = vbt.Data.from_sql(engine="sqlite:///data.db")  # (19)!
data = vbt.Data.from_sql("AAPL", engine=engine)  # (20)!
data = vbt.Data.from_sql("yahoo:AAPL", engine=engine)  # (21)!
data = vbt.Data.from_sql("AAPL", schema="yahoo", engine=engine)  # (22)!
data = vbt.Data.from_sql("AAPL", query="SELECT * FROM AAPL", engine=engine)  # (23)!

data = vbt.BinanceData.from_csv("BTCUSDT.csv", fetch_kwargs=dict(timeframe="hourly"))  # (24)!
```

1. Load from the pickle file named "YFData.pickle" and deserialize it into a Python object (recommended).

2. Load all CSV files in the directory named "data".

3. Same as above.

4. Same as above, but recursively.

5. Load two symbols into a symbol-oriented data instance.

6. Load two features into a feature-oriented data instance.

7. Load data from the file "polygon_btc_1hour.csv" and rename the symbol to "BTC-USD".

8. Load one symbol stored in the tab-delimited file "AAPL.tsv".

9. Load one symbol from the comma-delimited file "MSFT.csv" and another from the tab-delimited file "AAPL.tsv".

10. Load CSV data from a URL.

11. Load all symbols from all HDF files in the "data" directory recursively.

12. Load all symbols from the HDF file named "data.h5".

13. Load the symbol "AAPL" from the HDF file named "data.h5".

14. Load the symbols "AAPL" and "MSFT" from the HDF file named "data.h5".

15. Load all symbols under the "stocks" group in the HDF file named "data.h5".

16. Load all Parquet files and partitioned subdirectories in the "data" directory.

17. Load the symbol "AAPL" from the Parquet file named "AAPL.parquet".

18. Load the symbol "AAPL" from the partitioned Parquet directory named "AAPL".

19. Load all symbols from a SQLite database stored locally in the file "data.db".

20. Load the symbol "AAPL" by reading a table with the same name.

21. Load the symbol "AAPL" by reading a table with the same name from the "yahoo" schema.

22. Same as above, but the schema will not become part of the symbol name.

23. Load the symbol "AAPL" by executing an arbitrary SQL query.

24. Load the symbol from the file "BTCUSDT.csv" and wrap it with the `BinanceData` class to allow updating later.
To avoid specifying the timeframe when updating, provide it using `fetch_kwargs`.

## Updating

Some data classes support fetching and appending new data to previously saved data by overriding
the [Data.update_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update_symbol) method.
This method scans the data for the latest timestamp and uses it as the start timestamp when
fetching new data with [Data.fetch_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_symbol).
The [Data.update](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update) method performs this process for
each symbol in the data instance. There is no need to provide the client, timeframe, or other arguments,
since they are stored during fetching and reused automatically (unless they are lost by converting
the data instance to Pandas, CSV, or HDF!).

**Download 1-minute data and update it later**

```python
data = vbt.YFData.pull("AAPL", timeframe="1 minute")

# (1)!

data = data.update()  # (2)!
```

1. ...wait a minute...

2. Returns a new instance.

**Download one year of data at a time**

```python
start = 2010
end = 2020
data = None
while start < end:
    if data is None:
        data = vbt.YFData.pull("AAPL", start=str(start), end=str(start + 1))
    else:
        data = data.update(end=str(start + 1))
    start += 1
```

## Wrapping

A custom DataFrame can be wrapped into a data instance using [Data.from_data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_data).
This method accepts either a single DataFrame for one symbol or a dict containing multiple
DataFrames keyed by their symbols.

**Create a new Data instance**

```python
data = ohlc_df.vbt.ohlcv.to_data()  # (1)!
data = vbt.Data.from_data(ohlc_df)

data = close_df.vbt.to_data()  # (2)!
data = vbt.Data.from_data(close_df, columns_are_symbols=True)

data = close_df.vbt.to_data(invert_data=True)  # (3)!
data = vbt.Data.from_data(close_df, columns_are_symbols=True, invert_data=True)

data = vbt.Data.from_data(vbt.symbol_dict({"AAPL": aapl_ohlc_df, "MSFT": msft_ohlc_df}))  # (4)!
data = vbt.Data.from_data(vbt.feature_dict({"High": high_df, "Low": low_df}))  # (5)!
```

1. OHLC DataFrame.

2. Close DataFrame where columns are symbols. Store in a feature-oriented format.

3. Close DataFrame where columns are symbols. Store in a symbol-oriented format.

4. Multiple feature DataFrames keyed by symbol.

5. Multiple symbol DataFrames keyed by feature.

!!! tip
    You do not have to use data instances; you can work with Pandas and even NumPy arrays as well,
    since VBT will convert any array-like object to a NumPy array anyway. However, the Pandas format
    is generally better than the NumPy format, because it also includes a datetime index and backtest
    configuration metadata, such as symbols and parameter combinations in column form.
    Data instances are especially useful for symbol alignment, stacking, resampling, and updating.

## Extracting

Depending on your use case, there are several ways to extract the underlying Pandas Series/DataFrame
from a data instance. To get the original data with one DataFrame per symbol, access the `data` attribute.
This data includes OHLC and other features (possibly of various data types) concatenated together,
which can be helpful for plotting. Note that VBT does not support this format directly;
instead, you are encouraged to represent each feature as a separate DataFrame with columns as symbols.
A feature can be accessed as an attribute (for example, `data.close` for closing price) or
by using [Data.get](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get).

**How to get symbol-oriented data**

```python
data_per_symbol = data.data  # (1)!
aapl_data = data_per_symbol["AAPL"]  # (2)!

sr_or_df = data.get("Close")  # (3)!
sr_or_df = data["Close"].get()
sr_or_df = data.close

sr_or_df = data.get(["Close"])  # (4)!
sr_or_df = data[["Close"]].get()

sr = data.get("Close", "AAPL")  # (5)!
sr = data["Close"].get(symbols="AAPL")
sr = data.select("AAPL").close

df = data.get("Close", ["AAPL"])  # (6)!
df = data["Close"].get(symbols=["AAPL"])
df = data.select(["AAPL"]).close

aapl_df = data.get(["Open", "Close"], "AAPL")  # (7)!
close_df = data.get("Close", ["AAPL", "MSFT"])  # (8)!
open_df, close_df = data.get(["Open", "Close"], ["AAPL", "MSFT"])  # (9)!
```

1. Get a dictionary with one (OHLC) Series/DataFrame per symbol ("AAPL", "MSFT", etc.).

2. Extract the (OHLC) Series/DataFrame for "AAPL".

3. Get the closing price as a Series (for one symbol) or DataFrame (for multiple symbols, one per column).

4. Get the closing price as a DataFrame, regardless of the number of symbols.

5. Get the closing price for "AAPL" as a Series.

6. Get the closing price for "AAPL" as a DataFrame.

7. Get the opening and closing prices for "AAPL" as a DataFrame with two columns.

8. Get the closing price for "AAPL" and "MSFT" as a DataFrame with two columns.

9. Get the opening and closing prices as a tuple of two DataFrames, each with columns "AAPL" and "MSFT".

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If a data instance is feature-oriented, the behavior of features and symbols is reversed.

**How to get feature-oriented data**

```python
data_per_feature = feat_data.data  # (1)!
close_data = data_per_feature["Close"]

sr_or_df = data.get("Close") 
sr_or_df = data.select("Close").get()
sr_or_df = data.close

sr = feat_data.get("Close", "AAPL")
sr = feat_data["AAPL"].get(features="Close")  # (2)!
sr = feat_data.select("Close").get(symbols="AAPL")  # (3)!

aapl_df = data.get(["Open", "Close"], "AAPL")
close_df = data.get("Close", ["AAPL", "MSFT"])
aapl_df, msft_df = data.get(["Open", "Close"], ["AAPL", "MSFT"])  # (4)!
```

1. In feature-oriented instances, data dictionaries use features as keys and symbols as columns,
so it is easier to extract feature Series or DataFrames.

2. Indexing (such as `[]`) is applied to columns, which are now symbols.

3. Various methods (such as `select`) are applied to keys, which are now features.

4. DataFrames are per symbol rather than per feature if a tuple is returned.

!!! tip
    To ensure consistent behavior between symbol-oriented and feature-oriented instances, always
    use [Data.get](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get) to extract your data.

## Changing

There are four main operations to modify features and symbols: adding, selecting, renaming, and removing.
You can add one feature or symbol at a time, while the other operations can be performed on multiple.
Usually, you do not need to specify whether you want to operate on symbols or features,
as this is determined automatically. Both features and symbols are case-insensitive.
Note that each operation does not modify the original data instance, but returns a new one.

**How to add features and symbols**

```python
new_data = data.add_symbol("BTC-USD")  # (1)!
new_data = data.add_symbol("BTC-USD", fetch_kwargs=dict(start="2020"))  # (2)!
btc_df = vbt.YFData.pull("ETH-USD", start="2020").get()
new_data = data.add_symbol("BTC-USD", btc_df)  # (3)!

new_data = data.add_feature("SMA")  # (4)!
new_data = data.add_feature("SMA", run_kwargs=dict(timeperiod=20, hide_params=True))  # (5)!
sma_df = data.run("SMA", timeperiod=20, hide_params=True, unpack=True)
new_data = data.add_feature("SMA", sma_df)  # (6)!

new_data = data.add("BTC-USD", btc_df)  # (7)!
new_data = data.add("SMA", sma_df)  # (8)!
```

1. Pull "BTC-USD" and add it as a symbol.

2. Pull "BTC-USD" from 2020 onwards and add it as a symbol.

3. Add a custom DataFrame as a symbol.

4. Run the SMA indicator and add it as a feature.

5. Run the 20-period SMA indicator and add it as a feature.

6. Add a custom DataFrame as a feature.

7. If some columns are found among the features of the data instance and not among the symbols,
the DataFrame will be added as a symbol.

8. If some columns are found among the symbols of the data instance and not among the features,
the DataFrame will be added as a feature.

!!! note
    Only one feature or symbol can be added at a time. To add a data instance, use `merge` instead.

**How to select features and symbols**

```python
new_data = data.select_symbols("BTC-USD")  # (1)!
new_data = data.select_symbols(["BTC-USD", "ETH-USD"])  # (2)!

new_data = data.select_features("SMA")  # (3)!
new_data = data.select_features(["SMA", "EMA"])  # (4)!

new_data = data.select("BTC-USD")  # (5)!
new_data = data.select("SMA")  # (6)!
new_data = data.select("sma")  # (7)!
```

1. Select one symbol.

2. Select multiple symbols.

3. Select one feature.

4. Select multiple features.

5. If some keys are found among the symbols of the data instance and not among the features,
a symbol will be selected.

6. If some keys are found among the features of the data instance and not among the symbols,
a feature will be selected.

7. Case does not matter!

**How to rename features and symbols**

```python
new_data = data.rename_symbols("BTC-USD", "BTCUSDT")  # (1)!
new_data = data.rename_symbols(["BTC-USD", "ETH-USD"], ["BTCUSDT", "ETHUSDT"])  # (2)!
new_data = data.rename_symbols({"BTC-USD": "BTCUSDT", "ETH-USD": "ETHUSDT"})

new_data = data.rename_features("Price", "Close")  # (3)!
new_data = data.rename_features(["Price", "MovAvg"], ["Close", "SMA"])  # (4)!
new_data = data.rename_features({"Price": "Close", "MovAvg": "SMA"})

new_data = data.rename("BTC-USD", "BTCUSDT")  # (5)!
new_data = data.rename("Price", "Close")  # (6)!
new_data = data.rename("price", "Close")  # (7)!
```

1. Rename the symbol "BTC-USD" to "BTCUSDT".

2. Rename the symbol "BTC-USD" to "BTCUSDT" and the symbol "ETH-USD" to "ETHUSDT".

3. Rename the feature "Price" to "Close".

4. Rename the feature "Price" to "Close" and the feature "MovAvg" to "SMA".

5. If some keys are found among the symbols of the data instance and not among the features,
they will be renamed as symbols.

6. If some keys are found among the features of the data instance and not among the symbols,
they will be renamed as features.

7. The case of the source key does not matter, but the case of the target key does!

**How to remove features and symbols**

```python
new_data = data.remove_symbols("BTC-USD")  # (1)!
new_data = data.remove_symbols(["BTC-USD", "ETH-USD"])  # (2)!

new_data = data.remove_features("SMA")  # (3)!
new_data = data.remove_features(["SMA", "EMA"])  # (4)!

new_data = data.remove("BTC-USD")  # (5)!
new_data = data.remove("SMA")  # (6)!
new_data = data.remove("sma")  # (7)!
```

1. Remove one symbol.

2. Remove multiple symbols.

3. Remove one feature.

4. Remove multiple features.

5. If some keys are found among the symbols of the data instance and not among the features,
they will be removed as symbols.

6. If some keys are found among the features of the data instance and not among the symbols,
they will be removed as features.

7. Case does not matter!

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Instances can be merged along symbols, rows, and columns by using [Data.merge](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.merge).

**Merge multiple different Data instances**

```python
data1 = vbt.YFData.pull("BTC-USD")
data2 = vbt.BinanceData.pull("BTCUSDT")
data3 = vbt.CCXTData.pull("BTC-USDT", exchange="kucoin")
data = vbt.Data.merge(data1, data2, data3, missing_columns="drop")
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To apply a function to each DataFrame and return a new instance, use the method
[Data.transform](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.transform). By default,
it passes a single DataFrame where all individual DataFrames are concatenated along columns.
This is useful for dropping missing values across all symbols. To transform DataFrames
individually, use `per_symbol=True` and/or `per_feature=True`. The only requirement is that
the returned column names are identical across all features and symbols.

**Drop rows with missing values**

```python
new_data = data.transform(lambda df: df.dropna(how="any"))  # (1)!
new_data = data.dropna()  # (2)!
new_data = data.dropna(how="all")  # (3)!

new_data = data.transform(your_func, per_feature=True)
new_data = data.transform(your_func, per_symbol=True)
new_data = data.transform(your_func, per_feature=True, per_symbol=True)  # (4)!
new_data = data.transform(your_func, per_feature=True, per_symbol=True, pass_frame=True)  # (5)!
```

1. Remove any row that has at least one missing value across all features and symbols.

2. Same as above.

3. Remove any row that has all values missing across all features and symbols.

4. One column at a time is passed as a Series.

5. One column at a time is passed as a DataFrame.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If symbols have different timezones, the final timezone will be set to "UTC". This may cause
some symbols to be shifted in time. For example, if one symbol uses UTC+0200 and another uses UTC+0400,
this will effectively double the common index and result in missing values about half the time.
To align their indexes into a single index, use [Data.realign](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.realign),
which is a special resampling method that produces a single index and correctly orders data by time.

**Realign multiple timezones**

```python
new_data = data.realign()  # (1)!
```

1. To avoid forward filling missing values, pass `ffill=False`.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can easily chain operations that return a new data instance by using dot notation or the `pipe` method.

**Chain multiple operations**

```python
data = (
    vbt.YFData.pull("BTC-USD")
    .add_symbol("ETH-USD")
    .rename({"btc-usd": "BTCUSDT", "eth-usd": "ETHUSDT"})
    .remove(["dividends", "stock splits"])
    .add_feature("SMA")
    .add_feature("EMA")
)

# ______________________________________________________________

data = (
    vbt.YFData
    .pipe("pull", "BTC-USD")  # (1)!
    .pipe("add_symbol", "ETH-USD")
    .pipe("rename", {"btc-usd": "BTCUSDT", "eth-usd": "ETHUSDT"})
    .pipe("remove", ["dividends", "stock splits"])
    .pipe("add_feature", "SMA")
    .pipe("add_feature", "EMA")
)
```

1. The method can be called on both data classes and instances. You can pass a string to call
a method of the data class or instance, or pass any function that expects the instance as the first argument.
To provide arguments to the function, pass the function as a tuple. The second element is the argument's
position or name.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/datetime/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Datetime
type: page
icon: material-calendar
---

# Datetime

VBT values flexibility, allowing you to create a variety of datetime-related objects from human-readable strings.

## Timestamps

Timestamps represent a single point in time, similar to a datetime object in Python's `datetime` module,
but with enhanced features for data analysis and manipulation.

**How to construct a timestamp**

```python
vbt.timestamp()  # (1)!
vbt.utc_timestamp()  # (2)!
vbt.local_timestamp()  # (3)!
vbt.timestamp(tz="America/New_York")  # (4)!
vbt.timestamp("1 Jul 2020")  # (5)!
vbt.timestamp("7 days ago")  # (6)!
```

1. `pd.Timestamp.now()` (without timezone).

2. `pd.Timestamp.now(tz="utc")` (UTC timezone).

3. `pd.Timestamp.now(tz="tzlocal()")` (local timezone).

4. `pd.Timestamp.now(tz="New_York/America")` (New York timezone).

5. `pd.Timestamp("2020-07-01")`.

6. `pd.Timestamp.now() - pd.Timedelta(days=7)`.

## Timezones

Timezones can be used in timestamps, making them a powerful tool for global time-based data analysis.

**How to construct a timezone**

```python
vbt.timezone()  # (1)!
vbt.timezone("utc")  # (2)!
vbt.timezone("America/New_York")  # (3)!
vbt.timezone("+0500")  # (4)!
```

1. Local timezone.

2. UTC timezone.

3. New York timezone.

4. UTC+5 timezone.

## Timedeltas

Timedeltas handle continuous time spans and precise time differences. They are commonly used for adding or
subtracting durations from timestamps, or for measuring the difference between two timestamps.

**How to construct a timedelta**

```python
vbt.timedelta()  # (1)!
vbt.timedelta("7 days")  # (2)!
vbt.timedelta("weekly")
vbt.timedelta("Y", approximate=True)  # (3)!
```

1. `pd.Timedelta(nanoseconds=1)`.

2. `pd.Timedelta(days=7)`.

3. Approximation for a year.

## Date offsets

Date offsets handle calendar-specific offsets, such as adding a month or skipping weekends with business
days. They are often used for calendar-aware adjustments and recurring periods.

**How to construct a date offset**

```python
vbt.offset("Y")  # (1)!
vbt.offset("YE")  # (2)!
vbt.offset("weekstart")  # (3)!
vbt.offset("monday")
vbt.offset("july")  # (4)!
```

1. `pd.offsets.YearBegin()` (every year begin).

2. `pd.offsets.YearEnd()` (every year end).

3. `pd.offsets.Week(weekday=0)` (every Monday).

4. `pd.offsets.YearBegin(month=7)` (every July).

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/discovery/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Discovery
type: page
icon: material-file-tree
---

# Discovery

You can display the arguments and, optionally, the description of any Python function or class by using
[phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp). For example, this makes it easy to
quickly view the inputs, outputs, and parameters accepted by an indicator's `run()` function.

**Print the specification of the TA-Lib's ATR**

```python
vbt.phelp(vbt.talib("atr").run)
```

!!! note
    This is not the same as calling Python's `help` command. It only works on functions.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can list the attributes of any Python object by using 
[pdir](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.pdir). This is helpful when you want to 
check if an object has a specific attribute without searching through the API documentation.

**Print the properties and methods of the Portfolio class**

```python
vbt.pdir(vbt.PF)
```

!!! tip
    You can even use this on third-party objects, such as packages!

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Most VBT objects can be expanded and pretty-formatted to quickly reveal their contents
with [pprint](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.pprint). This is a simple way to
visually confirm that an object has the correct shape and grouping.

**Print the configuration of a Data instance**

```python
vbt.pprint(data)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Most VBT objects can be linked to the API reference on the website and the source code on GitHub
by using [open_api_ref](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module.open_api_ref). The function
accepts a VBT object, its name, or its absolute path inside the package. It can also take third-party
objects; in this case, it will search for them using [:simple-duckduckgo: DuckDuckGo](https://duckduckgo.com/)
and open the first link it finds.

**How to open the online API reference**

```python
vbt.open_api_ref(vbt.nb)  # (1)!
vbt.open_api_ref(vbt.nb.rolling_mean_nb)  # (2)!
vbt.open_api_ref(vbt.PF)  # (3)!
vbt.open_api_ref(vbt.Data.run)  # (4)!
vbt.open_api_ref(vbt.Data.features)  # (5)!
vbt.open_api_ref(vbt.ADX.adx_crossed_above)  # (6)!
vbt.open_api_ref(vbt.settings)  # (7)!
vbt.open_api_ref(pf.get_sharpe_ratio)  # (8)!
vbt.open_api_ref((pf, "sharpe_ratio"))  # (9)!
vbt.open_api_ref(pd.DataFrame)  # (10)!
vbt.open_api_ref("vbt.PF")  # (11)!
vbt.open_api_ref("SizeType")  # (12)!
vbt.open_api_ref("DataFrame", module="pandas")  # (13)!
vbt.open_api_ref("numpy.char.find", resolve=False)  # (14)!
```

1. Module.

2. Function.

3. Class.

4. Method.

5. Property.

6. Generated method or property.

7. Instance.

8. Method of an instance.

9. Any attribute of an instance (note: you cannot use `pf.sharpe_ratio` here).

10. Third-party objects work too.

11. Shortcut.

12. Find an object across the package.

13. Find an object across a third-party package.

14. Use the reference without checking its validity.

!!! tip
    To get the link without opening it, use [get_api_ref](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module.get_api_ref).
    It takes the same arguments.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To open the first result for any search query, use [imlucky](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module.imlucky).

**Ask a question if you feel lucky**

```python
vbt.imlucky("How to create a structured NumPy array?")  # (1)!
```

1. Opens https://numpy.org/doc/stable/user/basics.rec.html.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/indexing/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Indexing
type: page
icon: material-button-pointer
---

# Indexing

Most VBT objects, such as data instances and portfolios, can be indexed like regular Pandas objects
using the `[]`, `iloc`, `loc`, and `xs` selectors. The operation applies to all arrays within the
instance, and creates a new instance containing the selected arrays.

**Select a date range of a Data instance**

```python
new_data = data.loc["2020-01-01":"2020-12-31"]
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

There is also a special selector, `xloc`, that accepts a smart indexing instruction. Such an
instruction can include one or more positions, labels, dates, times, ranges, frequencies, or even date
offsets. It is automatically parsed and translated into an array with integer positions, which are then
internally passed to the `iloc` selector.

**Various smart row indexing operations on a Data instance**

```python
new_data = data.xloc[::2]  # (1)!
new_data = data.xloc[np.array([10, 20, 30])]  # (2)!
new_data = data.xloc["2020-01-01 17:30"]  # (3)!
new_data = data.xloc["2020-01-01"]  # (4)!
new_data = data.xloc["2020-01"]  # (5)!
new_data = data.xloc["2020"]  # (6)!
new_data = data.xloc["2020-01-01":"2021-01-01"]  # (7)!
new_data = data.xloc["january":"april"]  # (8)!
new_data = data.xloc["monday":"saturday"]  # (9)!
new_data = data.xloc["09:00":"16:00"]  # (10)!
new_data = data.xloc["16:00":"09:00"]  # (11)!
new_data = data.xloc["monday 09:00":"friday 16:00"]  # (12)!
new_data = data.xloc[
    vbt.autoidx(slice("monday", "friday"), closed_end=True) &  # (13)!
    vbt.autoidx(slice("09:00", "16:00"), closed_end=False)
]
new_data = data.xloc["Y"]  # (14)!
new_data = data.xloc[pd.Timedelta(days=7)]  # (15)!
new_data = data.xloc[df.index.weekday == 0]  # (16)!
new_data = data.xloc[pd.tseries.offsets.BDay()]  # (17)!
```

1. Every second row.

2. Rows 10, 20, and 30.

3. All rows that match the datetime 2020-01-01 17:30.

4. All rows that match the day 2020-01-01.

5. All rows that match the month 2020-01.

6. All rows that match the year 2020.

7. All rows from 2020-01-01 to 2020-12-31 (exclusive).

8. From January to April (exclusive).

9. From Monday to Saturday (exclusive).

10. Between 09:00 and 16:00 (exclusive).

11. Between 16:00 and 09:00 (exclusive).

12. Datetime components can also be combined. Here, between Monday 09:00 and Friday 16:00
(exclusive).

13. Indexers can also be combined like masks. Here, from Monday to Friday (inclusive) between 09:00
and 16:00 (exclusive).

14. Every year start (frequency string).

15. Every 7 days (timedelta).

16. Every Monday (mask).

17. Every business day ([date offset](https://pandas.pydata.org/docs/user_guide/timeseries.html#dateoffset-objects)).

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can select not only rows but also columns by combining
[rowidx](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.RowIdxr) and
[colidx](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.ColIdxr) instructions.

**Various smart row and/or column indexing operations on a DataFrame accessor**

```python
new_df = df.vbt.xloc[vbt.colidx(0)].get()  # (1)!
new_df = df.vbt.xloc[vbt.colidx("BTC-USD")].get()  # (2)!
new_df = df.vbt.xloc[vbt.colidx((10, "simple", "BTC-USD"))].get()  # (3)!
new_df = df.vbt.xloc[vbt.colidx("BTC-USD", level="symbol")].get()  # (4)!
new_df = df.vbt.xloc["2020", "BTC-USD"].get()  # (5)!
```

1. First column.

2. Column label "BTC-USD".

3. Column label (10, "simple", "BTC-USD").

4. Any column with symbol "BTC-USD".

5. Year 2020 and column label "BTC-USD". Both can also be wrapped with `rowidx` and `colidx`
respectively, for example, to pass indexer-related keyword arguments.

!!! info
    Without the `get()` call, the accessor will be returned. There is __no need__ for this call
    when indexing other VBT objects, such as portfolios.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Pandas accessors can also be used to modify values under certain rows and columns.
This is not possible for more complex VBT objects.

**Enter at the beginning of the business day, exit at the end**

```python
entries.vbt.xloc[vbt.autoidx(slice("mon", "sat")) & vbt.autoidx("09:00")] = True  # (1)!
exits.vbt.xloc[vbt.autoidx(slice("mon", "sat")) & (vbt.autoidx("16:00") << 1)] = True  # (2)!

entries.vbt.xloc[vbt.pointidx(every="B", at_time="09:00")] = True  # (3)!
exits.vbt.xloc[vbt.pointidx(every="B", at_time="16:00", indexer_method="before")] = True
```

1. Saturday is exclusive.

2. The operation `<< 1` shifts the mask backward to get the last time before 16:00.

3. Another approach by using [PointIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PointIdxr)
with a business day frequency.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/indicators/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Indicators
type: page
icon: material-chart-timeline-variant
---

# Indicators

!!! question
    Learn more in the [Indicators documentation](https://vectorbt.pro/pvt_41531c19/documentation/indicators/).

## Listing

To list the currently supported indicators, use
[IndicatorFactory.list_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_indicators).
You can filter the returned indicator names by location, which can be listed with
[IndicatorFactory.list_locations](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_locations),
or by applying a glob or regex pattern.

**List supported indicators and locations**

```python
indicator_names = vbt.IF.list_indicators()  # (1)!
indicator_names = vbt.IF.list_indicators("vbt")  # (2)!
indicator_names = vbt.IF.list_indicators("talib")  # (3)!
indicator_names = vbt.IF.list_indicators("RSI*")  # (4)!
indicator_names = vbt.IF.list_indicators("*ma")  # (5)!
indicator_names = vbt.IF.list_indicators("[a-z]+ma$", use_regex=True)  # (6)!
indicator_names = vbt.IF.list_indicators("*ma", location="pandas_ta")  # (7)!

location_names = vbt.IF.list_locations()  # (8)!
```

1. List all indicators.

2. List all custom-implemented VBT indicators.

3. List all TA-Lib indicators.

4. List any indicators that start with "RSI", such as "RSI" and "RSIIndicator".

5. List any indicators that end with "ma", such as "MA" and "SMA".

6. Same as above but as a regular expression.

7. Same as above but only those offered by Pandas TA.

8. List all locations.

!!! note
    If you do not specify a location, indicators from all locations will be parsed, which may take some time.
    To avoid repeated calls, save the results to a variable.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To get the class of an indicator, use [IndicatorFactory.get_indicator](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.get_indicator).

**How to get the indicator class**

```python
vbt.BBANDS  # (1)!

BBANDS = vbt.IF.get_indicator("pandas_ta:BBANDS")  # (2)!
BBANDS = vbt.indicator("pandas_ta:BBANDS")  # (3)!
BBANDS = vbt.IF.from_pandas_ta("BBANDS")  # (4)!
BBANDS = vbt.pandas_ta("BBANDS")  # (5)!

RSI = vbt.indicator("RSI")  # (6)!
```

1. Custom-implemented VBT indicators can be accessed directly.

2. Get the indicator class by name as returned by `list_indicators`.

3. Same as above.

4. Same as above using the respective function.

5. Same as above.

6. Search for an indicator with the name "RSI" and return the first one found.
Custom, TA-LIB, and Pandas-TA indicators are preferred in that order.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To get familiar with an indicator class, call [phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp)
on the `run` class method, which is used to run the indicator. The specification,
such as input names, is also available through various properties that can be accessed programmatically.

**How to get the specification of an indicator class**

```python
vbt.phelp(vbt.OLS.run)  # (1)!

print(vbt.OLS.input_names)  # (2)!
print(vbt.OLS.param_names)  # (3)!
print(vbt.OLS.param_defaults)  # (4)!
print(vbt.OLS.in_output_names)  # (5)!
print(vbt.OLS.output_names)  # (6)!
print(vbt.OLS.lazy_output_names)  # (7)!
```

1. Get the signature of the `run` class method along with the specification.

2. Get the input names, such as "close" in `talib:SMA`.
These are the arrays on which the indicator is computed.

3. Get the parameter names, such as "timeperiod" in `talib:SMA`.
These are the parameters being tested.

4. Get the parameter defaults.

5. Get the in-place output names, such as "stop_ts" in `vbt:STX`.
These are the arrays written in-place by the indicator.

6. Get the output names, such as "entries" and "exits" in `vbt:RANDNX`.
These are the arrays returned by the indicator.

7. Get the lazy output names, such as "bandwidth" in `vbt:BBANDS`.
These arrays can be optionally calculated after the indicator has finished computing.

## Running

To run an indicator, call the [IndicatorBase.run](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run)
class method of its class. Pass the input arrays (any array-like objects,
such as Pandas DataFrames or NumPy arrays), parameters (which can be single values or lists for testing
multiple parameter combinations), and other arguments expected by the indicator. Running the indicator
returns __an indicator instance__ (not the actual arrays!).

**How to run an indicator**

```python
bbands = vbt.BBANDS.run(close)  # (1)!
bbands = vbt.BBANDS.run(open)  # (2)!
bbands = vbt.BBANDS.run(close, window=20)  # (3)!
bbands = vbt.BBANDS.run(close, window=vbt.Default(20))  # (4)!
bbands = vbt.BBANDS.run(close, window=20, hide_params=["window"])  # (5)!
bbands = vbt.BBANDS.run(close, window=20, hide_params=True)  # (6)!
bbands = vbt.BBANDS.run(close, window=[10, 20, 30])  # (7)!
bbands = vbt.BBANDS.run(close, window=[10, 20, 30], alpha=[2, 3, 4])  # (8)!
bbands = vbt.BBANDS.run(close, window=[10, 20, 30], alpha=[2, 3, 4], param_product=True)  # (9)!
```

1. Run the indicator on close price with default parameters.

2. Run the indicator on open price. You can pass any time series if the indicator expects only "close".
But if it expects other time series such as "open", "high", or "low", use only "close".

3. Test a window of 20. If the close price is a DataFrame, this will append a new column level called "window".
If the close price is a Series, its name will become a tuple.

4. To avoid adding a new column level, wrap with `Default`.

5. Alternatively, hide the parameter.

6. Hide all parameters. This is only useful when testing one parameter combination,
and you want the output arrays to have the same columns or names as the input arrays.

7. Test three windows. This will produce three Series or DataFrames stacked into one DataFrame along columns.

8. Test three parameter combinations: (10, 2), (20, 3), and (30, 4).

9. Test all combinations between both parameters, resulting in a total of 9 tests.

!!! warning
    Testing a wide grid of parameter combinations will produce large arrays. For example,
    testing 10000 parameter combinations on one year of daily data would create an array that
    takes 30MB of RAM. If the indicator returns three arrays, RAM consumption would be at least 120MB.
    For one year of minute data, this would result in about 40GB. To avoid excessive memory use,
    test only a subset of combinations at a time, such as by using
    [parameterization](https://vectorbt.pro/pvt_41531c19/cookbook/optimization#parameterization) or [chunking](https://vectorbt.pro/pvt_41531c19/cookbook/optimization#chunking).

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Often, you may want an indicator to skip missing values. For this, use `skipna=True`.
This argument works for any indicator, not just TA-Lib indicators, with one requirement:
the jitted loop must be disabled. When passing a two-dimensional input array, make sure to also
set `split_columns=True` to split its columns and process one column at a time.

**Run an indicator on valid values only**

```python
bbands = vbt.BBANDS.run(close_1d, skipna=True)
bbands = vbt.BBANDS.run(close_2d, split_columns=True, skipna=True)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Another approach is to remove missing values entirely.

**Remove missing values in an indicator**

```python
bbands = bbands.dropna()  # (1)!
bbands = bbands.dropna(how="all")  # (2)!
```

1. Remove any row that has at least one missing value across all outputs.

2. Remove any row that has all values missing across all outputs.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To retrieve the output arrays from an indicator instance, you can access each one as an attribute,
or use various unpacking options such as [IndicatorBase.unpack](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.unpack).

**How to retrieve output arrays**

```python
bbands = vbt.talib("BBANDS").run(close)
upperband_df = bbands.upperband  # (1)!
middleband_df = bbands.middleband
lowerband_df = bbands.lowerband
upperband_df, middleband_df, lowerband_df = bbands.unpack()  # (2)!
output_dict = bbands.to_dict()  # (3)!
output_df = bbands.to_frame()  # (4)!

sma = vbt.talib("SMA").run(close)
sma_df = sma.real  # (5)!
sma_df = sma.sma  # (6)!
sma_df = sma.output  # (7)!
sma_df = sma.unpack()
```

1. Retrieve the upper band array from the indicator instance.

2. Retrieve the output arrays in the same order as in `bbands.output_names`.

3. Retrieve a dictionary with the output arrays.

4. Retrieve a DataFrame with the output arrays stacked along columns.

5. If a TA-Lib indicator has only one output array, it is usually named "real".

6. The only output can also be accessed by the short name of the indicator.

7. The only output also has an alias "output".

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To keep outputs in NumPy format and/or skip any shape checks, use `return_raw="outputs"`.

**Keep NumPy format**

```python
upperband, middleband, lowerband = vbt.talib("BBANDS").run(close, return_raw="outputs")
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

A simpler way to run indicators is by using [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run),
which takes an indicator name or class, determines what input names the indicator expects,
and runs the indicator by automatically passing all the inputs found in the data instance.
This method also supports unpacking and running multiple indicators, which is very useful for
feature engineering.

**How to run indicators automatically**

```python
bbands = data.run("vbt:BBANDS")  # (1)!
bbands = data.run("vbt:BBANDS", window=20)  # (2)!
upper, middle, lower = data.run("vbt:BBANDS", unpack=True)  # (3)!

features_df = data.run(["talib:BBANDS", "talib:RSI"])  # (4)!
bbands, rsi = data.run(["talib:BBANDS", "talib:RSI"], concat=False)  # (5)!
features_df = data.run(  # (6)!
    ["talib:BBANDS", "talib:RSI"], 
    timeperiod=vbt.run_func_dict(talib_bbands=20, talib_rsi=30),
    hide_params=True
)
features_df = data.run(  # (7)!
    ["talib:BBANDS", "vbt:RSI"], 
    talib_bbands=vbt.run_arg_dict(timeperiod=20),
    vbt_rsi=vbt.run_arg_dict(window=30),
    hide_params=True
)
features_df = data.run("talib_all")  # (8)!
```

1. Run `vbt.BBANDS` with the "close" input automatically substituted by `data.close`.

2. Any arguments will be passed to the indicator's `run` class method.

3. The output arrays can be unpacked directly. The `unpack` argument also accepts the options
"dict" and "frame".

4. Run multiple indicators. By default, they will be concatenated into a DataFrame along columns.
The function and output names will appear in the column levels "run_func" and "output" respectively.

5. Do not concatenate indicators but return them as a list.

6. Pass different values under the same parameter name using `run_func_dict`.

7. Pass different keyword arguments to each indicator using `run_arg_dict`.

8. Run all TA-Lib indicators.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To quickly run and plot a TA-Lib indicator on a single parameter combination without using the
indicator factory, use [talib_func](https://vectorbt.pro/pvt_41531c19/api/indicators/talib_/#vectorbtpro.indicators.talib.talib_func)
and [talib_plot_func](https://vectorbt.pro/pvt_41531c19/api/indicators/talib_/#vectorbtpro.indicators.talib.talib_plot_func)
respectively. Unlike the official TA-Lib implementation, these handle DataFrames,
NaNs, broadcasting, and timeframes properly. The indicator factory's TA-Lib version is based
on these functions.

**Quickly run and plot a TA-Lib indicator**

```python
run_bbands = vbt.talib_func("BBANDS")
upperband, middleband, lowerband = run_bbands(close, timeperiod=2)
upperband, middleband, lowerband = data.run("talib_func:BBANDS", timeperiod=2)  # (1)!

plot_bbands = vbt.talib_plot_func("BBANDS")
fig = plot_bbands(upperband, middleband, lowerband)
```

1. Same as above.

### Parallelization

Parameter combinations are processed using [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute)
so it is straightforward to parallelize their execution.

**Various parallelization configurations**

```python
any_indicator.run(...)  # (1)!

# ______________________________________________________________

numba_indicator.run(  # (2)!
    ...,
    jitted_loop=True,  # (3)!
    jitted_warmup=True,    # (4)!
    execute_kwargs=dict(n_chunks="auto", engine="threadpool")
)

# ______________________________________________________________

python_indicator.run(  # (5)!
    ...,
    execute_kwargs=dict(n_chunks="auto", distribute="chunks", engine="pathos")
)
```

1. By default, the method processes parameter combinations serially (not in parallel).

2. If the indicator is compiled with Numba and `nogil` is enabled, divide parameter combinations
into an optimal number of chunks, and execute all chunks in parallel with multithreading (one chunk per thread).

3. Wrap the loop with Numba that iterates over parameter combinations (required).

4. Dry-run one parameter combination to compile the indicator ahead of distribution (optional).

5. If the indicator is __not__ compiled with Numba, divide parameter combinations into an optimal
number of chunks and execute all chunks in parallel with multiprocessing (one chunk per process),
while processing all parameter combinations within each chunk serially.

## Registration

Custom indicators can be registered with the indicator factory to appear in the list of all indicators.
This allows you to refer to the indicator by name when running a data instance.
Upon registration, you can assign the indicator to a custom location (the default is "custom"),
which serves as a tag or group. This lets you create arbitrary indicator groups. One indicator can be
assigned to multiple locations. Custom indicators take priority over built-in indicators.

```python
vbt.IF.register_custom_indicator(sma_indicator)  # (1)!
vbt.IF.register_custom_indicator(sma_indicator, "SMA")  # (2)!
vbt.IF.register_custom_indicator(sma_indicator, "SMA", location="rolling")  # (3)!
vbt.IF.register_custom_indicator(sma_indicator, "rolling:SMA")
vbt.IF.register_custom_indicator("talib:sma", location="rolling")  # (4)!

vbt.IF.deregister_custom_indicator("SMA", location="rolling")  # (5)!
vbt.IF.deregister_custom_indicator("rolling:SMA")
vbt.IF.deregister_custom_indicator("SMA")  # (6)!
vbt.IF.deregister_custom_indicator(location="rolling")  # (7)!
vbt.IF.deregister_custom_indicator()  # (8)!
```

1. Register an indicator class. The class name will be used as the indicator name.

2. Register an indicator class with a specific name.

3. Register an indicator class with a specific name and under a specific location.

4. Assign a built-in indicator class to a custom location (for example, for tagging).

5. Deregister all indicators with a specific name under a specific location.

6. Deregister all indicators with a specific name under all locations.

7. Deregister all indicators under a specific location.

8. Deregister all indicators under all locations.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
children:
  - https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Knowledge
type: page
icon: material-message-text-outline
---

Recipes for managing knowledge assets in VectorBT PRO

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#assets
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Assets
type: heading 2
---

## Assets

Knowledge assets are instances of [KnowledgeAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset)
that contain a list of Python objects (most often dicts) and provide various methods to manipulate them.
For usage examples, see the API documentation for each specific method.

### VBT assets

There are two knowledge assets in VBT: 1) website pages, and 2) Discord messages. The first asset includes
pages and headings from the (mainly private) website. Each data item represents either a page or a page
heading. Pages generally point to one or more other pages or headings, while headings contain text content,
all reflecting the structure of Markdown files. The second asset consists of messages from the "vectorbt.pro"
Discord server, where each data item is a Discord message, which may reference other messages through replies.

The assets are attached to each [release](https://github.com/polakowo/vectorbt.pro/releases) as
`pages.json.zip` and `messages.json.zip`. These are ZIP-compressed JSON files, managed by
[PagesAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.PagesAsset) and
[MessagesAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.MessagesAsset)
classes, respectively. You can load them automatically or manually. For automatic loading, a GitHub token
is required.

!!! tip
    The first pull will download the assets, and subsequent pulls will use cached versions.
    After upgrading VBT, new assets will be downloaded automatically.

**How to load an asset**

```python
env["GITHUB_TOKEN"] = "<YOUR_GITHUB_TOKEN>"  # (1)!
pages_asset = vbt.PagesAsset.pull()
messages_asset = vbt.MessagesAsset.pull()

# ______________________________________________________________

vbt.settings.set("knowledge.assets.vbt.token", "YOUR_GITHUB_TOKEN")  # (2)!
pages_asset = vbt.PagesAsset.pull()
messages_asset = vbt.MessagesAsset.pull()

# ______________________________________________________________

pages_asset = vbt.PagesAsset(/MessagesAsset).pull(release_name="v2024.8.20") # (3)!
pages_asset = vbt.PagesAsset(/MessagesAsset).pull(cache_dir="my_cache_dir") # (4)!
pages_asset = vbt.PagesAsset(/MessagesAsset).pull(clear_cache=True) # (5)!
pages_asset = vbt.PagesAsset(/MessagesAsset).pull(cache=False)  # (6)!

# ______________________________________________________________

pages_asset = vbt.PagesAsset.from_json_file("pages.json.zip") # (7)!
messages_asset = vbt.MessagesAsset.from_json_file("messages.json.zip")
```

1. Set the token as an environment variable.

2. Set the token as a global setting.

3. Use the asset from a different release. By default, the asset of the installed release is used (see `vbt.version`).

4. Specify a custom cache directory. By default, data is stored under 
__$user_cache_dir/knowledge/vbt/$release_name/pages/assets/__ for pages and 
__$user_cache_dir/knowledge/vbt/$release_name/messages/assets/__ for messages.

5. Clear the cache directory and pull the asset again. By default, the asset will not be pulled if it already exists.

6. Create a temporary directory.

7. Load the asset file that has already been downloaded.

### Generic assets

Knowledge assets are not limited to VBT assets—you can build an asset from any list!

**How to load an asset**

```python
asset = vbt.KnowledgeAsset(my_list)  # (1)!
asset = vbt.KnowledgeAsset.from_json_file("my_list.json")  # (2)!
asset = vbt.KnowledgeAsset.from_json_bytes(vbt.load_bytes("my_list.json"))  # (3)!
```

1. Create an instance by wrapping a list directly.

2. Create an instance from a JSON file.

3. Create an instance from JSON bytes.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#describing
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Describing
type: heading 2
---

## Describing

Knowledge assets behave like regular lists, so to describe an asset, you should describe it as a list.
This enables many analysis options, such as checking the length, printing out a random data item, and
more advanced options like printing out the field schema. Most data items in an asset are dicts, so you 
can describe them by their fields.

**How to describe an asset**

```python
print(len(asset))  # (1)!

asset.sample().print()  # (2)!
asset.print_sample()

asset.print_schema()  # (3)!

vbt.pprint(messages_asset.describe())  # (4)!

pages_asset.print_site_schema()  # (5)!
```

1. Get the number of data items.

2. Pick a random data item and print it.

3. Visualize the asset's field schema as a tree. Shows each field, including frequency and type.
Works on all assets where data items are dicts.

4. Describe each field. Works on all assets where data items are dicts.

5. Visualize the URL schema as a tree. Works on pages and headings only.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#manipulating
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Manipulating
type: heading 2
---

## Manipulating

A knowledge asset is simply an advanced list: it looks like a VBT object, but behaves like a list.
For manipulation, it provides a collection of methods ending with `item` or `items` to get, set, or
remove data items—either by returning a new asset instance (default) or modifying the asset in place.

**How to manipulate an asset**

```python
d = asset.get_items(0)  # (1)!
d = asset[0]
data = asset[0:100]  # (2)!
data = asset[mask]  # (3)!
data = asset[indices]  # (4)!

# ______________________________________________________________

new_asset = asset.set_items(0, new_d)  # (5)!
asset.set_items(0, new_d, inplace=True)  # (6)!
asset[0] = new_d  # (7)!
asset[0:100] = new_data
asset[mask] = new_data
asset[indices] = new_data

# ______________________________________________________________

new_asset = asset.delete_items(0)  # (8)!
asset.delete_items(0, inplace=True)
asset.remove(0)
del asset[0]
del asset[0:100]
del asset[mask]
del asset[indices]

# ______________________________________________________________

new_asset = asset.append_item(new_d)  # (9)!
asset.append_item(new_d, inplace=True)
asset.append(new_d)

# ______________________________________________________________

new_asset = asset.extend_items([new_d1, new_d2])  # (10)!
asset.extend_items([new_d1, new_d2], inplace=True)
asset.extend([new_d1, new_d2])
asset += [new_d1, new_d2]

# ______________________________________________________________

print(d in asset)  # (11)!
print(asset.index(d))  # (12)!
print(asset.count(d))  # (13)!

# ______________________________________________________________

for d in asset:  # (14)!
    ...
```

1. Get the first data item.

2. Get the first 100 data items.

3. Get the data items that correspond to True in the mask.

4. Get the data items at the positions in the index array.

5. Set the first data item and return a new asset instance.

6. Set the first data item by modifying the asset in place.

7. Built-in methods always modify the existing asset instance.

8. Remove the first data item.

9. Append a new data item.

10. Extend with new data items.

11. Check if the asset contains a data item.

12. Get the position of a data item in the asset.

13. Get the number of data items matching a specific data item.

14. Iterate over data items.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#querying
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Querying
type: heading 2
---

## Querying

A wide range of methods are available to query an asset:
[get](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.get) /
[select](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.select),
[query](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.query) /
[filter](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.filter),
and [find](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.find).
The first pair is used to get and process one or more fields from each data item. The `get` method returns the
raw output, while the `select` method returns a new asset instance. The second pair lets you run queries on
the asset using various engines, such as JMESPath. Again, the `query` method returns the raw output, while
the `filter` method returns a new asset instance. Finally, the `find` method is specialized for searching
across one or more fields. By default, it returns a new asset instance.

**How to query an asset**

```python
messages = messages_asset.get()  # (1)!
total_reactions = sum(messages_asset.get("reactions"))  # (2)!
first_attachments = messages_asset.get("attachments[0]['content']", skip_missing=True)  # (3)!
first_attachments = messages_asset.get("attachments.0.content", skip_missing=True)  # (4)!
stripped_contents = pages_asset.get("content", source="x.strip() if x else ''")  # (5)!
stripped_contents = pages_asset.get("content", source=lambda x: x.strip() if x else '')  # (6)!
stripped_contents = pages_asset.get(source="content.strip() if content else ''")  # (7)!

# (8)!

all_contents = pages_asset.select("content").remove_empty().get()  # (9)!
all_attachments = messages_asset.select("attachments").merge().get()  # (10)!
combined_content = messages_asset.select(source=vbt.Sub('[$author] $content')).join()  # (11)!

# ______________________________________________________________

user_questions = messages_asset.query("content if '@maintainer' in mentions else vbt.NoResult")  # (12)!
is_user_question = messages_asset.query("'@maintainer' in mentions", return_type="bool")  # (13)!
all_attachments = messages_asset.query("[].attachments | []", query_engine="jmespath")  # (14)!
all_classes = pages_asset.query("name[obj_type == 'class'].sort_values()", query_engine="pandas")  # (15)!

# (16)!

support messages = messages_asset.filter("channel == 'support'")  # (17)!

# ______________________________________________________________

new_messages_asset = messages_asset.find("@maintainer")  # (18)!
new_messages_asset = messages_asset.find("@maintainer", path="author")  # (19)!
new_messages_asset = messages_asset.find(vbt.Not("@maintainer"), path="author")  # (20)!
new_messages_asset = messages_asset.find(  # (21)!
    ["@maintainer", "from_signals"], 
    path=["author", "content"], 
    find_all=True
)

found_fields = messages_asset.find(  # (22)!
    ["vbt.Portfolio", "vbt.PF"], 
    return_type="field"
).get()
found_code_matches = messages_asset.find(  # (23)!
    r"(?<!`)`([^`]*)`(?!`)", 
    mode="regex", 
    return_type="match",
).sort().get()
```

1. Get all data items. This is the same as `messages_asset.data`.

2. Get the value from a simple field in each data item. Here, retrieve and sum the number of reactions.

3. Get the value from a nested field in each data item. If the field does not always exist,
skip data items where it is missing.

4. You can also use dot notation for the path to the value.

5. Post-process the value using a source expression. Here, strip the selected value for "content".

6. The source can also be a function.

7. If no value is selected, the source expression or function is applied to the entire data item,
with the data item referred to as "x" and its fields accessed by name.

8. If you need to process results further, use `select` instead of `get`. It accepts the same arguments.

9. Select contents while removing None and empty strings.

10. Select attachments, merge them into a list, and extract.

11. Format the author and content into a string, then join those strings.

12. Return the content if @polakowo is in mentions; otherwise, ignore. The expression here works
similarly to the source expression in `get`.

13. Return True if @polakowo is in mentions; otherwise, return False. Without `return_type="bool"`,
it would act as a filter and return the data items matching the condition.

14. Use JMESPath to extract all attachments.

15. You can also use Pandas as a query engine, where each field becomes a Series.
Here, get the heading name for every data item with object type "class", and sort.

16. If you need to process results further, use `filter` instead of `query`. It accepts the same arguments.

17. Filter messages that belong to the support channel.

18. Find messages that mention @polakowo in any field.

19. Find messages where @polakowo is the author.

20. Find messages where @polakowo is not the author.

21. Find messages where @polakowo is the author and `from_signals` is mentioned in the content.
If `find_all` is False, the conjunction would be "or".

22. Find all fields mentioning either `vbt.Portfolio` or `vbt.PF`.

23. Find all regex matches for code snippets using a single backtick.

!!! tip
    To make chained calls easier to read, use either of the following styles:

    **How to find admonition types**

```python
    admonition_types = (
        pages_asset.find(
            r"!!!\s+(\w+)", 
            mode="regex", 
            return_type="match"
        )
        .sort()
        .get()
    )
    admonition_types = pages_asset.chain([
        ("find", (r"!!!\s+(\w+)",), dict(mode="regex", return_type="match")),
        "sort",
        "get"
    ])
    ```

### Code

There is a specialized method for finding code, either in single backticks or code blocks.

```python title="How to find code"
found_code_blocks = messages_asset.find_code().get()  # (1)!
found_code_blocks = messages_asset.find_code(language="python").get()  # (2)!
found_code_blocks = messages_asset.find_code(language=True).get()  # (3)!
found_code_blocks = messages_asset.find_code("from_signals").get()  # (4)!
found_code_blocks = messages_asset.find_code("from_signals", in_blocks=False).get()  # (5)!
found_code_blocks = messages_asset.find_code("from_signals", path="attachments").get()  # (6)!
```

1. Find any code blocks across all fields.

2. Find any Python code blocks across all fields.

3. Find code blocks annotated with any language across all fields.

4. Find code blocks that mention `from_signals` across all fields.

5. Find code that mentions `from_signals` across all fields.

6. Find code blocks that mention `from_signals` within attachments.

### Links

Custom knowledge assets like pages and messages have specialized methods for finding data items by
their link. The default behavior matches the target against the end of each link. This approach ensures
that searching for both "https://vectorbt.pro/become-a-member/" and "become-a-member/" will consistently
return "https://vectorbt.pro/become-a-member/". When using "exact" or "end" mode, a variant with or without
a slash is automatically included. This way, searching for "become-a-member" (without a slash) will still
return "https://vectorbt.pro/become-a-member/". The method also ignores other matched links like
"https://vectorbt.pro/become-a-member/#become-a-member" since they belong to the same page.

**How to find links**

```python
new_messages_asset = messages_asset.find_link(  # (1)!
    "https://discord.com/channels/918629562441695344/919715148896301067/923327319882485851"
)
new_messages_asset = messages_asset.find_link("919715148896301067/923327319882485851")  # (2)!

new_pages_asset = pages_asset.find_page(  # (3)!
    "https://vectorbt.pro/pvt_xxxxxxxx/getting-started/installation/"
)
new_pages_asset = pages_asset.find_page("https://vectorbt.pro/pvt_41531c19/getting-started/installation/")  # (4)!
new_pages_asset = pages_asset.find_page("installation/")
new_pages_asset = pages_asset.find_page("installation")  # (5)!
new_pages_asset = pages_asset.find_page("installation", aggregate=True)  # (6)!
```

1. Find the message using a Discord URL.

2. Find the message using a suffix such as `channel_id/message_id`.

3. Find the page using a website URL.

4. Find the page using a suffix.

5. A slash will be added automatically if needed.

6. Find the page and also [aggregate](#aggregating) it.

### Objects

You can also find headings that correspond to VBT objects.

**How to find an object**

```python
new_pages_asset = pages_asset.find_obj(vbt.Portfolio)  # (1)!
new_pages_asset = pages_asset.find_obj(vbt.Portfolio, aggregate=True)  # (2)!
new_pages_asset = pages_asset.find_obj(vbt.PF.from_signals, aggregate=True)
new_pages_asset = pages_asset.find_obj(vbt.pf_nb, aggregate=True)
new_pages_asset = pages_asset.find_obj("SignalContext", aggregate=True)
```

1. Get the top-level heading corresponding to the class `Portfolio`.

2. Get the top-level heading corresponding to the class `Portfolio` along with all sub-headings.

### Nodes

You can also traverse pages and messages in a way similar to navigating nodes within a graph.

**How to traverse an asset**

```python
new_vbt_asset = vbt_asset.select_previous(link)  # (1)!
new_vbt_asset = vbt_asset.select_next(link)

# ______________________________________________________________

new_pages_asset = pages_asset.select_parent(link)  # (2)!
new_pages_asset = pages_asset.select_children(link)
new_pages_asset = pages_asset.select_siblings(link)
new_pages_asset = pages_asset.select_descendants(link)
new_pages_asset = pages_asset.select_branch(link)
new_pages_asset = pages_asset.select_ancestors(link)
new_pages_asset = pages_asset.select_parent_page(link)
new_pages_asset = pages_asset.select_descendant_headings(link)

# ______________________________________________________________

new_messages_asset = messages_asset.select_reference(link)
new_messages_asset = messages_asset.select_replies(link)
new_messages_asset = messages_asset.select_block(link)  # (3)!
new_messages_asset = messages_asset.select_thread(link)
new_messages_asset = messages_asset.select_channel(link)
```

1. These methods and those below work on both pages and messages.

2. These methods and those below do not include the link by default (use `incl_link=True` to include it).

3. These methods and those below include the link by default (use `incl_link=False` to exclude it).

!!! note
    Each operation requires at least one full data pass; use sparingly.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#applying
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Applying
type: heading 2
---

## Applying

"Find" and many other methods rely on [KnowledgeAsset.apply](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.apply),
which executes a function on each data item. These are called asset functions and consist of two
parts: argument preparation and function calling. The main advantage is that arguments are prepared
only once, then passed to each function call. Execution is managed by the powerful
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function, which supports parallelization.

**How to apply a function to an asset**

```python
links = messages_asset.apply("get", "link")  # (1)!

from vectorbtpro.knowledge.base_asset_funcs import GetAssetFunc  # (2)!
args, kwargs = GetAssetFunc.prepare("link")
links = [GetAssetFunc.call(d, *args, **kwargs) for d in messages_asset]

# ______________________________________________________________

links_asset = messages_asset.apply(lambda d: d["link"])  # (3)!
links = messages_asset.apply(lambda d: d["link"], wrap=False)  # (4)!
json_asset = messages_asset.apply(vbt.dump, dump_engine="json")  # (5)!

# ______________________________________________________________

new_asset = asset.apply(  # (6)!
    ...,
    execute_kwargs=dict(
        n_chunks="auto",
        distribute="chunks",
        engine="processpool"
    )
)
```

1. Apply a built-in function. This is equivalent to `get("link")`. Depending on the function,
either a new asset instance or the raw output is returned.

2. The same as above, but performed manually.

3. Apply a custom function. By default, returns a new asset instance.

4. Apply a custom function and return the raw output.

5. Any function that takes a single argument can be used. Here, each message is serialized with JSON.

6. Any operation can be distributed by specifying execution-related keyword arguments.

### Pipelines

Most examples show how to run a chain of standalone operations, but each operation processes the
data at least once. To process data just once, regardless of the number of operations, use asset pipelines.
There are two types: basic and complex. Basic pipelines take a list of tasks (such as functions and
their arguments) and compose them into a single operation that acts on a data item. This composed
operation is then applied to all data items. Complex pipelines use a Python expression in a
functional style, where one function receives a data item and returns a result that becomes
the argument for the next function.

**How to apply a pipeline to an asset**

```python
tasks = [("find", ("@maintainer",), dict(return_type="match")), len, "get"]  # (1)!
tasks = [vbt.Task("find", "@maintainer", return_type="match"), vbt.Task(len), vbt.Task("get")]  # (2)!
mention_count = messages_asset.apply(tasks)  # (3)!

asset_pipeline = vbt.BasicAssetPipeline(tasks) # (4)!
mention_count = [asset_pipeline(d) for d in messages_asset]

# ______________________________________________________________

expression = "get(len(find(d, '@maintainer', return_type='match')))"
mention_count = messages_asset.apply(expression)  # (5)!

asset_pipeline = vbt.ComplexAssetPipeline(expression)  # (6)!
mention_count = [asset_pipeline(d) for d in messages_asset]
```

1. Tasks can be provided as strings, tuples, or functions.

2. Tasks can also be given as instances of [Task](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Task).

3. Use a list of tasks to get the number of @polakowo mentions in each message.

4. The same as above, but performed manually.

5. Use an expression to get the number of @polakowo mentions in each message.

6. The same as above, but performed manually.

!!! info
    In both pipeline types, arguments are prepared only once during initialization.

### Reducing

[Reducing](https://realpython.com/python-reduce-function/) means merging all data items into one.
This requires a function that takes two data items. Initially, these two data items are the initializer
(such as an empty dict) and the first data item. If the initializer is unknown, the first two
data items are used. The result of this first iteration is then used as the first data item in
the next iteration. Execution is handled by [KnowledgeAsset.reduce](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.reduce)
and cannot be parallelized since each iteration depends on the previous one.

**How to reduce an asset**

```python
all_attachments = messages_asset.select("attachments").reduce("merge_lists")  # (1)!

from vectorbtpro.knowledge.base_asset_funcs import MergeListsAssetFunc  # (2)!
args, kwargs = MergeListsAssetFunc.prepare()
d1 = []
for d2 in messages_asset.select("attachments"):
    d1 = MergeListsAssetFunc.call(d1, d2, *args, **kwargs)
all_attachments = d1

# ______________________________________________________________

total_reactions = messages_asset.select("reactions").reduce(lambda d1, d2: d1 + d2)  # (3)!
```

1. Apply a built-in function. This is equivalent to `select("attachments").merge_lists()`.
Depending on the function, either a new asset instance or the raw output is returned.

2. The same as above, but performed manually.

3. Apply a custom function. By default, returns the raw output.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can also split a knowledge asset into groups and reduce those groups. Group iteration is handled
by the [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function, which supports
parallelization.

**How to reduce groups of an asset**

```python
reactions_by_channel = messages_asset.groupby_reduce(  # (1)!
    lambda d1, d2: d1 + d2["reactions"],
    by="channel",
    initializer=0,
    return_group_keys=True
)

# ______________________________________________________________

result = asset.groupby_reduce(  # (2)!
    ...,
    execute_kwargs=dict(
        n_chunks="auto",
        distribute="chunks",
        engine="processpool"
    )
)
```

1. Get the total number of reactions per channel.

2. Any group-by operation can be distributed by specifying execution-related keyword arguments.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#aggregating
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Aggregating
type: heading 2
---

## Aggregating

Since headings are represented as individual data items, they can be aggregated back into their
parent page. This is useful when you want to [format](#formatting) or [display](#browsing) the page.
Note that only headings can be aggregated; pages themselves cannot be aggregated into other pages.

**How to aggregate pages**

```python
new_pages_asset = pages_asset.aggregate()  # (1)!
new_pages_asset = pages_asset.aggregate(append_obj_type=False, append_github_link=False)  # (2)!
```

1. Aggregate headings into the content of the parent page.

2. The same as above, but do not append object type and GitHub source to the API headings.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Messages, on the other hand, can be aggregated across multiple levels: _"message"_, _"block"_, _"thread"_, 
and _"channel"_. Aggregation here means collecting messages that belong to the specified level,
and dumping them into the content of a single, bigger message.

* The _"message"_ level means attachments are included in the content of the message.

* The _"block"_ level combines messages from the same author that reference the same block or
do not reference anything. The link for the block is the link of the first message in the block.

* The _"thread"_ level combines messages from the same channel that are connected by a chain of replies.
The link for the thread is the link of the first message in the thread.

* The _"channel"_ level combines all messages that belong to the same channel.

**How to aggregate messages**

```python
new_messages_asset = messages_asset.aggregate()  # (1)!
new_messages_asset = messages_asset.aggregate(by="message")  # (2)!
new_messages_asset = messages_asset.aggregate(by="block")  # (3)!
new_messages_asset = messages_asset.aggregate(by="thread")  # (4)!
new_messages_asset = messages_asset.aggregate(by="channel")  # (5)!
new_messages_asset = messages_asset.aggregate(
    ...,
    minimize_metadata=True  # (6)!
)
new_messages_asset = messages_asset.aggregate(
    ...,
    dump_metadata_kwargs=dict(dump_engine="nestedtext")  # (7)!
)
```

1. Aggregate into the content of the parent if only one parent exists; otherwise, an error will be raised.

2. Aggregate attachments into the content of the parent message.

3. Aggregate messages into the content of the parent block.

4. Aggregate messages into the content of the parent thread.

5. Aggregate messages into the content of the parent channel.

6. Remove irrelevant keys from metadata before dumping.

7. When adding messages into the parent content, dump their metadata using the selected engine (here, NestedText).

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#formatting
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Formatting
type: heading 2
---

## Formatting

Most Python objects can be dumped (that is, serialized) into strings.

**How to dump an asset**

```python
new_asset = asset.dump()  # (1)!
new_asset = asset.dump(dump_engine="nestedtext", indent=4)  # (2)!

# ______________________________________________________________

print(asset.dump().join())  # (3)!
print(asset.dump().join(separator="\n\n---------------------\n\n"))  # (4)!
print(asset.dump_all())  # (5)!
```

1. Dump each data item. By default, items are dumped into YAML using Ruamel (if installed) or PyYAML.

2. Dump each data item using a custom engine. In this example, NestedText is used with 4 spaces for indentation.

3. Join all dumped data items. The separator is selected automatically.

4. Join all dumped data items using a custom separator.

5. Dump the entire list as a single object.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Custom knowledge assets such as pages and messages can be converted, and optionally saved, in Markdown
or HTML format. Only the "content" field will be converted; other fields are used to build the metadata
block shown at the start of each file.

!!! note
    Without [aggregation](#aggregating), each page heading will become a separate file.

**How to format an asset**

```python
new_pages_asset = pages_asset.to_markdown()  # (1)!
new_pages_asset = pages_asset.to_markdown(root_metadata_key="pages")  # (2)!
new_pages_asset = pages_asset.to_markdown(clear_metadata=False)  # (3)!
new_pages_asset = pages_asset.to_markdown(remove_code_title=False, even_indentation=False)  # (4)!

dir_path = pages_asset.save_to_markdown()  # (5)!
dir_path = pages_asset.save_to_markdown(cache_dir="markdown")  # (6)!
dir_path = pages_asset.save_to_markdown(clear_cache=True)  # (7)!
dir_path = pages_asset.save_to_markdown(cache=False)  # (8)!

# (9)!

# ______________________________________________________________

new_pages_asset = pages_asset.to_html()  # (10)!
new_pages_asset = pages_asset.to_html(to_markdown_kwargs=dict(root_metadata_key="pages"))  # (11)!
new_pages_asset = pages_asset.to_html(make_links=False)  # (12)!
new_pages_asset = pages_asset.to_html(extensions=[], use_pygments=False)  # (13)!

extensions = vbt.settings.get("knowledge.formatting.markdown_kwargs.extensions")
new_pages_asset = pages_asset.to_html(extensions=extensions + ["pymdownx.smartsymbols"])  # (14)!

extensions = vbt.settings.get("knowledge.formatting.markdown_kwargs.extensions")
extensions.append("pymdownx.smartsymbols")  # (15)!

extension_configs = vbt.settings.get("knowledge.formatting.markdown_kwargs.extension_configs")
extension_configs["pymdownx.superfences"]["preserve_tabs"] = False  # (16)!

new_pages_asset = pages_asset.to_html(format_html_kwargs=dict(pygments_kwargs=dict(style="dracula")))  # (17)!
vbt.settings.set("knowledge.formatting.pygments_kwargs.style", "dracula")  # (18)!

style_extras = vbt.settings.get("knowledge.formatting.style_extras")
style_extras.append("""
.admonition.success {
    background-color: #00c8531a;
    border-left-color: #00c853;
}
""")  # (19)!

head_extras = vbt.settings.get("knowledge.formatting.head_extras")
head_extras.append('<link ...>')  # (20)!

body_extras = vbt.settings.get("knowledge.formatting.body_extras")
body_extras.append('<script>...</script>')  # (21)!

vbt.settings.get("knowledge.formatting").reset()  # (22)!

dir_path = pages_asset.save_to_html()  # (23)!

# (24)!
```

1. Convert each data item to a Markdown-formatted string. Returns a new asset instance with a list of strings.

2. Prepend a root key called "pages" (or "messages" for messages) to the metadata block.

3. Keep empty fields in the metadata block. By default, empty fields are removed.

4. Keep the content as is, without removing code titles or adjusting uneven indentation.

5. Save each data item to a Markdown-formatted file. Returns the path to the parent directory.
By default, saves to __$user_cache_dir/knowledge/vbt/$release_name/pages/markdown/__ for pages and
__$user_cache_dir/knowledge/vbt/$release_name/messages/markdown/__ for messages.

6. Specify a different cache directory.

7. Clear the cache directory before saving. By default, a data item is skipped if its file already exists.

8. Create a temporary directory.

9. The `save_to_markdown` method accepts the same arguments as `to_markdown`.

10. Convert each data item to an HTML-formatted string. Returns a new asset instance with a list of strings.

11. The same options available for the `to_markdown` method can be set using `to_markdown_kwargs`.

12. If a URL is found in text, do not make it a link. By default, all URLs become clickable.

13. Disable all Markdown extensions and do not use Pygments for highlighting.

14. Add a new extension to the preset list of Markdown extensions (locally).

15. Add a new extension to the preset list of Markdown extensions (globally).

16. Change a configuration of a Markdown extension.

17. Change the default Pygments highlighting style (locally).

18. Change the default Pygments highlighting style (globally).

19. Add extra CSS at the end of the `<style>` element.

20. Add extra HTML, such as links, at the end of the `<head>` element.

21. Add extra HTML, such as JavaScript, at the end of the `<body>` element.

22. Reset formatting.

23. Save each data item to an HTML-formatted file. Returns the path to the parent directory.
By default, saves to __$user_cache_dir/knowledge/vbt/$release_name/pages/html/__ for pages and
__$user_cache_dir/knowledge/vbt/$release_name/pages/html/__ for messages.

24. The `save_to_html` method accepts the same arguments as `to_html` and `save_to_markdown`.

### Browsing

Pages and messages can be displayed and browsed using static HTML files. When displaying a single item,
VBT creates a temporary HTML file and opens it in the default browser. All links in this file remain
__external__. When displaying multiple items, VBT creates a single HTML file
where the items are shown as iframes that you can navigate using pagination.

**How to display an asset**

```python
file_path = pages_asset.display()  # (1)!
file_path = pages_asset.display(link="documentation/fundamentals")  # (2)!
file_path = pages_asset.display(link="documentation/fundamentals", aggregate=True)  # (3)!

# ______________________________________________________________

file_path = messages_asset.display()  # (4)!
file_path = messages_asset.display(link="919715148896301067/923327319882485851")  # (5)!
file_path = messages_asset.filter("channel == 'announcements'").display()  # (6)!
```

1. Display one or more pages.

2. Select a page to display.

3. If the asset is not aggregated, select a page, find its headings, merge them into one page, and display.

4. Display one or more messages.

5. Choose a message to display.

6. Select messages from the specified channel, aggregate them, and display.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If you want to browse one or more pages (or headings) like a website, VBT can convert all data items to HTML
and replace all external links with __internal__ links, allowing you to navigate from one page to another locally.
But which page appears first? Pages and headings create a directed graph. If there is a single page from which
all other pages can be accessed, that page is displayed first. If there are multiple such pages, VBT creates
an index page with metadata blocks that let you access the other pages (unless you specify `entry_link`).

**How to browse an asset**

```python
dir_path = pages_asset.browse()  # (1)!
dir_path = pages_asset.browse(aggregate=True)  # (2)!
dir_path = pages_asset.browse(entry_link="documentation/fundamentals", aggregate=True)  # (3)!
dir_path = pages_asset.browse(entry_link="documentation", descendants_only=True, aggregate=True)  # (4)!
dir_path = pages_asset.browse(cache_dir="website")  # (5)!
dir_path = pages_asset.browse(clear_cache=True)  # (6)!
dir_path = pages_asset.browse(cache=False)  # (7)!

# ______________________________________________________________

dir_path = messages_asset.browse()  # (8)!
dir_path = messages_asset.browse(entry_link="919715148896301067/923327319882485851")  # (9)!

# (10)!
```

1. Generate an HTML file for every page and heading (unless already aggregated).

2. Aggregate headings into pages and generate an HTML file for each page.

3. Generate an HTML file for every page and start browsing from the selected page.

4. Generate an HTML file for every page that is a descendant of the selected page.

5. Specify a different cache directory. By default, data is stored under 
__$user_cache_dir/knowledge/vbt/$release_name/pages/html/__ for pages and 
__$user_cache_dir/knowledge/vbt/$release_name/messages/html/__ for messages.

6. Clear the cache directory before saving. By default, a page or heading is skipped if its file already exists.

7. Create a temporary directory.

8. Generate an HTML file for every message (note that there may be a large number of messages!).

9. In addition to the above, select a message to open in the default browser.

10. Messages also accept the same caching-related arguments as pages.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#combining
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Combining
type: heading 2
---

## Combining

Assets can be easily combined. If the target class is not specified, their common superclass is used.
For example, combining [PagesAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.PagesAsset)
and [MessagesAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.MessagesAsset)
produces an instance of [VBTAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset),
which includes features shared by both assets, such as the "link" and "content" fields.

**How to combine multiple assets**

```python
vbt_asset = pages_asset + messages_asset  # (1)!
vbt_asset = pages_asset.combine(messages_asset)  # (2)!
vbt_asset = vbt.VBTAsset.combine(pages_asset, messages_asset)  # (3)!
```

1. Concatenate both lists and wrap them with the common superclass.

2. Concatenate both lists and wrap them with the first class.

3. Concatenate both lists and wrap them with the selected class.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If both assets contain the same number of data items, you can also merge them at the data item level.
This method works for complex containers, such as nested dictionaries and lists, by flattening their
nested structures into flat dictionaries, merging them, and then unflattening them back into the original
container types.

**How to merge multiple assets**

```python
new_asset = asset1.merge(asset2)  # (1)!
new_asset = vbt.KnowledgeAsset.merge(asset1, asset2)  # (2)!
```

1. Merge both lists and wrap them with the first class.

2. Merge both lists and wrap them with the selected class.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can also merge all data items in a single asset into one data item.

**How to merge one asset**

```python
new_asset = asset.merge()  # (1)!
new_asset = asset.merge_dicts()  # (2)!
new_asset = asset.merge_lists()  # (3)!
```

1. Calls either `merge_dicts` or `merge_lists` depending on the data type.

2. Concatenate all lists into one.

3. (Deep-)merge all dictionaries into one.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#for-objects
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#searching
name: For objects
type: heading 3
---

### For objects

There are four methods for searching for any VBT object in pages and messages. The first method
searches for the API documentation of the object. The second method searches for mentions of the object in
the non-API (human-readable) documentation. The third method searches for mentions of the object in Discord
messages. The last method searches for mentions of the object in the code of both pages and messages.

**How to find API-related knowledge about an object**

```python
api_asset = vbt.find_api(vbt.PFO)  # (1)!
api_asset = vbt.find_api(vbt.PFO, incl_bases=False, incl_ancestors=False)  # (2)!
api_asset = vbt.find_api(vbt.PFO, use_parent=True)  # (3)!
api_asset = vbt.find_api(vbt.PFO, use_refs=True)  # (4)!
api_asset = vbt.find_api(vbt.PFO.row_stack)  # (5)!
api_asset = vbt.find_api(vbt.PFO.from_uniform, incl_refs=False)  # (6)!
api_asset = vbt.find_api([vbt.PFO.from_allocate_func, vbt.PFO.from_optimize_func])  # (7)!

# ______________________________________________________________

api_asset = vbt.PFO.find_api()  # (8)!
api_asset = vbt.PFO.find_api(attr="from_optimize_func")
```

1. Find the (aggregated) API page for [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer).
It includes the (aggregated) base classes, such as [Configured](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Configured),
and (non-aggregated) parent modules, such as [portfolio.pfopt.base](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base).

2. Do not include base classes and parent modules; include only this class.

3. Use the (aggregated) parent of this class instead. Here, [portfolio.pfopt.base](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base).

4. Include the (non-aggregated) pages of objects referenced by this object.

5. Find the API page for [PortfolioOptimizer.row_stack](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.row_stack).
It includes (aggregated) base methods, such as [Wrapping.row_stack](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.row_stack),
(non-aggregated) parent objects such as [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer),
and (non-aggregated) references.

6. Do not include references.

7. Search for multiple objects.

8. Call the method directly from the object's interface.

**How to find documentation-related knowledge about an object**

```python
docs_asset = vbt.find_docs(vbt.PFO)  # (1)!
docs_asset = vbt.find_docs(vbt.PFO, incl_shortcuts=False, incl_instances=False)  # (2)!
docs_asset = vbt.find_docs(vbt.PFO, incl_custom=["pf_opt"])  # (3)!
docs_asset = vbt.find_docs(vbt.PFO, incl_custom=[r"pf_opt\s*=\s*.+"], is_custom_regex=True)  # (4)!
docs_asset = vbt.find_docs(vbt.PFO, as_code=True)  # (5)!
docs_asset = vbt.find_docs([vbt.PFO.from_allocate_func, vbt.PFO.from_optimize_func])  # (6)!

docs_asset = vbt.find_docs(vbt.PFO, up_aggregate_th=0)  # (7)!
docs_asset = vbt.find_docs(vbt.PFO, up_aggregate_pages=True)  # (8)!
docs_asset = vbt.find_docs(vbt.PFO, incl_pages=["documentation", "tutorials"])  # (9)!
docs_asset = vbt.find_docs(vbt.PFO, incl_pages=[r"(features|cookbook)"], page_find_mode="regex")  # (10)!
docs_asset = vbt.find_docs(vbt.PFO, excl_pages=["release-notes"])  # (11)!

# ______________________________________________________________

docs_asset = vbt.PFO.find_docs()  # (12)!
docs_asset = vbt.PFO.find_docs(attr="from_optimize_func")
```

1. Find mentions of [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
across all non-API pages. It searches for full reference names, shortcuts (such as `vbt.PFO`),
imports (such as `from ... import PFO`), typical instance names (such as `pfo =`), and access or call
notations (such as `PFO.`).

2. Include only full reference names and imports.

3. Include custom literal mentions.

4. Include custom regex mentions.

5. Find mentions only as code.

6. Search for multiple objects.

7. By default, if the method matches two-thirds of all headings sharing the same parent,
it takes the (aggregated) parent instead. Here, it takes the entire page if any mention is found.

8. Similarly, if the method matches two-thirds of all pages sharing the same parent page,
it includes the parent page as well.

9. Include only the links from "documentation" and "tutorials" (the targets are substrings).

10. Include only the links from "features" and "cookbook" (the target is a regex).

11. Exclude the links from "release-notes" (the target is a substring).

12. Call the method directly from the object's interface.

**How to find Discord-related knowledge about an object**

```python
messages_asset = vbt.find_messages(vbt.PFO)  # (1)!

# ______________________________________________________________

messages_asset = vbt.PFO.find_messages()  # (2)!
messages_asset = vbt.PFO.find_messages(attr="from_optimize_func")
```

1. This method works the same as above but finds mentions in messages. It accepts the same arguments
related to targets (the first block in `find_docs` recipes) and does not accept arguments related
to pages (the second block in `find_docs` recipes).

2. Call the method directly from the object's interface.

**How to find code examples of an object**

```python
examples_asset = vbt.find_examples(vbt.PFO)  # (1)!

# ______________________________________________________________

examples_asset = vbt.PFO.find_examples()  # (2)!
examples_asset = vbt.PFO.find_examples(attr="from_optimize_func")
```

1. This method works the same as above but finds code mentions in both pages and messages.

2. Call the method directly from the object's interface.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The first three methods are guaranteed not to overlap, while the last method may return examples
that can also be found by the first three methods. There is also another method that,
by default, calls the first three methods and combines their results into a single asset.
This approach allows you to gather all relevant knowledge about a VBT object.

**How to combine knowledge about an object**

```python
combined_asset = vbt.find_assets(vbt.Trades)  # (1)!
combined_asset = vbt.find_assets(vbt.Trades, asset_names=["api", "docs"])  # (2)!
combined_asset = vbt.find_assets(vbt.Trades, asset_names=["messages", ...])  # (3)!
combined_asset = vbt.find_assets(vbt.Trades, asset_names="all")  # (4)!
combined_asset = vbt.find_assets(  # (5)!
    vbt.Trades, 
    api_kwargs=dict(incl_ancestors=False),
    docs_kwargs=dict(as_code=True),
    messages_kwargs=dict(as_code=True),
)
combined_asset = vbt.find_assets(vbt.Trades, minimize=False)  # (6)!
asset_list = vbt.find_assets(vbt.Trades, combine=False)  # (7)!
combined_asset = vbt.find_assets([vbt.EntryTrades, vbt.ExitTrades])  # (8)!

# ______________________________________________________________

combined_asset = vbt.find_assets("SQL", resolve=False)  # (9)!
combined_asset = vbt.find_assets(["SQL", "database"], resolve=False)  # (10)!

# ______________________________________________________________

messages_asset = vbt.Trades.find_assets()  # (11)!
messages_asset = vbt.Trades.find_assets(attr="plot")
messages_asset = pf.trades.find_assets(attr="expectancy")
```

1. Combine assets for [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades).

2. Use only the API asset and the documentation asset.

3. Place the messages asset first and keep other assets in their usual order by using `...` (Ellipsis).

4. Use all assets, excluding the examples asset (which is already included in other assets).

5. Provide keyword arguments specific to each asset.

6. Disable minimization. This keeps all fields but takes up more context space.

7. Disable combining. Returns a dictionary of assets by their name.

8. Search for multiple objects.

9. Search for arbitrary keywords (not actual VBT objects).

10. Search for multiple keywords.

11. Make a call directly from the object's interface.

**How to browse combined knowledge about an object**

```python
vbt.Trades.find_assets().select("link").print()  # (1)!

file_path = vbt.Trades.find_assets( # (2)!
    asset_names="docs", 
    docs_kwargs=dict(excl_pages="release-notes")
).display()

dir_path = vbt.Trades.find_assets( # (3)!
    asset_names="docs", 
    docs_kwargs=dict(excl_pages="release-notes")
).browse(cache=False)
```

1. Print all found links.

2. Browse all found documentation (excluding release notes) as a single HTML file.

3. Browse all found documentation (excluding release notes) as multiple HTML files.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#globally
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#searching
name: Globally
type: heading 3
---

### Globally

You can search not only for knowledge related to a specific VBT object but also for any VBT items
that match a query in natural language. This process works by embedding both the query and
the data items, calculating their pairwise similarity scores, and sorting the data items by their mean
score in descending order. Since the result contains all the data items from the original set, just
in a different order, it is recommended to select the top-k results before displaying them.

All the methods discussed in [objects](#for-objects) can also be used on queries!

**How to search for knowledge using natural language**

```python
api_asset = vbt.find_api("How to rebalance weekly?", top_k=20)
docs_asset = vbt.find_docs("How to hedge a position?", top_k=20)
messages_asset = vbt.find_messages("How to trade live?", top_k=20)
combined_asset = vbt.find_assets("How to create a custom data class?", top_k=20)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

There is also a specialized [search](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.search)
function that calls [find_assets](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_assets),
caches the documents (so the next search runs much faster), and displays the top results
as a static HTML page.

!!! info
    The first time you run this command, it may take up to 15 minutes to prepare and embed documents.
    However, most preparation steps are cached and stored, so future searches will be significantly
    faster without repeating the process.

**How to search for knowledge on VBT using natural language and display top results**

```python
file_path = vbt.search("How to turn df into data?")  # (1)!
found_asset = vbt.search("How to turn df into data?", display=False)  # (2)!
file_path = vbt.search("How to turn df into data?", display_kwargs=dict(open_browser=False))  # (3)!
file_path = vbt.search("How to fix 'Symbols have mismatching columns'?", asset_names="messages")  # (4)!
file_path = vbt.search("How to use templates in signal_func_nb?", asset_names="examples", display=100)  # (5)!
file_path = vbt.search("How to turn df into data?", search_method="embeddings")  # (6)!
```

1. Search API pages, documentation pages, and Discord messages for a query, then display the 20 most relevant results.

2. Return the results instead of displaying them.

3. Do not automatically open the HTML file in the browser.

4. Search Discord messages only.

5. Search and display the 100 most relevant code examples.

6. Disable hybrid mode.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Building an index of embeddings for searching is not always necessary. Instead, you can use BM25,
a fast and reliable algorithm that works completely offline.

**How to search for knowledge on VBT quickly**

```python
file_path = vbt.quick_search("How to fix 'Symbols have mismatching columns'?")
```

!!! tip
    Use this method when your query contains specific keywords. For vague queries, embeddings are a better choice.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#chatting
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Chatting
type: heading 2
---

## Chatting

Knowledge assets can be used as context when chatting with LLMs. The main method for chatting is
[Contextable.chat](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_ranking/#vectorbtpro.knowledge.doc_ranking.Contextable.chat),
which [dumps](#formatting) the asset instance, combines it with your question and chat history
into messages, sends them to the LLM service, then displays and saves the response. The response
can be shown in various formats, including raw text, Markdown, and HTML. All three formats
support streaming. This method is compatible with multiple LLM APIs, such as OpenAI, LiteLLM, and LLamaIndex.

**How to chat about an asset**

```python
env["OPENAI_API_KEY"] = "<OPENAI_API_KEY>"  # (1)!

# ______________________________________________________________

patterns_tutorial = pages_asset.find_page( # (2)!
    "https://vectorbt.pro/pvt_xxxxxxxx/tutorials/patterns-and-projections/patterns/", 
    aggregate=True
)
patterns_tutorial.chat("How to detect a pattern?")

data_documentation = pages_asset.select_branch("documentation/data").aggregate()  # (3)!
data_documentation.chat("How to convert DataFrame into vbt.Data?")

pfo_api = pages_asset.find_obj(vbt.PFO, aggregate=True)  # (4)!
pfo_api.chat("How to rebalance weekly?")

combined_asset = pages_asset + messages_asset
signal_func_nb_code = combined_asset.find_code("signal_func_nb")  # (5)!
signal_func_nb_code.chat("How to pass an array to signal_func_nb?")

polakowo_messages = messages_asset.filter("author == '@maintainer'").minimize().shuffle()
polakowo_messages.chat("Describe the author of these messages", max_tokens=10_000)  # (6)!

mention_fields = combined_asset.find(
    "parameterize", 
    mode="substring", 
    return_type="field", 
    merge_fields=False
)
mention_counts = combined_asset.find(
    "parameterize", 
    mode="substring", 
    return_type="match", 
    merge_matches=False
).apply(len)
sorted_fields = mention_fields.sort(keys=mention_counts, reverse=True).merge()
sorted_fields.chat("How to parameterize a function?")  # (7)!

vbt.settings.set("knowledge.chat.max_tokens", None)  # (8)!

# ______________________________________________________________

chat_history = []
signal_func_nb_code.chat("How to check if we're in a long position?", chat_history)  # (9)!
signal_func_nb_code.chat("How about short one?", chat_history)  # (10)!
chat_history.clear()  # (11)!
signal_func_nb_code.chat("How to access close price?", chat_history)

# ______________________________________________________________

asset.chat(..., completions="openai", model="gpt-5-mini")  # (12)!
# (13)!
# vbt.settings.set("knowledge.chat.completions_configs.openai.model", "gpt-5-mini")
# (14)!
# vbt.OpenAICompletions.set_settings({"model": "gpt-5-mini"})

env["OPENAI_API_KEY"] = "<YOUR_OPENROUTER_API_KEY>"
asset.chat(..., completions="openai", base_url="https://openrouter.ai/api/v1", model="openai/gpt-4o") 
# vbt.settings.set("knowledge.chat.completions_configs.openai.base_url", "https://openrouter.ai/api/v1")
# vbt.settings.set("knowledge.chat.completions_configs.openai.model", "openai/gpt-4o")
# vbt.OpenAICompletions.set_settings({
#     "base_url": "https://openrouter.ai/api/v1", 
#     "model": "openai/gpt-4o"
# })

env["DEEPSEEK_API_KEY"] = "<YOUR_DEEPSEEK_API_KEY>"
asset.chat(..., completions="litellm", model="deepseek/deepseek-coder")
# vbt.settings.set("knowledge.chat.completions_configs.litellm.model", "deepseek/deepseek-coder")
# vbt.LiteLLMCompletions.set_settings({"model": "deepseek/deepseek-coder"})

asset.chat(..., completions="llama_index", llm="perplexity", model="claude-3-5-sonnet-20240620")  # (15)!
# vbt.settings.set("knowledge.chat.completions_configs.llama_index.llm", "anthropic")
# anthropic_config = {"model": "claude-3-5-sonnet-20240620"}
# vbt.settings.set("knowledge.chat.completions_configs.llama_index.anthropic", anthropic_config)
# vbt.LlamaIndexCompletions.set_settings({"llm": "anthropic", "anthropic": anthropic_config})

vbt.settings.set("knowledge.chat.completions", "litellm")  # (16)!

# ______________________________________________________________

asset.chat(..., stream=False)  # (17)!

asset.chat(..., formatter="plain")  # (18)!
asset.chat(..., formatter="ipython_markdown")  # (19)!
asset.chat(..., formatter="ipython_html")  # (20)!

file_path = asset.chat(..., formatter="html")  # (21)!
file_path = asset.chat(..., formatter="html", formatter_kwargs=dict(cache_dir="chat"))  # (22)!
file_path = asset.chat(..., formatter="html", formatter_kwargs=dict(clear_cache=True))  # (23)!
file_path = asset.chat(..., formatter="html", formatter_kwargs=dict(cache=False))  # (24)!
file_path = asset.chat(  # (25)!
    ..., 
    formatter="html", 
    formatter_kwargs=dict(
        to_markdown_kwargs=dict(...),
        to_html_kwargs=dict(...),
        format_html_kwargs=dict(...)
    )
)

asset.chat(..., formatter_kwargs=dict(update_interval=1.0))  # (26)!

asset.chat(..., formatter_kwargs=dict(output_to="response.txt"))  # (27)!

asset.chat(  # (28)!
    ..., 
    system_prompt="You are a helpful assistant",
    context_prompt="Here's what you need to know: $context"
)
```

1. Setting the API key as an environment variable makes it available to all packages.
Another option is passing `api_key` directly or saving it in the settings, as shown for `model` below.

2. Paste a link from the website (for example, the patterns tutorial) and use it as context.
If you do not know the private hash, you can paste the suffix. See [querying](#querying).

3. Select and aggregate all documentation pages related to data, then use them as context.

4. Select the API documentation page for the portfolio optimizer and use it as context.

5. Find all mentions of `signal_func_nb` throughout the code of all pages and messages, then use them as context.

6. Filter all messages by @polakowo and keep only relevant fields to fit more data.
By default, the context is trimmed to 100,000 tokens (the exact allowance depends on the model,
e.g., GPT-4o supports 128,000). Content is shuffled to prevent placing more weight on older or
newer content when trimming the context.

7. Get all fields with at least one "parameterize" mention, sort them by the number of mentions
in descending order, and merge them into a single list. When embeddings are unavailable, this is a common
workflow when there is too much data.

8. Allow unlimited context.

9. Append the question and answer to the chat history.

10. Reuse the chat history to ask another question in the current chat.

11. Clear the chat history to start a new chat.

12. Specify the model and other client and chat-related arguments for the OpenAI API.

13. Save the model in the settings to use by default next time.

14. Same as above.

15. You can define configs by LLM when using LLamaIndex.

16. Set the default class for completions.

17. Disable streaming. By default, streaming is enabled.

18. Print the response.

19. Display the response in Markdown format (requires IPython environment).

20. Display the response in HTML format (requires IPython environment).

21. Store the response in a static HTML file and display it.

22. Specify a different cache directory. By default, data is stored under 
__$user_cache_dir/knowledge/vbt/$release_name/chat__.

23. Clear the cache directory before saving.

24. Create a temporary file.

25. When working with HTML, you can provide many arguments accepted by
[VBTAsset.to_html](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset.to_html).

26. When using an update interval, streaming data is buffered and released periodically.
Note that when displaying HTML, the minimum update interval is 1 second.

27. In addition to displaying, the raw response can also be saved to a file.

28. Customize the system and context prompts.

### About objects

You can chat about a VBT object using [chat_about](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.chat_about).
This function calls the method above, but operates only on code examples. When you pass arguments,
they are automatically distributed between [find_assets](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_assets)
and [KnowledgeAsset.chat](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.chat)
(see [chatting](#chatting) for recipes).

**How to chat about an object**

```python
vbt.chat_about(vbt.Portfolio, "How to get trading expectancy?")  # (1)!
vbt.chat_about(  # (2)!
    vbt.Portfolio, 
    "How to get returns accessor with log returns?", 
    asset_names="api",
    api_kwargs=dict(incl_bases=False, incl_ancestors=False)
)
vbt.chat_about(  # (3)!
    vbt.Portfolio, 
    "How to backtest a basic strategy?", 
    model="gpt-5-mini",
    max_tokens=100_000,
    shuffle=True
)

# ______________________________________________________________

vbt.Portfolio.chat("How to create portfolio from order records?")  # (4)!
vbt.Portfolio.chat("How to get grouped stats?", attr="stats")
```

1. Find knowledge about [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
and ask a question using this knowledge as context.

2. Pass knowledge-related arguments. Here, find API knowledge that includes only the
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) class.

3. Pass chat-related arguments. In this example, use the gpt-5-mini model on a shuffled context with up to
100,000 tokens. Also, set the user role instead of the system role for the initial instruction.

4. Call the chat function directly from the object's interface.

You can also ask questions about objects that do not technically exist in VBT or about general keywords,
such as "quantstats", which will search for mentions of "quantstats" within pages and messages.

**How to chat about keywords**

```python
vbt.chat_about(
    "sql", 
    "How to import data from a SQL database?", 
    resolve=False,  # (1)!
    find_kwargs=dict(
        ignore_case=True,
        allow_prefix=True,  # (2)!
        allow_suffix=True  # (3)!
    )
)
```

1. Use this option to avoid searching for a VBT object with the same name.

2. Allow prefixes for "sql", such as "from_sql".

3. Allow suffixes for "sql", such as "SQLData".

### Globally

Similar to the global [search](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.search)
function, there is a global function for chatting: [chat](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.chat).
It processes documents in the same way, but instead of displaying results, it sends them to
an LLM for completion.

!!! info
    The first time you run this command, it may take up to 15 minutes to prepare and embed documents.
    Most preparation steps are cached and stored, so future searches will be much
    faster and do not need to repeat the process.

**How to chat about VBT**

```python
vbt.chat("How to turn df into data?")  # (1)!
file_path = vbt.chat("How to turn df into data?", formatter="html")  # (2)!
vbt.chat("How to fix 'Symbols have mismatching columns'?", asset_names="messages")  # (3)!
vbt.chat(
    "How to use templates in signal_func_nb?", 
    asset_names="examples", 
    top_k=None, 
    cutoff=None, 
    return_chunks=False
)  # (4)!

chat_history = []
vbt.chat("How to turn df into data?", chat_history)  # (5)!
vbt.chat("What if I have symbols as columns?", chat_history)  # (6)!
vbt.chat("How to replace index of data?", chat_history, incl_past_queries=False)  # (7)!

_, chat = vbt.chat("How to turn df into data?", return_chat=True)  # (8)!
chat.complete("What if I have symbols as columns?")
```

1. Search API pages, documentation pages, and Discord messages for a query, and chat about them.
By default, up to 100 document chunks are used.

2. Accepts the same arguments as `asset.chat`.

3. Chat about Discord messages only.

4. Use the entire (ranked) asset as context.

5. Reuse the chat history to ask a question in the current chat.

6. Take the chat history into account. All user messages will be considered when ranking the context.

7. Take the chat history into account. Only the current query will be considered when ranking the context.

8. Same as above, except the context remains fixed for all later queries.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Building an index of embeddings for chatting is not always required. Instead, you can use BM25,
a quick and reliable algorithm that works entirely offline. Additionally, this function will use
a smaller context and a less expensive model for completions, such as "gpt-4o-mini"
instead of "gpt-4o".

**How to chat about VBT quickly**

```python
vbt.quick_chat("How to fix 'Symbols have mismatching columns'?")
```

!!! tip
    Use this when your query contains distinctive keywords. For vague queries, embeddings are a better option.

### QuantGPT

If you have a basic question, want to save money, or prefer chatting from a mobile device,
check out [QuantGPT](https://www.quantgpt.chat/), a free service generously hosted by our member
@simrell. QuantGPT uses the same knowledge base (website and Discord) as our ChatVBT function,
powered by the OpenAI Assistants API, which we have replicated with our own RAG.

<div class="grid cards width-fifty" markdown>

- :material-robot:{ .lg .middle } __Ask QuantGPT__

    ---

    [![](https://vectorbt.pro/pvt_41531c19/assets/quantgpt.light.webp#only-light){: loading=lazy }](https://www.quantgpt.chat/)
    [![](https://vectorbt.pro/pvt_41531c19/assets/quantgpt.dark.webp#only-dark){: loading=lazy }](https://www.quantgpt.chat/)

</div>

!!! info
    Both tools should give similar answers, but ChatVBT (that is, `vbt.chat()`) offers clickable references,
    full control over the knowledge base (which may improve completions), and the flexibility to use
    any LLM, not just OpenAI.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#interacting
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Interacting
type: heading 2
tags:
  - recently_added
---

## Interacting

In addition to the context-based chat functionality, VBT also supports tool calls to interact with
the MCP server and any user-defined functions. This functionality is also available in `vbt.chat()`.

**How to interact with tools**

```python
vbt.interact("How to plot a portfolio?", tools="mcp")  # (1)!

vbt.interact("What methods can I use for plotting a vbt.Portfolio?", tools=["vbt.mcp_server.get_attrs"])  # (2)!
vbt.interact("What methods can I use for plotting a vbt.Portfolio?", tools=[vbt.mcp_server.get_attrs])  # (3)!

mcp_tools = {k: v for k, v in vbt.mcp_server.tool_registry.items() if k != "run_code"}
vbt.interact("How to backtest a SMA crossover?", tools=mcp_tools)  # (4)!

@vbt.register_tool  # (5)!
def backtest_sma_crossover(symbol: str, long_window: int, short_window: int, metric: str) -> str:
    """Backtest a simple moving average crossover strategy.
    
    Args:
        symbol (str): Yahoo Finance symbol to backtest.
        long_window (int): Long moving average window.
        short_window (int): Short moving average window.
        metric (str): Performance metric to return.
    """
    data = vbt.YFData.pull(symbol)
    sma_long = data.get("Close").vbt.rolling_mean(long_window)
    sma_short = data.get("Close").vbt.rolling_mean(short_window)
    entries = sma_long.vbt.crossed_above(sma_short)
    exits = sma_long.vbt.crossed_below(sma_short)
    pf = vbt.Portfolio.from_signals(data, entries, exits)
    return getattr(pf, metric)

vbt.interact("What's the total return of a (20, 50) crossover for AAPL?", tools="mcp")

# ______________________________________________________________

vbt.interact("What's 1234 * (5678 + 91011)?", tools={  # (6)
    "add": lambda x, y: x + y,
    "subtract": lambda x, y: x - y,
    "multiply": lambda x, y: x * y,
    "divide": lambda x, y: x / y,
})

# ______________________________________________________________

vbt.settings.set("knowledge.chat.completions_config.tool_registry.add", lambda x, y: x + y)
vbt.settings.set("knowledge.chat.completions_config.tool_registry.subtract", lambda x, y: x - y)
vbt.settings.set("knowledge.chat.completions_config.tool_registry.multiply", lambda x, y: x * y)
vbt.settings.set("knowledge.chat.completions_config.tool_registry.divide", lambda x, y: x / y)

vbt.interact("What's 1234 * (5678 + 91011)?", tools="registry")  # (7)

# ______________________________________________________________

vbt.interact("...")  # (8)!

vbt.interact("...", tool_display_format="none")  # (9)!
vbt.interact("...", tool_display_format="minimal")  # (10)!
vbt.interact("...", tool_display_format="compact")  # (11)!
vbt.interact("...", tool_display_format="full")  # (12)!

vbt.interact("...", max_tool_calls=16)  # (13)!
vbt.interact("...", tool_dump_kwargs=dict(dump_engine="yaml"))  # (14)!
vbt.interact(
    "...", 
    tool_request_template="Requested:\n$payload",  # (15)!
    tool_response_template="Responded with:\n$payload"
)

# (16)!
```

1. Import the tools from the [MCP server](https://vectorbt.pro/pvt_41531c19/api/mcp_server/).

2. Import only specific tools using their reference names (i.e., fully qualified names).

3. Import actual function objects

4. Import the tools from the [MCP server](https://vectorbt.pro/pvt_41531c19/api/mcp_server/), excluding the [run_code](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.run_code) tool.

5. Register custom tools using the [register_tool](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.register_tool) decorator.
It will become immediately available in the MCP server.

6. Define custom tools using lambda functions and pass them as a dictionary.

7. The same as above but saving the tools in the global registry (not part of the MCP server).

8. Import tools from both the MCP server and the global registry (uses `tools="all"` by default).

9. Don't display tools in thought bubbles.

10. Display tools in thought bubbles but not their contents.

11. Display tool contents in thought bubbles but show only a preview.

12. Display tool contents in thought bubbles in full.

13. Force a maximum number of tool calls and produce the final answer once the limit is reached.

14. Display tool parameters using a different format, here YAML.

15. Customize tool request and response templates.

16. For argument descriptions, see [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions).

!!! tip
    When defining a tool function, 1) use `def` instead of `lambda`, 2) use annotations for the input types,
    and 3) write comprehensive documentation as a (preferably) Google-formatted docstring.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#rag
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: RAG
type: heading 2
---

## RAG

VBT uses a set of components for standard RAG. Most of these are orchestrated and deployed
automatically whenever you globally [search](#globally) for knowledge on VBT or [chat](#globally_1) about VBT.

### Tokenizer

The [Tokenizer](https://vectorbt.pro/pvt_41531c19/api/knowledge/tokenization/#vectorbtpro.knowledge.tokenization.Tokenizer) class
and its subclasses provide an interface for converting text into tokens.

**How to tokenize text**

```python
tokenizer = vbt.TikTokenizer()  # (1)!
tokenizer = vbt.TikTokenizer(encoding="o200k_base")
tokenizer = vbt.TikTokenizer(model="gpt-4o")

vbt.TikTokenizer.set_settings(encoding="o200k_base")  # (2)!

token_count = tokenizer.count_tokens(text)  # (3)!
tokens = tokenizer.encode(text)
text = tokenizer.decode(tokens)

# ______________________________________________________________

tokens = vbt.tokenize(text)  # (4)!
text = vbt.detokenize(tokens)

tokens = vbt.tokenize(text, tokenizer="tiktoken", model="gpt-4o")  # (5)!
```

1. Use the `tiktoken` package as a tokenizer.

2. Set the default settings.

3. Use the tokenizer to count tokens in a text, encode text into tokens, or decode tokens back into text.

4. There are also shortcut methods that create a tokenizer for you.

5. The tokenizer type and parameters can be passed as keyword arguments to both methods.

### Embeddings

The [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) class
and its subclasses provide an interface for generating vector representations of text.

**How to embed text**

```python
embeddings = vbt.OpenAIEmbeddings()  # (1)!
embeddings = vbt.OpenAIEmbeddings(batch_size=256)  # (2)!
embeddings = vbt.OpenAIEmbeddings(model="text-embedding-3-large")  # (3)!
embeddings = vbt.GeminiEmbeddings(model="gemini-embedding-001")  # (4)!
embeddings = vbt.HFInferenceEmbeddings(model="BAAI/bge-small-en-v1.5")  # (5)!
embeddings = vbt.LiteLLMEmbeddings(model="openai/text-embedding-3-large")  # (6)!
embeddings = vbt.LlamaIndexEmbeddings(embedding="openai", model="text-embedding-3-large")  # (7)!
embeddings = vbt.OllamaEmbeddings(model="dengcao/Qwen3-Embedding-0.6B:Q8_0")  # (8)!

vbt.OpenAIEmbeddings.set_settings(model="text-embedding-3-large")  # (9)!

emb = embeddings.get_embedding(text)  # (10)!
embs = embeddings.get_embeddings(texts)

# ______________________________________________________________

emb = vbt.embed(text)  # (11)!
embs = vbt.embed(texts)

emb = vbt.embed(text, embeddings="openai", model="text-embedding-3-large")  # (12)!
```

1. Use the `openai` (OpenAI) package as an embeddings provider.

2. Process embeddings in batches of 256. Supported by all embeddings providers.

3. Specify the model. Supported by all embeddings providers.

4. Use the `google-genai` (Gemini) package as an embeddings provider.

5. Use the `huggingface-hub` (Hugging Face inference) package as an embeddings provider.

6. Use the `litellm` (LiteLLM) package as an embeddings provider.

7. Use the `llama-index` (LlamaIndex) package as an embeddings provider.

8. Use the `ollama` (Ollama) package as an embeddings provider.

9. Set the default settings.

10. Use the embeddings provider to get embeddings for one or more texts.

11. There is also a shortcut method that creates an embeddings provider for you. It accepts one or more texts.

12. The embeddings provider type and parameters can be passed as keyword arguments to the method.

### Completions

The [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) class
and its subclasses provide an interface for generating text completions based on user queries.
For arguments such as `formatter`, see [chatting](#chatting).

**How to (auto-)complete a query**

```python
completions = vbt.OpenAICompletions()  # (1)!
completions = vbt.OpenAICompletions(stream=False)
completions = vbt.OpenAICompletions(max_tokens=100_000, tokenizer="tiktoken")
completions = vbt.OpenAICompletions(model="gpt-5-mini", reasoning_effort="high")
completions = vbt.OpenAICompletions(formatter="html", formatter_kwargs=dict(cache=False))
completions = vbt.AnthropicCompletions(model="claude-sonnet-4-0")  # (2)!
completions = vbt.GeminiCompletions(model="gemini-2.5-flash")  # (3)!
completions = vbt.HFInferenceCompletions(model="openai/gpt-oss-120b")  # (4)!
completions = vbt.LiteLLMCompletions(model="openai/gpt-5-mini", reasoning_effort="high")  # (5)!
completions = vbt.LlamaIndexCompletions(llm="openai", model="gpt-5-mini", reasoning_effort="high")  # (6)!
completions = vbt.OllamaCompletions(model="qwen3:latest")  # (7)!

vbt.OpenAICompletions.set_settings(model="gpt-5-mini", reasoning_effort="high")  # (8)!

completions.get_completion(text)  # (9)!
content = completions.get_completion_content(text)  # (10)!

# ______________________________________________________________

vbt.complete(text)  # (11)!

vbt.complete(text, completions="openai", model="gpt-5-mini", reasoning_effort="high")  # (12)!
```

1. Use the `openai` (OpenAI) package as a completions provider.

2. Use the `anthropic` (Anthropic) package as a completions provider.

3. Use the `google-genai` (Gemini) package as a completions provider.

4. Use the `huggingface-hub` (Hugging Face inference) package as a completions provider.

5. Use the `litellm` (LiteLLM) package as a completions provider.

6. Use the `llama-index` (LlamaIndex) package as a completions provider.

7. Use the `ollama` (Ollama) package as a completions provider.

8. Set the default settings.

9. Use the completions provider to get a completion for a text.

10. Get the content string of a completion without formatting.

11. There is also a shortcut method that creates a completions provider for you.

12. The completions provider type and parameters can be passed as keyword arguments to the method.

### Text splitter

The [TextSplitter](https://vectorbt.pro/pvt_41531c19/api/knowledge/text_splitting/#vectorbtpro.knowledge.text_splitting.TextSplitter) class
and its subclasses provide an interface for splitting text.

**How to split text**

```python
text_splitter = vbt.TokenSplitter()  # (1)!
text_splitter = vbt.TokenSplitter(chunk_size=1000, chunk_overlap=200)
text_splitter = vbt.SegmentSplitter()  # (2)!
text_splitter = vbt.SegmentSplitter(separators=r"\s+")  # (3)!
text_splitter = vbt.SegmentSplitter(separators=[r"(?<=[.!?])\s+", r"\s+", None])  # (4)!
text_splitter = vbt.SegmentSplitter(tokenizer="tiktoken", tokenizer_kwargs=dict(model="gpt-4o"))
text_splitter = vbt.LlamaIndexSplitter(node_parser="SentenceSplitter")  # (5)!

vbt.TokenSplitter.set_settings(chunk_size=1000, chunk_overlap=200)  # (6)!

text_chunks = text_splitter.split_text(text)  # (7)!

# ______________________________________________________________

text_chunks = vbt.split_text(text)  # (8)!

text_chunks = vbt.split_text(text, text_splitter="llamaindex", node_parser="SentenceSplitter")  # (9)!
```

1. Create a splitter that splits text into tokens.

2. Create a splitter that splits text into segments. By default, it splits into paragraphs and sentences,
words (if the sentence is too long), and then tokens (if the word is too long).

3. Split text strictly into words.

4. Split text into sentences, words (if the sentence is too long), and then tokens (if the word is too long).

5. Use the `SentenceSplitter` from the `llamaindex` package as a text splitter.

6. Set the default settings.

7. Use the text splitter to split a text, which returns a generator.

8. There is also a shortcut method that creates a text splitter for you.

9. The text splitter type and parameters can be passed as keyword arguments to the method.

### Object store

The [ObjectStore](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_storing/#vectorbtpro.knowledge.doc_storing.ObjectStore) class
and its subclasses provide an interface for efficiently storing and retrieving arbitrary Python objects,
such as text documents and embeddings. These objects must subclass
[StoreObject](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_storing/#vectorbtpro.knowledge.doc_storing.StoreObject).

**How to store objects**

```python
obj_store = vbt.DictStore()  # (1)!
obj_store = vbt.MemoryStore(store_id="abc")  # (2)!
obj_store = vbt.MemoryStore(purge_on_open=True)  # (3)!
obj_store = vbt.FileStore(dir_path="./file_store")  # (4)!
obj_store = vbt.FileStore(consolidate=True, use_patching=False)  # (5)!
obj_store = vbt.LMDBStore(dir_path="./lmdb_store")  # (6)!
obj_store = vbt.CachedStore(obj_store=vbt.FileStore())  # (7)!
obj_store = vbt.CachedStore(obj_store=vbt.FileStore(), mirror=True)  # (8)!

vbt.FileStore.set_settings(consolidate=True, use_patching=False)  # (9)!

obj = vbt.TextDocument(id_, text)  # (10)!
obj = vbt.TextDocument.from_data(text)  # (11)!
obj = vbt.TextDocument.from_data(  # (12)!
    {"timestamp": timestamp, "content": text}, 
    text_path="content",
    excl_embed_metadata=["timestamp"],
    dump_kwargs=dict(dump_engine="nestedtext")
)
obj1 = vbt.StoreEmbedding(id1, child_ids=[id2, id3])  # (13)!
obj2 = vbt.StoreEmbedding(id2, parent_id=id1, embedding=embedding2)
obj3 = vbt.StoreEmbedding(id3, parent_id=id1, embedding=embedding3)

with obj_store:  # (14)!
    obj = obj_store[obj.id_]
    obj_store[obj.id_] = obj
    del obj_store[obj.id_]
    print(len(obj_store))
    for id_, obj in obj_store.items():
        ...
```

1. Create an object store based on a simple dictionary, where data persists only for the lifetime of the instance.

2. Create an object store based on a global dictionary (`memory_store`), where data persists for the
lifetime of the Python process.

3. Clear the store upon opening, removing any data saved by previous instances.

4. Create an object store based on a file (without patching) or folder (with patching).
Patching means that additional changes are saved as separate files.

5. Consolidate the folder (if any) into a file and disable patching.

6. Create an object store based on LMDB.

7. Create an object store that caches the operations of another object store internally.

8. Same as above, but also mirrors operations in the global dictionary (`memory_store`) to persist the cache
for the lifetime of the Python process.

9. Set default settings.

10. Create a text document object.

11. Generate an id automatically from the text.

12. Create a text document object with metadata.

13. Create three embedding objects with relationships.

14. An object store is very easy to use: it behaves just like a regular dictionary.

### Document ranker

The [DocumentRanker](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_ranking/#vectorbtpro.knowledge.doc_ranking.DocumentRanker) class
provides an interface for embedding, scoring, and ranking documents.

**How to rank documents**

```python
doc_ranker = vbt.DocumentRanker()  # (1)!
doc_ranker = vbt.DocumentRanker(dataset_id="abc")  # (2)!
doc_ranker = vbt.DocumentRanker(  # (3)!
    embeddings="litellm", 
    embeddings_kwargs=dict(model="openai/text-embedding-3-large")
)
doc_ranker = vbt.DocumentRanker(  # (4)!
    doc_store="file",
    doc_store_kwargs=dict(dir_path="./doc_file_store"),
    emb_store="file",
    emb_store_kwargs=dict(dir_path="./emb_file_store"),
)
doc_ranker = vbt.DocumentRanker(score_func="dot", score_agg_func="max")  # (5)!

vbt.DocumentRanker.set_settings(doc_store="memory", emb_store="memory")  # (6)!

documents = [vbt.TextDocument("text1"), vbt.TextDocument("text2")]  # (7)!

doc_ranker.embed_documents(documents)  # (8)!
emb_documents = doc_ranker.embed_documents(documents, return_documents=True)
embs = doc_ranker.embed_documents(documents, return_embeddings=True)
doc_ranker.embed_documents(documents, refresh=True)  # (9)!

doc_scores = doc_ranker.score_documents("How to use VBT?", documents)  # (10)!
chunk_scores = doc_ranker.score_documents("How to use VBT?", documents, return_chunks=True)
scored_documents = doc_ranker.score_documents("How to use VBT?", documents, return_documents=True)

documents = doc_ranker.rank_documents("How to use VBT?", documents)  # (11)!
scored_documents = doc_ranker.rank_documents("How to use VBT?", documents, return_scores=True)
documents = doc_ranker.rank_documents("How to use VBT?", documents, top_k=50)  # (12)!
documents = doc_ranker.rank_documents("How to use VBT?", documents, top_k=0.1)  # (13)!
documents = doc_ranker.rank_documents("How to use VBT?", documents, top_k="elbow")  # (14)!
documents = doc_ranker.rank_documents("How to use VBT?", documents, cutoff=0.5, min_top_k=20)  # (15)!

# ______________________________________________________________

vbt.embed_documents(documents)  # (16)!
vbt.embed_documents(documents, embeddings="openai", model="text-embedding-3-large")
documents = vbt.rank_documents("How to use VBT?", documents)
```

1. Create a document ranker.

2. Set the store id for both the document store and embedding store.

3. Specify the embeddings provider type and parameters.

4. Specify the object store types for documents and embeddings.

5. Specify the score function and score aggregation function. Both can be callables.

6. Set default settings.

7. A document ranker accepts an iterable of store objects, such as text documents.

8. Use the document ranker to embed documents. Embeddings are stored in an embedding store.
You can also choose to return the embedded documents or the embeddings themselves.

9. If a document or embedding already exists in the store, override it.

10. Assign each document a similarity score relative to the query and return the scores.
You can also specify whether to return the scores for the chunks (by default they are aggregated),
or the documents along with their scores.

11. Rank documents based on similarity scores relative to the query. By default,
this reorders the documents, but you can also choose to return the documents with their scores.

12. Select the top 50 documents.

13. Select the top 10 percent of documents.

14. Select top documents based on an algorithm such as the Elbow method.

15. Select at least the top 20 documents with a similarity score above 0.5.

16. There are also shortcut methods for embedding and ranking that create a document ranker for you.

### Pipeline

The components described above can enhance RAG pipelines, extending their utility beyond the VBT scope.

**How to create a basic RAG pipeline**

```python
data = [
    "The Eiffel Tower is not located in London.",
    "The Great Wall of China is not visible from Jupiter.",
    "HTML is not a programming language."
]
query = "Where the Eiffel Tower is not located?"

documents = map(vbt.TextDocument.from_data, data)
retrieved_documents = vbt.rank_documents(query, documents, top_k=1)
context = "\n\n".join(map(str, retrieved_documents))
vbt.complete(query, context=context)
```

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#provider-configs
parent: https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/#knowledge
name: Provider configs
type: heading 2
---

## Provider configs

VBT supports a variety of providers for document embedding and completion tasks. Below you can find
common configurations for each provider. These configurations can be set globally or per-query.

### OpenAI (ChatGPT)

[OpenAI](https://openai.com/) is the default provider in VBT.

OpenAI requires you to set the `OPENAI_API_KEY` environment variable or pass it directly as `api_key`.

**Set OpenAI API key**

```python
env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"
```

??? abstract "Embeddings"

    To use OpenAI (default), override the default `embeddings` configuration.

    **Set OpenAI as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "openai")
    ```

    The embedding model can be changed in the settings.

    ```python title="Set embedding model"
    vbt.settings.set("knowledge.chat.embeddings_configs.openai.model", "text-embedding-3-large")
    ```

    By default, the number of dimensions are truncated to 256 for faster processing.
    You can revert this by setting the `dimensions` parameter to None.

    ```python title="Use the original dimensions"
    vbt.settings.set("knowledge.chat.embeddings_configs.openai.dimensions", None)
    ```

    By default, the number of embeddings processed at once (`batch_size`) is 256, which can also be increased
    (not recommended, unless the number of tokens in each document is small) or decreased.

    ```python title="Set batch size to 64"
    vbt.settings.set("knowledge.chat.embeddings_configs.openai.batch_size", 64)
    ```

    Use `client_kwargs` and `embeddings_kwargs` to pass additional arguments to the OpenAI client
    and the embeddings API, respectively. Whenever you set a custom key outside of these dictionaries,
    it will be assigned to either of these dictionaries automatically. See the
    [OpenAI documentation](https://platform.openai.com/docs/api-reference) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.embeddings_configs.openai.client_kwargs.timeout", 20.0)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) and
        [OpenAIEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.OpenAIEmbeddings).

    To revert all changes, reset the settings.

    ```python title="Reset embedding settings"
    vbt.settings.get("knowledge.chat.embeddings_configs.openai").reset()
    ```

??? abstract "Completions"

    To use OpenAI (default), override the default `completions` configuration.

    ```python title="Set OpenAI as default completions provider"
    vbt.settings.set("knowledge.chat.completions", "openai")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    can be changed in the settings.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.openai.model", "gpt-5")
    vbt.settings.set("knowledge.chat.completions_configs.openai.quick_model", "gpt-5-mini")
    ```

    Reasoning must be enabled explicitly.

    ```python title="Enable reasoning"
    vbt.settings.set("knowledge.chat.completions_configs.openai.reasoning.effort", "medium")
    ```

    Chain-of-thought (COT) summaries must be enabled explicitly.

    ```python title="Enable COT summaries"
    vbt.settings.set("knowledge.chat.completions_configs.openai.reasoning.summary", "auto")
    ```

    By default, the Responses API is used, but you can easily switch to the Completions API.

    ```python title="Switch to Completions API"
    vbt.settings.set("knowledge.chat.completions_configs.openai.use_responses", "False")
    vbt.settings.set("knowledge.chat.completions_configs.openai.reasoning_effort", "medium")
    ```

    !!! note
        COT summaries are not supported by the Completions API.

    If the selected model returns reasoning output, you can disable it by setting `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.openai.hide_thoughts", True)
    ```

    Use `client_kwargs` and `completions_kwargs` to pass additional arguments to the OpenAI client
    and the completions API, respectively. Whenever you set a custom key outside of these dictionaries,
    it will be assigned to either of these dictionaries automatically. See the
    [OpenAI documentation](https://platform.openai.com/docs/api-reference) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.completions_configs.openai.client_kwargs.timeout", 20.0)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [OpenAICompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.OpenAICompletions).

    To revert all changes, reset the settings.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.openai").reset()
    ```

#### Example: OpenRouter

Set the appropriate base URL and API key to use [OpenRouter](https://openrouter.ai/)
or other OpenAI-compatible providers.

```python title="Configure OpenRouter for DeepSeek R1"
vbt.settings.set("knowledge.chat.completions", "openai")
vbt.settings.set("knowledge.chat.completions_configs.openai.base_url", "https://openrouter.ai/api/v1")
vbt.settings.set("knowledge.chat.completions_configs.openai.api_key", "<YOUR_OPENROUTER_API_KEY>")
vbt.settings.set("knowledge.chat.completions_configs.openai.model", "deepseek/deepseek-r1")
vbt.settings.set("knowledge.chat.completions_configs.openai.use_responses", False)  # (1)!
```

1. Use Completions API for OpenRouter, as Responses API is not supported.

### Anthropic (Claude)

[Anthropic](https://www.anthropic.com/) with Claude is a popular LLM provider.

Anthropic requires you to set the `ANTHROPIC_API_KEY` environment variable or pass it directly as `api_key`.

**Set Anthropic API key**

```python
env["ANTHROPIC_API_KEY"] = "<YOUR_ANTHROPIC_API_KEY>"
```

!!! info
    Anthropic does not currently support embeddings.

??? abstract "Completions"

    To use Anthropic, override the default `embeddings` configuration.

    **Set Anthropic as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "anthropic")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    can be changed in the settings.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.model", "claude-sonnet-4-0")
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.quick_model", "claude-3-5-haiku-latest")
    ```

    Reasoning must be enabled explicitly by setting the `thinking` parameters.

    ```python title="Enable reasoning"
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.thinking.type", "enabled")
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.thinking.budget_tokens", 2048)
    ```

    If the selected model returns reasoning output, you can disable it by setting `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.hide_thoughts", True)
    ```

    AWS Bedrock and Google Vertex can be used by setting `client_type` to "bedrock" or "vertex", respectively.

    ```python title="Use AWS Bedrock"
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.client_type", "bedrock")
    ```

    Use `client_kwargs` and `messages_kwargs` to pass additional arguments to the Anthropic client
    and the messages API, respectively. Whenever you set a custom key outside of these dictionaries,
    it will be assigned to either of these dictionaries automatically. See the
    [Anthropic documentation](https://docs.anthropic.com/) for more details.

    ```python title="Set maximum output tokens to 4096"
    vbt.settings.set("knowledge.chat.completions_configs.anthropic.messages_kwargs.max_tokens", 4096)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [AnthropicCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.AnthropicCompletions).

    To revert all changes, reset the settings.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.anthropic").reset()
    ```

### Google (Gemini)

[Gemini](https://ai.google.dev/) is Google's latest LLM offering.

Google requires you to set the `GEMINI_API_KEY` environment variable or pass it directly as `api_key`.

```python title="Set Gemini API key"
env["GEMINI_API_KEY"] = "<YOUR_GEMINI_API_KEY>"
```

??? abstract "Embeddings"

    To use Gemini, override the default `embeddings` configuration.

    **Set Gemini as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "gemini")
    ```

    The embedding model can be changed in the settings.

    ```python title="Set embedding model"
    vbt.settings.set("knowledge.chat.embeddings_configs.gemini.model", "gemini-embedding-001")
    ```

    By default, the number of dimensions are truncated to 768 (and normalized) for faster processing.
    You can revert this by setting the `output_dimensionality` parameter to None.

    ```python title="Use the original dimensions"
    vbt.settings.set("knowledge.chat.embeddings_configs.gemini.config.output_dimensionality", None)
    ```

    By default, the number of embeddings processed at once (`batch_size`) is 256, which can also be increased
    (not recommended, unless the number of tokens in each document is small) or decreased.

    ```python title="Set batch size to 64"
    vbt.settings.set("knowledge.chat.embeddings_configs.gemini.batch_size", 64)
    ```

    Use `client_kwargs` and `embeddings_kwargs` to pass additional arguments to the Gemini client
    and the embeddings API, respectively. Whenever you set a custom key outside of these dictionaries,
    it will be assigned to either of these dictionaries automatically. See the
    [Gemini documentation](https://ai.google.dev/gemini-api/docs/) for more details.

    ```python title="Use Vertex AI"
    vbt.settings.set("knowledge.chat.embeddings_configs.gemini.client_kwargs.vertexai", True)
    vbt.settings.set("knowledge.chat.embeddings_configs.gemini.client_kwargs.project", "my-project-id")
    vbt.settings.set("knowledge.chat.embeddings_configs.gemini.client_kwargs.location", "us-central1")
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) and
        [GeminiEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.GeminiEmbeddings).

    To revert all changes, reset the settings.

    ```python title="Reset embedding settings"
    vbt.settings.get("knowledge.chat.embeddings_configs.gemini").reset()
    ```

??? abstract "Completions"

    To use Gemini, override the default `completions` configuration.

    ```python title="Set Gemini as default completions provider"
    vbt.settings.set("knowledge.chat.completions", "gemini")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    can be changed in the settings.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.gemini.model", "gemini-2.5-flash")
    vbt.settings.set("knowledge.chat.completions_configs.gemini.quick_model", "gemini-2.5-flash-lite")
    ```

    Reasoning can be controlled by the `thinking_config` parameters.

    ```python title="Enable dynamic reasoning"
    vbt.settings.set("knowledge.chat.completions_configs.gemini.config.thinking_config.thinking_budget", -1)
    ```

    Chain-of-thought (COT) summaries must be enabled explicitly.

    ```python title="Enable COT summaries"
    vbt.settings.set("knowledge.chat.completions_configs.gemini.config.thinking_config.include_thoughts", True)
    ```

    If the selected model returns reasoning output, you can disable it by setting VBT's `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.gemini.hide_thoughts", True)
    ```

    Use `client_kwargs` and `completions_kwargs` to pass additional arguments to the Gemini client
    and the completions API, respectively. Whenever you set a custom key outside of these dictionaries,
    it will be assigned to either of these dictionaries automatically. See the
    [Gemini documentation](https://ai.google.dev/gemini-api/docs/) for more details.

    ```python title="Use Vertex AI"
    vbt.settings.set("knowledge.chat.completions_configs.gemini.client_kwargs.vertexai", True)
    vbt.settings.set("knowledge.chat.completions_configs.gemini.client_kwargs.project", "my-project-id")
    vbt.settings.set("knowledge.chat.completions_configs.gemini.client_kwargs.location", "us-central1")
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [GeminiCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.GeminiCompletions).

    To revert all changes, reset the settings.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.gemini").reset()
    ```

### Hugging Face Inference

[Hugging Face](https://huggingface.co/) provides a wide range of models and APIs.
The Inference API allows you to use these models for embeddings and completions
without needing to host your own infrastructure.

Hugging Face requires you to set the `HF_TOKEN` environment variable or pass it directly as `api_key`.

```python title="Set Hugging Face API key"
env["HF_TOKEN"] = "<YOUR_HF_TOKEN>"
```

??? abstract "Embeddings"

    To use Hugging Face Inference, override the default `embeddings` configuration.

    **Set Hugging Face Inference as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "hf_inference")
    ```

    The embedding model can be changed in the settings.

    ```python title="Set embedding model"
    vbt.settings.set("knowledge.chat.embeddings_configs.hf_inference.model", "Qwen/Qwen3-Embedding-8B")
    ```

    By default, the number of embeddings processed at once (`batch_size`) is 256, which can also be increased
    (not recommended, unless the number of tokens in each document is small) or decreased.

    ```python title="Set batch size to 64"
    vbt.settings.set("knowledge.chat.embeddings_configs.hf_inference.batch_size", 64)
    ```

    Use `client_kwargs` and `feature_extraction_kwargs` to pass additional arguments to the
    Hugging Face Inference client and the feature-extraction API, respectively. Whenever you set
    a custom key outside of these dictionaries, it will be assigned to either of these dictionaries
    automatically. See the [Hugging Face documentation](https://huggingface.co/docs) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.embeddings_configs.hf_inference.client_kwargs.timeout", 20)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) and
        [HFInferenceEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.HFInferenceEmbeddings).

    To revert all changes, reset the settings.

    ```python title="Reset embedding settings"
    vbt.settings.get("knowledge.chat.embeddings_configs.hf_inference").reset()
    ```

??? abstract "Completions"

    To use Hugging Face Inference, override the default `completions` configuration.

    ```python title="Set Hugging Face Inference as default completions provider"
    vbt.settings.set("knowledge.chat.completions", "hf_inference")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    can be changed in the settings.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.hf_inference.model", "openai/gpt-oss-120b")
    vbt.settings.set("knowledge.chat.completions_configs.hf_inference.quick_model", "openai/gpt-oss-20b")
    ```

    If the selected model returns reasoning output, you can disable it by setting `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.hf_inference.hide_thoughts", True)
    ```

    Use `client_kwargs` and `chat_completion_kwargs` to pass additional arguments to the
    Hugging Face Inference client and the chat-completion API, respectively. Whenever you set
    a custom key outside of these dictionaries, it will be assigned to either of these dictionaries
    automatically. See the [Hugging Face documentation](https://huggingface.co/docs) for more details.

    ```python title="Use Together AI as inference provider"
    vbt.settings.set("knowledge.chat.completions_configs.hf_inference.chat_completion.provider", "together")
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [HFInferenceCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.HFInferenceCompletions).

    To revert all changes, reset the settings.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.hf_inference").reset()
    ```

### LiteLLM

[LiteLLM](https://www.litellm.ai/) is an LLM gateway in OpenAI format.

There isn't a specific API key for LiteLLM; you need to set the environment variable for
the provider you want to use, such as `DEEPSEEK_API_KEY` for DeepSeek, or pass it directly as `api_key`.
See the [LiteLLM documentation](https://docs.litellm.ai/) for more details.

```python title="Set API keys"
env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"
env["ANTHROPIC_API_KEY"] = "<YOUR_ANTHROPIC_API_KEY>"
env["GEMINI_API_KEY"] = "<YOUR_GEMINI_API_KEY>"
...
```

??? abstract "Embeddings"

    To use LiteLLM, override the default `embeddings` configuration.

    **Set LiteLLM as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "litellm")
    ```

    The embedding provider and/or model can be changed in the settings.

    ```python title="Set embedding model"
    vbt.settings.set("knowledge.chat.embeddings_configs.litellm.model", "gemini/gemini-embedding-001")
    ```

    By default, the number of dimensions are truncated to 256 for faster processing.
    You can revert this by setting the `dimensions` parameter to None.

    ```python title="Use the original dimensions"
    vbt.settings.set("knowledge.chat.embeddings_configs.litellm.dimensions", None)
    ```

    By default, the number of embeddings processed at once (`batch_size`) is 256, which can also be increased
    (not recommended, unless the number of tokens in each document is small) or decreased.

    ```python title="Set batch size to 64"
    vbt.settings.set("knowledge.chat.embeddings_configs.litellm.batch_size", 64)
    ```

    Use `embedding_kwargs` to pass additional arguments to the LiteLLM embedding API.
    See the [LiteLLM documentation](https://docs.litellm.ai/) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.embeddings_configs.litellm.embedding_kwargs.timeout", 20.0)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) and
        [LiteLLMEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.LiteLLMEmbeddings).

    To revert all changes, reset the settings.

    ```python title="Reset embedding settings"
    vbt.settings.get("knowledge.chat.embeddings_configs.litellm").reset()
    ```

??? abstract "Completions"

    To use LiteLLM, override the default `completions` configuration.

    ```python title="Set LiteLLM as default completions provider"
    vbt.settings.set("knowledge.chat.completions", "litellm")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    can be changed in the settings.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.litellm.model", "gemini/gemini-2.5-flash")
    vbt.settings.set("knowledge.chat.completions_configs.litellm.quick_model", "gemini/gemini-2.5-flash-lite")
    ```

    Reasoning must be enabled explicitly by setting `reasoning_effort`.

    ```python title="Enable reasoning"
    vbt.settings.set("knowledge.chat.completions_configs.litellm.reasoning_effort", "medium")
    ```

    If the selected model returns reasoning output, you can disable it by setting `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.litellm.hide_thoughts", True)
    ```

    Use `completion_kwargs` to pass additional arguments to the LiteLLM completion API.
    See the [LiteLLM documentation](https://docs.litellm.ai/) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.completions_configs.litellm.completion_kwargs.timeout", 20.0)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [LiteLLMCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.LiteLLMCompletions).

    To revert all changes, reset the settings.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.litellm").reset()
    ```

### LlamaIndex

[LlamaIndex](https://llamaindex.ai/) is a framework for building LLM applications that supports
various data sources and integrations. While it provides a (somewhat) unified interface for working
with different LLM providers, you may need to install and configure each provider individually.

There isn't a specific API key for LlamaIndex; you need to set the environment variable for
the provider you want to use, such as `DEEPSEEK_API_KEY` for DeepSeek, or pass it directly as `api_key`.
See the [LlamaIndex documentation](https://docs.llamaindex.ai/) for more details.

```python title="Set API keys"
env["OPENAI_API_KEY"] = "<YOUR_OPENAI_API_KEY>"
env["ANTHROPIC_API_KEY"] = "<YOUR_ANTHROPIC_API_KEY>"
env["GEMINI_API_KEY"] = "<YOUR_GEMINI_API_KEY>"
...
```

??? abstract "Embeddings"

    To use LlamaIndex, override the default `embeddings` configuration.

    **Set LlamaIndex as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "llama_index")
    ```

    The embedding provider can be changed in the settings. For that, you need to
    set the `embedding` parameter to either the name of the class or its qualified name,
    such as "openai" or "llama_index.llms.openai.OpenAI".

    ```python title="Set embedding provider"
    vbt.settings.set("knowledge.chat.embeddings_configs.llama_index.embedding", "openai")
    ```

    The embedding model and other provider-specific settings can be configured under the
    lowercase-formatted provider name in `embedding_configs`.

    ```python title="Set embedding model"
    vbt.settings.set("knowledge.chat.embeddings_configs.llama_index.embedding_configs.openai.model", "text-embedding-3-large")
    ```

    By default, the number of embeddings processed at once (`batch_size`) is 256, which can also be increased
    (not recommended, unless the number of tokens in each document is small) or decreased.
    This can be done both globally and per-provider.

    ```python title="Set batch size to 64"
    vbt.settings.set("knowledge.chat.embeddings_configs.llama_index.embedding_configs.openai.batch_size", 64)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) and
        [LlamaIndexEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.LlamaIndexEmbeddings).

    To revert all changes, reset the settings. This can be done both globally and per-provider.

    ```python title="Reset embedding settings"
    vbt.settings.get("knowledge.chat.embeddings_configs.llama_index.embedding_configs.openai").reset()
    ```

??? abstract "Completions"

    To use LlamaIndex, override the default `completions` configuration.

    ```python title="Set LlamaIndex as default completions provider"
    vbt.settings.set("knowledge.chat.completions", "llama_index")
    ```

    The completion provider can be changed in the settings. For that, you need to
    set the `llm` parameter to either the name of the class or its qualified name
    (both are case-insensitive), such as "openai" or "llama_index.llms.openai.OpenAI".

    ```python title="Set completion provider"
    vbt.settings.set("knowledge.chat.completions_configs.llama_index.llm", "openai")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    and other provider-specific settings can be configured under the lowercase-formatted
    provider name in `llm_configs`.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.llama_index.llm_configs.openai.model", "gpt-5)
    vbt.settings.set("knowledge.chat.completions_configs.llama_index.llm_configs.openai.quick_model", "gpt-5-mini")
    ```

    If the selected model returns reasoning output, you can disable it by setting `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.llama_index.llm_configs.openai.hide_thoughts", True)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [LlamaIndexCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.LlamaIndexCompletions).

    To revert all changes, reset the settings. This can be done both globally and per-provider.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.llama_index.llm_configs.openai").reset()
    ```

#### Example: Local HF

LlamaIndex can be configured to use local Hugging Face models for embeddings and completions.

```python title="Configure LlamaIndex for Hugging Face"
vbt.settings.set("knowledge.chat.embeddings", "llama_index")
vbt.settings.set("knowledge.chat.embeddings_configs.llama_index.embedding", "huggingface")
vbt.settings.set("knowledge.chat.embeddings_configs.llama_index.model_name", "BAAI/bge-small-en-v1.5")  # (1)!
vbt.settings.set("knowledge.chat.embeddings_configs.llama_index.batch_size", 64)  # (2)!

vbt.settings.set("knowledge.chat.completions", "llama_index")
vbt.settings.set("knowledge.chat.completions_configs.llama_index.llm", "huggingface")
vbt.settings.set("knowledge.chat.completions_configs.llama_index.model_name", "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")  # (3)!
vbt.settings.set("knowledge.chat.completions_configs.llama_index.tokenizer_name", "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B")

vbt.settings.set("knowledge.chat.rank_kwargs.dataset_id", "local")
```

1. Basic embedding model. Take a look at the [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) for more models.

2. Number of embeddings to generate in parallel. Increase it if you have enough RAM and compute.

3. Discover [more DeepSeek models](https://huggingface.co/deepseek-ai).

### Ollama

[Ollama](https://ollama.com/) is a platform for running LLMs on your own hardware.

??? abstract "Embeddings"

    To use Ollama, override the default `embeddings` configuration.

    **Set Ollama as default embeddings provider**

```python
    vbt.settings.set("knowledge.chat.embeddings", "ollama")
    ```

    The model (search for available models [here](https://ollama.com/search)) can be changed in the settings.

    ```python title="Set embedding model"
    vbt.settings.set("knowledge.chat.embeddings_configs.ollama.model", "dengcao/Qwen3-Embedding-0.6B")
    ```

    !!! info
        The model will be automatically downloaded when you first use it. The progress bar
        will indicate the download status. To disable the progress bar, set `show_progress` to False.

    By default, the number of embeddings processed at once (`batch_size`) is 256, which can also be increased
    or decreased depending on your hardware specifications.

    ```python title="Set batch size to 64"
    vbt.settings.set("knowledge.chat.embeddings_configs.ollama.batch_size", 64)
    ```

    Use `client_kwargs` and `embed_kwargs` to pass additional arguments to the
    Ollama client and the embed API, respectively. Whenever you set a custom key outside of
    these dictionaries, it will be assigned to either of these dictionaries automatically.
    See the [Ollama documentation](https://github.com/ollama/ollama-python) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.embeddings_configs.ollama.embedding_kwargs.timeout", 20.0)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings) and
        [OllamaEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.OllamaEmbeddings).

    To revert all changes, reset the settings.

    ```python title="Reset embedding settings"
    vbt.settings.get("knowledge.chat.embeddings_configs.ollama").reset()
    ```

??? abstract "Completions"

    To use Ollama, override the default `completions` configuration.

    ```python title="Set Ollama as default completions provider"
    vbt.settings.set("knowledge.chat.completions", "ollama")
    ```

    The main and quick (used in `vbt.quick_search()` and `vbt.quick_chat()`) completion models
    (search for available models [here](https://ollama.com/search)) can be changed in the settings.

    ```python title="Set completion models"
    vbt.settings.set("knowledge.chat.completions_configs.ollama.model", "qwen3:4b")
    vbt.settings.set("knowledge.chat.completions_configs.ollama.quick_model", "qwen3:0.6b")
    ```

    !!! info
        The model will be automatically downloaded when you first use it. The progress bar
        will indicate the download status. To disable the progress bar, set `show_progress` to False.

    If the selected model returns reasoning output, you can disable it by setting `hide_thoughts` to True.

    ```python title="Disable reasoning output"
    vbt.settings.set("knowledge.chat.completions_configs.ollama.hide_thoughts", True)
    ```

    Use `client_kwargs` and `chat_kwargs` to pass additional arguments to the
    Ollama client and the chat API, respectively. Whenever you set a custom key outside of
    these dictionaries, it will be assigned to either of these dictionaries automatically.
    See the [Ollama documentation](https://github.com/ollama/ollama-python) for more details.

    ```python title="Set timeout to 20 seconds"
    vbt.settings.set("knowledge.chat.completions_configs.ollama.completion_kwargs.timeout", 20.0)
    ```

    !!! tip
        To learn more about the available parameters, check the arguments of
        [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions) and
        [OllamaCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.OllamaCompletions).

    To revert all changes, reset the settings.

    ```python title="Reset completion settings"
    vbt.settings.get("knowledge.chat.completions_configs.ollama").reset()
    ```

#### Example: GPT-OSS

[GPT-OSS](https://openai.com/index/introducing-gpt-oss/) is OpenAI's open-source alternative to
ChatGPT mini models, designed to provide similar capabilities while being freely available.

```python title="Configure Ollama for high-reasoning GPT-OSS"
vbt.settings.set("knowledge.chat.completions", "ollama")
vbt.settings.set("knowledge.chat.completions_configs.ollama.model", "gpt-oss:20b")
system_prompt = vbt.settings.get("knowledge.chat.system_prompt")
vbt.settings.set("knowledge.chat.completions_configs.ollama.system_prompt", f"{system_prompt}\n\n---\n\nReasoning: high")
```

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/optimization/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Optimization
type: page
icon: material-magnify-expand
---

# Optimization

Optimization involves running a function with various configurations to improve the performance of a
strategy or to enhance the CPU or RAM efficiency of a pipeline.

!!! question
    Learn more in [Pairs trading tutorial](https://vectorbt.pro/pvt_41531c19/tutorials/pairs-trading/).

## Parameterization

The simplest approach is to test one parameter combination at a time. This method uses minimal RAM,
but it may take longer to run if the function is not written in pure Numba and has a fixed overhead
(such as converting from Pandas to NumPy and back), which increases the total execution time for each
run. To use this approach, create a pipeline function that accepts individual parameter values and
decorate it with [`@vbt.parameterized`](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized).
To test multiple parameters, wrap each parameter argument with
[Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param).

!!! example
    See an example in [Parameterized decorator](https://vectorbt.pro/pvt_41531c19/features/optimization/#parameterized-decorator).

### Decoration

To parameterize any function, decorate (or wrap) it with `@vbt.parameterized`. This returns a new
function with the same name and arguments as the original. The only difference is that the new
function processes the provided arguments, builds all parameter combinations, invokes the original
function for each combination, and merges the results of all combinations.

**Process only one parameter combination at a time**

```python
@vbt.parameterized
def my_pipeline(data, fast_window, slow_window):  # (1)!
    ...
    return result  # (2)!

results = my_pipeline(  # (3)!
    data,
    vbt.Param(fast_windows),  # (4)!
    vbt.Param(slow_windows)
)
```

1. Arguments can be anything. In this example, we expect a data instance and two parameters:
    `fast_window` and `slow_window`. The decorator will pass them as single values.

2. Perform calculations using the current parameter combination and return a result, which can be anything.

3. Call the function the same way as without the decorator.

4. Wrap each set of multiple values in `vbt.Param` for every parameter.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To keep the original function separate from the decorated one, you can apply the decorator after defining
the function and assign the decorated function to a different name.

**Decorate a function later**

```python
def my_pipeline(data, fast_window, slow_window):
    ...
    return result

my_param_pipeline = vbt.parameterized(my_pipeline)
results = my_param_pipeline(...)
```

### Merging

The code above returns a list of results, one for each parameter combination. To also return the grid
of parameter combinations, pass `return_param_index=True` to the decorator. Alternatively, you can
have VBT merge the results into one or more Pandas objects and attach the grid to their index or columns
by specifying a merging function (see [resolve_merge_func](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.merging.resolve_merge_func)).

**Various merging configurations**

```python
@vbt.parameterized(return_param_index=True)  # (1)!
def my_pipeline(...):
    ...
    return result
    
results, param_index = my_pipeline(...)

# ______________________________________________________________

@vbt.parameterized(merge_func="concat")  # (2)!
def my_pipeline(...):
    ...
    return pf.sharpe_ratio
    
sharpe_ratio = my_pipeline(...)

# ______________________________________________________________

@vbt.parameterized(merge_func="concat")
def my_pipeline(...):
    ...
    return pf.sharpe_ratio, pf.win_rate
    
sharpe_ratio, win_rate = my_pipeline(...)

# ______________________________________________________________
    
@vbt.parameterized(merge_func="column_stack")  # (3)!
def my_pipeline(...):
    ...
    return entries, exits
    
entries, exits = my_pipeline(...)

# ______________________________________________________________

@vbt.parameterized(merge_func="row_stack")  # (4)!
def my_pipeline(...):
    ...
    return pf.value
    
value = my_pipeline(...)

# ______________________________________________________________

@vbt.parameterized(merge_func=("concat", "column_stack"))  # (5)!
def my_pipeline(...):
    ...
    return pf.sharpe_ratio, pf.value
    
sharpe_ratio, value = my_pipeline(...)

# ______________________________________________________________

def merge_func(results, param_index):
    return pd.Series(results, index=param_index)
    
@vbt.parameterized(
    merge_func=merge_func,  # (6)!
    merge_kwargs=dict(param_index=vbt.Rep("param_index"))  # (7)!
)
def my_pipeline(...):
    ...
    return pf.sharpe_ratio
    
sharpe_ratio = my_pipeline(...)
```

1. Return the results along with the parameter grid.

2. If the function returns a single number (or a tuple of numbers), concatenate all into a Series using
the parameter combinations as the index. This is useful for returning metrics like Sharpe ratio.

3. If the function returns an array (or a tuple of arrays), stack all arrays along columns into a DataFrame,
using the parameter combinations as the outermost column level. This is useful for indicator arrays.

4. If the function returns an array (or a tuple of arrays), stack all arrays along rows into a Series or
DataFrame, using the parameter combinations as the outermost index level. Useful for cross-validation.

5. If the function returns a number and an array, return a Series of concatenated numbers and a DataFrame
of arrays stacked along columns.

6. Pass a custom merging function.

7. Use an expression template to pass the parameter index as a keyword argument.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can also use annotations to specify the merging function or functions.

```python
@vbt.parameterized
def my_pipeline(...) -> "concat":  # (1)!
    ...
    return result

# ______________________________________________________________

@vbt.parameterized
def my_pipeline(...) -> ("concat", "column_stack"):  # (2)!
    ...
    return result1, result2

# ______________________________________________________________

@vbt.parameterized
def my_pipeline(...) -> (  # (3)!
    vbt.MergeFunc("concat", wrap=False), 
    vbt.MergeFunc("column_stack", wrap=False)
):
    ...
    return result1, result2
```

1. Concatenates the results.

2. Concatenates the first result and stacks the second result along columns.

3. The same as above but allows specifying keyword arguments for each merging function.

### Generation

You can control the grid of parameter combinations using individual parameters. By default,
vectorbtpro builds a Cartesian product of all parameters. To avoid building the product between
certain parameters, assign them to the same product `level`. To filter out unwanted parameter
configurations, specify a `condition` as a boolean expression using parameter names as variables.
This condition is evaluated for each parameter combination, and only those returning True are kept.
To change how a parameter appears in the parameter index, provide `keys` with human-readable strings.
A parameter can also be hidden entirely by setting `hide=True`.

**Various parameter configurations**

```python
sma_crossover(  # (1)!
    data=data,
    fast_window=vbt.Param(windows, condition="fast_window < slow_window"),
    slow_window=vbt.Param(windows),
)

# ______________________________________________________________

sma_crossover(  # (2)!
    data=vbt.Param(data),
    fast_window=vbt.Param(windows, condition="fast_window < slow_window"),
    slow_window=vbt.Param(windows),
)

# ______________________________________________________________

from itertools import combinations

fast_windows, slow_windows = zip(*combinations(windows, 2))  # (3)!
sma_crossover(
    data=vbt.Param(data, level=0),
    fast_window=vbt.Param(fast_windows, level=1),
    slow_window=vbt.Param(slow_windows, level=1),
)

# ______________________________________________________________

bbands_indicator(  # (4)!
    data=data,
    timeperiod=vbt.Param(timeperiods, level=0),
    upper_threshold=vbt.Param(thresholds, level=1, keys=pd.Index(thresholds, name="threshold")),
    lower_threshold=vbt.Param(thresholds, level=1, hide=True),
    _random_subset=1_000  # (5)!
)
```

1. Builds a product of fast and slow windows, removing those where the fast window is longer than the
slow window (for example, 20 and 50 is valid, but 50 and 20 does not make sense).

2. The same as above but tests only one symbol at a time.

3. The same as above but builds the window combinations manually. The window parameters are now on the
same level and will not create another product.

4. Tests two parameters: time periods and thresholds. Upper and lower thresholds share the same values,
and only one `threshold` level is displayed in the parameter index. Also, select a random subset of
    1,000 parameter combinations.

5. Arguments that are normally passed to the decorator can also be passed to the function itself by
prepending an underscore.

!!! example
    See an example in [Conditional parameters](https://vectorbt.pro/pvt_41531c19/features/optimization/#conditional-parameters).

!!! warning
    Testing 6 parameters with 10 values each generates a huge 1 million parameter combinations, so be
    careful not to make your grids too large. Otherwise, the grid generation alone will take a long time.
    This warning does not apply if you use `random_subset`. In that case, VBT selects random
    combinations dynamically instead of building the full grid. See
    [Lazy parameter grids](https://vectorbt.pro/pvt_41531c19/features/optimization/#lazy-parameter-grids) for an example.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can also use annotations to specify which arguments are parameters and their default configuration.

**Calculate the SMA crossover for one parameter combination at a time**

```python
@vbt.parameterized
def sma_crossover(
    data,
    fast_window: vbt.Param(condition="fast_window < slow_window"),
    slow_window: vbt.Param,
) -> "column_stack":
    fast_sma = data.run("talib:sma", fast_window, unpack=True)
    slow_sma = data.run("talib:sma", slow_window, unpack=True)
    upper_crossover = fast_sma.vbt.crossed_above(slow_sma)
    lower_crossover = fast_sma.vbt.crossed_below(slow_sma)
    signals = upper_crossover | lower_crossover
    return signals

signals = sma_crossover(data, fast_windows, slow_windows)
```

#### Pre-generation

To obtain the generated parameter combinations before (or without) calling the `@vbt.parameterized`
decorator, you can pass the same parameters to [combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params).

**Pre-generate parameter combinations**

```python
param_product, param_index = vbt.combine_params(
    dict(
        fast_window=vbt.Param(windows, condition="fast_window < slow_window"),
        slow_window=vbt.Param(windows),
    )
)

# ______________________________________________________________

param_product = vbt.combine_params(
    dict(
        fast_window=vbt.Param(windows, condition="fast_window < slow_window"),
        slow_window=vbt.Param(windows),
    ),
    build_index=False  # (1)!
)
```

1. Do not build the index. Only return the parameter product.

### Execution

Each parameter combination results in a single call to the pipeline function. To execute multiple calls
in parallel, provide a dictionary named `execute_kwargs` containing keyword arguments that will be
forwarded to the [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function, which
handles chunking and execution of the function calls.

**Various execution configurations**

```python
@vbt.parameterized  # (1)!
def my_pipeline(...):
    ...
    
# ______________________________________________________________

@vbt.parameterized(execute_kwargs=dict(chunk_len="auto", engine="threadpool"))  # (2)!
@njit(nogil=True)
def my_pipeline(...):
    ...
    
# ______________________________________________________________

@vbt.parameterized(execute_kwargs=dict(n_chunks="auto", distribute="chunks", engine="pathos"))  # (3)!
def my_pipeline(...):
    ...

# ______________________________________________________________

@vbt.parameterized  # (4)!
@njit(nogil=True)
def my_pipeline(...):
    ...

my_pipeline(
    ...,
    _execute_kwargs=dict(chunk_len="auto", engine="threadpool")
)

# ______________________________________________________________

@vbt.parameterized(execute_kwargs=dict(show_progress=False))  # (5)!
@njit(nogil=True)
def my_pipeline(...):
    ...

my_pipeline(
    ...,
    _execute_kwargs=dict(chunk_len="auto", engine="threadpool")  # (6)!
)
my_pipeline(
    ...,
    _execute_kwargs=vbt.atomic_dict(chunk_len="auto", engine="threadpool")  # (7)!
)
```

1. Execute parameter combinations serially.

2. Distribute parameter combinations into chunks of optimal length, and execute all parameter
combinations within each chunk in parallel using multithreading (one parameter combination per thread),
while executing chunks serially.

3. Divide parameter combinations into an optimal number of chunks, and execute all chunks in parallel
using multiprocessing (one chunk per process), while executing parameter combinations within each
chunk serially.

4. Parallelization can be enabled or disabled for each call by prepending an underscore to
`execute_kwargs` and passing it directly to the function.

5. If `execute_kwargs` is already set in the decorator, the values will be merged. To avoid merging,
wrap any of the dictionaries with `vbt.atomic_dict`.

6. `show_progress=False`

7. `show_progress=True` (default)

!!! note
    Threads are easier and faster to spawn than processes. To execute a function in its own process,
    all inputs and parameters must be serialized and then deserialized, which adds overhead.
    Multithreading is preferred, but the function needs to release the GIL, so use Numba compiled
    functions with `nogil=True`, or only use NumPy.

    If that is not possible, use multiprocessing, but make sure the function does not take large
    arrays, or that a single parameter combination takes a substantial amount of time to run.
    Otherwise, parallelization can end up making execution even slower.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To run code before or after the entire process, or even before or after each individual chunk,
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) provides several callbacks.

**Clear cache and collect garbage once in 3 chunks**

```python
def post_chunk_func(chunk_idx, flush_every):
    if (chunk_idx + 1) % flush_every == 0:
        vbt.flush()

@vbt.parameterized(
    post_chunk_func=post_chunk_func,
    post_chunk_kwargs=dict(
        chunk_idx=vbt.Rep("chunk_idx", eval_id="post_chunk_kwargs"), 
        flush_every=3
    ),
    chunk_len=10  # (1)!
)  
def my_pipeline(...):
    ...
```

1. Put 10 calls into each chunk, so flushing occurs every 30 calls.

!!! tip
    This works not just with `@vbt.parameterized`, but also with other functions that use
    [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) along with chunking!

### Total or partial?

You often need to decide whether your pipeline should be totally or partially parameterized.
Total parameterization means running the entire pipeline for each parameter combination.
This approach is simplest and is most suitable when parameters are used across several components
of the pipeline, or when you want to sacrifice some speed for reduced memory usage.

**Parameterize an entire MA crossover pipeline**

```python
@vbt.parameterized(merge_func="concat")  
def ma_crossover_sharpe(data, fast_window, slow_window):
    fast_ma = data.run("vbt:ma", window=fast_window, hide_params=True)
    slow_ma = data.run("vbt:ma", window=slow_window, hide_params=True)
    entries = fast_ma.ma_crossed_above(slow_ma)
    exits = fast_ma.ma_crossed_below(slow_ma)
    pf = vbt.PF.from_signals(data, entries, exits)
    return pf.sharpe_ratio

ma_crossover_sharpe(
    data, 
    vbt.Param(fast_windows, condition="fast_window < slow_window"), 
    vbt.Param(slow_windows)
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Partial parameterization is a good option when only a small part of the pipeline uses parameters,
and the rest can process results from those parameterized components. This can
lead to faster execution, but often results in higher memory usage.

**Parameterize only the signal part of a MA crossover pipeline**

```python
@vbt.parameterized(merge_func="column_stack")  
def ma_crossover_signals(data, fast_window, slow_window):
    fast_ma = data.run("vbt:ma", window=fast_window, hide_params=True)
    slow_ma = data.run("vbt:ma", window=slow_window, hide_params=True)
    entries = fast_ma.ma_crossed_above(slow_ma)
    exits = fast_ma.ma_crossed_below(slow_ma)
    return entries, exits

def ma_crossover_sharpe(data, fast_windows, slow_windows):
    entries, exits = ma_crossover_signals(data, fast_windows, slow_windows)  # (1)!
    pf = vbt.PF.from_signals(data, entries, exits)  # (2)!
    return pf.sharpe_ratio

ma_crossover_sharpe(
    data, 
    vbt.Param(fast_windows, condition="fast_window < slow_window"), 
    vbt.Param(slow_windows)
)
```

1. Parameter combinations become columns in the entry and exit arrays.

2. The simulator is able to process these additional columns.

### Flat or nested?

Another decision to make is whether to use one decorator for all parameters (flat parameterization)
or to place parameters across multiple decorators to implement a parameter hierarchy (nested parameterization).
Use the former if you want to treat all parameters equally and combine them together for generation
and processing. In this case, the order of the parameter combinations is determined by the order in
which parameters are passed to the function. For example, the values of the first parameter will be
processed sequentially, while the values of any additional parameter will be rotated.

**Process all parameters at the same time in a MA crossover pipeline**

```python
@vbt.parameterized(merge_func="concat")  
def ma_crossover_sharpe(data, symbol, fast_window, slow_window):
    symbol_data = data.select(symbol)  # (1)!
    fast_ma = symbol_data.run("vbt:ma", window=fast_window, hide_params=True)
    slow_ma = symbol_data.run("vbt:ma", window=slow_window, hide_params=True)
    entries = fast_ma.ma_crossed_above(slow_ma)
    exits = fast_ma.ma_crossed_below(slow_ma)
    pf = vbt.PF.from_signals(symbol_data, entries, exits)
    return pf.sharpe_ratio

ma_crossover_sharpe(
    data, 
    vbt.Param(data.symbols), 
    vbt.Param(fast_windows, condition="fast_window < slow_window"), 
    vbt.Param(slow_windows),
)
```

1. Symbol selection depends only on the symbol but is run for every combination of
`symbol`, `fast_window`, and `slow_window`—often unnecessarily!

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The second approach should be used if you want to set up your own custom parameter hierarchy.
For example, you may want to process some parameters (such as in parallel) differently, limit
the number of times certain parameters are invoked, or add special preprocessing or postprocessing
to specific parameters.

**First process symbols and then windows in a MA crossover pipeline**

```python
@vbt.parameterized(merge_func="concat", eval_id="inner")  # (1)!
def symbol_ma_crossover_sharpe(symbol_data, fast_window, slow_window):
    fast_ma = symbol_data.run("vbt:ma", window=fast_window, hide_params=True)
    slow_ma = symbol_data.run("vbt:ma", window=slow_window, hide_params=True)
    entries = fast_ma.ma_crossed_above(slow_ma)
    exits = fast_ma.ma_crossed_below(slow_ma)
    pf = vbt.PF.from_signals(symbol_data, entries, exits)
    return pf.sharpe_ratio

@vbt.parameterized(merge_func="concat", eval_id="outer")  # (2)!
def ma_crossover_sharpe(data, symbol, fast_windows, slow_windows):
    symbol_data = data.select(symbol)  # (3)!
    return symbol_ma_crossover_sharpe(symbol_data, fast_windows, slow_windows)  # (4)!

ma_crossover_sharpe(  # (5)!
    data, 
    vbt.Param(data.symbols, eval_id="outer"),
    vbt.Param(fast_windows, eval_id="inner", condition="fast_window < slow_window"),
    vbt.Param(slow_windows, eval_id="inner")
)

# ______________________________________________________________

@vbt.parameterized(merge_func="concat", eval_id="outer")
@vbt.parameterized(merge_func="concat", eval_id="inner")
def ma_crossover_sharpe(data, fast_window, slow_window):  # (6)!
    fast_ma = data.run("vbt:ma", window=fast_window, hide_params=True)
    slow_ma = data.run("vbt:ma", window=slow_window, hide_params=True)
    entries = fast_ma.ma_crossed_above(slow_ma)
    exits = fast_ma.ma_crossed_below(slow_ma)
    pf = vbt.PF.from_signals(data, entries, exits)
    return pf.sharpe_ratio

ma_crossover_sharpe(
    vbt.Param(data, eval_id="outer"),
    vbt.Param(fast_windows, eval_id="inner", condition="fast_window < slow_window"),
    vbt.Param(slow_windows, eval_id="inner")
)
```

1. The inner decorator, which iterates over fast and slow windows.

2. The outer decorator, which iterates over symbols.

3. This line now runs only once per symbol, making it more efficient.

4. Call the inner function from within the outer function.

5. Call the outer function with all parameters, specifying for each which function it should be evaluated at.

6. The same function can be parameterized multiple times, with each decorator handling a subset of the parameters.

### Skipping

Parameter combinations can be skipped dynamically by returning
[NoResult](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.NoResult) instead of an actual result.

**Skip the parameter combination if an error occurred**

```python
@vbt.parameterized
def my_pipeline(data, fast_window, slow_window):
    try:
        ...
        return result
    except Exception:
        return vbt.NoResult

results = my_pipeline(
    data,
    vbt.Param(fast_windows),
    vbt.Param(slow_windows)
)
```

## Hybrid (mono-chunks)

The approach above calls the original function for each individual parameter combination, which can be
slow when working with many combinations, especially if each function call comes with overhead,
such as when converting a NumPy array to a Pandas object. Keep in mind that 1 millisecond of overhead
adds up to about 17 minutes of extra execution time for one million combinations.

For functions that accept only one combination at a time, there is nothing (aside from parallelization)
that can be done to speed them up. However, if your function can be modified to accept multiple combinations
at once—where each parameter argument is an array rather than a single value—you can instruct
`@vbt.parameterized` to merge all combinations into chunks and call the function on each chunk.
This approach can greatly reduce the number of function calls.

**Test a grid of parameters using mono-chunks**

```python
@vbt.parameterized(mono_n_chunks=?, mono_chunk_len=?, mono_chunk_meta=?)  # (1)!
def my_pipeline(data, fast_windows, slow_windows):  # (2)!
    ...
    return result  # (3)!

results = my_pipeline(  # (4)!
    data,
    vbt.Param(fast_windows),
    vbt.Param(slow_windows)
)

# ______________________________________________________________

@vbt.parameterized(mono_n_chunks="auto")  # (5)!
...

# ______________________________________________________________

@vbt.parameterized(mono_chunk_len=100)  # (6)!
...
```

1. Instruct VBT to create chunks from parameter combinations. Use `mono_n_chunks` to set a target number
of chunks, `mono_chunk_len` to specify the maximum combinations per chunk, or
`mono_chunk_meta` to define chunk metadata directly.

2. The function should now accept multiple values of `fast_windows` and `slow_windows` instead of
single values like `fast_window` and `slow_window`. Each set of values contains
combinations within the same chunk.

3. Perform calculations on the given parameter combinations and return a result
(which should include an outcome for each combination).

4. Call the function as before.

5. Create as many chunks as there are CPU cores.

6. Create chunks with no more than 100 combinations each.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

By default, parameter values are passed as lists to the original function. To pass them as arrays,
or in another format, set a merging function `mono_merge_func` for each parameter.

```python
my_pipeline(
    param_a=vbt.Param(param_a),  # (1)!
    param_b=vbt.Param(param_b, mono_reduce=True),  # (2)!
    param_c=vbt.Param(param_c, mono_merge_func="concat"),  # (3)!
    param_d=vbt.Param(param_d, mono_merge_func="row_stack"),  # (4)!
    param_e=vbt.Param(param_e, mono_merge_func="column_stack"),  # (5)!
    param_f=vbt.Param(param_f, mono_merge_func=vbt.MergeFunc(...))  # (6)!
)
```

1. Places chunk values into a list.

2. The same as above, but returns a single value if all values in the chunk are the same.

3. Concatenates values into a NumPy array or Pandas Series.

4. Stacks chunk values along rows into a NumPy array or Pandas Series/DataFrame.

5. Stacks chunk values along columns into a NumPy array or Pandas DataFrame.

6. Merges chunk values using a custom merging function.

Execution works the same way as in [Parameterization](#parameterization), and chunks can be easily
parallelized. However, keep an eye on RAM usage since multiple parameter combinations
are processed at the same time.

!!! example
    Check out an example in [Mono-chunks](https://vectorbt.pro/pvt_41531c19/features/optimization/#mono-chunks).

## Chunking

Chunking is the process of splitting a value (such as an array) of one or more arguments into smaller
parts (called chunks), running the function on each part, and then merging the results back together.
This allows VBT to process only a subset of data at a time, which helps reduce RAM usage
and improves performance through parallelization. Chunking is also highly convenient—
most of the time, you do not need to modify your function, and you will get the same results whether
or not chunking is enabled. To use chunking, create a pipeline function, decorate it with
[`@vbt.chunked`](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked), and specify
how arguments should be chunked and how results should be merged.

!!! example
    See an example in [Chunking](https://vectorbt.pro/pvt_41531c19/features/performance/#chunking).

### Decoration

To make any function chunkable, decorate (or wrap) it with `@vbt.chunked`.
This returns a new function with the same name and arguments as the original. The only
difference is that this new function processes the arguments, chunks them, calls the original
function on each chunk, and then merges the results from all chunks.

**Process only a subset of values at a time**

```python
@vbt.chunked
def my_pipeline(data, fast_windows, slow_windows):  # (1)!
    ...
    return result  # (2)!

results = my_pipeline(  # (3)!
    data,
    vbt.Chunked(fast_windows),  # (4)!
    vbt.Chunked(slow_windows)
)
```

1. Arguments can be anything. Here, data is expected, along with already combined fast and slow windows,
as in [Hybrid (mono-chunks)](#hybrid-mono-chunks).

2. Perform calculations on the received chunk of values and return a result, which can be anything.

3. Call the function in the same way as before, just as without the decorator.

4. Wrap any chunkable argument with `vbt.Chunked` or another class.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To keep the original function separate from the decorated one, you can decorate it after its
definition and assign a different name to the decorated function.

**Decorate a function later**

```python
def my_pipeline(data, fast_windows, slow_windows):
    ...
    return result

my_chunked_pipeline = vbt.chunked(my_pipeline)
results = my_chunked_pipeline(...)
```

### Specification

To chunk an argument, you need to provide a chunking specification for that argument. There are three
main ways to specify this.

Approach 1: Pass a dictionary `arg_take_spec` to the decorator. This is the most versatile approach, as it allows
chunking of any nested objects of arbitrary depth, such as lists inside lists.

**Specify chunking rules via arg_take_spec**

```python
@vbt.chunked(
    arg_take_spec=dict(  # (1)!
        array1=vbt.ChunkedArray(axis=1),  # (2)!
        array2=vbt.ChunkedArray(axis=1),
        combine_func=vbt.NotChunked  # (3)!
    ),
    size=vbt.ArraySizer(arg_query="array1", axis=1),  # (4)!
    merge_func="column_stack"  # (5)!
)
def combine_arrays(array1, array2, combine_func):
    return combine_func(array1, array2)

new_array = combine_arrays(array1, array2, np.add)
```

1. A dictionary where keys are argument names and values are chunking rules for those arguments.

2. Split arguments `array1` and `array2` along columns. They must be multidimensional NumPy or Pandas arrays.

3. Provide rules for all arguments. If any argument is missing in `arg_take_spec`, a warning will be shown.

4. Specify from where to take the total size. This is needed to build chunks. It is mostly
optional, as newer versions of VBT can determine it automatically.

5. The merging function must work with the chunked arrays. Here, we stack columns of output arrays back together.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Approach 2: Annotate the function. This is the most convenient approach, allowing you to specify chunking rules
right next to each argument in the function definition.

**Specify chunking rules via annotations**

```python
@vbt.chunked
def combine_arrays(
    array1: vbt.ChunkedArray(axis=1) | vbt.ArraySizer(axis=1),  # (1)!
    array2: vbt.ChunkedArray(axis=1),
    combine_func
) -> "column_stack":
    return combine_func(array1, array2)

new_array = combine_arrays(array1, array2, np.add)
```

1. You can combine multiple VBT annotations with the `|` operator. It does not matter whether
a chunking annotation is a class or an instance. Providing the sizer is mostly optional, as
newer versions of VBT can determine it automatically.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Approach 3: Wrap argument values directly. This lets you switch chunking rules on the fly.

**Specify chunking rules via argument values**

```python
@vbt.chunked
def combine_arrays(array1, array2, combine_func):
    return combine_func(array1, array2)

new_array = combine_arrays(  # (1)!
    vbt.ChunkedArray(array1),
    vbt.ChunkedArray(array2),
    np.add,
    _size=len(array1),  # (2)!
    _merge_func="concat"
)
new_array = combine_arrays(  # (3)!
    vbt.ChunkedArray(array1, axis=0),
    vbt.ChunkedArray(array2, axis=0),
    np.add,
    _size=array1.shape[0],
    _merge_func="row_stack"
)
new_array = combine_arrays(  # (4)!
    vbt.ChunkedArray(array1, axis=1),
    vbt.ChunkedArray(array2, axis=1),
    np.add,
    _size=array1.shape[1],
    _merge_func="column_stack"
)
```

1. Split one-dimensional input arrays and concatenate the output arrays back together.

2. Providing the total size is mostly optional, as newer versions of VBT can determine it automatically.

3. Split two-dimensional input arrays along rows and stack the rows of output arrays together.

4. Split two-dimensional input arrays along columns and stack the columns of output arrays together.

Merging and execution work in the same way as in [Parameterization](#parameterization).

## Hybrid (super-chunks)

You can combine the [parameterized decorator](#parameterization) and the chunked decorator to process only
a subset of parameter combinations at a time, without needing to change the function's design as in
[Hybrid (mono-chunks)](#hybrid-mono-chunks). Although super-chunking may not be as fast as mono-chunking,
it is still useful when you want to process only part of the parameter combinations at a time
(but not all; otherwise, you should just use `distribute="chunks"` in the parameterized decorator
without the chunked decorator) to manage RAM usage, or when you need to perform preprocessing and/or
postprocessing, such as flushing per batch of parameter combinations.

**Execute at most n parameter combinations per process**

```python
@vbt.parameterized
def my_pipeline(data, fast_window, slow_window):  # (1)!
    ...
    return result

@vbt.chunked(
    chunk_len=?,  # (2)!
    execute_kwargs=dict(chunk_len="auto", engine="pathos")  # (3)!
)
def chunked_pipeline(data, fast_windows, slow_windows):  # (4)!
    return my_pipeline(
        data,
        vbt.Param(fast_windows, level=0),
        vbt.Param(slow_windows, level=0)
    )

param_product = vbt.combine_params(  # (5)!
    dict(
        fast_windows=fast_windows,
        slow_windows=slow_windows,
    ),
    build_index=False
)

chunked_pipeline(
    data,
    vbt.Chunked(param_product["fast_windows"]),
    vbt.Chunked(param_product["slow_windows"])
)
```

1. The parameterized function expects a single parameter combination, with each parameter argument as a single value.

2. Split each sequence of parameter values into chunks of n elements.

3. Build super-chunks from chunks, where chunks within each super-chunk execute in parallel,
and super-chunks themselves execute in sequence.

4. The chunked function expects a grid of parameter combinations, with each parameter argument as a sequence
of values. All sequences must have the same length.

5. Build a grid of parameter combinations.

## Raw execution

Whenever VBT needs to execute a function on multiple sets of arguments, it uses the
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function. This function takes
a list of tasks (functions and their arguments) and runs them using the engine selected by the user.
It accepts all the same arguments that you usually provide in `execute_kwargs`.

**Execute multiple indicator configurations in parallel**

```python
sma_func = vbt.talib_func("sma")
ema_func = vbt.talib_func("ema")
tasks = [
    vbt.Task(sma_func, arr, 10),  # (1)!
    vbt.Task(sma_func, arr, 20),
    vbt.Task(ema_func, arr, 10),
    vbt.Task(ema_func, arr, 20),
]
keys = pd.MultiIndex.from_tuples([  # (2)!
    ("sma", 10),
    ("sma", 20),
    ("ema", 10),
    ("ema", 20),
], names=["indicator", "timeperiod"])

indicators_df = vbt.execute(  # (3)!
    tasks,
    keys=keys,
    merge_func="column_stack",
    engine="threadpool"
)
```

1. Each task consists of the function and the (positional and keyword) arguments it needs.

2. Keys appear in the progress bar and in the columns of the new DataFrame.

3. Execute tasks in separate threads and merge them into a DataFrame.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To parallelize a workflow inside a for-loop, place it in a function and decorate
the function with [iterated](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.iterated).
Then, when you execute the decorated function, pass the total number of iterations or a range
to the argument where you expect the iteration variable.

**Execute a regular for-loop in parallel**

```python
# ______________________________ FROM ______________________________

results = []
keys = []
for timeperiod in range(20, 50, 5):
    result = sma_func(arr, timeperiod)
    results.append(result)
    keys.append(timeperiod)
keys = pd.Index(keys, name="timeperiod")
sma_df = pd.concat(map(pd.Series, results), axis=1, keys=keys)

# ______________________________ TO ______________________________

@vbt.iterated(over_arg="timeperiod", merge_func="column_stack", engine="threadpool")
def sma(arr, timeperiod):
    return sma_func(arr, timeperiod)

sma = vbt.iterated(  # (1)!
    sma_func,
    over_arg="timeperiod",
    engine="threadpool",
    merge_func="column_stack"
)

sma_df = sma(arr, range(20, 50, 5))
```

1. Another way to decorate a function.

**Execute a nested for-loop in parallel**

```python
# ______________________________ FROM ______________________________

results = []
keys = []
for fast_window in range(20, 50, 5):
    for slow_window in range(20, 50, 5):
        if fast_window < slow_window:
            fast_sma = sma_func(arr, fast_window)
            slow_sma = sma_func(arr, slow_window)
            result = fast_sma - slow_sma
            results.append(result)
            keys.append((fast_window, slow_window))
keys = pd.MultiIndex.from_tuples(keys, names=["fast_window", "slow_window"])
sma_diff_df = pd.concat(map(pd.Series, results), axis=1, keys=keys)

# ______________________________ TO ______________________________

@vbt.iterated(over_arg="fast_window", merge_func="column_stack", engine="pathos")  # (1)!
@vbt.iterated(over_arg="slow_window", merge_func="column_stack", raise_no_results=False)
def sma_diff(arr, fast_window, slow_window):
    if fast_window >= slow_window:
        return vbt.NoResult
    fast_sma = sma_func(arr, fast_window)
    slow_sma = sma_func(arr, slow_window)
    return fast_sma - slow_sma

sma_diff_df = sma_diff(arr, range(20, 50, 5), range(20, 50, 5))
```

1. Execute the outer loop in parallel using multiprocessing.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/persistence/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Persistence
type: page
icon: material-tray-arrow-down
---

# Persistence

Any Python object can be serialized and saved to disk as a pickle file using [save](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.save).

**Save a dict to a file**

```python
cache = dict(data=data, indicator=indicator, pf=pf)
vbt.save(cache, "cache.pickle")
```

!!! important
    If a file with the same name already exists, it will be overwritten.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

A pickle file can then be loaded and deserialized using [load](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.load).

**Load the dict back**

```python
cache = vbt.load("cache.pickle")
```

!!! note
    The file can be read in another Python environment or even on another machine (such as a cloud
    environment). Just make sure that the Python and package versions on both ends are approximately
    the same.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Pickle files usually take up a considerable amount of space. To reduce the size, compression can be used.
The recommended compression algorithm for binary files is [blosc](https://github.com/Blosc/c-blosc).
To later load the compressed file, pass the `compression` argument in exactly the same way to the loader, 
or simply append the ".blosc" extension to the filename so the loader recognizes it automatically.
The supported algorithms and their possible extensions are listed under `extensions` in
[settings.pickling](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.pickling).

**Specify the compression explicitly**

```python
vbt.save(cache, "cache.pickle", compression="blosc")
cache = vbt.load("cache.pickle", compression="blosc")
```

**Specify the compression implicitly**

```python
vbt.save(cache, "cache.pickle.blosc")
cache = vbt.load("cache.pickle.blosc")
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

VBT objects that subclass [Pickleable](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable)
can also be saved individually. The benefit is that the class name and, optionally, the compression 
algorithm will be included in the filename by default, making loading simpler. The object can be 
loaded back using the `load()` method of the object's class.

**Save a portfolio under 'Portfolio.pickle.blosc' and load it back**

```python
pf.save(compression="blosc")
pf = vbt.PF.load()
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If a VBT object was saved with an older package version and loading it with a newer version throws 
an error (for example, due to a different argument order), the object can still be reconstructed by 
creating and registering a [RecInfo](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.RecInfo) instance 
before loading.

**Reconstruct an older BinanceData instance**

```python
def modify_state(rec_state):  # (1)!
    return vbt.RecState(
        init_args=rec_state.init_args,
        init_kwargs=rec_state.init_kwargs,
        attr_dct=rec_state.attr_dct,
    )

rec_info = vbt.RecInfo(
    vbt.get_id_from_class(vbt.BinanceData),
    vbt.BinanceData,
    modify_state
)
rec_info.register()
data = vbt.BinanceData.load()
```

1. Change the arguments to be passed to the class here.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If you encounter issues with saving an instance of a specific class, set the reconstruction id
`_rec_id` with any string, and then reconstruct the object using this id (the first argument of
`RecInfo`).

**Set a custom identifier to a class and reconstruct its instance using another class**

```python
class MyClass1(vbt.Configured):  # (1)!
    _rec_id = "MyClass"
    ...
    
my_obj = MyClass1()
vbt.save(my_obj, "my_obj")

# (2)!

class MyClass2(vbt.Configured):
    ...

rec_info = vbt.RecInfo("MyClass", MyClass2)
rec_info.register()
my_obj = vbt.load("my_obj")  # (3)!
```

1. `vbt.Configured` can be replaced by any other VBT class that subclasses it.

2. ...restart the kernel...

3. The resulting object is of the class `MyClass2` even though it was saved as `MyClass1`.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/plotting/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Plotting
type: page
icon: material-chart-bubble
---

# Plotting

Any Pandas Series or DataFrame can be plotted using an accessor. There are two main ways to plot data:

1. [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor) offers
methods designed for typical vectorbtpro workflows.

2. [PXAccessor](https://vectorbt.pro/pvt_41531c19/api/px/accessors/#vectorbtpro.px.accessors.PXAccessor) provides methods based on
[Plotly express](https://plotly.com/python/plotly-express/).

**How to plot a Pandas object**

```python
fig = sr_or_df.vbt.plot()  # (1)!

fig = pd.Series(
    np.asarray(y), 
    index=np.asarray(x)
).vbt.scatterplot()  # (2)!
fig = pf.value.vbt.lineplot()  # (3)!
fig = pf.sharpe_ratio.vbt.barplot()  # (4)!
fig = pf.returns.vbt.qqplot()  # (5)!
fig = pf.allocations.vbt.areaplot(line_shape="hv")  # (6)!
fig = pf.returns.vbt.histplot(trace_kwargs=dict(nbinsx=100))  # (7)!

monthly_returns = pf.returns_acc.resample("M").get()
fig = monthly_returns.vbt.boxplot()   # (8)!
fig = monthly_returns.vbt.heatmap()  # (9)!
fig = monthly_returns.vbt.ts_heatmap()  # (10)!

fig = pf.sharpe_ratio.vbt.heatmap(  # (11)!
    x_level="fast_window", 
    y_level="slow_window",
    symmetric=True
)
fig = pf.sharpe_ratio.vbt.heatmap(  # (12)!
    x_level="fast_window", 
    y_level="slow_window",
    slider_level="symbol",
    symmetric=True
)
fig = pf.sharpe_ratio.vbt.volume(  # (13)!
    x_level="timeperiod", 
    y_level="upper_threshold",
    z_level="lower_threshold",
    symmetric=True
)

# ______________________________________________________________

fig = sr_or_df.vbt.px.ecdf()  # (14)!
```

1. Create a line plot with visible markers. Each column will be a separate trace.

2. Create a scatter plot of variable `x` against variable `y`. Both must be one-dimensional.

3. Display equity as a line plot.

4. Compare Sharpe ratios for different symbols or parameter combinations using a bar plot.

5. Create a Q-Q plot to check if returns are normally distributed. Only one column at a time.

6. Plot allocations as an area plot.

7. Create a histogram of returns with 100 bins. Remove `nbinsx` for automatic binning.

8. Create a box plot of monthly returns.

9. Create a heatmap showing time (y-axis) versus monthly returns (x-axis).

10. Create a heatmap showing monthly returns (y-axis) versus time (x-axis).

11. Create a symmetrical heatmap of Sharpe ratios, with fast windows on the x-axis and slow windows on
the y-axis. The Pandas object must be a Series with two index levels.

12. Create a symmetrical heatmap of Sharpe ratios, with fast windows on the x-axis, slow windows on the
y-axis, and symbols displayed as a slider. The Pandas object must be a Series with three index levels.

13. For three or four parameters, create a volume plot instead.

14. Plot an Empirical Cumulative Distribution Plot using Plotly express.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

A figure created with VBT can be displayed either interactively or as a static image.

**How to display a figure**

```python
fig.show()  # (1)!
fig.show_svg()  # (2)!
fig.show_png()  # (3)!
```

1. Display an interactive figure.

2. Display as a static SVG.

3. Display as a static PNG.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To plot multiple items on the same figure, retrieve the figure from the first plot method and pass it to
each subsequent plot call.

**Plot equity lines of two portfolios in the same figure**

```python
fig = pf1.value.vbt.lineplot()
fig = pf2.value.vbt.lineplot(fig=fig)
fig.show()
```

This also works for plotting multiple columns of a portfolio or other complex objects. When creating a
graph with subplots, you can automatically overlay each column.

```python
pf.plot(per_column=True).show()  # (1)!

fig = pf["BTC-USD"].plot(show_legend=False, show_column_label=True)
fig = pf["ETH-USD"].plot(show_legend=False, show_column_label=True, fig=fig)
fig.show()  # (2)!
```

1. Plot each column on the same figure.

2. Same as above.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The default theme can be set globally in the settings. Available themes are listed under `themes` in
[settings.plotting](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.plotting).

**Set the dark theme**

```python
vbt.settings.set_theme("dark")
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can change trace parameters such as line color and marker shape using `trace_kwargs`. Some plot
methods have multiple such arguments. For the full list of allowed parameters, see the Plotly
documentation for the corresponding trace type, such as
[Scatter](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Scatter.html) for lines.

**Change the color of the upper and lower lines of a Bollinger Bands indicator**

```python
fig = bbands.plot(
    upper_trace_kwargs=dict(line=dict(color="green")),
    lower_trace_kwargs=dict(line=dict(color="red"))
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can change layout parameters by passing them directly to the plot method as keyword arguments.

**Make the width and height of the plot variable instead of fixed**

```python
fig = df.vbt.plot(width=None, height=None)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can create a plot with multiple subplots using `vbt.make_subplots()`, which accepts [the same
arguments](https://plotly.com/python-api-reference/generated/plotly.subplots.make_subplots.html) as Plotly.

**Create two subplots, one per row**

```python
fig = vbt.make_subplots(rows=2, cols=1)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Most plot methods accept the argument `add_trace_kwargs`
(see [Figure.add_trace](https://plotly.com/python-api-reference/generated/plotly.graph_objects.Figure.html#plotly.graph_objects.Figure.add_trace)),
which you can use to specify which subplot to draw the traces on.

**Plot the first and second DataFrame on the first and second subplot**

```python
df1.vbt.plot(add_trace_kwargs=dict(row=1, col=1), fig=fig)
df2.vbt.plot(add_trace_kwargs=dict(row=2, col=1), fig=fig)
```

!!! note
    The figure `fig` must be created using `vbt.make_subplots()`.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Traces with two different scales but the same time axis can be plotted together by creating a
secondary y-axis.

**Plot the first and second DataFrame on the first and second y-axis**

```python
fig = vbt.make_subplots(specs=[[{"secondary_y": True}]])
df1.vbt.plot(add_trace_kwargs=dict(secondary_y=False), fig=fig)
df2.vbt.plot(add_trace_kwargs=dict(secondary_y=True), fig=fig)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can modify the figure manually after creation. Here, `0` is the index of the trace in the figure.

**Update the title and markers of the scatter plot after creation**

```python
fig = df.vbt.scatterplot()
fig.layout.title.text = "Scatter"
fig.data[0].marker.line.width = 4
fig.data[0].marker.line.color = "black"
```

!!! note
    A plotting method can add multiple traces to the figure.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Plotting-related settings can be defined or changed globally in [settings.plotting](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.plotting).

**Change the background of any figure to black**

```python
vbt.settings["plotting"]["layout"]["paper_bgcolor"] = "rgb(0,0,0)"
vbt.settings["plotting"]["layout"]["plot_bgcolor"] = "rgb(0,0,0)"
vbt.settings["plotting"]["layout"]["template"] = "vbt_dark"
```

**Same by registering your own theme**

```python
import plotly.io as pio
import plotly.graph_objects as go

pio.templates["my_black"] = go.layout.Template(
    layout_paper_bgcolor="rgb(0,0,0)",
    layout_plot_bgcolor="rgb(0,0,0)",
)
vbt.settings["plotting"]["layout"]["template"] = "vbt_dark+my_black"
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

By default, Plotly displays a continuous datetime index, including time gaps such as non-business hours
and weekends. To skip these gaps, you can use the `rangebreaks` property.

**Skip non-business hours and weekends**

```python
fig = df.vbt.plot()
fig.update_xaxes(
    rangebreaks=[
        dict(bounds=['sat', 'mon']),
        dict(bounds=[16, 9.5], pattern='hour'),
        # (1)!
    ]
)
```

1. Pass `dict(values=df.isnull().all(axis=1).index)` to also skip NaN rows, or any other index.

!!! note
    Ensure your data has the correct timezone to apply this approach.

**Skip all gaps automatically**

```python
fig = df.vbt.plot()
fig.auto_rangebreaks()  # (1)!
fig.auto_rangebreaks(freq="D")  # (2)!

# ______________________________________________________________

vbt.settings.plotting.auto_rangebreaks = True
vbt.settings.plotting.auto_rangebreaks = dict(freq="D")

# ______________________________________________________________

def pre_show_func(fig):
    fig.auto_rangebreaks(freq="D")
    
vbt.settings.plotting.pre_show_func = pre_show_func  # (4)!
fig = df.vbt.plot()
fig.show()  # (5)!
```

1. Skip gaps based on the detected frequency of the index.

2. Skip gaps using any other frequency. For example, use daily frequency to skip weekends
when the detected frequency is business days.

3. Skip NaN rows, or provide any other index to skip.

4. Run auto-rangebreaks before displaying any figure.

5. You must call the `show()` method for this to take effect.

!!! note
    This approach works only for figures produced by VBT methods.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To display a figure on an interactive HTML page, see [Interactive HTML Export](https://plotly.com/python/interactive-html-export/).

**Save the figure to an HTML file**

```python
fig.write_html("fig.html")
```

**Save multiple figures to the same HTML file**

```python
with open("fig.html", "a") as f:
    f.write(fig1.to_html(full_html=False))
    f.write(fig2.to_html(full_html=False))
    f.write(fig3.to_html(full_html=False))
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To display a figure in a separate browser tab, see [Renderers](https://plotly.com/python/renderers/).

**Make browser the default renderer**

```python
import plotly.io as pio
pio.renderers.default = "browser"
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If a figure takes too long to display, the amount of data may be the cause.
In this case, [plotly-resampler](https://github.com/predict-idlab/plotly-resampler)
can help by resampling any (primarily scatter) data on the fly.

**Enable plotly-resampler globally**

```python
vbt.settings.plotting.use_resampler = True
```

Another option is to select a specific date range of interest.

**Display one year of data**

```python
fig = fig.select_range(start="2023", end="2024")
```

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/portfolio/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Portfolio
type: page
icon: material-chart-areaspline
---

# Portfolio

!!! question
    Find more information in the [Portfolio documentation](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/).

## From data

To quickly simulate a portfolio from any OHLC data, you can use [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run)
or pass the data instance (or simply a symbol or `class_name:symbol`) to the simulation method.

**Various way to quickly simulate a portfolio from a Data instance**

```python
pf = data.run("from_holding")  # (1)!
pf = data.run("from_random_signals", n=10)  # (2)!

pf = vbt.PF.from_holding(data)  # (3)!
pf = vbt.PF.from_holding("BTC-USD")  # (4)!
pf = vbt.PF.from_holding("BinanceData:BTCUSDT")  # (5)!
```

1. Simulate a buy-and-hold portfolio from data.

2. Simulate a portfolio from data with 10 entries and 10 exits placed randomly.

3. Same as the first line.

4. Retrieve the symbol "BTC-USD" from Yahoo Finance (default) and simulate a buy-and-hold portfolio.

5. Retrieve the symbol "BTCUSDT" from Binance and simulate a buy-and-hold portfolio.

## From signals

This simulation method is easy to use yet powerful, as long as your strategy can be defined
using signals, such as buy, sell, short sell, and buy to cover.

**Various signal configurations**

```python
pf = vbt.PF.from_signals(data, ...)  # (1)!
pf = vbt.PF.from_signals(open=open, high=high, low=low, close=close, ...)  # (2)!
pf = vbt.PF.from_signals(close, ...)  # (3)!

pf = vbt.PF.from_signals(data, entries, exits)  # (4)!
pf = vbt.PF.from_signals(data, entries, exits, direction="shortonly")  # (5)!
pf = vbt.PF.from_signals(data, entries, exits, direction="both")  # (6)!
pf = vbt.PF.from_signals(  # (7)!
    data, 
    long_entries=long_entries, 
    long_exits=long_exits,
    short_entries=short_entries, 
    short_exits=short_exits,
)
```

1. When a data instance is passed (not a DataFrame!), VBT will extract OHLC data automatically.

2. Same as above, but all price features are passed manually.

3. For tick data or when OHL is not available, pass only the close price.

4. Enter a long position when `entries` is True and exit it when `exits` is True (no short positions).

5. Enter a short position when `entries` is True and exit it when `exits` is True (no long positions).

6. Enter a long position when `entries` is True and a short position when `exits` is True, using reversals.

7. Enter a long position when `long_entries` is True, exit it when `long_exits` is True, enter a short
position when `short_entries` is True, and exit it when `short_exits` is True.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To set different prices or other arguments for long and short signals, create an empty array 
and use each signal type as a mask to assign the corresponding value.

**Manually apply a 1% slippage to the price**

```python
price = data.symbol_wrapper.fill()
price[entries] = data.close * (1 + 0.01)  # (1)!
price[exits] = data.close * (1 - 0.01)
```

1. Entries and exits here are masks: assign values where their value is True.
If you want to replace `data.close` with a NumPy array, use `arr[entries]` on the right side as well.

**Use the ask price for buying and the bid price for selling**

```python
price = (bid_price + ask_price) / 2
price[entries] = ask_price
price[exits] = bid_price
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To exit a trade after a specific amount of time or number of rows, use the `td_stop` argument.
The measurement starts from the opening time of the entry row.

**How to close out a position after some time**

```python
pf = vbt.PF.from_signals(..., td_stop="7 days")  # (1)!
pf = vbt.PF.from_signals(..., td_stop=pd.Timedelta(days=7))
pf = vbt.PF.from_signals(..., td_stop=td_arr)  # (2)!

pf = vbt.PF.from_signals(..., td_stop=7, time_delta_format="rows")  # (3)!
pf = vbt.PF.from_signals(..., td_stop=int_arr, time_delta_format="rows")  # (4)!

pf = vbt.PF.from_signals(..., td_stop=vbt.Param(["1 day", "7 days"]))  # (5)!
```

1. Exit after 7 days.

2. Specify a custom timedelta for each row using an array. For example, you can use 
[`pd.TimedeltaIndex`](https://pandas.pydata.org/docs/reference/api/pandas.TimedeltaIndex.html).

3. Exit after 7 rows (i.e., bars).

4. For each potential entry row, specify after how many rows to exit.

5. Any of the above examples can be tested as a parameter.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To exit a trade at a specific time or row count, use the `dt_stop` argument.
If you pass a timedelta (as above), the position will be exited at the last bar _before_ the target date.
Otherwise, if you provide an exact date or time, the position will be exited _at_ or _after_ that point.
You can override this behavior using the argument config.

**How to close out a position at some time**

```python
import datetime

pf = vbt.PF.from_signals(..., dt_stop="daily")  # (1)!
pf = vbt.PF.from_signals(..., dt_stop=pd.Timedelta(days=1))
pf = vbt.PF.from_signals(  # (2)!
    ..., 
    dt_stop="daily", 
    arg_config=dict(dt_stop=dict(last_before=False))
)

pf = vbt.PF.from_signals(..., dt_stop="16:00")  # (3)!
pf = vbt.PF.from_signals(..., dt_stop=datetime.time(16, 0))
pf = vbt.PF.from_signals(  # (4)!
    ..., 
    dt_stop="16:00", 
    arg_config=dict(dt_stop=dict(last_before=True))
)

pf = vbt.PF.from_signals(..., dt_stop="2024-01-01")  # (5)!
pf = vbt.PF.from_signals(..., dt_stop=pd.Timestamp("2024-01-01"))
pf = vbt.PF.from_signals(  # (6)!
    ..., 
    dt_stop="2024-01-01", 
    arg_config=dict(dt_stop=dict(last_before=True))
)
pf = vbt.PF.from_signals(..., dt_stop=dt_arr)  # (7)!

pf = vbt.PF.from_signals(..., dt_stop=int_arr, time_delta_format="rows")  # (8)!

pf = vbt.PF.from_signals(..., dt_stop=vbt.Param(["1 day", "7 days"]))  # (9)!
```

1. Exit at the last bar before the end of the day.

2. Exit at or after the end of the day.

3. Exit at or after 16:00.

4. Exit at the last bar before 16:00.

5. Exit on or after January 1, 2024. This applies only to entries issued before that date.

6. Exit at the last bar before January 1, 2024. This applies only to entries issued before that date.

7. Specify a custom datetime for each row using an array. For example, you can use 
[`pd.DatetimeIndex`](https://pandas.pydata.org/docs/reference/api/pandas.DatetimeIndex.html).

8. For each potential entry row, specify at which row to exit.

9. Any of the above examples can be tested as a parameter.

!!! note
    Do not confuse `td_stop` with `dt_stop`. "td" stands for timedelta, while 
    "dt" stands for datetime.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To perform multiple actions within a single bar, split each bar into three sub-bars:
opening nanosecond, middle, and closing nanosecond. For example, you can execute your signals at the
end of the bar, and your stop orders will be guaranteed to execute at the first two sub-bars. This lets
you close out a position and enter a new one within the same bar.

**Execute entries at open, exits at close**

```python
x3_open = open.vbt.repeat(3, axis=0)  # (1)!
x3_high = high.vbt.repeat(3, axis=0)
x3_low = low.vbt.repeat(3, axis=0)
x3_close = close.vbt.repeat(3, axis=0)
x3_entries = entries.vbt.repeat(3, axis=0)
x3_exits = exits.vbt.repeat(3, axis=0)

bar_open = slice(0, None, 3)  # (2)!
bar_middle = slice(1, None, 3)
bar_close = slice(2, None, 3)

x3_high.iloc[bar_open] = open.copy()  # (3)!
x3_low.iloc[bar_open] = open.copy()
x3_close.iloc[bar_open] = open.copy()

x3_open.iloc[bar_close] = close.copy()  # (4)!
x3_high.iloc[bar_close] = close.copy()
x3_low.iloc[bar_close] = close.copy()

x3_entries.iloc[bar_middle] = False  # (5)!
x3_entries.iloc[bar_close] = False

x3_exits.iloc[bar_open] = False  # (6)!
x3_exits.iloc[bar_middle] = False

x3_index = pd.Series(x3_close.index)  # (7)!
x3_index.iloc[bar_middle] += pd.Timedelta(nanoseconds=1)
x3_index.iloc[bar_close] += index.freq - pd.Timedelta(nanoseconds=1)
x3_index = pd.Index(x3_index)
x3_open.index = x3_index
x3_high.index = x3_index
x3_low.index = x3_index
x3_close.index = x3_index
x3_entries.index = x3_index
x3_exits.index = x3_index

x3_pf = vbt.PF.from_signals(  # (8)!
    open=x3_open,
    high=x3_high,
    low=x3_low,
    close=x3_close,
    entries=x3_entries,
    exits=x3_exits,
)
pf = x3_pf.resample(close.index, freq=False, silence_warnings=True)  # (9)!
```

1. Split each array into three sub-bars.

2. These slices are used to index a specific sub-bar. They can be applied only with `iloc` or on NumPy
arrays.

3. Set OHLC to open at the first nanosecond.

4. Set OHLC to close at the last nanosecond.

5. Enable entries only at the open, and disable them at other sub-bars.

6. Enable exits only at the close, and disable them at other sub-bars.

7. Prepare the index.

8. Run the simulation.

9. Resample back to the original index.

### Callbacks

To save a piece of information at one timestamp and reuse it at a later timestamp in a callback,
create a NumPy array and pass it to the callback. The array should be one-dimensional and have the
same number of elements as there are columns. You can then read and write the element under the
current column using the same method as accessing the latest position via `c.last_position[c.col]`.
If you need to store more pieces of information, use additional arrays or a single structured array.
For convenience, you can combine multiple arrays into a named tuple.

**Execute only the first signal**

```python
from collections import namedtuple

Memory = namedtuple("Memory", ["signal_executed"])

@njit
def signal_func_nb(c, entries, exits, memory):
    is_entry = vbt.pf_nb.select_nb(c, entries)
    is_exit = vbt.pf_nb.select_nb(c, exits)
    if is_entry and not memory.signal_executed[c.col]:  # (1)!
        memory.signal_executed[c.col] = True  # (2)!
        return True, False, False, False
    if is_exit:
        return False, True, False, False
    return False, False, False, False
    
def init_memory(target_shape):
    return Memory(
        signal_executed=np.full(target_shape[1], False)  # (3)!
    )
    
pf = vbt.PF.from_signals(
    ...,
    entries=entries,
    exits=exits,
    signal_func_nb=signal_func_nb,
    signal_args=(
        vbt.Rep("entries"), 
        vbt.Rep("exits"), 
        vbt.RepFunc(init_memory)
    )
)
```

1. Read the element under the current column.

2. Write the element under the current column.

3. Create a boolean array filled with False, with one element per column.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To overcome the limitation of having only one active built-in limit order at a time, you can create
custom limit orders. This allows you to have multiple active orders at once. Store relevant data in
memory and manually check if the limit order price has been reached on each bar. When the price is
hit, simply generate a signal.

**Breakout strategy by straddling current price with opposing limit orders**

```python
Memory = namedtuple("Memory", ["signal_price"])  # (1)!

@njit
def signal_func_nb(c, signals, memory, limit_delta):
    if np.isnan(memory.signal_price[c.col]):
        signal = vbt.pf_nb.select_nb(c, signals)
        if signal:
            memory.signal_price[c.col] = vbt.pf_nb.select_nb(c, c.close)  # (2)!
    else:
        open = vbt.pf_nb.select_nb(c, c.open)
        high = vbt.pf_nb.select_nb(c, c.high)
        low = vbt.pf_nb.select_nb(c, c.low)
        close = vbt.pf_nb.select_nb(c, c.close)
        above_price = vbt.pf_nb.resolve_limit_price_nb(  # (3)!
            init_price=memory.signal_price[c.col],
            limit_delta=limit_delta,
            hit_below=False
        )
        hit_price, _, _ = vbt.pf_nb.check_price_hit_nb(  # (4)!
            open=open,
            high=high,
            low=low,
            close=close,
            price=above_price,
            hit_below=False
        )
        if not np.isnan(hit_price):
            memory.signal_price[c.col] = np.nan
            return True, False, False, False  # (5)!
        below_price = vbt.pf_nb.resolve_limit_price_nb(  # (6)!
            init_price=memory.signal_price[c.col],
            limit_delta=limit_delta,
            hit_below=True
        )
        hit_price, _, _ = vbt.pf_nb.check_price_hit_nb(
            open=open,
            high=high,
            low=low,
            close=close,
            price=below_price,
            hit_below=True
        )
        if not np.isnan(hit_price):
            memory.signal_price[c.col] = np.nan
            return False, False, True, False
    return False, False, False, False

def init_memory(target_shape):
    return Memory(
        signal_price=np.full(target_shape[1], np.nan)
    )

pf = vbt.PF.from_signals(
    ...,
    signal_func_nb=signal_func_nb,
    signal_args=(
        vbt.Rep("signals"), 
        vbt.RepFunc(init_memory),
        0.1
    ),
    broadcast_named_args=dict(
        signals=signals
    )
)
```

1. Memory with a single array: the price at the time when the latest user signal occurred.

2. Whenever a user signal occurs, save the current price and skip the current bar.

3. Resolve the limit price (i.e., signal price plus delta) for going long.

4. Check whether the limit price has been hit.

5. If so, clear the memory for this asset and return a long entry.

6. The same logic applies for a short entry.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

If signals are generated dynamically and only a subset are actually executed, you may want to keep 
track of all generated signals for later analysis. To do this, use function templates to create 
__global__ custom arrays and fill these arrays during the simulation.

**Place entries and exits randomly and access them outside the simulation**

```python
custom_arrays = dict()

def create_entries_out(wrapper):  # (1)!
    entries_out = np.full(wrapper.shape_2d, False)
    custom_arrays["entries"] = entries_out  # (2)!
    return entries_out

def create_exits_out(wrapper):
    exits_out = np.full(wrapper.shape_2d, False)
    custom_arrays["exits"] = exits_out
    return exits_out

@njit
def signal_func_nb(c, entry_prob, exit_prob, entries_out, exits_out):
    entry_prob_now = vbt.pf_nb.select_nb(c, entry_prob)
    exit_prob_now = vbt.pf_nb.select_nb(c, exit_prob)
    if np.random.uniform(0, 1) < entry_prob_now:
        is_entry = True
        entries_out[c.i, c.col] = True  # (3)!
    else:
        is_entry = False
    if np.random.uniform(0, 1) < exit_prob_now:
        is_exit = True
        exits_out[c.i, c.col] = True
    else:
        is_exit = False
    return is_entry, is_exit, False, False

pf = vbt.PF.from_signals(
    ...,
    signal_func_nb=signal_func_nb,
    signal_args=(
        vbt.Rep("entry_prob"), 
        vbt.Rep("exit_prob"), 
        vbt.RepFunc(create_entries_out),  # (4)!
        vbt.RepFunc(create_exits_out),
    ),
    broadcast_named_args=dict(
        entry_prob=0.1,
        exit_prob=0.1
    )
)

print(custom_arrays)
```

1. Function to create an empty NumPy array. The wrapper contains the shape of the simulation.

2. Store a reference to the array for later use.

3. Write information to the array.

4. Use a template to call the function once the shape of the simulation is known.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To limit the number of active positions within a group, disable any entry signal in a custom signal
function whenever the limit has been reached. The exit signal should be allowed to execute at any time.

**Allow at most one active position at a time**

```python
@njit
def signal_func_nb(c, entries, exits, max_active_positions):
    is_entry = vbt.pf_nb.select_nb(c, entries)
    is_exit = vbt.pf_nb.select_nb(c, exits)
    n_active_positions = vbt.pf_nb.get_n_active_positions_nb(c)
    if n_active_positions >= max_active_positions:
        return False, is_exit, False, False  # (1)!
    return is_entry, is_exit, False, False

pf = vbt.PF.from_signals(
    ...,
    entries=entries,
    exits=exits,
    signal_func_nb=signal_func_nb,
    signal_args=(vbt.Rep("entries"), vbt.Rep("exits"), 1),
    group_by=True  # (2)!
)
```

1. Disable any entry signal.

2. Combine all assets into one group. This can be customized.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To access information on the current or previous position, query the position information records.

**Ignore entries for a number of days after a losing trade**

```python
@njit
def signal_func_nb(c, entries, exits, cooldown):
    entry = vbt.pf_nb.select_nb(c, entries)
    exit = vbt.pf_nb.select_nb(c, exits)
    if not vbt.pf_nb.in_position_nb(c):
        if vbt.pf_nb.has_orders_nb(c):
            if c.last_pos_info[c.col]["pnl"] < 0:  # (1)!
                last_exit_idx = c.last_pos_info[c.col]["exit_idx"]
                if c.index[c.i] - c.index[last_exit_idx] < cooldown:
                    return False, exit, False, False
    return entry, exit, False, False
    
pf = vbt.PF.from_signals(
    ...,
    signal_func_nb=signal_func_nb,
    signal_args=(
        vbt.Rep("entries"), 
        vbt.Rep("exits"), 
        vbt.dt.to_ns(vbt.timedelta("7D"))
    )
)
```

1. If not in a position, the position information records contain information about the last (closed) position.

2. Cooldown should be an integer representing the number of nanoseconds. Any timedelta
should be converted to nanoseconds before execution.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To activate a stop-loss (SL) or another stop order after a certain condition is met, set it initially 
to infinity. Then, update the stop value in a callback once the condition is satisfied.

**Set SL to breakeven after price has moved a certain %**

```python
@njit
def adjust_func_nb(c, perc):
    ...
    sl_info = c.last_sl_info[c.col]
    if c.i > 0 and np.isinf(sl_info["stop"]):  # (1)!
        prev_close = vbt.pf_nb.select_nb(c, c.close, i=c.i - 1)
        price_change = prev_close / sl_info["init_price"] - 1
        if c.last_position[c.col] < 0:
            price_change *= -1
        if price_change >= perc:
            sl_info["stop"] = 0.0  # (2)!

pf = vbt.PF.from_signals(
    ...,
    sl_stop=np.inf,
    stop_entry_price="fillprice",
    adjust_func_nb=adjust_func_nb, 
    adjust_args=(0.1,),
)
```

1. SL has not yet been activated.

2. Activate an SL order by setting the stop price to the initial price.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The stop value can be updated not just once, but on every bar.

**ATR-based TSL order**

```python
@njit
def adjust_func_nb(c, atr):
    ...
    if c.i > 0:
        tsl_info = c.last_tsl_info[c.col]
        tsl_info["stop"] = vbt.pf_nb.select_nb(c, atr, i=c.i - 1)

pf = vbt.PF.from_signals(
    ...,
    tsl_stop=np.inf,
    stop_entry_price="fillprice",
    delta_format="absolute",
    broadcast_named_args=dict(atr=atr),
    adjust_func_nb=adjust_func_nb,
    adjust_args=(vbt.Rep("atr"),)
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To set a ladder dynamically, use `stop_ladder="dynamic"` and then use the current ladder step in a 
callback to pull information from a custom array and override the stop information with that value.

**Set a ladder based on ATR multipliers**

```python
@njit
def adjust_func_nb(c, atr, multipliers, exit_sizes):
    tp_info = c.last_tp_info[c.col]
    if vbt.pf_nb.is_stop_info_ladder_active_nb(tp_info):
        if np.isnan(tp_info["stop"]):
            step = tp_info["step"]
            init_atr = vbt.pf_nb.select_nb(c, atr, i=tp_info["init_idx"])
            tp_info["stop"] = init_atr * multipliers[step]
            tp_info["delta_format"] = vbt.pf_enums.DeltaFormat.Absolute
            tp_info["exit_size"] = exit_sizes[step]
            tp_info["exit_size_type"] = vbt.pf_enums.SizeType.Percent
            
pf = vbt.PF.from_signals(
    ...,
    adjust_func_nb=adjust_func_nb,
    adjust_args=(
        vbt.Rep("atr"),
        np.array([1, 2]),
        np.array([0.5, 1.0])
    ),
    stop_ladder="dynamic",
    broadcast_named_args=dict(atr=atr)
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Position metrics, such as the current open P&L and return, are available through the 
`last_pos_info` context field. This is an array with one record per column, using the data type
[trade_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.trade_dt).

**By hitting an unrealized profit of 100%, lock in 50% of it with SL**

```python
@njit
def adjust_func_nb(c, x, y):  # (1)!
    pos_info = c.last_pos_info[c.col]
    if pos_info["status"] == vbt.pf_enums.TradeStatus.Open:
        if pos_info["return"] >= x:
            sl_info = c.last_sl_info[c.col]
            if not vbt.pf_nb.is_stop_info_active_nb(sl_info):
                entry_price = pos_info["entry_price"]
                if vbt.pf_nb.in_long_position_nb(c):
                    x_price = entry_price * (1 + x)  # (2)!
                    y_price = entry_price * (1 + y)  # (3)!
                else:
                    x_price = entry_price * (1 - x)
                    y_price = entry_price * (1 - y)
                vbt.pf_nb.set_sl_info_nb(
                    sl_info, 
                    init_idx=c.i, 
                    init_price=x_price,
                    stop=y_price,
                    delta_format=vbt.pf_enums.DeltaFormat.Target
                )
                
pf = vbt.PF.from_signals(
    ..., 
    adjust_func_nb=adjust_func_nb,
    adjust_args=(1.0, 0.5)
)
```

1. Both arguments (x = return as %, y = SL as %) should be provided as single values.

2. The SL is triggered at the price where `x` was reached.

3. The target price of the SL is calculated relative to the entry price so that `x` and `y` use the same scale
    (for example, 10% and 10% -> 0% final return).

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To dynamically determine and apply an optimal position size, create an empty size array filled with NaN.
Then, in a callback, calculate the target size and write it to the size array.

**Risk only 1% of the cash balance with each trade**

```python
@njit
def adjust_func_nb(c, size, sl_stop, delta_format, risk_amount):
    close_now = vbt.pf_nb.select_nb(c, c.close)
    sl_stop_now = vbt.pf_nb.select_nb(c, sl_stop)
    delta_format_now = vbt.pf_nb.select_nb(c, delta_format)
    risk_amount_now = vbt.pf_nb.select_nb(c, risk_amount)
    free_cash_now = vbt.pf_nb.get_free_cash_nb(c)
        
    stop_price = vbt.pf_nb.resolve_stop_price_nb(
        init_price=close_now,
        stop=sl_stop_now,
        delta_format=delta_format_now,
        hit_below=True
    )
    price_diff = abs(close_now - stop_price)
    size[c.i, c.col] = risk_amount_now * free_cash_now / price_diff

pf = vbt.PF.from_signals(
    ...,
    adjust_func_nb=adjust_func_nb,
    adjust_args=(
        vbt.Rep("size"), 
        vbt.Rep("sl_stop"), 
        vbt.Rep("delta_format"), 
        vbt.Rep("risk_amount")
    ),
    size=vbt.RepFunc(lambda wrapper: np.full(wrapper.shape_2d, np.nan)),
    sl_stop=0.1,
    delta_format="percent",
    broadcast_named_args=dict(risk_amount=0.01)
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To ensure SL/TP considers the average entry price, rather than just the first order entry price,
when accumulation is enabled, set the initial price of the stop record to the position's entry price.

**Apply an SL of 10% to the accumulated position**

```python
@njit
def post_signal_func_nb(c):
    if vbt.pf_nb.order_increased_position_nb(c):
        c.last_sl_info[c.col]["init_price"] = c.last_pos_info[c.col]["entry_price"]
        
pf = vbt.PF.from_signals(
    ...,
    accumulate="addonly",
    sl_stop=0.1,
    post_signal_func_nb=post_signal_func_nb,
)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To check at the end of a bar whether a signal has been executed, use `post_signal_func_nb`
or `post_segment_func_nb`. The first is called right after an order is executed and can
access the result of the executed order through `c.order_result`. The second is called
after all columns in the current group have been processed (just one column if there is no grouping),
cash deposits and earnings have been applied, and the portfolio value and returns have been updated.

**Apply a 20% tax on any positive P&L generated from closing out a position**

```python
@njit
def post_signal_func_nb(c, cash_earnings, tax):
    if vbt.pf_nb.order_closed_position_nb(c):
        pos_info = c.last_pos_info[c.col]
        pnl = pos_info["pnl"]
        if pnl > 0:
            cash_earnings[c.i, c.col] = -tax * pnl
    
tax = 0.2
pf = vbt.PF.from_signals(
    ...,
    post_signal_func_nb=post_signal_func_nb,
    post_signal_args=(vbt.Rep("cash_earnings"), tax),
    cash_earnings=vbt.RepEval("np.full(wrapper.shape_2d, 0.0)")
)
```

!!! tip
    An alternative approach after creating the portfolio:

    ```python
    winning_positions = pf.positions.winning
    winning_idxs = winning_positions.end_idx.values
    winning_pnl = winning_positions.pnl.values
    cash_earnings = pf.get_cash_earnings(group_by=False)
    if pf.wrapper.ndim == 2:
        winning_cols = winning_positions.col_arr
        cash_earnings.values[winning_idxs, winning_cols] += -tax * winning_pnl
    else:
        cash_earnings.values[winning_idxs] += -tax * winning_pnl
    new_pf = pf.replace(cash_earnings=cash_earnings)
    ```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To access the running total return during the simulation, create an empty array for cumulative
returns and update it within the `post_segment_func_nb` callback. The same array can be accessed
by other callbacks to get the total return at any time step.

**Access the running total return from within a simulation**

```python
@njit
def adjust_func_nb(c, cum_return):
    if c.cash_sharing:
        total_return = cum_return[c.group] - 1
    else:
        total_return = cum_return[c.col] - 1
    ...  # (1)!

@njit
def post_segment_func_nb(c, cum_return):
    if c.cash_sharing:
        cum_return[c.group] *= 1 + c.last_return[c.group]
    else:
        for col in range(c.from_col, c.to_col):
            cum_return[col] *= 1 + c.last_return[col]
          
cum_return = None
def init_cum_return(wrapper):
    global cum_return
    if cum_return is None:
        cum_return = np.full(wrapper.shape_2d[1], 1.0)
    return cum_return
    
pf = vbt.PF.from_signals(
    ...,
    adjust_func_nb=adjust_func_nb,
    adjust_args=(vbt.RepFunc(init_cum_return),),
    post_segment_func_nb=post_segment_func_nb,
    post_segment_args=(vbt.RepFunc(init_cum_return),),
)
```

1. Your logic here.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can use the same process to access the running trade records during the simulation.

**Access the running trade records from within a simulation**

```python
from collections import namedtuple

TradeMemory = namedtuple("TradeMemory", ["trade_records", "trade_counts"])

@njit
def adjust_func_nb(c, trade_memory):
    trade_count = trade_memory.trade_counts[c.col]
    trade_records = trade_memory.trade_records[:trade_count, c.col]
    ...  # (1)!

@njit
def post_signal_func_nb(c, trade_memory):
    if vbt.pf_nb.order_filled_nb(c):
        exit_trade_records = vbt.pf_nb.get_exit_trade_records_nb(c)
        trade_memory.trade_records[:len(exit_trade_records), c.col] = exit_trade_records
        trade_memory.trade_counts[c.col] = len(exit_trade_records)
        
trade_memory = None
def init_trade_memory(target_shape):
    global trade_memory
    if trade_memory is None:
        trade_memory = TradeMemory(
            trade_records=np.empty(target_shape, dtype=vbt.pf_enums.trade_dt),  # (2)!
            trade_counts=np.full(target_shape[1], 0)
        )
    return trade_memory
    
pf = vbt.PF.from_random_signals(
    ...,
    post_signal_func_nb=post_signal_func_nb,
    post_signal_args=(vbt.RepFunc(init_trade_memory),),
)
```

1. Your logic here.

2. Create an array with the full shape to handle the worst case of one trade per bar.
If you expect fewer trades, you can limit the number of rows in the shape.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To execute an SL (or any other order type) on the same bar as the entry, you can check
whether the stop order is triggered on this bar and, if so, execute it as a regular signal on the next bar.

**Execute each entry using open price and potentially SL at the same bar**

```python
Memory = namedtuple("Memory", ["stop_price", "order_type"])
memory = None

def init_memory(target_shape):
    global memory
    if memory is None:
        memory = Memory(
            stop_price=np.full(target_shape, np.nan),
            order_type=np.full(target_shape, -1),
        )
    return memory

@njit
def signal_func_nb(c, price, memory, ...):
    if c.i > 0 and not np.isnan(memory.stop_price[c.i - 1, c.col]):
        price[c.i, c.col] = memory.stop_price[c.i - 1, c.col]
        return False, True, False, True
    ...

@njit
def post_signal_func_nb(c, memory, ...):
    if vbt.pf_nb.order_opened_position_nb(c):
        open = vbt.pf_nb.select_nb(c, c.open)
        high = vbt.pf_nb.select_nb(c, c.high)
        low = vbt.pf_nb.select_nb(c, c.low)
        close = vbt.pf_nb.select_nb(c, c.close)
        sl_stop_price, _, _ = vbt.pf_nb.check_stop_hit_nb(
            open=open,
            high=high,
            low=low,
            close=close,
            is_position_long=c.last_position[c.col] > 0,
            init_price=c.last_sl_info["init_price"][c.col],
            stop=c.last_sl_info["stop"][c.col],
            delta_format=c.last_sl_info["delta_format"][c.col],
            hit_below=True,
            can_use_ohlc=True,
            check_open=False,
            hard_stop=c.last_sl_info["exit_price"][c.col] == vbt.pf_enums.StopExitPrice.HardStop,
        )
        if not np.isnan(sl_stop_price):
            memory.stop_price[c.i, c.col] = sl_stop_price
            memory.order_type[c.i, c.col] = vbt.sig_enums.StopType.SL
            vbt.pf_nb.clear_sl_info_nb(c.last_sl_info[c.col])
            vbt.pf_nb.clear_tp_info_nb(c.last_tp_info[c.col])
        
    elif vbt.pf_nb.order_closed_position_nb(c):
        if memory.order_type[c.i - 1, c.col] != -1:
            order = c.order_records[c.order_counts[c.col] - 1, c.col]
            order["stop_type"] = memory.order_type[c.i - 1, c.col]
            order["signal_idx"] = c.i - 1
            order["creation_idx"] = c.i - 1
            order["idx"] = c.i - 1
    ...

pf = vbt.PF.from_signals(
    ...,
    signal_func_nb=signal_func_nb,
    signal_args=(vbt.Rep("price"), vbt.RepFunc(init_memory), ...),
    post_signal_func_nb=post_signal_func_nb,
    post_signal_args=(vbt.RepFunc(init_memory), ...),
    price=vbt.RepFunc(lambda wrapper: np.full(wrapper.shape_2d, -np.inf)),
    sl_stop=0.1,
    stop_entry_price="fillprice"
)
```

## Records

There are several ways to examine the orders, trades, and positions generated by a simulation.
Each one corresponds to a different concept in vectorbtpro. Be sure to understand their differences
by reviewing the examples at the top of the [trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/) module.

**Print out information on various records**

```python
print(pf.orders.readable)  # (1)!
print(pf.entry_trades.readable)  # (2)!
print(pf.exit_trades.readable)  # (3)!
print(pf.trades.readable)  # (4)!
print(pf.positions.readable)  # (5)!

print(pf.trade_history)  # (6)!
```

1. Information on orders.

2. Information on trades that open or increase a position.

3. Information on trades that close or decrease a position.

4. Same as `exit_trades` by default.

5. Information on positions.

6. Information on orders and trades merged together.

## Metrics

By default, the year frequency is set to 365 days, assuming each trading day lasts 24 hours. However,
for stocks or other securities, you should change it to 252 days or less. Also, consider trading hours
when working with sub-daily data frequency.

**Change the year frequency**

```python
vbt.settings.returns.year_freq = "auto"  # (1)!

vbt.settings.returns.year_freq = "252 days"  # (2)!
vbt.settings.returns.year_freq = pd.Timedelta(days=252)  # (3)!
vbt.settings.returns.year_freq = pd.offsets.BDay() * 252  # (4)!
vbt.settings.returns.year_freq = pd.Timedelta(hours=6.5) * 252  # (5)!

returns_df.vbt.returns(year_freq="252 days").stats()  # (6)!
pf = vbt.PF.from_signals(..., year_freq="252 days")  # (7)!
```

1. Determine the year frequency automatically.

2. Change the year frequency to 252 days. Assumes daily frequency.

3. Same as above, set as a timedelta.

4. Same as above, set as a date offset (business days).

5. Account for 6.5 trading hours per day. Assumes intra-daily frequency.

6. Year frequency can also be set locally per DataFrame.

7. Year frequency can also be passed directly to the portfolio.

!!! info
    The year frequency will be divided by the frequency of your data to calculate the annualization
    factor. For example, `pd.Timedelta(hours=6.5) * 252` divided by `15 minutes` will result in a
    factor of 6552.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To tell VBT to place zero instead of infinity and NaN in any generated returns, create a
[configuration](https://vectorbt.pro/pvt_41531c19/cookbook/configuration/#settings) file with the following content:

=== "INI"

    **File vbt.cfg**

```ini
    [returns]
    inf_to_nan = True
    nan_to_zero = True
    ```

=== "YAML"

    ```yaml title="File vbt.yml"
    returns:
      inf_to_nan: true
      nan_to_zero: true
    ```

=== "TOML"

    ```toml title="File vbt.toml"
    [returns]
    inf_to_nan = true
    nan_to_zero = true
    ```

!!! note
    If this does not work, run `vbt.clear_pycache()` and restart the kernel.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To compute a metric based on the returns or other time series of each trade, rather than for the
entire equity, use projections to extract the time series range corresponding to each trade.

```python title="Calculate the average total log return of winning and losing trades"
winning_trade_returns = pf.trades.winning.get_projections(pf.log_returns, rebase=False)
losing_trade_returns = pf.trades.losing.get_projections(pf.log_returns, rebase=False)
avg_winning_trade_return = vbt.pd_acc.returns(winning_trade_returns, log_returns=True).total().mean()
avg_losing_trade_return = vbt.pd_acc.returns(losing_trade_returns, log_returns=True).total().mean()
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

To calculate a trade metric purely in Numba, convert order records to trade records, compute the column
map for the trade records, and then reduce each column to a single number.

**Calculate trade win rate in Numba**

```python
order_records = sim_out.order_records  # (1)!

order_col_map = vbt.rec_nb.col_map_nb(
    order_records["col"],
    close.shape[1]  # (2)!
)
trade_records = vbt.pf_nb.get_exit_trades_nb(
    order_records, 
    close, 
    order_col_map
)
trade_col_map = vbt.rec_nb.col_map_nb(
    trade_records["col"], 
    close.shape[1]
)
win_rate = vbt.rec_nb.reduce_mapped_nb(
    trade_records["pnl"], 
    trade_col_map, 
    np.nan, 
    vbt.pf_nb.win_rate_reduce_nb
)
```

1. Simulation output is from a simulation function such as `from_signals_nb`.

2. Close must be a two-dimensional NumPy array.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

The same method applies to drawdown records, which are based on cumulative returns.

**Calculate maximum drawdown duration in Numba**

```python
returns = sim_out.in_outputs.returns

cumulative_returns = vbt.ret_nb.cumulative_returns_nb(returns)  # (1)!
drawdown_records = vbt.nb.get_drawdowns_nb(None, None, None, cumulative_returns)
dd_duration = vbt.nb.range_duration_nb(  # (2)!
    drawdown_records["start_idx"], 
    drawdown_records["end_idx"], 
    drawdown_records["status"]
)
dd_col_map = vbt.rec_nb.col_map_nb(
    drawdown_records["col"],
    returns.shape[1]
)
max_dd_duration = vbt.rec_nb.reduce_mapped_nb(  # (3)!
    dd_duration,
    dd_col_map,
    np.nan,
    vbt.nb.max_reduce_nb
)
```

1. Expects a two-dimensional NumPy array.

2. Returns one value per record.

3. Returns one value per column.

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

Return metrics are not based on records and can be calculated directly from returns.

**Calculate various return metrics in Numba**

```python
returns = sim_out.in_outputs.returns

total_return = vbt.ret_nb.total_return_nb(returns)  # (1)!
max_dd = vbt.ret_nb.max_drawdown_nb(returns)  # (2)!
sharpe_ratio = vbt.ret_nb.sharpe_ratio_nb(returns, ann_factor=ann_factor)  # (3)!
```

1. Expects a two-dimensional NumPy array.

2. Returns one value per column.

3. Annualization factor is the average number of bars in a year, such as 365.

## Metadata

You can access the columns and groups of the portfolio through its wrapper and grouper, respectively.

**How to get the columns and groups of a Portfolio instance**

```python
print(pf.wrapper.columns)  # (1)!
print(pf.wrapper.grouper.is_grouped())  # (2)!
print(pf.wrapper.grouper.grouped_index)  # (3)!
print(pf.wrapper.get_columns())  # (4)!

columns_or_groups = pf.wrapper.get_columns()
first_pf = pf[columns_or_groups[0]]  # (5)!
```

1. Get the columns, regardless of any grouping.

2. Check whether the object is grouped.

3. If the above is True, get the groups.

4. Get the columns or groups (if grouped). Recommended.

5. Select the first column or group (if grouped) from the object.

## Stacking

Multiple compatible array-based strategies can be included in the same portfolio by stacking their
respective arrays along the columns.

**Simulate and analyze multiple strategies jointly**

```python
strategy_keys = pd.Index(["strategy1", "strategy2"], name="strategy")
entries = pd.concat((entries1, entries2), axis=1, keys=strategy_keys)
exits = pd.concat((exits1, exits2), axis=1, keys=strategy_keys)
pf = vbt.PF.from_signals(data, entries, exits)
```

<div class="separator-container">
    <hr class="separator">
        <span class="separator-text">+</span>
    <hr class="separator">
</div>

You can also stack multiple incompatible strategies—such as those requiring different simulation methods
or argument combinations—by simulating them independently and then stacking them for joint analysis.
This combines their data, order records, initial states, in-place output arrays, and more as if they were
stacked before simulation, with grouping disabled.

**Simulate multiple strategies separately but analyze them jointly**

```python
strategy_keys = pd.Index(["strategy1", "strategy2"], name="strategy")
pf1 = vbt.PF.from_signals(data, entries, exits)
pf2 = vbt.PF.from_orders(data, size, price)
pf = vbt.PF.column_stack((pf1, pf2), wrapper_kwargs=dict(keys=strategy_keys))

# ______________________________________________________________

pf = vbt.PF.column_stack(
    (pf1, pf2), 
    wrapper_kwargs=dict(keys=strategy_keys), 
    group_by=strategy_keys.name  # (1)!
)
```

1. Represent each portfolio as a group in the new portfolio.

## Parallelizing

If you want to simulate multiple columns (without cash sharing) or multiple groups (with or without
cash sharing), you can easily parallelize execution in several ways.

**How to parallelize simulation**

```python
pf = vbt.PF.from_signals(..., chunked="threadpool")  # (1)!

# ______________________________________________________________

pf = vbt.PF.from_signals(..., jitted=dict(parallel=True))  # (2)!
```

1. Use chunking with multithreading.

2. Use automatic parallelization within Numba.

You can also parallelize statistics after your portfolio has been simulated.

```python
@vbt.chunked(engine="threadpool")
def chunked_stats(pf: vbt.ChunkedArray(axis=1)) -> "row_stack":
    return pf.stats(agg_func=None)

chunked_stats(pf)  # (1)!

# ______________________________________________________________

pf.chunk_apply(  # (2)!
    "stats", 
    agg_func=None, 
    execute_kwargs=dict(engine="threadpool", merge_func="row_stack")
)

# ______________________________________________________________

pf.stats(agg_func=None, settings=dict(jitted=dict(parallel=True)))  # (3)!
```

1. Use chunking with multithreading.

2. Same as above.

3. Use automatic parallelization within Numba.

---
link: https://vectorbt.pro/pvt_41531c19/cookbook/signals/
parent: https://vectorbt.pro/pvt_41531c19/cookbook/
name: Signals
type: page
icon: material-broadcast
---

# Signals

!!! question
    Learn more in the [Signal development tutorial](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development/).

## Cleaning

You can clean only two arrays at a time. For more than two arrays, create a custom Numba function to
handle the task.

**Clean 4 arrays**

```python
@njit
def custom_clean_nb(long_en, long_ex, short_en, short_ex):
    new_long_en = np.full_like(long_en, False)
    new_long_ex = np.full_like(long_ex, False)
    new_short_en = np.full_like(short_en, False)
    new_short_ex = np.full_like(short_ex, False)
    
    for col in range(long_en.shape[1]):  # (1)!
        position = 0  # (2)!
        for i in range(long_en.shape[0]):  # (3)!
            if long_en[i, col] and position != 1:
                new_long_en[i, col] = True  # (4)!
                position = 1
            elif short_en[i, col] and position != -1:
                new_short_en[i, col] = True
                position = -1
            elif long_ex[i, col] and position == 1:
                new_long_ex[i, col] = True
                position = 0
            elif short_ex[i, col] and position == -1:
                new_short_ex[i, col] = True
                position = 0
            
    return new_long_en, new_long_ex, new_short_en, new_short_ex
```

1. Iterate over columns (i.e., assets).

2. Set the initial position per column: 0 for no position, 1 for long, and -1 for short.

3. Iterate over rows (i.e., bars).

4. Keep a signal only if the condition above is satisfied; otherwise, skip the signal.

!!! tip
    Convert each input array to NumPy with `arr = vbt.to_2d_array(df)`, and then convert each output
    array back to Pandas with `new_df = df.vbt.wrapper.wrap(arr)`.

---
link: https://www.pyquantnews.com/the-pyquant-newsletter/backtest-powerful-intraday-trading-strategies
parent: https://www.pyquantnews.com/
name: Backtest powerful intraday trading strategies
type: page
---

# How to backtest a multi-timeframe strategy

Multi-timeframe (MTF) analysis is an essential trading approach that involves analyzing an asset's price in different timeframes.

Despite its popularity, MTF analysis comes with several pitfalls when working with arrays, including look-ahead bias and information loss.

Many native pandas implementations mistakenly assume that events, such as indicator calculations, take place at the same timestamp as the data provided by the exchange, which is typically the opening time of a bar.

VBT operates under the assumption that the exact timing of most events is unknown and occurs at some point between the opening (best-case) and closing (worst-case) times of a bar. Consequently, VBT employs a set of features designed to resample data in the most sensitive way, without looking into the future.

In today's newsletter, we'll use VectorBT PRO to backtest trading on multiple timeframes simultaneously.

## Imports and set up

In the newer versions of VBT PRO, the star-import (*) loads all the relevant stuff for us, such as `np` for NumPy.

```python
from vectorbtpro import *
```

Configure our graphs to be dark and gap-free.

```python
vbt.settings.set_theme("dark")
vbt.settings.plotting.auto_rangebreaks = True
```

Grab the data of a higher frequency for your favorite asset. We'll use hourly TSLA.

```python
data = vbt.YFData.pull("TSLA", start="2023", end="2024", timeframe="hourly")
```

## Multi-timeframe indicators

Instruct VBT to calculate the fast and slow SMA indicators across multiple timeframes.

Under the hood, data is first resampled to the target timeframe; then, the actual TA-Lib indicator is applied exclusively to non-missing values. Finally, the result is realigned back to the original timeframe in a manner that eliminates the possibility of look-ahead bias.

```python
fast_sma = data.run(
    "talib:sma", 
    timeframe=["1h", "4h", "1d"], 
    timeperiod=vbt.Default(20),
    skipna=True
)
slow_sma = data.run(
    "talib:sma", 
    timeframe=["1h", "4h", "1d"], 
    timeperiod=vbt.Default(50),
    skipna=True
)
```

The result of each call is a DataFrame with three columns, one for each timeframe.

If we plot the DataFrame, we'll observe that the line corresponding to the highest frequency is smooth, whereas the line representing the lowest frequency appears stepped since the indicator values are updated less frequently.

```python
fast_sma.real.vbt.plot().show_svg()
```

![svg](output_19_0.svg)

## Unified portfolio

Next, we'll set up a portfolio in which we go long whenever the fast SMA crosses above the slow SMA and go short when the opposite occurs, across each timeframe.

However, since hourly signals occur more frequently than daily signals, we'll allocate less capital to more frequent signals. For instance, we'll allocate 5% of the equity to hourly signals, 10% to 4-hour signals, and 20% to daily signals.

We'll begin with a cash balance of $10,000, shared across all timeframes. Additionally, we'll implement a 20% trailing stop loss (TSL).

```python
pf = vbt.PF.from_signals(
    data, 
    long_entries=fast_sma.real_crossed_above(slow_sma), 
    short_entries=fast_sma.real_crossed_below(slow_sma), 
    size=[[0.05, 0.1, 0.2]],
    size_type="valuepercent",
    init_cash=10_000,
    group_by=["pf", "pf", "pf"],
    cash_sharing=True,
    tsl_stop=0.2
)
```

Plot the cumulative return for each timeframe and compare these to the cumulative return of the entire portfolio.

```python
fig = pf.get_cumulative_returns().vbt.plot(trace_kwargs=dict(line_color="gray", line_dash="dot"))
fig = pf.get_cumulative_returns(group_by=False).vbt.plot(fig=fig)
fig.show_svg()
```

![svg](output_26_0.svg)

To delve deeper into one of the timeframes, we can plot the indicators alongside the executed trade signals.

Here, we can observe that the majority of positions on the daily timeframe were closed out by the TSL.

```python
fig = fast_sma.real.vbt.plot(column="1d", trace_kwargs=dict(name="Fast", line_color="limegreen"))
fig = slow_sma.real.vbt.plot(column="1d", trace_kwargs=dict(name="Slow", line_color="orangered"), fig=fig)
fig = pf.plot_trade_signals(column="1d", fig=fig)
fig.show_svg()
```

![svg](output_29_0.svg)

## Timeframe product

Since our MTF indicators share the same index, we can combine one timeframe with another. For instance, we can generate signals from the crossover of two timeframes and identify the pair of timeframes that yield the highest expectancy.

```python
fast_sma_real = fast_sma.real.vbt.rename_levels({"sma_timeframe": "fast_sma_timeframe"})
slow_sma_real = slow_sma.real.vbt.rename_levels({"sma_timeframe": "slow_sma_timeframe"})
fast_sma_real, slow_sma_real = fast_sma_real.vbt.x(slow_sma_real)
long_entries = fast_sma_real.vbt.crossed_above(slow_sma_real)
short_entries = fast_sma_real.vbt.crossed_below(slow_sma_real)
pf = vbt.PF.from_signals(data, long_entries=long_entries, short_entries=short_entries)
pf.trades.expectancy.sort_values(ascending=False)
```

fast_sma_timeframe  slow_sma_timeframe
    1h                  4h                    4.570061
    4h                  4h                    1.304871
    1d                  4h                    1.058039
    1h                  1h                   -0.486195
    4h                  1h                   -1.937727
    1h                  1d                   -2.262228
    4h                  1d                   -4.692057
    1d                  1h                   -5.062149
                        1d                   -5.630190
    Name: expectancy, dtype: float64

## Next steps

Timeframe is yet another parameter of your strategy that can be tweaked. For example, you can go to uncharted territory and test more unconventional timeframes like "1h 30min" to discover potentially novel insights. Similar to other parameters, timeframes should also undergo cross-validation.

However, unlike regular parameters, timeframes should be regarded as a distinct dimension that provides a unique perspective on your strategy.

---
link: https://www.pyquantnews.com/easily-cross-validate-parameters-boost-strategy/
parent: https://www.pyquantnews.com/
name: Easily cross-validate parameters to boost your trading strategy
type: page
---

# How to cross-validate a parameterized trading strategy

Trading strategies often rely on parameters. Enhancing and effectively cross-validating these parameters can provide a competitive advantage in the market. However, creating a reliable cross-validation schema is challenging due to risks like look-ahead bias and other pitfalls that can lead to overestimating a strategy's performance. With [VectorBT PRO](https://vectorbt.pro/), you can easily access and implement a variety of sophisticated cross-validation methods with just a few lines of code.

## Imports and data

Let's import VBT PRO and the few libraries relevant for our analysis.

```python
from vectorbtpro import *
# whats_imported()

vbt.settings.set_theme("dark")
```

The first step involves acquiring data.

```python
SYMBOL = "AAPL"
START = "2010"
END = "now"
TIMEFRAME = "day"

data = vbt.YFData.pull(SYMBOL, start=START, end=END, timeframe=TIMEFRAME)
```

## Cross-validation schema

Next, we'll set up a "splitter," which divides a date range into smaller segments according to a chosen schema. For instance, let's allocate 12 months for training data and another 12 months for testing data, with this cycle repeating every 3 months.

```python
TRAIN = 12
TEST = 12
EVERY = 3
OFFSET = vbt.offset("M")

splitter = vbt.Splitter.from_ranges(
    data.index, 
    every=EVERY * OFFSET, 
    lookback_period=(TRAIN + TEST) * OFFSET,
    split=(
        vbt.RepFunc(lambda index: index < index[0] + TRAIN * OFFSET),
        vbt.RepFunc(lambda index: index >= index[0] + TRAIN * OFFSET),
    ),
    set_labels=["train", "test"]
)
splitter.plots().show_svg()
```

![svg](output_9_0.svg)

In the first subplot, we see that each split (or row) contains adjacent training and testing sets, progressively rolling from past to present. The second subplot illustrates the overlap of each data point across different ranges. Tip: For non-overlapping testing sets, use the setting `EVERY = TRAIN`.

## Objective function

Next, we'll create a function to execute a trading strategy within a specified date range using a single parameter set, returning one key metric. Our strategy will be a simple EMA crossover combined with an ATR trailing stop.

```python
def objective(data, fast_period=10, slow_period=20, atr_period=14, atr_mult=3):
    fast_ema = data.run("talib:ema", fast_period, short_name="fast_ema", unpack=True)
    slow_ema = data.run("talib:ema", slow_period, short_name="slow_ema", unpack=True)
    atr = data.run("talib:atr", atr_period, unpack=True)
    pf = vbt.PF.from_signals(
        data, 
        entries=fast_ema.vbt.crossed_above(slow_ema), 
        exits=fast_ema.vbt.crossed_below(slow_ema), 
        tsl_stop=atr * atr_mult, 
        save_returns=True,
        freq=TIMEFRAME
    )
    return pf.sharpe_ratio

print(objective(data))
```

    1.2335189648721099

## Parameter optimization

Let's harness the power of VBT PRO! By decorating (or wrapping) our function with `parameterized`, we enable `objective` to accept a list of parameters and execute them across all combinations. We'll then further enhance the function with another decorator, `split`, which runs the strategy on each date range specified by the splitter. This approach allows us to apply our strategy across every possible date range and parameter combination, compiling the outcomes into a single Pandas Series.

```python
param_objective = vbt.parameterized(
    objective,
    merge_func="concat",
    mono_n_chunks="auto",  # merge parameter combinations into chunks
    execute_kwargs=dict(warmup=True, engine="pathos")  # run chunks in parallel using Pathos
)
cv_objective = vbt.split(
    param_objective,
    splitter=splitter, 
    takeable_args=["data"],  # select date range from data
    merge_func="concat", 
)

sharpe_ratio = cv_objective(
    data,
    vbt.Param(np.arange(10, 50), condition="slow_period - fast_period >= 5"),
    vbt.Param(np.arange(10, 50)),
    vbt.Param(np.arange(10, 50), condition="fast_period <= atr_period <= slow_period"),
    vbt.Param(np.arange(2, 5))
)
print(sharpe_ratio)
```

0%|          | 0/48 [00:00<?, ?it/s]

split  set    fast_period  slow_period  atr_period  atr_mult
    0      train  10           15           10          2           0.996887
                                                        3           0.996887
                                                        4           0.996887
                                            11          2           0.996887
                                                        3           0.996887
                                                                      ...   
    47     test   44           49           48          3           1.066327
                                                        4           1.066327
                                            49          2           1.066327
                                                        3           1.066327
                                                        4           1.066327
    Name: sharpe_ratio, Length: 3144960, dtype: float64

We tested over 3 million combinations of date ranges and parameters in just a few minutes.

## Analysis

Let's find out if there's a correlation between the results of the training and testing sets.

```python
train_sharpe_ratio = sharpe_ratio.xs("train", level="set")
test_sharpe_ratio = sharpe_ratio.xs("test", level="set")
print(train_sharpe_ratio.corr(test_sharpe_ratio))
```

    -0.28711631959312456

The analysis indicates a weak negative correlation or no substantial correlation. This suggests that the strategy tends to perform oppositely compared to its results in previous months.

And here's an analysis segmented by fast and slow EMA periods. It highlights the minimal variation in the Sharpe ratio from the training to the testing set across at least 50% of the splits, where blue indicates a positive change.

```python
sharpe_ratio_diff = test_sharpe_ratio - train_sharpe_ratio
sharpe_ratio_diff_median = sharpe_ratio_diff.groupby(["fast_period", "slow_period"]).median()
sharpe_ratio_diff_median.vbt.heatmap(trace_kwargs=dict(colorscale="RdBu")).show_svg()
```

![svg](output_23_0.svg)

## Conclusion

Although you might have developed a promising strategy on paper, cross-validating it is essential to confirm its consistent performance over time and to ensure it's not merely a result of random fluctuations.

---
link: https://www.pyquantnews.com/vectorbt-pro-algorithmically-find-chart-patterns/
parent: https://www.pyquantnews.com/
name: How to use VectorBT PRO to algorithmically find chart patterns
type: page
---

# How to backtest chart patterns with VectorBT PRO

VectorBT PRO (https://vectorbt.pro/) is a proprietary Python package designed for backtesting and analyzing quantitative trading strategies. It provides a comprehensive suite of tools for every stage of an algorithmic trading workflow, including data acquisition, signal generation and analysis, portfolio optimization, strategy simulation, hyperparameter tuning, and cross-validation. These modular components empower users to flexibly customize their analysis, setting it apart from monolithic backtesting frameworks.

One of these components is a data pattern detector that efficiently scans data using variable-length windows, assessing their similarity to a specified pattern. This process, optimized with Numba (https://numba.pydata.org/), operates on any hardware without the need for machine learning. To showcase the detector's capabilities, we will conduct backtesting on a range of patterns and their combinations on a single dataset.

## Imports and set up

Due to VectorBT PRO's self-contained design, only minimal imports are necessary.

```python
from vectorbtpro import *
# whats_imported()

vbt.settings.set_theme("dark")
```

VectorBT PRO features built-in data downloading from sources such as Yahoo Finance, Alpaca, Polygon, TradingView, and many more. We will perform pattern detection on hourly price data pulled from TradingView.

```python
symbols = [
    "NASDAQ:META",
    "NASDAQ:AMZN",
    "NASDAQ:AAPL",
    "NASDAQ:NFLX",
    "NASDAQ:GOOG",
]

data = vbt.TVData.pull(symbols, timeframe="hourly")
```

0%|          | 0/5 [00:00<?, ?it/s]

/Users/olegpolakow/Documents/GitHub/vectorbt.pro/vectorbtpro/data/base.py:1565: UserWarning: Symbols have mismatching index. Setting missing data points to NaN.
      data = cls_or_self.align_index(data, missing=missing_index, silence_warnings=silence_warnings)

TradingView does not offer the option to specify a date range in advance, so we will need to select it afterward.

```python
start_date = "2020"
end_date = None

data = data.xloc[start_date:end_date]
```

Ensure that our data spans the correct date period and is free of NaN values.

```python
print(data.stats())
```

    Start                       2020-01-02 14:30:00+00:00
    End                         2023-09-22 19:30:00+00:00
    Period                              273 days 08:00:00
    Total Symbols                                       5
    Null Counts: NASDAQ:META                           35
    Null Counts: NASDAQ:AMZN                           25
    Null Counts: NASDAQ:AAPL                           15
    Null Counts: NASDAQ:NFLX                           40
    Null Counts: NASDAQ:GOOG                           40
    Name: agg_stats, dtype: object

As pattern detection requires only a single time series, we must choose the suitable feature. We'll utilize HLC/3, which effectively captures price fluctuations.

```python
price = data.hlc3
```

## Define patterns

Numerous chart patterns can be translated into numerical sequences, like the "Double Top" pattern (https://www.investopedia.com/terms/d/doubletop.asp) represented as [1, 3, 2, 3, 1]. It's important to note that while the numbers themselves can be arbitrary, their relative spacing should mirror the relative distance between the pattern's chart points. For instance, in this sequence, 2 aligns with the midpoint between valley point 1 and peak point 3. The same principle applies to temporal distribution: points should be equidistant from one another.

```python
bullish_patterns = {
    "double_bottom": [5, 1, 3, 1, 5],
    "exp_triangle": [3, 4, 2, 5, 1, 6],
    "asc_triangle": [1, 5, 2, 5, 3, 6],
    "symm_triangle": [1, 6, 2, 5, 3, 6],
    "pennant": [6, 1, 5, 2, 4, 3, 6]
}
bearish_patterns = {
    "head_and_shoulders": [1, 4, 2, 6, 2, 4, 1],
    "double_top": [1, 5, 3, 5, 1],
    "desc_triangle": [6, 2, 5, 2, 4, 1],
    "symm_triangle": [6, 1, 5, 2, 4, 1],
    "pennant": [1, 6, 2, 5, 3, 4, 1]
}
```

Confirm the visual representation of a pattern by plotting its corresponding line graph.

```python
pd.Series(bullish_patterns["double_bottom"]).vbt.plot().show_svg()
```

![svg](output_18_0.svg)

Each generated sequence serves as a rough approximation of the desired chart pattern, and there's no need for precise adjustments: VectorBT PRO's similarity-based algorithm is flexible and can identify patterns, even if they are not perfectly consistent in their design.

## Detect patterns in data

Iterate through each pattern, dataset, and timestamp within the dataset. Search for matches within windows spanning from 1 to 30 days, and create a record for each match that exceeds a pre-defined minimum similarity score, which is set by default to 85%.

```python
min_window = 24
max_window = 24 * 30

def detect_patterns(patterns):
    return vbt.PatternRanges.from_pattern_search(
        price,
        open=data.open,  # OHLC for plotting
        high=data.high,
        low=data.low,
        close=data.close,
        pattern=patterns,
        window=min_window,
        max_window=max_window,
        execute_kwargs=dict(  # multithreading
            engine="threadpool", 
            chunk_len="auto", 
        )
    )

bullish_matches = detect_patterns(vbt.Param(bullish_patterns, name="bullish_pattern"))
bearish_matches = detect_patterns(vbt.Param(bearish_patterns, name="bearish_pattern"))
```

0%|          | 0/4 [00:00<?, ?it/s]

0%|          | 0/4 [00:00<?, ?it/s]

In just several minutes, VectorBT PRO seamlessly detected matches among all patterns. This process, involving around 230 million unique pattern and window combinations, was executed in parallel.

Get the number of matches for each pattern and dataset.

```python
print(bullish_matches.count())
```

    bullish_pattern  symbol     
    double_bottom    NASDAQ:META     5
                      NASDAQ:AMZN    14
                      NASDAQ:AAPL     6
                      NASDAQ:NFLX    10
                      NASDAQ:GOOG    11
    exp_triangle     NASDAQ:META     0
                      NASDAQ:AMZN     0
                      NASDAQ:AAPL     4
                      NASDAQ:NFLX     0
                      NASDAQ:GOOG     2
    asc_triangle     NASDAQ:META     4
                      NASDAQ:AMZN     4
                      NASDAQ:AAPL    10
                      NASDAQ:NFLX     6
                      NASDAQ:GOOG     4
    symm_triangle    NASDAQ:META     1
                      NASDAQ:AMZN     1
                      NASDAQ:AAPL     3
                      NASDAQ:NFLX     1
                      NASDAQ:GOOG     3
    pennant          NASDAQ:META     1
                      NASDAQ:AMZN     0
                      NASDAQ:AAPL     2
                      NASDAQ:NFLX     0
                      NASDAQ:GOOG     0
    Name: count, dtype: int64

Plot the pattern and dataset with the most matches.

```python
vbt.settings.plotting.auto_rangebreaks = True  # for stocks

display_column = bullish_matches.count().idxmax()

bullish_matches.plot(column=display_column, fit_ranges=True).show_svg()
```

![svg](output_27_0.svg)

Zoom in on a match.

```python
display_match = 3

bullish_matches.plot(column=display_column, fit_ranges=display_match).show_svg()
```

![svg](output_29_0.svg)

The window data closely aligns with the pattern. This functionality is highly comprehensive, offering the flexibility to adjust fitness levels, modify rescaling and interpolation algorithms, and more to suit specific requirements.

## Transform matches to signals

To conduct backtesting on the identified patterns, we will convert them into signals, triggering a signal once a pattern has fully developed.

```python
entries = bullish_matches.last_pd_mask
exits = bearish_matches.last_pd_mask
```

Generate a Cartesian product of bullish and bearish patterns to systematically test each bullish pattern against each bearish pattern.

```python
entries, exits = entries.vbt.x(exits)
```

Both arrays have been converted into equally-shaped DataFrames, each comprising 125 columns. Each column represents an individual backtest, encompassing three parameters: bullish pattern, bearish pattern, and symbol.

```python
print(entries.columns)
```

    MultiIndex([('double_bottom', 'head_and_shoulders', 'NASDAQ:META'),
                ('double_bottom', 'head_and_shoulders', 'NASDAQ:AMZN'),
                ('double_bottom', 'head_and_shoulders', 'NASDAQ:AAPL'),
                ('double_bottom', 'head_and_shoulders', 'NASDAQ:NFLX'),
                ('double_bottom', 'head_and_shoulders', 'NASDAQ:GOOG'),
                ('double_bottom',         'double_top', 'NASDAQ:META'),
                ('double_bottom',         'double_top', 'NASDAQ:AMZN'),
                ('double_bottom',         'double_top', 'NASDAQ:AAPL'),
                ('double_bottom',         'double_top', 'NASDAQ:NFLX'),
                ('double_bottom',         'double_top', 'NASDAQ:GOOG'),
                ...
                (      'pennant',      'symm_triangle', 'NASDAQ:META'),
                (      'pennant',      'symm_triangle', 'NASDAQ:AMZN'),
                (      'pennant',      'symm_triangle', 'NASDAQ:AAPL'),
                (      'pennant',      'symm_triangle', 'NASDAQ:NFLX'),
                (      'pennant',      'symm_triangle', 'NASDAQ:GOOG'),
                (      'pennant',            'pennant', 'NASDAQ:META'),
                (      'pennant',            'pennant', 'NASDAQ:AMZN'),
                (      'pennant',            'pennant', 'NASDAQ:AAPL'),
                (      'pennant',            'pennant', 'NASDAQ:NFLX'),
                (      'pennant',            'pennant', 'NASDAQ:GOOG')],
                names=['bullish_pattern', 'bearish_pattern', 'symbol'], length=125)

## Backtest signals

Establish a portfolio by simulating signals.

```python
pf = vbt.Portfolio.from_signals(data, entries, exits)
```

Get the mean total return for every combination of bullish and bearish patterns.

```python
mean_total_return = pf.total_return.groupby(["bullish_pattern", "bearish_pattern"]).mean()

print(mean_total_return)
```

    bullish_pattern  bearish_pattern   
    asc_triangle     desc_triangle         0.141544
                      double_top            0.078194
                      head_and_shoulders    0.382325
                      pennant               0.340322
                      symm_triangle         0.488653
    double_bottom    desc_triangle         0.332584
                      double_top            0.164022
                      head_and_shoulders    0.361569
                      pennant               0.342995
                      symm_triangle         0.463775
    exp_triangle     desc_triangle         0.196349
                      double_top            0.058434
                      head_and_shoulders    0.323352
                      pennant               0.305847
                      symm_triangle         0.384134
    pennant          desc_triangle         0.351857
                      double_top            0.253997
                      head_and_shoulders    0.357806
                      pennant               0.394544
                      symm_triangle         0.394544
    symm_triangle    desc_triangle         0.193384
                      double_top            0.037126
                      head_and_shoulders    0.326350
                      pennant               0.462505
                      symm_triangle         0.488226
    Name: total_return, dtype: float64

As visual beings, let's represent these values as a heatmap.

```python
mean_total_return.vbt.heatmap(x_level="bearish_pattern", y_level="bullish_pattern").show_svg()
```

![svg](output_44_0.svg)

Although the displayed performance of each pattern combination does not guarantee future results, it provides insight into how the market responded to pattern events in the past. For instance, it's noteworthy that the "Bearish Symmetrical Triangle" exhibited a notably bullish trend. Cross-validation and robustness testing are next essential steps for a comprehensive assessment.

Read more at https://vectorbt.pro/tutorials/patterns-and-projections/

---
link: https://www.pyquantnews.com/forecast-future-price-trends-with-projections/
parent: https://www.pyquantnews.com/
name: Forecast future price trends (with projections)
type: page
---

# Forecasting future price trends by projecting historical price patterns

In our previous newsletter focusing on VectorBT PRO (VBT), we dived into the pattern detection capabilities of this powerful library. An additional key functionality is VBT's capacity to extrapolate identified price segments into the future and aggregate them for statistical analysis. This feature can be an invaluable tool for real-time decision-making in market analysis.

## Imports and set up

Given the self-contained design of VBT, a single import suffices.

```python
from vectorbtpro import *
# whats_imported()

vbt.settings.set_theme("dark")
```

Let's define a set of variables for our analysis.

```python
SYMBOL = "BTCUSDT"
TIMEFRAME = "1 hour"
START = "one year ago"

LAST_N_BARS = 24
PRED_N_BARS = 12

GIF_FNAME = "projections.gif"
GIF_N_BARS = 72
GIF_FPS = 4
GIF_PAD = 0.01
```

We will execute the analysis using price data retrieved from BinanceData, based on the parameters we previously defined.

```python
data = vbt.BinanceData.pull(SYMBOL, timeframe=TIMEFRAME, start=START)
```

0it [00:00, ?it/s]

## Find and plot projections

Let's write a function that analyzes the most recent price trend and employs it as a pattern to identify similar price movements in historical data. This pattern recognition function will focus exclusively on segments of price history having a comparable percentage change from their respective starting points.

```python
def find_patterns(data):
    price = data.hlc3
    pattern = price.values[-LAST_N_BARS:]
    pattern_ranges = price.vbt.find_pattern(
        pattern=pattern,
        rescale_mode="rebase",
        overlap_mode="allow",
        wrapper_kwargs=dict(freq=TIMEFRAME)
    )
    pattern_ranges = pattern_ranges.status_closed
    return pattern_ranges

pattern_ranges = find_patterns(data)
print(pattern_ranges.count())
```

    6

We have identified a number of price segments that closely resemble the latest price trend. Now, we'll write a function that extracts the price data immediately succeeding each identified segment and plots these as extensions of the price trend. These subsequent segments are known as "projections."

```python
def plot_projections(data, pattern_ranges, **kwargs):
    projection_ranges = pattern_ranges.with_delta(
        PRED_N_BARS,
        open=data.open,
        high=data.high,
        low=data.low,
        close=data.close,
    )
    projection_ranges = projection_ranges.status_closed
    return projection_ranges.plot_projections(
        plot_past_period=LAST_N_BARS, 
        **kwargs,
    )

plot_projections(data, pattern_ranges, plot_bands=False).show_svg()
```

![svg](output_13_0.svg)

As we can see, similar price movements have historically branched into a diverse set of trajectories. For a visually compelling and statistically robust forecast, we will display the confidence bands encompassing all the projections, with 60% of these projections falling between the upper and lower bands.

```python
plot_projections(data, pattern_ranges, plot_bands=True).show_svg()
```

![svg](output_15_0.svg)

## Generate animation

Lastly, we will compile a GIF animation that iterates through a specified range of bars, applying the aforementioned procedure to each bar within that range.

```python
def plot_frame(frame_index, **kwargs):
    sub_data = data.loc[:frame_index[-1]]
    pattern_ranges = find_patterns(sub_data)
    if pattern_ranges.count() < 3:
        return None
    return plot_projections(sub_data, pattern_ranges, **kwargs)

vbt.save_animation(
    GIF_FNAME,
    data.index[-GIF_N_BARS:],
    plot_frame,
    plot_projections=False,
    delta=1,
    fps=GIF_FPS,
    writer_kwargs=dict(loop=0),
    yaxis_range=[
        data.low.iloc[-GIF_N_BARS:].min() * (1 - GIF_PAD), 
        data.high.iloc[-GIF_N_BARS:].max() * (1 + GIF_PAD)
    ],
)
```

0%|          | 0/72 [00:00<?, ?it/s]

Bear in mind that while the confidence bands describe past performance, they should not be used as guarantees of future results.

---
link: https://qubitquants.github.io/discretionary-signals-bactesting/index.html
parent: https://qubitquants.github.io/
name: Discretionary Signal Backtesting
type: page
---

# Discretionary Signal Backtesting

## Prepare data and signals

```python
from vectorbtpro import *
# whats_imported()

vbt.settings.set_theme("dark")
```

```python
# Pull data

def date_parser(timestamps):
    # First column are integer timestamps, parse them into DatetimeIndex
    return pd.to_datetime(timestamps, utc=True, unit="ms")

data = vbt.CSVData.pull("download/xauusd-m1-bid-2021-09-01-2023-03-14.csv", date_parser=date_parser)

print(data.wrapper.shape)
```

    (485280, 4)

```python
# Pull signals
signal_data = vbt.CSVData.pull("download/TG_Extracted_Signals.csv", index_col=1)

print(signal_data.wrapper.shape)
```

    (538, 10)

```python
# Numba doesn't understand strings, thus create an enumerated type for stop types

# Create a type first
OrderTypeT = namedtuple("OrderTypeT", ["BUY", "SELL", "BUYSTOP", "SELLSTOP"])

# Then create a tuple
OrderType = OrderTypeT(*range(len(OrderTypeT._fields)))

print(OrderType)
```

    OrderTypeT(BUY=0, SELL=1, BUYSTOP=2, SELLSTOP=3)

```python
# Prepare signals

def transform_signal_data(df):
    # Select only one symbol, the one we pulled the data for
    df = df[df["Symbol"] == "XAUUSD"]
    
    # Select columns of interest
    df = df.iloc[:, -7:]
    
    # Map order types using OrderType
    df["OrderType"] = df["OrderType"].map(lambda x: OrderType._fields.index(x.replace(" ", "")))
    
    # Some entry prices are zero
    df = df[df["EntryPrice"] > 0]
    
    return df

signal_data = signal_data.transform(transform_signal_data)

print(signal_data.wrapper.shape)
```

    (225, 7)

## Simulate with from_order_func

```python
# Create named tuples which will act as containers for various arrays

# SignalInfo will contain signal information in a vbt-friendly format
# Rows in each array correspond to signals
SignalInfo = namedtuple("SignalInfo", [
    "timestamp",  # 1d array with timestamps in nanosecond format (int64)
    "order_type",  # 1d array with order types in integer format (int64, see order_type_map)
    "entry_price",  # 1d array with entry price (float64)
    "sl",  # 2d array where columns are SL levels (float64)
    "tp",  # 2d array where columns are TP levels (float64)
])

# TempInfo will contain temporary information that will be written during backtesting
# You can imagine being buffer that we write and then access at a later time
# Rows in each array correspond to signals
TempInfo = namedtuple("TempInfo", [
    "ts_bar",  # 1d array with row indices where signal was hit (int64)
    "entry_price_bar",  # 1d array with row indices where entry price was hit (int64)
    "sl_bar",  # 2d array with row indices where each SL level was hit, same shape as SignalInfo.sl (int64)
    "tp_bar",  # 2d array with row indices where each TP level was hit, same shape as SignalInfo.tp (int64)
])
```

```python
# Here's what we will do:
# Represent each signal as a separate column with its own starting capital
# Run an order function using Portfolio.from_order_func
# The order function is executed at each bar and column (signal in our case)
# If the current bar contains a signal, execute the signal logic
# Order functions can issue only one order at bar, thus we if multiple stops were hit, we will aggregate them
# We will go all in and then gradually reduce the position based on the number of stops

@njit
def has_data_nb(c):
    # Numba function to check whether OHLC is not NaN
    if np.isnan(vbt.pf_nb.select_nb(c, c.open)):
        return False
    if np.isnan(vbt.pf_nb.select_nb(c, c.high)):
        return False
    if np.isnan(vbt.pf_nb.select_nb(c, c.low)):
        return False
    if np.isnan(vbt.pf_nb.select_nb(c, c.close)):
        return False
    return True

@njit
def check_price_hit_nb(c, price, hit_below, can_use_ohlc):
    # Numba function to check whether a price level was hit during this bar
    # Use hit_below=True to check against low and hit_below=False to check against high
    # If can_use_ohlc is False, will check only against the close price
    
    order_price, hit_on_open, hit_on_close = vbt.pf_nb.check_price_hit_nb(
        open=vbt.pf_nb.select_nb(c, c.open),  # OHLC are flexible arrays, always use select_nb!
        high=vbt.pf_nb.select_nb(c, c.high),
        low=vbt.pf_nb.select_nb(c, c.low),
        close=vbt.pf_nb.select_nb(c, c.close),
        price=price,
        hit_below=hit_below,
        can_use_ohlc=can_use_ohlc
    )
    # Order price here isn't necessarily the price that has been hit
    # For example, if the price was hit before open, order price is set to the open price
    # The price is considered to be hit if a non-NaN value is returned
    return order_price

@njit(boundscheck=True)
def order_func_nb(c, signal_info, temp_info):  # first argument is context, other are our containers
    if not has_data_nb(c):
        # If this bar contains no data, skip it
        return vbt.pf_nb.order_nothing_nb()
    
    # Each column corresponds to a signal
    signal = c.col
    
    # Each row corresponds to a bar
    bar = c.i
    
    # Define various flags for pure convenience
    buy_market = signal_info.order_type[signal] == OrderType.BUY
    sell_market = signal_info.order_type[signal] == OrderType.SELL
    buy_stop = signal_info.order_type[signal] == OrderType.BUYSTOP
    sell_stop = signal_info.order_type[signal] == OrderType.SELLSTOP
    buy = buy_market or buy_stop
    
    # First, we need to check whether the current bar contains a signal
    can_use_ohlc = True
    if temp_info.ts_bar[signal] == -1:
        if c.index[bar] == signal_info.timestamp[signal]:
            # If so, store the current row index in a temporary array
            # such that later we know that we already discovered a signal
            temp_info.ts_bar[signal] = bar

            # The signal has the granularity of seconds, thus it belongs somewhere in the bar
            # We need to notify the functions below that they cannot use full OHLC information, only close
            # This is to avoid using prices that technically happened before the signal
            can_use_ohlc = False
        
    # Here comes the entry order
    # Check whether the signal has been discovered
    # -1 means hasn't been discovered yet
    if temp_info.ts_bar[signal] != -1:
        
        # Then, check whether the entry order hasn't been executed
        if temp_info.entry_price_bar[signal] == -1:
            
            # If so, execute the entry order
            if buy_market:
                # Buy market order (using close price)
                
                # Store the current row index in a temporary array such that future bars know
                # that the order has already been executed
                temp_info.entry_price_bar[signal] = bar
                order_price = signal_info.entry_price[signal]
                return vbt.pf_nb.order_nb(np.inf, np.inf)  # size, price
            
            if sell_market:
                # Sell market order (using close price)
                temp_info.entry_price_bar[signal] = bar
                order_price = signal_info.entry_price[signal]
                return vbt.pf_nb.order_nb(-np.inf, np.inf)
            
            if buy_stop:
                # Buy stop order
                # A buy stop order is entered at a stop price above the current market price
                
                # Since it's a pending order, we first need to check whether the entry price has been hit
                order_price = check_price_hit_nb(
                    c,
                    price=signal_info.entry_price[signal],
                    hit_below=False,
                    can_use_ohlc=can_use_ohlc,
                )
                if not np.isnan(order_price):
                    # If so, execute the order
                    temp_info.entry_price_bar[signal] = bar
                    return vbt.pf_nb.order_nb(np.inf, order_price)
                
            if sell_stop:
                # Sell stop order
                # A sell stop order is entered at a stop price below the current market price
                order_price = check_price_hit_nb(
                    c,
                    price=signal_info.entry_price[signal],
                    hit_below=True,
                    can_use_ohlc=can_use_ohlc,
                )
                if not np.isnan(order_price):
                    temp_info.entry_price_bar[signal] = bar
                    return vbt.pf_nb.order_nb(-np.inf, order_price)
                
        # Here comes the stop order
        # Check whether the entry order has been executed
        if temp_info.entry_price_bar[signal] != -1:
            
            # We also need to check whether we're still in a position
            # in case stops have already closed out the position
            if c.last_position[signal] != 0:
                
                # If so, start with checking for potential SL orders
                # (remember that SL pessimistically comes before TP)
                # First, we need to know the number of potential and already executed SL levels
                # since we want to gradually reduce the position proportially to the number of levels
                # For example, one signal may define [12.35, 12.29] and another [17.53, nan]
                n_sl_levels = 0
                n_sl_hits = 0
                sl_levels = signal_info.sl[signal]  # select 1d array from 2d array
                sl_bar = temp_info.sl_bar[signal]  # same here
                for k in range(len(sl_levels)):
                    if not np.isnan(sl_levels[k]):
                        n_sl_levels += 1
                    if sl_bar[k] != -1:
                        n_sl_hits += 1
                
                # We can execute only one order at the current bar
                # Thus, if the price crossed multiple SL levels, we need to pack them into one order
                # Since SL levels are guaranteed to be sorted, we will check the most distant levels first
                # because if a distant stop has been hit, the closer stops are automatically hit too
                for k in range(n_sl_levels - 1, n_sl_hits - 1, -1):
                    if not np.isnan(sl_levels[k]) and sl_bar[k] == -1:
                        # Check against low for buy orders and against high for sell orders
                        order_price = check_price_hit_nb(
                            c,
                            price=sl_levels[k],
                            hit_below=buy,
                            can_use_ohlc=can_use_ohlc,
                        )
                        if not np.isnan(order_price):
                            sl_bar[k] = bar
                            # The further away the stop is, the more of the position needs to be closed
                            # We will specify a target percentage
                            # For example, for two stops it would be 0.5 (SL1) and 0.0 (SL2)
                            # while for three stops it would be 0.66 (SL1), 0.33 (SL2), and 0.0 (SL3)
                            # This works only if we went all in before (size=np.inf)!
                            size = 1 - (k + 1) / n_sl_levels
                            size_type = vbt.pf_enums.SizeType.TargetPercent
                            if buy:
                                return vbt.pf_nb.order_nb(size, order_price, size_type)
                            else:
                                # Size must be negative for short positions
                                return vbt.pf_nb.order_nb(-size, order_price, size_type)
                        
                # Same for potential TP orders
                n_tp_levels = 0
                n_tp_hits = 0
                tp_levels = signal_info.tp[signal]
                tp_bar = temp_info.tp_bar[signal]
                for k in range(len(tp_levels)):
                    if not np.isnan(tp_levels[k]):
                        n_tp_levels += 1
                    if tp_bar[k] != -1:
                        n_tp_hits += 1
                
                for k in range(n_tp_levels - 1, n_tp_hits - 1, -1):
                    if not np.isnan(tp_levels[k]) and tp_bar[k] == -1:
                        # Check against high for buy orders and against low for sell orders
                        order_price = check_price_hit_nb(
                            c,
                            price=tp_levels[k],
                            hit_below=not buy,
                            can_use_ohlc=can_use_ohlc,
                        )
                        if not np.isnan(order_price):
                            tp_bar[k] = bar
                            size = 1 - (k + 1) / n_tp_levels
                            size_type = vbt.pf_enums.SizeType.TargetPercent
                            if buy:
                                return vbt.pf_nb.order_nb(size, order_price, size_type)
                            else:
                                return vbt.pf_nb.order_nb(-size, order_price, size_type)
                    
    # If neither of orders has been executed, order nothing
    return vbt.pf_nb.order_nothing_nb()
```

```python
# Prepare signal information

timestamp = vbt.dt.to_ns(signal_data.index)  # nanoseconds
order_type = signal_data.get("OrderType").values
entry_price = signal_data.get("EntryPrice").values
sl = signal_data.get("SL").values
tp1 = signal_data.get("TP1").values
tp2 = signal_data.get("TP2").values
tp3 = signal_data.get("TP3").values
tp4 = signal_data.get("TP4").values

n_signals = len(timestamp)
print(n_signals)
```

    225

```python
# Since the signals are of the second granularity while the data is of the minute granularity,
# we need to round the timestamp of the signal to the nearest minute
# Timestamps represent the opening time, thus the second "19:28:59" belongs to the minute "19:28:00"

timestamp = timestamp - timestamp % vbt.dt_nb.m_ns
```

```python
# Create a named tuple for signal information

signal_info = SignalInfo(
    timestamp=timestamp,
    order_type=order_type,
    entry_price=entry_price,
    sl=np.column_stack((sl,)),
    tp=np.column_stack((tp1, tp2, tp3, tp4))
)

n_sl_levels = signal_info.sl.shape[1]
print(n_sl_levels)

n_tp_levels = signal_info.tp.shape[1]
print(n_tp_levels)
```

    1
    4

```python
# Important: re-run this cell every time you're running the simulation!
# Create a named tuple for temporary information
# All arrays below hold row indices, thus the default value is -1

def build_temp_info(signal_info):
    return TempInfo(
        ts_bar=np.full(len(signal_info.timestamp), -1),
        entry_price_bar=np.full(len(signal_info.timestamp), -1),
        sl_bar=np.full(signal_info.sl.shape, -1),
        tp_bar=np.full(signal_info.tp.shape, -1)
    )

temp_info = build_temp_info(signal_info)
```

```python
# By default, vectorbt initializes an empty order array of the same shape as data
# But since our data is highly granular, it would take a lot of RAM
# Let's limit the number of records to one entry order and the maximum number of SL and TP orders
# It will be applied per column

max_order_records = 1 + n_sl_levels + n_tp_levels

print(max_order_records)
```

    6

```python
# Perform the actual simulation
# Since we don't broadcast data against any other array, vectorbt doesn't know anything about
# our signal arrays and will simulate only the one column in our data
# Thus, we need to tell it to expand the number of columns by the number of signals using tiling
# But don't worry: thanks to flexible indexing vectorbt won't actually tile the data - good for RAM!
# (it would tile the data if it had multiple columns though!)

pf = vbt.Portfolio.from_order_func(
    data,
    order_func_nb=order_func_nb,
    order_args=(signal_info, temp_info),
    broadcast_kwargs=dict(tile=n_signals),  # tiling here
    max_order_records=max_order_records,
    freq="minute"  # we have an irregular one-minute frequency
)
# (may take a minute...)
```

```python
# Let's print out the order records in a human-readable format

print(pf.orders.records_readable)
```

          Order Id  Column                     Index      Size     Price  Fees  \
    0           0       0 2021-09-14 12:43:00+00:00  0.055835  1790.994   0.0   
    1           1       0 2021-09-14 12:44:00+00:00  0.018612  1790.900   0.0   
    2           2       0 2021-09-14 12:46:00+00:00  0.018597  1789.400   0.0   
    3           3       0 2021-09-14 12:55:00+00:00  0.018626  1796.400   0.0   
    4           0       3 2021-09-16 10:39:00+00:00  0.056191  1779.634   0.0   
    ..        ...     ...                       ...       ...       ...   ...   
    432         3     221 2023-03-09 14:39:00+00:00  0.027615  1830.000   0.0   
    433         0     222 2023-03-09 08:48:00+00:00  0.055008  1817.918   0.0   
    434         1     222 2023-03-09 14:34:00+00:00  0.055008  1828.000   0.0   
    435         0     223 2023-03-09 14:26:00+00:00  0.054760  1826.135   0.0   
    436         1     223 2023-03-09 15:07:00+00:00  0.054760  1835.000   0.0   
    
          Side  
    0    Sell  
    1     Buy  
    2     Buy  
    3     Buy  
    4     Buy  
    ..    ...  
    432   Buy  
    433  Sell  
    434   Buy  
    435  Sell  
    436   Buy  
    
    [437 rows x 7 columns]

## Extend order records

```python
# We can notice above that there's no information whether an order is an SL or TP order
# What we can do is to create our own order records with custom fields, copy the old ones over,
# and tell the portfolio to use them instead of the default ones

# First, we need to create an enumerated field for stop types
# SL levels will come first, TP levels second, in an incremental fashion
StopTypeT = namedtuple("StopTypeT", [
    *[f"SL{i + 1}" for i in range(n_sl_levels)],
    *[f"TP{i + 1}" for i in range(n_tp_levels)]
])
StopType = StopTypeT(*range(len(StopTypeT._fields)))

print(StopType)
```

    StopTypeT(SL1=0, TP1=1, TP2=2, TP3=3, TP4=4)

```python
# To extend order records, we just need to append new fields and construct a new data type

custom_order_dt = np.dtype(vbt.pf_enums.order_fields + [("order_type", int_), ("stop_type", int_)])

def fix_order_records(order_records, signal_info, temp_info):
    # This is a function that will "fix" our default records and return the fixed ones
    
    # Create a new empty record array with the new data type
    # Empty here means that the array isn't initialized yet and contains junk data
    # Thus, make sure to override each single element
    custom_order_records = np.empty(order_records.shape, dtype=custom_order_dt)
    
    # Copy over the information from our default records
    for field, _ in vbt.pf_enums.order_fields:
        custom_order_records[field] = order_records[field]
        
    # Iterate over the new records and fill the stop type
    for i in range(len(custom_order_records)):
        record = custom_order_records[i]
        signal = record["col"]  # each column corresponds to a signal
        
        # Fill the order type
        record["order_type"] = signal_info.order_type[signal]
        
        # Concatenate SL and TP row indices of this signal into a new list
        # We must do it the same way as we did in StopTypeT
        bar = [
            *temp_info.sl_bar[signal],
            *temp_info.tp_bar[signal]
        ]
        
        # Check whether the row index of this order is in this list
        # (which means that this order is a stop order)
        if record["idx"] in bar:
            # If so, get the matching position in this list and use it as order type
            # It will correspond to a field in StopType
            record["stop_type"] = bar.index(record["idx"])
        else:
            record["stop_type"] = -1
    return custom_order_records
            
custom_order_records = fix_order_records(pf.order_records, signal_info, temp_info)
print(custom_order_records[:10])
```

    [(0, 0, 13723, 0.05583492, 1790.994, 0., 1, 1, -1)
      (1, 0, 13724, 0.01861164, 1790.9  , 0., 0, 1,  1)
      (2, 0, 13726, 0.01859683, 1789.4  , 0., 0, 1,  2)
      (3, 0, 13735, 0.01862645, 1796.4  , 0., 0, 1,  0)
      (0, 3, 15039, 0.05619133, 1779.634, 0., 0, 0, -1)
      (1, 3, 18200, 0.02809566, 1781.3  , 0., 1, 0,  1)
      (2, 3, 41058, 0.02809566, 1783.3  , 0., 1, 0,  2)
      (0, 4, 15168, 0.05672607, 1762.858, 0., 0, 0, -1)
      (1, 4, 15173, 0.01890869, 1763.8  , 0., 1, 0,  1)
      (2, 4, 15209, 0.03781738, 1758.3  , 0., 1, 0,  0)]

```python
# Having raw order records is not enough as vbt.Orders doesn't know what to do with the new field
# (remember that vbt.Orders is used to analyze the records)
# Let's create our custom class that subclasses vbt.Orders
# and override the field config to also include the information on the new field

from vectorbtpro.records.decorators import attach_fields, override_field_config

@attach_fields(dict(stop_type=dict(attach_filters=True)))
@override_field_config(dict(
    dtype=custom_order_dt,  # specify the new data type
    settings=dict(
        order_type=dict(
            title="Order Type",  # specify a human-readable title for the field
            mapping=OrderType,  # specify the mapper for the field
        ),
        stop_type=dict(
            title="Stop Type",  # specify a human-readable title for the field
            mapping=StopType,  # specify the mapper for the field
        ),
    )
))
class CustomOrders(vbt.Orders):
    pass
```

```python
# Finally, let's replace the order records and the class in the portfolio

pf = pf.replace(order_records=custom_order_records, orders_cls=CustomOrders)
```

## Analyze portfolio

```python
# We can now effortlessly analyze the stop type

print(pf.orders.records_readable)
```

          Order Id  Column                     Index      Size     Price  Fees  \
    0           0       0 2021-09-14 12:43:00+00:00  0.055835  1790.994   0.0   
    1           1       0 2021-09-14 12:44:00+00:00  0.018612  1790.900   0.0   
    2           2       0 2021-09-14 12:46:00+00:00  0.018597  1789.400   0.0   
    3           3       0 2021-09-14 12:55:00+00:00  0.018626  1796.400   0.0   
    4           0       3 2021-09-16 10:39:00+00:00  0.056191  1779.634   0.0   
    ..        ...     ...                       ...       ...       ...   ...   
    432         3     221 2023-03-09 14:39:00+00:00  0.027615  1830.000   0.0   
    433         0     222 2023-03-09 08:48:00+00:00  0.055008  1817.918   0.0   
    434         1     222 2023-03-09 14:34:00+00:00  0.055008  1828.000   0.0   
    435         0     223 2023-03-09 14:26:00+00:00  0.054760  1826.135   0.0   
    436         1     223 2023-03-09 15:07:00+00:00  0.054760  1835.000   0.0   
    
          Side Order Type Stop Type  
    0    Sell       SELL      None  
    1     Buy       SELL       TP1  
    2     Buy       SELL       TP2  
    3     Buy       SELL       SL1  
    4     Buy        BUY      None  
    ..    ...        ...       ...  
    432   Buy       SELL       SL1  
    433  Sell       SELL      None  
    434   Buy       SELL       SL1  
    435  Sell       SELL      None  
    436   Buy       SELL       SL1  
    
    [437 rows x 9 columns]

```python
# And here are the signals that correspond to these records for verification

print(signal_data.get())
```

                                OrderType  EntryPrice      SL     TP1     TP2  \
    date                                                                       
    2021-09-14 12:43:51+00:00          1      1792.4  1796.4  1790.9  1789.4   
    2021-09-15 04:24:47+00:00          0      1800.0  1797.5  1805.0  1810.0   
    2021-09-15 08:08:45+00:00          1      1802.5  1806.5  1801.0  1799.5   
    2021-09-16 10:39:52+00:00          0      1780.0     NaN  1781.3  1783.3   
    2021-09-16 12:48:36+00:00          0      1762.3  1758.3  1763.8  1765.3   
    ...                              ...         ...     ...     ...     ...   
    2023-03-08 15:15:01+00:00          1      1820.0  1830.0  1817.0  1812.0   
    2023-03-08 15:38:05+00:00          1      1823.0  1830.0  1820.0  1815.0   
    2023-03-09 08:48:33+00:00          1      1818.0  1828.0  1815.0  1810.0   
    2023-03-09 14:26:07+00:00          1      1827.0  1835.0  1824.0  1820.0   
    2023-03-10 15:45:43+00:00          3      1865.0  1875.0  1862.0  1857.0   
    
                                  TP3     TP4  
    date                                       
    2021-09-14 12:43:51+00:00  1787.4     NaN  
    2021-09-15 04:24:47+00:00     NaN     NaN  
    2021-09-15 08:08:45+00:00  1797.5     NaN  
    2021-09-16 10:39:52+00:00     NaN     NaN  
    2021-09-16 12:48:36+00:00  1767.3     NaN  
    ...                           ...     ...  
    2023-03-08 15:15:01+00:00  1805.0  1770.0  
    2023-03-08 15:38:05+00:00  1805.0  1770.0  
    2023-03-09 08:48:33+00:00  1800.0  1770.0  
    2023-03-09 14:26:07+00:00  1810.0  1770.0  
    2023-03-10 15:45:43+00:00  1850.0  1770.0  
    
    [225 rows x 7 columns]

```python
# We can see that some signals were skipped, let's remove them from the portfolio

pf = pf.loc[:, pf.orders.count() >= 1]

print(len(pf.wrapper.columns))
```

    127

```python
# There are various ways to analyze the data
# For example, we can count how many times each stop type was triggered
# Since we want to combine all trades in each statistic, we need to provide grouping

print(pf.orders.stop_type.stats(group_by=True))
```

    Start                 2021-09-01 00:00:00+00:00
    End                   2023-03-11 23:59:00+00:00
    Period                        337 days 00:00:00
    Count                                       437
    Value Counts: None                          127
    Value Counts: SL1                            95
    Value Counts: TP1                            96
    Value Counts: TP2                            82
    Value Counts: TP3                            29
    Value Counts: TP4                             8
    Name: group, dtype: object

```python
# We can also get the position stats for P&L information

print(pf.positions.stats(group_by=True))
```

    Start                         2021-09-01 00:00:00+00:00
    End                           2023-03-11 23:59:00+00:00
    Period                                337 days 00:00:00
    First Trade Start             2021-09-14 12:43:00+00:00
    Last Trade End                2023-03-09 15:07:00+00:00
    Coverage                              183 days 13:36:00
    Overlap Coverage                       86 days 11:37:00
    Total Records                                       127
    Total Long Trades                                    20
    Total Short Trades                                  107
    Total Closed Trades                                 126
    Total Open Trades                                     1
    Open Trade PnL                                 0.119989
    Win Rate [%]                                  32.539683
    Max Win Streak                                        1
    Max Loss Streak                                       1
    Best Trade [%]                                 2.541454
    Worst Trade [%]                               -0.763337
    Avg Winning Trade [%]                           0.72646
    Avg Losing Trade [%]                          -0.258267
    Avg Winning Trade Duration    4 days 00:53:54.146341463
    Avg Losing Trade Duration     1 days 20:42:29.647058823
    Profit Factor                                  1.356775
    Expectancy                                      0.06216
    SQN                                            1.155675
    Edge Ratio                                     2.216966
    Name: group, dtype: object

```python
# Let's plot a random trade
# The only issue: we have too much data for that (thanks to Plotly)
# Thus, crop it before plotting to remove irrelevant data

signal = np.random.choice(len(pf.wrapper.columns))
pf.trades.iloc[:, signal].crop().plot().show_svg()
```

![svg](output_28_0.svg)

```python
# Let's verify that the entry price stays within each candle

print(pd.concat((
    pf.orders.records_readable[["Column", "Order Type", "Stop Type", "Price"]],
    pf.orders.bar_high.to_readable(title="High", only_values=True),
    pf.orders.bar_low.to_readable(title="Low", only_values=True),
    pf.orders.price_status.to_readable(title="Price Status", only_values=True),
), axis=1))

print(pf.orders.price_status.stats(group_by=True))
```

          Column Order Type Stop Type     Price      High       Low Price Status
    0         0       SELL      None  1790.994  1791.578  1790.694           OK
    1         0       SELL       TP1  1790.900  1791.088  1789.848           OK
    2         0       SELL       TP2  1789.400  1790.488  1789.234           OK
    3         0       SELL       SL1  1796.400  1796.877  1794.507           OK
    4         3        BUY      None  1779.634  1779.907  1779.324           OK
    ..      ...        ...       ...       ...       ...       ...          ...
    432     221       SELL       SL1  1830.000  1830.075  1828.485           OK
    433     222       SELL      None  1817.918  1818.098  1817.698           OK
    434     222       SELL       SL1  1828.000  1828.345  1826.565           OK
    435     223       SELL      None  1826.135  1826.595  1825.878           OK
    436     223       SELL       SL1  1835.000  1835.565  1833.635           OK
    
    [437 rows x 7 columns]
    Start               2021-09-01 00:00:00+00:00
    End                 2023-03-11 23:59:00+00:00
    Period                      337 days 00:00:00
    Count                                     437
    Value Counts: OK                          437
    Name: group, dtype: object

## Merge order records

```python
# Now, what if we're interested in portfolio metrics, such as the Sharpe ratio?
# The problem is that most metrics are producing multiple (intermediate) time series 
# of the full shape, which is disastrous for RAM since our data will have to be tiled 
# by the number of columns. But here's a trick: merge order records of all columns into one, 
# as if we did the simulation on just one column!

def merge_order_records(order_records):
    merged_order_records = order_records.copy()
    
    # New records should have only one column
    merged_order_records["col"][:] = 0
    
    # Sort the records by the timestamp
    merged_order_records = merged_order_records[np.argsort(merged_order_records["idx"])]
    
    # Reset the order ids
    merged_order_records["id"][:] = np.arange(len(merged_order_records))
    return merged_order_records

merged_order_records = merge_order_records(custom_order_records)
print(merged_order_records[:10])
```

    [(0, 0, 13723, 0.05583492, 1790.994, 0., 1, 1, -1)
      (1, 0, 13724, 0.01861164, 1790.9  , 0., 0, 1,  1)
      (2, 0, 13726, 0.01859683, 1789.4  , 0., 0, 1,  2)
      (3, 0, 13735, 0.01862645, 1796.4  , 0., 0, 1,  0)
      (4, 0, 15039, 0.05619133, 1779.634, 0., 0, 0, -1)
      (5, 0, 15168, 0.05672607, 1762.858, 0., 0, 0, -1)
      (6, 0, 15173, 0.01890869, 1763.8  , 0., 1, 0,  1)
      (7, 0, 15209, 0.03781738, 1758.3  , 0., 1, 0,  0)
      (8, 0, 16500, 0.05676728, 1761.578, 0., 0, 0, -1)
      (9, 0, 16610, 0.05676728, 1755.   , 0., 1, 0,  0)]

```python
# We also need to change the wrapper because it holds the information on our columns

merged_wrapper = pf.wrapper.replace(columns=[0], ndim=1)
```

```python
# Is there any other array that requires merging?
# Let's introspect the portfolio instance and search for arrays of the full shape

print(pf)
```

    Portfolio(
        wrapper=ArrayWrapper(
            index=<pandas.core.indexes.datetimes.DatetimeIndex object at 0x7fdc0c0d02b0 with shape (485280,)>,
            columns=<pandas.core.indexes.numeric.Int64Index object at 0x7fdc0adc53d0 with shape (127,)>,
            ndim=2,
            freq='minute',
            column_only_select=None,
            range_only_select=None,
            group_select=None,
            grouped_ndim=None,
            grouper=Grouper(
                index=<pandas.core.indexes.numeric.Int64Index object at 0x7fdc0adc53d0 with shape (127,)>,
                group_by=None,
                def_lvl_name='group',
                allow_enable=True,
                allow_disable=True,
                allow_modify=True
            )
        ),
        order_records=<numpy.ndarray object at 0x7fdc29119210 with shape (437,)>,
        open=<numpy.ndarray object at 0x7fdc0c2d9f30 with shape (485280, 1)>,
        high=<numpy.ndarray object at 0x7fdc28e65f30 with shape (485280, 1)>,
        low=<numpy.ndarray object at 0x7fdc28e65e70 with shape (485280, 1)>,
        close=<numpy.ndarray object at 0x7fdc28e65db0 with shape (485280, 1)>,
        log_records=<numpy.ndarray object at 0x7fdc0c0b20f0 with shape (0,)>,
        cash_sharing=False,
        init_cash=<numpy.ndarray object at 0x7fdbe8b1c0f0 with shape (127,)>,
        init_position=<numpy.ndarray object at 0x7fdbe8b1c930 with shape (127,)>,
        init_price=<numpy.ndarray object at 0x7fdbe8b1c870 with shape (127,)>,
        cash_deposits=<numpy.ndarray object at 0x7fdc0c1feab0 with shape (1, 1)>,
        cash_earnings=<numpy.ndarray object at 0x7fdc28e65e10 with shape (1, 1)>,
        call_seq=None,
        in_outputs=None,
        use_in_outputs=None,
        bm_close=None,
        fillna_close=None,
        trades_type=None,
        orders_cls=<class '__main__.CustomOrders'>,
        logs_cls=None,
        trades_cls=None,
        entry_trades_cls=None,
        exit_trades_cls=None,
        positions_cls=None,
        drawdowns_cls=None
    )

```python
# There are none, thus replace only the records and the wrapper
# Also, the previous individual portfolios were each using the starting capital of $100
# Which was used by 100%, but since we merge columns together, we now may require less starting capital
# Thus, we will determine it automatically

merged_pf = pf.replace(
    order_records=merged_order_records, 
    wrapper=merged_wrapper,
    init_cash="auto"
)
```

```python
# We can now get any portfolio statistic

print(merged_pf.stats())
```

    Start                         2021-09-01 00:00:00+00:00
    End                           2023-03-11 23:59:00+00:00
    Period                                337 days 00:00:00
    Start Value                                  315.787606
    Min Value                                    313.387374
    Max Value                                     331.72069
    End Value                                     323.73976
    Total Return [%]                               2.518197
    Benchmark Return [%]                           2.934614
    Total Time Exposure [%]                       54.470821
    Max Gross Exposure [%]                       100.448203
    Max Drawdown [%]                               3.719242
    Max Drawdown Duration                  83 days 02:09:00
    Total Orders                                        437
    Total Fees Paid                                     0.0
    Total Trades                                        320
    Win Rate [%]                                  63.636364
    Best Trade [%]                                  4.69009
    Worst Trade [%]                               -3.091344
    Avg Winning Trade [%]                          0.572676
    Avg Losing Trade [%]                          -0.567391
    Avg Winning Trade Duration    2 days 19:19:48.472906403
    Avg Losing Trade Duration     3 days 19:46:24.521739130
    Profit Factor                                  1.274841
    Expectancy                                     0.026842
    Sharpe Ratio                                   0.875265
    Calmar Ratio                                   0.441779
    Omega Ratio                                    1.007658
    Sortino Ratio                                  1.299684
    dtype: object

```python
# You may wonder why the win rate and other trade metrics are different here
# There are two reasons: 
# 1) portfolio stats uses exit trades (previously we used positions), 
#     that is, each stop order is a trade
# 2) after merging, there's no more information which order belongs to which trade, 
#     thus positions are built in a sequential order

# But to verify that both portfolio match, we can compare to the total profit to the previous trade P&L
print(merged_pf.total_profit)
print(pf.trades.pnl.sum(group_by=True))
```

    7.952153959905239
    7.952153959905058

```python
# We can now plot the entire portfolio

merged_pf.resample("daily").plot().show_svg()
```

![svg](output_37_0.svg)

## Custom simulator

```python
# The main issue with using from_order_func is that we need to go over the entire data 
# as many times as there are signals because the order function is run on single each element
# A far more time-efficient approach would be processing trades in a sequential order
# This is easily possible because our trades are perfectly sorted - we don't need
# to process a signal if the previous signal hasn't been processed yet
# Also, because the scope of this notebook assumes that signals are independent, 
# we can simulate them independently and stop each signal's simulation once its position has been closed out
# This is only possible by writing an own simulator (which isn't as scary as it sounds!)

# To avoid duplicating our signal logic, we will re-use order_func_nb by passing our own limited context
# It will consist only of the fields that are required by our order_func_nb

OrderContext = namedtuple("OrderContext", [
    "i",
    "col",
    "index",
    "open",  
    "high",
    "low",
    "close",
    "last_position"
])
```

```python
# Let's build the simulator
# Technically, it's just a regular Numba function that does whatever we want
# What's special about it is that it calls the vectorbt's low-level API to place orders and 
# updates the simulation state such as cash balances and positions
# We'll first determine the bars where the signals happen, and then run a smaller simulation
# on the first signal. Once the signal's position has been closed out, we'll terminate the simulation
# and continue with the next signal, until all signals are processed.

@njit(boundscheck=True)
def signal_simulator_nb(
    index, 
    open, 
    high, 
    low, 
    close, 
    signal_info,
    temp_info
):
    # Determine the number of signals, levels, and potential orders
    n_signals = len(signal_info.timestamp)
    n_sl_levels = signal_info.sl.shape[1]
    n_tp_levels = signal_info.tp.shape[1]
    max_order_records = 1 + n_sl_levels + n_tp_levels
    
    # Temporary arrays
    
    # This array will hold the bar where each signal happens
    signal_bars = np.full(n_signals, -1, dtype=int_)
    
    # This array will hold order records
    # Initially, order records are uninitialized (junk data) but we will fill them gradually
    # Notice how we use our own data type custom_order_dt - we can fill order type and stop type 
    # fields right during the simulation
    order_records = np.empty((max_order_records, n_signals), dtype=custom_order_dt)
    
    # To be able to distinguish between uninitialized and initialized (filled) orders,
    # we'll create another array holding the number of filled orders for each signal
    # For example, if order_records has a maximum of 6 rows and only one record is filled,
    # order_counts will be 1 for this signal, so vectorbt can remove 5 unfilled orders later
    order_counts = np.full(n_signals, 0, dtype=int_)
    
    # order_func_nb requires last_position, which holds the position of each signal
    last_position = np.full(n_signals, 0.0, dtype=float_)
    
    # First, we need to determine the bars where the signals happen
    # Even though we know their timestamps, we need to translate them into absolute indices
    signal = 0
    bar = 0
    while signal < n_signals and bar < len(index):
        if index[bar] == signal_info.timestamp[signal]:
            # If there's a match, save the bar and continue with the next signal on the next bar
            signal_bars[signal] = bar
            signal += 1
            bar += 1
        elif index[bar] > signal_info.timestamp[signal]:
            # If we're past the signal, continue with the next signal on the same bar
            signal += 1
        else:
            # If we haven't hit the signal yet, continue on the next bar
            bar += 1

    # Once we know the bars, we can iterate over signals in a loop and simulate them independently
    for signal in range(n_signals):
        
        # If there was no match in the previous level, skip the simulation
        from_bar = signal_bars[signal]
        if from_bar == -1:
            continue
            
        # This is our initial execution state, which holds the most important balances
        # We'll start with a starting capital of $100
        exec_state = vbt.pf_enums.ExecState(
            cash=100.0,
            position=0.0,
            debt=0.0,
            locked_cash=0.0,
            free_cash=100.0,
            val_price=np.nan,
            value=np.nan
        )
            
        # Here comes the actual simulation that starts from the signal's bar and
        # ends either once we processed all bars or once the position has been closed out (see below)
        for bar in range(from_bar, len(index)):
            
            # Create a named tuple holding the current context (this is "c" in order_func_nb)
            c = OrderContext(  
                i=bar,
                col=signal,
                index=index,
                open=open,
                high=high,
                low=low,
                close=close,
                last_position=last_position,
            )
            
            # If the first bar has no data, skip the simulation
            if bar == from_bar and not has_data_nb(c):
                break

            # Price area holds the OHLC of the current bar
            price_area = vbt.pf_enums.PriceArea(
                vbt.flex_select_nb(open, bar, signal), 
                vbt.flex_select_nb(high, bar, signal), 
                vbt.flex_select_nb(low, bar, signal), 
                vbt.flex_select_nb(close, bar, signal)
            )
            
            # Why do we need to redefine the execution state?
            # Because we need to manually update the valuation price and the value of the column
            # to be able to use complex size types such as target percentages
            # As in order_func_nb, we will use the open price as the valuation price
            # Why doesn't vectorbt do it on its own? Because it doesn't know anything
            # about other columns. For example, imagine having a grouped simulation with 100 columns sharing
            # the same cash: using the formula below wouldn't consider the positions of other 99 columns.
            exec_state = vbt.pf_enums.ExecState(
                cash=exec_state.cash,
                position=exec_state.position,
                debt=exec_state.debt,
                locked_cash=exec_state.locked_cash,
                free_cash=exec_state.free_cash,
                val_price=price_area.open,
                value=exec_state.cash + price_area.open * exec_state.position
            )
            
            # Let's run the order function, which returns an order
            # Remember when we used order_nothing_nb()? It also returns an order but with filled with nans
            order = order_func_nb(c, signal_info, temp_info)
            
            # Here's the main function in the entire simulation, which 1) executes the order,
            # 2) updates the execution state, and 3) updates the order_records and order_counts
            order_result, exec_state = vbt.pf_nb.process_order_nb(
                signal, 
                signal, 
                bar,
                exec_state=exec_state,
                order=order,
                price_area=price_area,
                order_records=order_records,
                order_counts=order_counts
            )
                    
            # If the order was successful (i.e., it's now in order_records),
            # we need to manually set the order type and stop type
            if order_result.status == vbt.pf_enums.OrderStatus.Filled:
                
                # Use this line to get the last order of any signal
                filled_order = order_records[order_counts[signal] - 1, signal]
                
                # Fill the order type
                filled_order["order_type"] = signal_info.order_type[signal]
                
                # Fill the stop type by going through the SL and TP levels and checking whether 
                # the order bar matches the level bar
                order_is_stop = False
                for k in range(n_sl_levels):
                    if filled_order["idx"] == temp_info.sl_bar[signal, k]:
                        filled_order["stop_type"] = k
                        order_is_stop = True
                        break
                for k in range(n_tp_levels):
                    if filled_order["idx"] == temp_info.tp_bar[signal, k]:
                        filled_order["stop_type"] = n_sl_levels + k  # TP indices come after SL indices
                        order_is_stop = True
                        break
                
                # If order bar hasn't been matched, it's not a stop order
                if not order_is_stop:
                    filled_order["stop_type"] = -1
                    
            # If we're not in position after an entry anymore, terminate the simulation
            if temp_info.entry_price_bar[signal] != -1:
                if exec_state.position == 0:
                    break
                    
            # Don't forget to update the position array
            last_position[signal] = exec_state.position
        
    # Remove uninitialized order records and flatten 2d array into a 1d array
    return vbt.nb.repartition_nb(order_records, order_counts)
```

```python
# Numba requires arrays in a NumPy format, and to avoid preparing them each time,
# let's create a function that only takes the data and signal information, and does everything else for us

def signal_simulator(data, signal_info):
    temp_info = build_temp_info(signal_info)
    
    custom_order_records = signal_simulator_nb(
        index=data.index.vbt.to_ns(),  # convert to nanoseconds
        open=vbt.to_2d_array(data.open),  # flexible indexing requires inputs to be 2d
        high=vbt.to_2d_array(data.high),
        low=vbt.to_2d_array(data.low),
        close=vbt.to_2d_array(data.close),
        signal_info=signal_info,
        temp_info=temp_info
    )
    
    # We have order records, what's left is wrapping them with a Portfolio
    # Required are three things: 1) array wrapper with index and columns, 2) order records, and 3) prices
    # We also need to specify the starting capital that we used during the simulation
    return vbt.Portfolio(
        wrapper=vbt.ArrayWrapper(
            index=data.index, 
            columns=range(len(signal_info.timestamp)),  # one column per signal
            freq="minute"
        ),
        order_records=custom_order_records,
        open=data.open,
        high=data.high,
        low=data.low,
        close=data.close,
        init_cash=100.0,
        orders_cls=CustomOrders
    )

# That's it!
pf = signal_simulator(data, signal_info)

print(pf.trades.pnl.sum(group_by=True))
```

    7.952153959905058

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/
parent: https://vectorbt.pro/pvt_41531c19/getting-started/
children:
  - https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2024/
  - https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2023/
  - https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2022/
  - https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2021/
name: Latest release notes
type: page
icon: material-alert-decagram
---

# Latest release notes

All notable changes are listed in reverse chronological order.

## Version 2025.10.15

- Added support for YAML configurations through the [ruamel.yaml](https://pypi.org/project/ruamel.yaml/)
and [PyYAML](https://pypi.org/project/PyYAML/) packages by implementing
[Pickleable.encode_yaml](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.encode_yaml)
and [Pickleable.decode_yaml](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.decode_yaml).
Supports all YAML features such as anchors, aliases, and complex data types. Also, has been enhanced
to represent any Python object through custom YAML tags, including custom VBT classes.

- Added support for TOML configurations through the [tomlkit](https://pypi.org/project/tomlkit/)
package by implementing [Pickleable.encode_toml](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.encode_toml)
and [Pickleable.decode_toml](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.decode_toml).
Supports all TOML features such as tables, arrays, and complex data types. Also, has been enhanced
to represent any Python object through custom TOML keys, including custom VBT classes.

- Implemented [HuggingFace Inference](https://huggingface.co/docs/inference-providers/en/index) API support in
[HFInferenceEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.HFInferenceEmbeddings)
and [HFInferenceCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.HFInferenceCompletions).
This allows you to use any HuggingFace-hosted open-source model for generating embeddings and completions.
Uses the [huggingface_hub](https://pypi.org/project/huggingface-hub/) package under the hood.

- Implemented [Gemini](https://ai.google.dev/) API support in
[GeminiEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.GeminiEmbeddings)
and [GeminiCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.GeminiCompletions).
This allows you to use Google's Gemini models for generating embeddings and completions.
Uses the [google-genai](https://pypi.org/project/google-genai/) package under the hood.

- Implemented [Anthropic](https://www.anthropic.com/) API support in
[AnthropicCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.AnthropicCompletions).
This allows you to use Anthropic's Claude models for generating completions.
Uses the [anthropic](https://pypi.org/project/anthropic/) package under the hood.

- Implemented [Ollama](https://ollama.com/docs/api/) API support in
[OllamaEmbeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.OllamaEmbeddings)
and [OllamaCompletions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.OllamaCompletions).
This allows you to use Ollama's models for generating embeddings and completions on your local machine.
The models are downloaded automatically when you first use them.
Uses the [ollama](https://pypi.org/project/ollama/) package under the hood.

- Standardized the way of handling thought/reasoning processes in chat-based completions
by introducing the [ThoughtProcessor](https://vectorbt.pro/pvt_41531c19/api/knowledge/formatting/#vectorbtpro.knowledge.formatting.ThoughtProcessor) class.
Whenever a completion contains a thought region, it is now represented as a dedicated section
wrapped into a

---

> *Thinking...*
>
>  HTML tag (customize with `think_open_tag` and `think_close_tag`).
> This allows for better formatting and easier extraction of the thought region.
> In Markdown and HTML, a thought region is rendered as a blockquote (disable with `think_to_blockquote=False`).
> To hide thought regions, set `hide_thoughts=True`.
>
> - Reorganized the [knowledge](https://vectorbt.pro/pvt_41531c19/api/knowledge/) module into submodules for better structure and maintainability.
>
> - All synthetic data generation classes, such as [RandomData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random/#vectorbtpro.data.custom.random.RandomData), 
> now accept a `randomize` argument to control whether to randomize the generated timestamps and 
> the amount of randomization. Another argument, `step`, allows you to specify the time step
> used in randomization (basically, the granularity of the grid from which timestamps are drawn).
> This allows you to generate more realistic tick data for testing and development, which can be 
> converted to OHLC data using [OHLCVDFAccessor.from_prices](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.from_prices)
> or `vbt.df_acc.ohlcv.from_prices(...)`.
>
> - Added support for custom tool and function calls across all completion classes.
> Each completion class takes a `tool_registry` argument, which is a dictionary
> of tool names to callables that can be defined per completion class or shared across multiple classes.
> Also, each class takes a `tools` argument, which is either a dictionary of tool names to callables
> or a list of callables or their names (in which case, the actual callables are looked up in the `tool_registry`).
> When `tools` is provided, all callables are converted to tool specifications in JSON Schema format using
> [Completions.function_to_tool_spec](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions.function_to_tool_spec)
> and added to the completion API call using the provided-specific mechanism (e.g., OpenAI function calling).
> Once requested, the tool calls are extracted from the completion response and executed.
> Both the tool calls and their results are rendered in the chat history inside a thought process
> and passed to the next completion call as part of the context. The argument `tools` can also
> be automatically populated with the MCP server tools by passing `tools="mcp"` or `tools="all"`
> (to include global tools as well). The way requests and responses are formatted can be customized
> using the `tool_display_format` argument (by default, payloads are not included).
> The `max_tool_calls` argument can be used to specify the maximum number of tool calls to execute
> per completion; once reached, the LLM will be forced to return the final answer.
> Finally, created the [interact](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.interact)
> function, which is based on [chat](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.chat)
> but isn't context-aware and includes the MCP server tools and global tools by default
> (similar behavior to Claude Desktop).
>
> - Implemented the [register_tool](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.register_tool) decorator
> for registering custom tools in the MCP server.
>
> - Optimized the context merging process in the MCP server.
>
> - Added `limit_delay` argument to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
> to specify whether to delay limit orders by one bar (default is False).
>
> - Made most fields in [ChunkMeta](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ChunkMeta) optional to improve usability.
>
> - BM25 fallback now defaults to True in MCP server mode only, not when used as a regular module.
>
> - Moved notebooks to a dedicated branch [notebooks](https://github.com/polakowo/vectorbt.pro/tree/notebooks).
>
> - Fixed `TA_OUTPUT_FLAGS` import in newer versions of TA-Lib.
>
> - Fixed data cleaning in custom Plotly classes.
>
> - Fixed evaluation of Python expressions separated by semicolons in [evaluate](https://vectorbt.pro/pvt_41531c19/api/utils/eval_/#vectorbtpro.utils.eval_.evaluate).
>
> - Fixed handling of non-unique and non-monotonic indexes in range breaking functions (Plotly).
>
> - Fixed the bug in [mcp_server.find](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.find)
> that caused it to return literal matches instead of complete documents. Also, reduced the default
> number of tokens from 5000 to 2000 to avoid exceeding the context length.
>
> - Fixed `find_earliest_date` not being triggered when `start` is None in 
> [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData).
>
> - Fixed handling of `width=None` and `height=None` (responsive layout) in plotting functions.
>
> ## Version 2025.7.27
>
> - Introduced the [ExamplesAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.ExamplesAsset)
> knowledge asset class, which is used to store code examples extracted from other knowledge assets and annotated with
> LLM-generated descriptions and other metadata. The new asset is referred to as "examples" (previously, examples
> were code blocks dynamically extracted from the knowledge base). In addition, to comply with GDPR and increase
> LLM compliance, the Discord history no longer contains exact timestamps (only dates) and fixed usernames
> (they are now sequential IDs per *thread*). The @polakowo's username has been replaced with `@maintainer`
> (i.e., the project maintainer).
>
> - Restructured the MCP server (now under [mcp_server](https://vectorbt.pro/pvt_41531c19/api/mcp_server/)) such that it can be run as
> as a standalone server or as a regular module. The server can be started with `python -m vectorbtpro.mcp_server`.
>
> - Implemented the [JupyterKernel](https://vectorbt.pro/pvt_41531c19/api/utils/eval_/#vectorbtpro.utils.eval_.JupyterKernel) class,
> which allows you to run code in a Jupyter kernel (similarly to REPL) and get the output as a string.
> Based on this, built the [mcp_server.run_code](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.run_code) tool,
> which allows LLMs to run VBT code in a Jupyter kernel.
>
> !!! warning
>     Letting LLMs run arbitrary code is not secure and should not be used in production.
>
> - The [mcp_server.search](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.search) tool now falls back to the BM25 search
> if the API key or some of the embeddings are not available. Also, the [mcp_server.search](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.search)
> and [mcp_server.find](https://vectorbt.pro/pvt_41531c19/api/mcp_server/#vectorbtpro.mcp_server.find) tools now return `max_tokens=5000` tokens
> by default (instead of 5 results of arbitrary length as before).
>
> - Made the [get_attrs](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.get_attrs) function more robust
> by operating on reference names. The new `refname` column can be used by LLMs to search for attribute names
> across the entire knowledge base.
>
> - Changed linear weighting in [DocumentRanker](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_ranking/#vectorbtpro.knowledge.doc_ranking.DocumentRanker)
> to RRF (Reciprocal Rank Fusion) for better performance.
>
> - Added support for 1-second data in [PolygonData](https://vectorbt.pro/pvt_41531c19/api/data/custom/polygon/#vectorbtpro.data.custom.polygon.PolygonData).
>
> - Made the [SUPERTREND](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/supertrend/#vectorbtpro.indicators.custom.supertrend.SUPERTREND)
> indicator more robust by switching to the classical Supertrend algorithm.
>
> - Fixed calculation of the right bound for indexes with a non-UTC time zone in [Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler).
>
> - Fixed "First Trade Start" and "Last Trade End" metrics in [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades),
> which were not calculated correctly for grouped trades.
>
> - Relaxed the credential requirements for [TVClient](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient).
>
> - Converted the error *"'ZMQInteractiveShell' object has no attribute 'magic'"* to a warning when importing QuantStats.
>
> - Switched from deprecated `pkg_resources` to `packaging.version` for version comparison.
>
> - Fixed default connection in [DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData).
>
> - Allowed zero length ranges for taking and applying by [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).
>
> - The following packages are not installed by default anymore: Dask (use `threadpool` instead) and
> QuantStats (has compatibility issues). On the other hand, Python SDK for MCP and Binance Exchange API
> are now installed by default.
>
> ## Version 2025.6.24
>
> - Crafted a lightweight Dockerfile specifically for development; it can be used to quickly set up a
> development environment with all necessary dependencies. Also, created `devcontainer.json` for
> [Visual Studio Code](https://code.visualstudio.com/docs/remote/devcontainerjson-reference) to enable
> development in a containerized environment.
>
> - Fixed incorrect import of Plotly resampler.
>
> - Renamed `refine_source` to `refactor_source` to better reflect its purpose of refactoring source code.
>
> - Implemented the [PythonSplitter](https://vectorbt.pro/pvt_41531c19/api/knowledge/text_splitting/#vectorbtpro.knowledge.text_splitting.PythonSplitter)
> class, which splits Python source code into clean chunks using AST while respecting class and function boundaries.
>
> - Implemented the [MarkdownSplitter](https://vectorbt.pro/pvt_41531c19/api/knowledge/text_splitting/#vectorbtpro.knowledge.text_splitting.MarkdownSplitter)
> class, which splits Markdown source code into clean chunks while respecting headings and paragraphs.
>
> - Fixed the issue with BM25 search in [DocumentRanker](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_ranking/#vectorbtpro.knowledge.doc_ranking.DocumentRanker),
> which would raise an error if the same ranker was used for different documents. Fixed another issue with
> returning results in the wrong order.
>
> - Implemented the [refactor_markdown](https://vectorbt.pro/pvt_41531c19/api/utils/source/#vectorbtpro.utils.source.refactor_markdown)
> and [refactor_docs](https://vectorbt.pro/pvt_41531c19/api/utils/source/#vectorbtpro.utils.source.refactor_docs) functions for refactoring
> Markdown and documentation files.
>
> - Added multiple output formats to the [refactor_source](https://vectorbt.pro/pvt_41531c19/api/utils/source/#vectorbtpro.utils.source.refactor_source) function,
> including "full", "patch", and "udiff", which allow you to get the full code, a patch, or a unified diff,
> respectively. The two latter formats are useful for applying smaller changes to large codebases.
>
> - The default cache directory for knowledge assets has been made global ("user_cache_dir" in [platformdirs](https://pypi.org/project/platformdirs/)), which is platform-dependent. This allows you to use the same cache across different projects and environments.
>
> - Implemented [VBTAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset)
> functions for generating the `/llms.txt` files.
>
> - Improved frontmatter formatting in [VBTAsset](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset)
>
> - Created a [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) server that allows you to
> use VectorBT PRO as a backend for LLMs that support MCP. This server can be used to quickly set up a knowledge base
> and chat with it using any MCP-compatible LLM, such as [Claude](https://claude.ai/).
>
> - Fixed the issue with the [combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params) function,
> which would occasionally return all combinations of parameters instead of a random subset when conditions were provided.
>
> - Made all property decorators serializable by default.
>
> - Implemented a more conservative stop order and limit order handling in 
> [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals):
>
>     - When only the close price is provided, the stop order is filled at the close price
>     (previously, it was filled at the stop price). Use `stop_exit_price="hardstop"` for the previous behavior.
>
>     - Limit orders are now filled with `limit_order_price="hardlimit"` by default, unless reversal is enabled;
>     in this case, the limit order behaves like a stop order. For the previous behavior, use
>     `limit_order_price="limit"` regardless of reversal.
>
> !!! warning
>     Be aware of the changes to the signatures of the following Numba functions:
>
>
>     * [check_price_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.check_price_hit_nb),
>
>     * [check_limit_expired_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.check_limit_expired_nb),
>
>     * [check_limit_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.check_limit_hit_nb),
>
>     * [check_stop_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.check_stop_hit_nb),
>
>     * [check_td_stop_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.check_td_stop_hit_nb), and
>
>     * [check_dt_stop_hit_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.check_dt_stop_hit_nb).
>
> - Discord history has been cleaned up, formatted, and made GDPR-compliant (as far as possible):
>
>     - Replaced usernames with sequential IDs (e.g., `@u1`, `@u2`, etc.), apart from the admin (`@polakowo`).
>
>     - Shifted timestamps by random amounts to prevent exact date and time identification.
>
>     - Redacted personal information (e.g., emails, user folder names, etc.).
>
>     - Formatted content to Markdown and rephrased any prose for clarity and conciseness.
>
> - The entire website content has been refined for clarity and conciseness.
>
> - Each website page now has a Markdown source file, which can be found at the same path as the page URL
> but with the `.md` extension. For example, the source file for this page is located at
> [https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/index.md](https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/index.md).
>
> - Added the `/llms.txt` files to the website.
>
> ## Version 2025.5.11
>
> - Integrated the [BM25 ranking algorithm](https://en.wikipedia.org/wiki/Okapi_BM25) into
> [DocumentRanker](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_ranking/#vectorbtpro.knowledge.doc_ranking.DocumentRanker)
> for fast lexical search. Three search methods are supported: "embeddings", "bm25", and "hybrid", which uses both.
> The new ranker operates fully offline and can be used instead of embeddings for quick retrieval of related information.
> Additionally, two new global functions were added: [quick_search](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.quick_search),
> which uses BM25 search, and [quick_chat](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.quick_chat),
> which uses BM25 search along with the quick LLM mode (a smaller, faster, and less expensive LLM).
>
> - Implemented the [completed](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.completed)
> function, which directly returns the LLM completion instead of formatting it.
>
> - [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory) now supports
> [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) instances as parameters.
> You can use `combine_kwargs` to configure how parameters are combined.
>
> - Added commodities and economic indicators to [AVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/av/#vectorbtpro.data.custom.av.AVData).
>
> - Added support for Plotly 6.0 and removed custom JSON templates in favor of a color map for better compatibility
> with future Plotly versions.
>
> - Increased flexibility in providing third-party embedding, LLM, and text splitter configurations.
>
> - Time-based datetime arguments (such as `dt_stop`) now behave correctly for time zones with daylight savings.
>
> - Connections in [SQLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData) are now closed automatically.
>
> - Renamed `imported_stuff` to [imported_star](https://vectorbt.pro/pvt_41531c19/api/#imported_star). The star is now used as a context in all
> templates by default.
>
> - Ranges with zero length no longer raise errors but are ignored by default in various factory methods of
> [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).
>
> - Fixed incorrect handling of NaN by Plotly 6.0 by internally replacing NaN with None.
> Also, resolved issues with handling datetime indexes by converting them to lists.
>
> - Implemented the function [refine_source](https://vectorbt.pro/pvt_41531c19/api/utils/source/#refine_source), which takes any object
> representing Python source code (such as a package, Python object, or raw string), splits it into clean
> chunks using AST while respecting class and function boundaries, refactors each chunk using an LLM,
> and merges the refactored chunks back into coherent code. It can return the result, modify the source directly,
> or copy it to the clipboard. Additionally, it can display the diff directly in the browser for verification.
> A special prompt and a helper function [refine_docstrings](https://vectorbt.pro/pvt_41531c19/api/utils/source/#refine_docstrings)
> were also developed to refine the API documentation of this package.
>
> - Moved code-cutting and source-related functionality to the [utils.source](https://vectorbt.pro/pvt_41531c19/api/utils/source/) module.
>
> - Improved autocompletion and static code analysis for `vbt.*`.
>
> - Plotly widgets are now disabled by default (set `use_widgets=True` in [plotting](https://vectorbt.pro/pvt_41531c19/api/_settings/#plotting)
> for the previous behavior).
>
> - Set `use_hash=True` in [VBTAsset.save_to_markdown](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset.save_to_markdown)
> and [VBTAsset.save_to_html](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset.save_to_html)
> to avoid generating long file paths on Windows.
>
> - Updated [YFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/yf/#vectorbtpro.data.custom.yf.YFData) to support the latest yfinance version.
> Introduced a new argument `ticker_kwargs` for passing session and proxy objects.
>
> - Fixed [BaseAccessor.split](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.split) so it returns
> the object in the "reset_stacked" format.
>
> - Updated the Dockerfile to use the latest Python 3.11, updated optional dependencies, and
> [uv](https://github.com/astral-sh/uv) for faster image building.
>
> - Added support for trades, quotes, and options in [AlpacaData](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData).
>
> - Fixed `typing.Self` import for some Python versions.
>
> - Corrected the helper function that determines whether the current stop ladder is active.
>
> - Implemented a documentation-friendly prettifier that hides memory addresses and other redundant information.
>
> - Improved formatting of initialization arguments in `@define`-decorated classes.
>
> - Refined the entire API documentation using LLMs (specifically, OpenAI o3-high).
>
>     - Standardized all docstrings to follow the [Google format](https://google.github.io/styleguide/pyguide.html).
>
>     - Type hints for arguments and return values are now visible in their descriptions and on the website.
>
>     - Manually verified all combinations of argument names and types for consistent descriptions.
>
>     - Enhanced arguments with references to related objects, such as 
>     [SizeType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType) for `size_type`.
>
>     - Fixed many smaller bugs along the way :bug:
>
> ## Version 2025.3.1
>
> - Introduced new modules for chatting that provide functionality for chatting with LLMs.
> Most of the classes offered are classic RAG components found in LlamaIndex and other
> knowledge assistant frameworks. They are flexible and generally not bound to any single framework.
> For example, the [Embeddings](https://vectorbt.pro/pvt_41531c19/api/knowledge/embeddings/#vectorbtpro.knowledge.embeddings.Embeddings)
> class for generating embeddings has subclasses for OpenAI, LiteLLM, and LlamaIndex,
> which can be used interchangeably. The same applies to [Completions](https://vectorbt.pro/pvt_41531c19/api/knowledge/completions/#vectorbtpro.knowledge.completions.Completions).
> Tokenization is primarily handled by the [TikTokenizer](https://vectorbt.pro/pvt_41531c19/api/knowledge/tokenization/#vectorbtpro.knowledge.tokenization.TikTokenizer)
> class, which is based on `tiktoken`. Other classes are designed to prepare the context,
> including document splitting ([TextSplitter](https://vectorbt.pro/pvt_41531c19/api/knowledge/text_splitting/#vectorbtpro.knowledge.text_splitting.TextSplitter)),
> storing ([ObjectStore](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_storing/#vectorbtpro.knowledge.doc_storing.ObjectStore)),
> and ranking ([DocumentRanker](https://vectorbt.pro/pvt_41531c19/api/knowledge/doc_ranking/#vectorbtpro.knowledge.doc_ranking.DocumentRanker)).
> With these components, context ranking is integrated into most knowledge searching and chatting functions.
> There are also two new functions based entirely on RAG for interacting with the entire knowledge base:
> [search](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.search)
>   (also known as SearchVBT) and [chat](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.chat)
>   (also known as ChatVBT) :robot:
>
> - Added a new [formatting](https://vectorbt.pro/pvt_41531c19/api/knowledge/formatting/) module that provides functionality
> for formatting input and output content. On one side, there are helper classes such as
> [ToHTML](https://vectorbt.pro/pvt_41531c19/api/knowledge/formatting/#vectorbtpro.knowledge.formatting.ToHTML)
> for converting content between string formats. On the other side, there is the
> [ContentFormatter](https://vectorbt.pro/pvt_41531c19/api/knowledge/formatting/#vectorbtpro.knowledge.formatting.ContentFormatter)
> class and its subclasses, which specialize in processing streaming and non-streaming input content.
> For instance, [IPythonHTMLFormatter](https://vectorbt.pro/pvt_41531c19/api/knowledge/formatting/#vectorbtpro.knowledge.formatting.IPythonHTMLFormatter)
> can display LLM output as HTML content in a Jupyter environment.
>
> - Added a feature to mirror OHLC data, so bullish phases become bearish and vice versa.
> Available in [Data.mirror_ohlc](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.mirror_ohlc),
> [OHLCVDFAccessor.mirror_ohlc](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.mirror_ohlc),
> and [ReturnsAccessor.mirror](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.mirror).
>
> - Introduced the [HasWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.HasWrapper) class,
> which implements a common interface for [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)
> and [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping).
>
> - Most VBT objects now have [HasWrapper.chunk](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.HasWrapper.chunk)
> and [HasWrapper.chunk_apply](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.HasWrapper.chunk_apply) methods,
> allowing the object to be split into chunks and a function to be applied to each chunk, respectively.
>
> - When broadcasting, index levels without a name are no longer treated as unique levels. For example,
> passing one index with a "symbol" level and another without will not create two separate levels now.
>
> - Added a `norm_window` argument to [OLS](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/ols/#vectorbtpro.indicators.custom.ols.OLS),
> which defaults to `window` and defines the window length used for computing the Z-score.
>
> - The client type in [AlpacaData](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData)
> is now determined automatically based on the symbol (crypto symbols contain "/").
>
> - Removed `Configured._expected_keys` in favor of `Configured._expected_keys_mode`, which (by default)
> automatically combines keys from bases and the signature. You no longer need to specify expected keys.
> Additionally, the design of metaclasses has been simplified.
>
> - Made it easier to get and set settings using [SettingsConfig.get](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.SettingsConfig.get)
> and [SettingsConfig.set](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.SettingsConfig.set), respectively. For example,
> `vbt.settings.set("portfolio.size_type", size_type)`.
>
> - Added a `hide_inner_progress` argument (True by default) to parallel execution engines,
> which allows showing progress inside tasks.
>
> - Allowed variable keyword arguments in [QSAdapter](https://vectorbt.pro/pvt_41531c19/api/returns/qs_adapter/#vectorbtpro.returns.qs_adapter.QSAdapter).
>
> - Renamed `yield_` to `iter_` everywhere. For example, `yield_chunk_meta` was renamed to
> [iter_chunk_meta](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.iter_chunk_meta).
>
> - Created a new template class [SafeSub](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.SafeSub)
> to safely substitute context without raising errors, making it optimal for prompts.
>
> - Removed the limitation of one HTML page in [VBTAsset.display](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.VBTAsset.display).
> This method can display any number of HTML pages as iframes inside an HTML page with pagination,
> providing a lightweight alternative to storing and browsing multiple HTML pages.
>
> - Added MathJax support for rendering HTML.
>
> - Fixed the "TypeError: 'method' object is not subscriptable" error raised by [Config.chat](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config.chat).
>
> - Fixed handling of integer data types in [nancorr_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.nancorr_1d_nb).
>
> - Fixed an issue with printing `attr`-generated objects.
>
> - Fixed the `TextIO` error in Python 3.8.
>
> - Fixed handling of tuples as parameter names in parameter conditions.
>
> - Fixed resolution of `xaxis` and `yaxis` during plotting.
>
> - Fixed handling of data types when updating data.
>
> - Fixed the error "ValueError: len(src) can only be a multiple of typesize (8)" raised by Blosc2.
>
> - Fixed handling of slices in [DTCIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.DTCIdxr),
> which previously prevented proper selection of date ranges from VBT objects.
>
> - Fixed `typing.Self` import error on Python versions lower than 3.11.
>
> - Made all VBT warnings more human-readable.
>
> - Made minor color adjustments to the dark theme.
>
> - Updated the [LICENSE :material-license:](https://vectorbt.pro/terms/software-license/) and
> proprietary notices in the code.

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2024/
parent: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/
name: Release notes for 2024
type: page
---

# Release notes for 2024

All notable changes are listed in reverse chronological order.

## Version 2024.12.15

- Indicators are now converted to arrays during broadcasting.

- Renamed `cum_returns` to `cumulative_returns` in [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor).

- Renamed `period` to `periods` in [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor)
and [BaseIDXAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseIDXAccessor).

- Added high-performance shortcuts for selecting features or symbols in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data).

- Added portfolio context helpers for dry-simulating orders.

- Implemented [Portfolio.get_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_signals)
to extract signals from orders and trades.

- Made all `column_stack` and `row_stack` methods hybrid; they can now also be called as instance methods.
For example, `wrapper.stack(*wrappers)` now works similarly to `vbt.ArrayWrapper.row_stack(*wrappers)`.

- The subplot builder ([PlotsBuilderMixin.plots](https://vectorbt.pro/pvt_41531c19/api/generic/plots_builder/#vectorbtpro.generic.plots_builder.PlotsBuilderMixin.plots))
now accepts a new `fig` argument to plot over an existing figure. Together with the metric builder
([StatsBuilderMixin.stats](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin.stats)),
it also accepts two additional arguments: `per_column` to process each column individually, and `split_columns`
to split the current instance into columns rather than to keep the current instance and pass each
column label as the `column` argument.

- Fixed resolution of valuation price in [execute_order_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.execute_order_nb).

- Passing a named tuple as a single argument without a comma, such as
`signal_args=(named_tuple)`, will now throw an error instead of treating the named tuple as arguments.

- Merged PR #152 on behalf of @kadeng, which fixes a warning about an invalid escape sequence in
[_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings) and improves version lookup when symlinking VBT into site-packages.

- Implemented the following cross-knowledge-asset methods that take a VBT object and operate on its knowledge:

    - [find_api](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_api): 
    Find API documentation based on the object's interface.

    - [find_docs](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_docs): 
    Find general (non-API) documentation based on object mentions.

    - [find_messages](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_messages): 
    Find Discord messages based on object mentions.

    - [find_examples](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_examples): 
    Find code examples based on object mentions.

    - [find_assets](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.find_assets): 
    Combine all of the above into one asset.

    - [chat_about](https://vectorbt.pro/pvt_41531c19/api/knowledge/custom_assets/#vectorbtpro.knowledge.custom_assets.chat_about): 
    Chat using the combined asset.

- Implemented a unified interface for most VBT classes - [Base](https://vectorbt.pro/pvt_41531c19/api/utils/base/#vectorbtpro.utils.base.Base) -
which exposes the above methods with the same naming (except `chat_about`, which becomes just `chat`).

- Added a `skip_empty` argument to [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
If disabled, no rows will be skipped, and all callbacks will be executed.

- Implemented [`vbt.get_obj(obj)`](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module_.get_obj)
to retrieve any VBT object by its name.

- Made the following functions and decorators more flexible:

    - [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute): 
    More flexible arguments; `executor` replaces `executor_cls`.

    - [@iterated](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.iterated): 
    More flexible arguments; `executor` replaces `executor_cls`.

    - [@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked): 
    More flexible arguments; `chunker` replaces `chunker_cls`.

    - [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized): 
    More flexible arguments; `parameterizer` replaces `parameterizer_cls`.

- The tokenizer in chatting now depends on the LLM model.

- Removed `max_context_chars` and `max_context_tokens` in favor of `max_tokens`, which
is the total number of tokens in __messages__ (default is 120,000).

- Matches and fields are now automatically merged and made unique in
[KnowledgeAsset.find](https://vectorbt.pro/pvt_41531c19/api/knowledge/base_assets/#vectorbtpro.knowledge.base_assets.KnowledgeAsset.find).

- Enabled callbacks with different names in staticization. Instead of passing an imported callback,
you can now provide it as a tuple with the module name/path and function name within the module,
such as `signal_func_nb=("my_callbacks.py", "my_signal_func_nb")`.

## Version 2024.11.12

- Implemented the sub-package [knowledge](https://vectorbt.pro/pvt_41531c19/api/knowledge/) for working with knowledge assets.
Knowledge assets are data containing knowledge about VBT, such as Discord messages and website pages.
See the [cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/knowledge/) for usage examples.

- Added native support for NumPy 2.0. The default integer and floating point precisions are 64 bits.
These can be changed in [NumPy settings](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.numpy).

- Fixed selection of date ranges in Plotly figures.

- Indicators now include a `get` method.

- Added portfolio context helpers for retrieving the current order size and value.
Also fixed [get_col_position_value_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/ctx_helpers/#vectorbtpro.portfolio.nb.ctx_helpers.get_col_position_value_nb).

- Renamed decorators: `classproperty` to `class_property`, `class_or_instanceproperty` to `hybrid_property`,
and `class_or_instancemethod` to `hybrid_method`.

- Fixed behavior for `size_granularity` and `allow_partial` when reversing positions.

- Renamed the setting `messaging.telegram` to `telegram.bot`.

- Added support for ZIP archives when saving and loading.

- Renamed `multiline_eval` to `evaluate`.

- Added functions for working with nested objects, such as [flatten_obj](https://vectorbt.pro/pvt_41531c19/api/utils/search/#vectorbtpro.utils.search.flatten_obj),
and path-like keys, such as [get_pathlike_key](https://vectorbt.pro/pvt_41531c19/api/utils/search/#vectorbtpro.utils.search.get_pathlike_key).

- Added functions to [find](https://vectorbt.pro/pvt_41531c19/api/utils/search/#vectorbtpro.utils.search.find) and
[replace](https://vectorbt.pro/pvt_41531c19/api/utils/search/#vectorbtpro.utils.search.replace) occurrences in text, including
[FuzzySearch](https://github.com/taleinat/fuzzysearch) and [RapidFuzz](https://github.com/rapidfuzz/RapidFuzz).

- Templates now have access not only to `np`, `pd`, and `vbt`, but to everything from `from vectorbtpro import *`.

- Each template subclass now includes a method [CustomTemplate.get_context_vars](https://vectorbt.pro/pvt_41531c19/api/utils/template/#vectorbtpro.utils.template.CustomTemplate.get_context_vars)
to detect context variables even before evaluation.

- Added functions for flexible reordering of [dicts](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.reorder_dict)
and [lists](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.reorder_list).

- Fixed indexing of indicators with multiple columns.

- Renamed `plot_cum_returns` to `plot_cumulative_returns` in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

- Any package can be searched for any object with [search_package](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module_.search_package).

- Most classes now have the [Chainable.chain](https://vectorbt.pro/pvt_41531c19/api/utils/chaining/#vectorbtpro.utils.chaining.Chainable.chain)
method to connect multiple pipes.

- Any object can be dumped into a string with [`vbt.dump(obj)`](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.dump).

- Made datetime components ([DTC](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.DTC)) Numba-resilient
by replacing `None` with `-1` in the enumerated tuple equivalent.

## Version 2024.8.20

- Portfolio instances now accept the `weights` argument, which is a vector containing one weight per column.
Weights are applied to the initial position, initial cash, cash deposits, cash earnings, and order records.
They are used to scale a column up or down relative to its group or the entire portfolio. Weights can also be
applied after creating a portfolio instance using [Portfolio.apply_weights](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.apply_weights),
and you can optionally rescale by group by passing `rescale=True` to the same method.

- Implemented the data class [FinPyData](https://vectorbt.pro/pvt_41531c19/api/data/custom/finpy/#vectorbtpro.data.custom.finpy.FinPyData)
for `findatapy` (https://github.com/cuemacro/findatapy), which allows pulling data from Bloomberg, Eikon,
Quandl, Dukascopy, and more.

- Implemented portfolio views (methods and properties that return other portfolio instances)
that show only orders of long positions ([Portfolio.long_view](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.long_view))
or short positions ([Portfolio.short_view](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.short_view)).

- Implemented methods to convert Pandas Series and DataFrames into [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data)
instances: OHLC(V) DataFrames can be converted using `data = df.vbt.ohlcv.to_data()`, and 
Series/DataFrames with symbols as columns can be converted using the more general `data = df.vbt.to_data()`.

- Indicators now behave like regular arrays when combined with other arrays. When an indicator has multiple
outputs and one output shares the same name as the indicator's short name (such as "atr" in [ATR](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/atr/#vectorbtpro.indicators.custom.atr.ATR)),
that output becomes the main output that is combined.

- Added options `PriceType.NextValidOpen` (or "nextvalidopen") and `PriceType.NextValidClose` (or "nextvalidclose"),
which use the next non-NA open and close price, respectively.

- Fixed [TVClient.search_symbol](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient.search_symbol).
A back-adjustment option was added for symbols ending with "!A". The user agent is now a global constant
(see `vbt.data.custom.tv.USER_AGENT`) and is used in most connection settings by default.

- [Data.add_symbol](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.add_symbol) now re-uses keyword arguments
previously used for fetching if `reuse_fetch_kwargs` is True, which is now the default.
Also, [Data.add_feature](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.add_feature) can now use pulling instead
of running as well, if `pull_feature` is True.

- Fixed plotting for [ADX](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/adx/#vectorbtpro.indicators.custom.adx.ADX).

- Fixed package version comparison that led to "NameError: name 'post1' is not defined".

- Fixed `read_only` argument resolution in [DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData).

- Allowed renaming the index in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data).

- As suggested and contributed by @rayanbenredjeb (thank you!), improved computation for covariance and
correlation functions by reducing two loops to a single loop.

- Fixed parameter index for `per_column=True` in [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory).

- Fixed mapping of start and end dates in [BentoData](https://vectorbt.pro/pvt_41531c19/api/data/custom/bento/#vectorbtpro.data.custom.bento.BentoData).

- Adapted scheduling functionality for the `schedule` package version 1.2.2 and above.

- Fixed integer size granularity, which caused incorrect order size due to floating-point errors.

- Fixed NaN resolution in [PIVOTINFO](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/pivotinfo/#vectorbtpro.indicators.custom.pivotinfo.PIVOTINFO).

- Re-added the arguments `use_asset_returns` and `bm_returns` to portfolio plotting methods.

## Version 2024.6.19

- [PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func)
and [PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func)
now accept the `group_execute_kwargs` argument, which controls the execution of groups.

- [PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func)
now accepts [Takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Takeable) instances.
Taking is performed using [Splitter.take_range_from_takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take_range_from_takeable),
which accepts many formats, including arrays with a different index.

- Most objects now have a `split_apply` method that splits the object and applies a function to each chunk.
The first argument is the function, and other arguments are distributed in the same way as in the `split` method.

- If the index of data pulled by [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) is not sorted,
it will be sorted while preserving the order of duplicates. If it is monotonically decreasing, it will be reversed.

- [SQLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData) sorts data by index directly within the database.

- In-house Plotly figures now have a `select_range` method that recursively selects a specified date range
across all traces and shapes in the figure. This is especially useful for quickly rendering granular data
or efficiently exploring specific parts of data. By default, it returns a new figure.

- Fixed NaN resolution in [PIVOTINFO](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/pivotinfo/#vectorbtpro.indicators.custom.pivotinfo.PIVOTINFO).

- The `tz` argument when pulling data with most [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data)
subclasses will now also default to `tz_localize` (not just to `tz_convert`).

- Added `eval_id` to all chunking classes. This enables nested chunking, allowing you to specify
which function (or decorator) should evaluate which array. For example, you can define different execution rules
for different arrays. If the `eval_id` of the function does not match the `eval_id` of the array,
the array will be forwarded downstream to be evaluated by the inner function.

- Added the [Task](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Task) class, which behaves
like the tuple format `(func, args, kwargs)`, but in a more user-friendly way. The `funcs_args` argument in
[Executor.run](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Executor.run) and other places has
been renamed to `tasks`. Also, the `distribute="calls"` option has been renamed to `distribute="tasks"`.

- [Executor](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Executor) can now merge execution results
using `merge_func` and `merge_kwargs`. It can also filter out `NoResult` instances if
`filter_results` is True (not default), and raise an error if all results are `NoResult` instances
and `raise_no_results` is True (default). In general, logic to filter out missing results has been
standardized and delegated to [filter_out_no_results](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.filter_out_no_results).

- Implemented the new [@iterated](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.iterated) decorator,
which takes a function and executes it repeatedly on values of an argument passed to the function
(usually the first argument, but it can be specified with `over_arg`).

- Connections in [DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData)
are closed automatically when provided as a string. This allows you to open two connections simultaneously
with different access modes.

- Cache clearing (`chunk_clear_cache`), garbage collection (`chunk_collect_garbage`), and delay
(`chunk_delay`) can be performed after executing each (super-)chunk using [Executor](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Executor).

- Progress bars are now hidden during parallel workflows. Additionally, tqdm bars are no longer opened
by default when progress should not be shown (this can be changed by setting `force_open_bar=True`).

- The signal factory now accepts a `jitted` argument to control jitting of generation functions.

- Integrated [Smart Money Concepts (SMC)](https://github.com/joshyattridge/smart-money-concepts)
into [IndicatorFactory.from_smc](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_smc).

- Added five different Numba-compiled implementations of the (rolling) Hurst exponent in
[HURST](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/hurst/#vectorbtpro.indicators.custom.hurst.HURST).

## Version 2024.5.15

- Refactored execution engines. In addition to the [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute)
function, there is now an [Executor](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Executor)
class that takes execution-related arguments and provides the [Executor.run](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Executor.run)
method to run these arguments on function call(s). When the [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute)
function is called, it constructs an instance of the executor and calls its run method. Many preprocessing
and postprocessing workflows have also been separated into standalone methods that you can override,
such as [Executor.call_pre_chunk_func](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.Executor.call_pre_chunk_func),
which is called before a chunk is processed. In addition, keys can be supplied with function calls.
Each key describes what the function call represents and can be used in progress bars.

- Implemented (super-)chunk caching in the executor. If (super-)chunking is enabled with `cache_chunks=True`
and an execution-specific caching directory is specified in `chunk_cache_dir` (which will be created if it does
not exist), the results of each chunk will be serialized and saved to disk. If execution is stopped and then
re-run, the results of each (previously cached) chunk will be loaded from disk. If `pre_clear_chunk_cache`
is True, the caching directory will be cleared before execution. If `release_chunk_cache` is True, results
will not be kept in memory, but replaced with a dummy object. When execution is complete, all dummy objects
are replaced with the actual results loaded from disk. By default (`post_clear_chunk_cache=True`), the
caching directory is removed after execution is complete.

- Resolution of defaults in all execution engines, the executor, the chunker, and the parameterizer
now happens during initialization for atomicity.

- Each progress bar can now display a relevant description of the item currently being processed.
For example, a daily range between the dates _"2024-01-01 00:00:00"_ and _"2025-01-01 00:00:00"_
is shown as _"2024-01-01 → 2025-01-01"_. The progress description is displayed as a postfix to avoid
progress bars of varying lengths. This option (`as_postfix`) can be changed in the pbar-related settings.

- Progress bars are now enabled by default (you no longer need to provide `show_progress=True`),
but they will only be displayed if the bars are not finished before a number of seconds (`delay`).

- Added a context manager for progress bars, [ProgressBar](https://vectorbt.pro/pvt_41531c19/api/utils/pbar/#vectorbtpro.utils.pbar.ProgressBar),
which replaces the `get_pbar` function. It wraps a `tqdm` instance and enhances it with useful features
while exposing identically named methods. For example, [ProgressBar.update](https://vectorbt.pro/pvt_41531c19/api/utils/pbar/#vectorbtpro.utils.pbar.ProgressBar.update)
updates the `tqdm` instance and also calls some pre-update and post-update workflows.

- Enabled reusable and nested progress bars by implementing a registry class,
[PBarRegistry](https://vectorbt.pro/pvt_41531c19/api/registries/pbar_registry/#vectorbtpro.registries.pbar_registry.PBarRegistry),
and its instance [pbar_reg](https://vectorbt.pro/pvt_41531c19/api/registries/pbar_registry/#vectorbtpro.registries.pbar_registry.pbar_reg),
which registers all currently active and pending, parent and child progress bars. Each progress bar
gets a unique identifier, which can be provided or generated automatically. When a child
progress bar finishes, it gets a pending status; if a progress bar with the same identifier is registered
again later, it reuses the already registered pending progress bar. When the parent progress bar stops,
all children are closed and deregistered. The parent-child relationship is determined by the time each
bar was opened, updated, or closed. To disable this feature, set `reuse` to False.

- All progress bars can be displayed or hidden within a code block using the context managers
[ProgressShown](https://vectorbt.pro/pvt_41531c19/api/utils/pbar/#vectorbtpro.utils.pbar.ProgressShown) and
[ProgressHidden](https://vectorbt.pro/pvt_41531c19/api/utils/pbar/#vectorbtpro.utils.pbar.ProgressHidden).

- The day unit is now uppercase ("D" instead of "d").

- Extended offset string translation. For example, "businessmonthstart" is now translated to "BMS".
All aliases now work the same way in both older and newer versions of Pandas.

- Added many convenient datetime shortcuts. For example, `vbt.timestamp()` returns the same value as
`pd.Timestamp.now()`, while `vbt.offset("M")` returns `pd.offsets.MonthBegin()`.

- Simulation range can now be modified during a simulation. Each context has new fields,
[SimulationContext.sim_start](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationContext.sim_start)
and [SimulationContext.sim_end](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationContext.sim_end),
which are guaranteed to be one-dimensional NumPy arrays with values per group. Lowering the
second array in place affects the current simulation. In particular, setting it to the next row or
earlier processes the current row to the end and stops the simulation at the next one. This can be
done conveniently by calling [`vbt.stop_group_sim_nb(c)`](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/ctx_helpers/#vectorbtpro.portfolio.nb.ctx_helpers.stop_group_sim_nb).

- Simulation range passed to a simulation method is retained in the simulation output
([SimulationOutput](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput)) per column,
rather than by group as when passed to the method.

- Portfolio analysis and return methods and properties have been adapted to simulation ranges.
When a simulation range is active, the corresponding Numba function will process data only within this range.
Any simulation range can be activated, modified, or deactivated dynamically, even in stats.
For example, `pf.get_sharpe_ratio(sim_start="2024")` returns the Sharpe ratio only for rows
from 2024 onwards. If `rec_sim_range` is False (default), only the latest analysis step
respects the simulation range. In this case, returns are computed for all rows, while only the Sharpe
ratio is computed within the range. If `rec_sim_range` is True, the entire analysis chain respects
the simulation range (similar to backpropagation). For example, `pf.get_sharpe_ratio(sim_start="2024", rec_sim_range=True)`
returns the Sharpe ratio as if orders before 2024 did not exist.

- Renamed the metrics "Start", "End", and "Period" to "Start Index", "End Index", and "Total Duration".
These metrics respect the currently active simulation range.

- Improved most portfolio subplots. They now respect the current simulation range and, like metrics,
can be called as class methods on already pre-calculated arrays.

- Redesigned Numba-compiled return metrics. Rolling metrics are now standalone Numba functions.

- Symbol sorting in the `list_symbols` method of custom data classes is now optional. If sorting has
been disabled, symbols are returned in the native order of the data provider.

- Stops will not be executed immediately when the `StopExitPrice.Close` option is active.

- A human-readable error will be raised if you fail to provide arguments as a tuple.

- OHLC DataFrames can now be passed as `close` in portfolio factory methods. They are recognized
as data-like instances if all features ("Open", "High", "Low", and "Close") are present.

- Refactored Numba functions in [portfolio.nb.ctx_helpers](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/ctx_helpers/).
Functions with the prefix "col" or "group" can now be called from any callback, not just from
callbacks with a compatible context.

- When [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData) fails to pull a symbol,
it will retry several times.

## Version 2024.4.1

- Limit and stop price checks are now more robust against floating-point errors.

- Fixed partial resolution of keyword arguments when merging VBT objects.

- Fixed `size_type` not being applied to `min_size` and `max_size` in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

- [BaseAccessor.combine](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.combine) always produces
a DataFrame when given a sequence of arrays, even if there is only one array.

- Providing a list with a single parameter value to an indicator will always produce a DataFrame.

- Created decorators for all profilers in [profiling](https://vectorbt.pro/pvt_41531c19/api/utils/profiling/).

- All simulation methods now take the arguments `sim_start` and `sim_end` (both default to None), which specify
at which row the simulation should start and end for each column or group. This skips all rows
before `sim_start` and after `sim_end`, allowing for faster execution, especially for simulations
with sparse orders (such as those caused by signal unraveling). Both arguments can be provided
as datetimes or human-readable strings and are automatically converted to indices.
Many simulation methods also accept `sim_start="auto"` and `sim_end="auto"` to determine the optimal
simulation range from inputs like signals or order size.

- If an indicator has a single output (especially one with a generic name such as "real"), VBT now creates 
two dynamic aliases for it: the short name of the indicator (such as "sma") and "output".
For example, the method `real_crossed_above` can also be accessed as `sma_crossed_above`,
`output_crossed_above`, and even just `crossed_above` (without any prefix).

- Renamed the field `sub_id` in templates to `eval_id`.

- Added `eval_id` to both parameters ([Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)) 
and takeables ([Takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Takeable)).
This enables nested parameterization and splitting, since you can now specify which function (or decorator)
should evaluate which parameter. For example, you can define different execution rules for different parameters. 
If the `eval_id` of the function does not match the `eval_id` of the parameter,
the parameter will be forwarded downstream to be evaluated by the inner function.

- Standardized attribute classes, such as [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param).
To define an attribute class, decorate it with [@define](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.define),
subclass [define.mixin](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.define.mixin), and define
fields with [define.field](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.define.field). Added the concept
of a missing value (`MISSING`), which enables required and optional fields that do not raise an error
during initialization, only at validation. This provides a key feature: instances of attribute
classes can track which arguments you explicitly provided, enabling better printing and more
granular merging of multiple instances.

- Fixed potential division-by-zero errors in portfolio records.

- Patched frequency-based `dt_stop`.

- When translating a datetime object to nanoseconds, there is now a global flag `tz_naive_ns` in
[settings.datetime_](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.datetime_) that controls
whether the timezone should be dropped (True) or converted to UTC (False) before conversion.

- Extended portfolio helper functions in [portfolio.nb.ctx_helpers](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/ctx_helpers/).
For example, you can now get most information for a custom column or group.

- Patched `length="optimize"` in [Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling).

- Renamed `index_stack_kwargs` to `clean_index_kwargs`.

- Most classes now subclass [HasItems](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.HasItems)
and [Paramable](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Paramable). When calling `obj.items()`,
the object is split along the column or group axis and returns an iterator of column or group labels
and sub-objects, where each sub-object contains only one column or group of data. Also, calling
`obj.as_param()` or `vbt.Param(obj)` returns a [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
instance where values are sub-objects and keys are column or group labels. This lets you use
objects of any complexity as parameters and process only one configuration at a time.

- Renamed `yield_group_lens_idxs` to `iter_group_lens`, `yield_group_map_idxs` to `iter_group_map`,
and `__iter__` to `iter_groups` in [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper).

- [Records.records_readable](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.records_readable)
and [MappedArray.mapped_readable](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.mapped_readable)
now have an alias `readable`.

- Most classes now subclass [IndexApplier](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.base.indexes.IndexApplier),
with methods for working with index and columns. For example, to drop a parameter from a portfolio:
`new_pf = pf.drop_levels(param_name)`.

- Parameter conditions ([Param.condition](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param.condition))
can be provided as a template. To access any key (or level in the parameter index) of a parameter
combination, wrap the parameter (or level) name with `__`. For example, when combining fast and slow
SMA indicators, the condition can be `"__fast_window__ < __slow_window__"` if `fast_window` and `slow_window`
are level names in the parameter index.

- The short name and level names of an indicator can be changed using
[IndicatorBase.rename](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.rename) and
[IndicatorBase.rename_levels](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.rename_levels).

- The `split` method of most objects, as well as the decorators `@split` and `@cv_split`, now allow
providing keyword arguments related to splitting, taking, and applying as variable arguments
(in addition to `splitter_kwargs`, `take_kwargs`, and `apply_kwargs`). These arguments are distributed
over the respective functions automatically. The splitting method can also be guessed by keyword
arguments when `splitter` is None. For example, calling `obj.split(n=10, into="stacked")` will call
[Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling)
with `n=10` and then [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
with `into="stacked"`.

- Adapted the package https://github.com/sam31415/timeseriescv in [generic.splitting.purged](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/purged/)
and added factory methods [Splitter.from_purged_walkforward](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_purged_walkforward)
and [Splitter.from_purged_kfold](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_purged_kfold).

- Renamed `SKLSplitter` to `SplitterCV`.

- Similar to [StopExitPrice](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopExitPrice) for stop orders,
added [LimitOrderPrice](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.LimitOrderPrice) for limit orders.

- Wrote [Backtest powerful intraday trading strategies](https://pyquantnews.com/backtest-powerful-intraday-trading-strategies/).:notebook_with_decorative_cover:

## Version 2024.2.22

- Using the star import `from vectorbtpro import *` now returns a minimal set of imports essential for working
with VBT, such as `pd`, `np`, and `vbt` itself. This helps avoid polluting the namespace with
VBT-specific objects, which remain accessible only through `vbt`. If you encounter static analysis issues,
you can still import VBT in the traditional way: `import vectorbtpro as vbt`. The star import behavior can
be controlled by the `star_import` setting in [settings.importing](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.importing),
which accepts the values "all", "vbt" (previous behavior), "minimal" (current behavior), and "none".
This setting can only be changed through a configuration file. All notebooks and website examples have
been updated accordingly.

- Implemented portfolio methods (and corresponding properties) to calculate the average entry and exit price
of each position and bar: [Portfolio.get_position_entry_price](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_position_entry_price)
and [Portfolio.get_position_exit_price](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_position_exit_price), respectively.

- The modeling of signal pairs for [SignalsAccessor.between_ranges](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.between_ranges)
and related methods has been redesigned and is now based on `relation` of type [SignalRelation](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.SignalRelation).
For example, the behavior of `from_other=True` can now be replicated with `relation="manyone"`. The
term `other` has also been changed to `target` throughout [SignalsAccessor](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor).

- Implemented the following methods for unraveling signals:

    - [SignalsAccessor.unravel](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.unravel):
    Unravels each signal into a separate column.

    - [SignalsAccessor.unravel_between](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.unravel_between):
    Unravels each signal pair in one or two masks into a separate column.

    - [SignalsAccessor.ravel](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.ravel):
    Ravels all separate columns back into one column.

- Standardized datetime and frequency functionality across the entire codebase. Added new frequency
aliases, such as week days and months; for example, `every="monday"` returns an index with only Mondays.
The frequency can now be a date offset of any complexity and is stored in its original format.
It is converted to a timedelta by approximation only when actually needed. The frequency can now
also be detected automatically from the index when set to `"auto"` (this is the new global default; see
[infer_index_freq](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.infer_index_freq) for other options).
Year frequency follows the same logic (the default remains "365 days" for backward compatibility; see
[ReturnsAccessor.get_year_freq](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.get_year_freq)
for options).

- [Records.to_readable](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.to_readable) and
[MappedArray.to_readable](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.to_readable)
accept `expand_columns=True` to split column tuples into multiple columns, one per level.

- Fixed row stacking for one-dimensional NumPy arrays.

- Fixed handling of colons in symbols such as `"BTC/USDT:USDT"` in [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData).
Previously, the first part could be mistaken for an exchange even if an exchange was provided.

- Added the `if_exists` option to [IndicatorFactory.register_custom_indicator](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.register_custom_indicator).

- Added the `magnet_kwargs` option to [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run).

- Added Python 3.12 support :zap:

## Version 2024.1.30

- Introduced a new option `cash_deposits_as_input` to control whether cash deposits should be added
to the input value (instead of being subtracted from the output value) when computing a return.

- Implemented the methods [IndicatorBase.dropna](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.dropna)
and [Data.dropna](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.dropna) to drop any missing values from an
indicator or data instance, respectively.

- Implemented or extended the four most basic operations on data. Each operation has five different methods:
one for columns, one for keys, one for features, one for symbols, and one that can determine the
right method automatically (see below). Features and symbols have also become case-insensitive.

    - [Data.select](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.select): Select one or more features or symbols.

    - [Data.add](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.add): Add (exactly) one feature or symbol.

    - [Data.rename](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.rename): Rename one or more features or symbols.

    - [Data.remove](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.remove): Remove one or more features or symbols.

- Expanded registration of custom indicators. A custom indicator can now be registered under a custom location
(which defaults to `"custom"`). Custom locations act as indicator groups or tags and are
stored as keys in [IndicatorFactory.custom_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.custom_indicators);
they can be listed with [IndicatorFactory.list_custom_locations](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_custom_locations).
Built-in indicators can also be added to custom locations.

- [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) can return a data instance.

- Fixed path resolution and handling of keys with a period (`.`) in [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData).

- Fixed filtering by the arguments `start` and `end` in [DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData).

- Improved resolution of datetime-like arguments (specifically `dt_stop`) in
[BasePreparer.prepare_dt_obj](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.preparing.BasePreparer.prepare_dt_obj).
When you provide a date such as `"2024-01-01"` or a time such as `"16:00"`, the stop is marked at the
given datetime. When you provide a frequency such as `"daily"`, the stop is marked at the end of
the last bar of the current period. You can change this behavior by overriding `last_before` in the
argument config for that particular argument. Also, if you provide just a time, the marked datetime will
overflow to the next day if the requested time is before the current time. In addition, you can
dynamically adjust the argument config by passing `arg_config` as a regular argument,
and it will be merged with the existing argument config.

- Implemented the NB function [init_FSInOutputs_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.init_FSInOutputs_nb)
to initialize in-place outputs.

- Implemented the function [get_api_ref](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module_.get_api_ref),
which returns the link to the private website and source code for any VBT object.
Another function, [open_api_ref](https://vectorbt.pro/pvt_41531c19/api/utils/module_/#vectorbtpro.utils.module_.open_api_ref), performs
the same operation and automatically opens the link in a browser. If you provide a third-party object,
it parses the object's path and opens the first search result on DuckDuckGo.

- Fixed an error that was raised when providing an integer (rather than a floating-point) account state
field to `long_buy_nb`, `long_sell_nb`, `short_sell_nb`, or `short_buy_nb` in [portfolio.nb.core](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/).

- Most classes now have a method [Chainable.pipe](https://vectorbt.pro/pvt_41531c19/api/utils/chaining/#vectorbtpro.utils.chaining.Chainable.pipe)
that enables built-in and user-defined methods in method chains. Deep attribute retrieval is supported.

- Added a light and dark color palette toggle to the website :waxing_gibbous_moon:

- Wrote [Easily cross-validate parameters to boost your trading strategy](https://pyquantnews.com/easily-cross-validate-parameters-boost-strategy/) :notebook_with_decorative_cover:

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2023/
parent: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/
name: Release notes for 2023
type: page
---

# Release notes for 2023

All notable changes are listed in reverse chronological order.

## Version 2023.12.23

- Implemented [DuckDBDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/saver/#vectorbtpro.data.saver.DuckDBDataSaver) for periodically
storing data in a DuckDB database.

- Improved performance of [PathosEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.PathosEngine).

- Fixed [fit_pattern_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.fit_pattern_nb).

- Fixed staticization in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

- Implemented a merging function "imageio" that merges Plotly figures and generates and saves a GIF animation.

- UTC is no longer the default timezone in [AVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/av/#vectorbtpro.data.custom.av.AVData).

- Made all indicator packages optional in [IndicatorFactory.list_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_indicators).

- Fixed processing of `benchmark` in [QSAdapter](https://vectorbt.pro/pvt_41531c19/api/returns/qs_adapter/#vectorbtpro.returns.qs_adapter.QSAdapter).

- Custom indicators can now be registered with [IndicatorFactory.register_custom_indicator](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.register_custom_indicator),
listed with [IndicatorFactory.list_custom_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_custom_indicators),
and retrieved with [IndicatorFactory.get_custom_indicator](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.get_custom_indicator).
Any registered custom indicator will also be visible in [IndicatorFactory.list_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_indicators).

- Moved creation of projection bands to [GenericDFAccessor.band](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericDFAccessor.band).

- Implemented a function [ptable](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.ptable) to pretty-print
a DataFrame of any size. Displays HTML when run in a notebook.

- Developed annotation functionality in [utils.annotations](https://vectorbt.pro/pvt_41531c19/api/utils/annotations/) that is now used by
many VBT components to control the behavior of function arguments.

- Chunk taking specification `arg_take_spec` can now be parsed from function annotations.

- The sizer (argument `size` in [@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked)) no
longer needs to be provided, as it can be reliably parsed from most
[ChunkTaker](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ChunkTaker) instances.

- Standardized merging functionality and created a new module [utils.merging](https://vectorbt.pro/pvt_41531c19/api/utils/merging/).
Keyword arguments can now be distributed across multiple merging functions when provided as a tuple.
Merging functions can also be specified via annotations (strings work too!).

- [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) can now parse
[Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) instances from function annotations.

- [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply)
and its decorators can parse [Takeable](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Takeable)
instances from function annotations.

- Refactored chunking functionality by moving most logic from
[@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked) to a new class
[Chunker](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.Chunker), which stores the function to
be chunked and the chunking configuration. By calling [Chunker.run](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.Chunker.run),
the pipeline runs on the provided arguments. This allows the same pipeline to be run on different
sets of arguments. The main reason for this class is that it can be easily subclassed to change
the default chunking behavior; the new class can be passed (or set globally) as `chunker_cls`.

- Developed new user-friendly classes for chunking, such as [ChunkedArray](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.ChunkedArray).
They can be used in `arg_take_spec`, annotations, and can also be passed as arguments for the chunk
taking specification to be parsed on the fly.

- The `minp` argument in the exponential mean and standard deviation functions now refers
to the minimum number of observations in `span`, rather than from the start as before.

- [TVData.list_symbols](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData.list_symbols) can now optionally
return fields other than symbols.

- Fixed a DST issue that resulted in a duplicated index in [QSAdapter](https://vectorbt.pro/pvt_41531c19/api/returns/qs_adapter/#vectorbtpro.returns.qs_adapter.QSAdapter).

- Standardized the process of searching for objects in arguments. This functionality now resides in
[utils.search](https://vectorbt.pro/pvt_41531c19/api/utils/search/) and is used by parameterization and templating.

- Similarly to [Chunker](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.Chunker) being based on
[@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked), there is now
an equivalent class [Parameterizer](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Parameterizer)
based on [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized).

- Extended [combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params) to preferentially
generate parameter combinations lazily instead of picking them from a (potentially large) materialized grid.
This significantly reduces overhead when `random_subset` is provided.
For example, if you want to select 10 random combinations from three parameters with 1000 values each,
VBT will no longer build a grid of 1,000,000,000 parameter combinations :ok_hand:

- Adapted [AVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/av/#vectorbtpro.data.custom.av.AVData) to use the
[alpha_vantage](https://github.com/RomelTorres/alpha_vantage) library by default (if installed).
The API documentation parser can still be used by passing `use_parser=True`.

- Implemented a new data class [BentoData](https://vectorbt.pro/pvt_41531c19/api/data/custom/bento/#vectorbtpro.data.custom.bento.BentoData)
for [Databento](https://databento.com/).

- Fixed [SQLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData) for Pandas < 2.0.

- Added functions [disable_caching](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.disable_caching)
and [enable_caching](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.enable_caching)
to globally disable and enable caching, respectively. Also implemented context managers
[CachingDisabled](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CachingDisabled) and
[CachingEnabled](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CachingEnabled)
to disable or enable caching within a code block. Optionally, they can take a class,
instance, method, or any other cacheable object and switch the caching behavior only for that
object. Additionally, implemented "caching rules" that can be registered globally and applied to any
new setups automatically, making passive cache management possible (previously, rules could be enforced
only on setups that were already registered). Finally, renamed any methods with the suffix `status` or
`status_overview` to just `stats`.

- [NDLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ndl/#vectorbtpro.data.custom.ndl.NDLData) supports datatables
by passing `data_format="datatable"`.

- Wrote [Forecast future price trends (with projections)](https://pyquantnews.com/forecast-future-price-trends-with-projections/) :notebook_with_decorative_cover:

- Wrote [Easily cross-validate parameters to boost your trading strategy](https://pyquantnews.com/easily-cross-validate-parameters-boost-strategy/) :notebook_with_decorative_cover:

## Version 2023.10.20

- Implemented [Data.to_feather](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_feather) for storing
data in Feather files, and [FeatherData](https://vectorbt.pro/pvt_41531c19/api/data/custom/feather/#vectorbtpro.data.custom.feather.FeatherData)
for loading data from Feather files using PyArrow.

- Implemented [Data.to_parquet](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_parquet) for storing
data in Parquet files, and [ParquetData](https://vectorbt.pro/pvt_41531c19/api/data/custom/parquet/#vectorbtpro.data.custom.parquet.ParquetData)
for loading data from Parquet files using PyArrow or FastParquet.

- Implemented [Data.to_sql](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_sql) for storing
data in a SQL database, [SQLDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/saver/#vectorbtpro.data.saver.SQLDataSaver)
for periodically storing data in a SQL database, and [SQLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/sql/#vectorbtpro.data.custom.sql.SQLData)
for loading data from a SQL database using SQLAlchemy.

- Implemented [Data.to_duckdb](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_duckdb) for storing
data in a DuckDB database, and [DuckDBData](https://vectorbt.pro/pvt_41531c19/api/data/custom/duckdb/#vectorbtpro.data.custom.duckdb.DuckDBData)
for loading data from a DuckDB database.

- Implemented [Data.sql](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.sql) to run analytical SQL queries
on entire data instances using DuckDB.

- Improved file discovery in [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData)
and [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData).

- Extended the market scanning functionality of [TVClient](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient)
to allow filtering by market(s) as well as additional information such as capitalization.

- Redesigned [Data.transform](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.transform) to attach
the correct feature and symbol classes depending on the selected mode.

- Redesigned setting resolution in various classes.

- Added a new execution engine [MpireEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.MpireEngine)
based on [mpire](https://github.com/sybrenjansen/mpire).

- Refactored some execution engines, such as [PathosEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.PathosEngine).

- Implemented class methods [PortfolioOptimizer.row_stack](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.row_stack)
and [PortfolioOptimizer.column_stack](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.column_stack).

- Optimized [PortfolioOptimizer.from_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocations)
and added [PortfolioOptimizer.from_initial](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_initial).

- Added an option `settings.plotting.auto_rangebreaks` to automatically skip date ranges with no data
when plotting with `fig.show`, `fig.show_png`, and `fig.show_svg`.

- Added an argument `fit_ranges` in [PatternRanges.plot](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.plot)
to zoom in on one or more ranges.

- State, value, and returns can now be saved in dynamic from-signals as well.

- Fixed [SIGDET](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/sigdet/#vectorbtpro.indicators.custom.sigdet.SIGDET)
on two-dimensional data.

- Fixed division-by-zero errors in pattern similarity calculation.

- Renamed `latest_at_index` to `realign`, `resample_opening` to `realign_opening`, and
`resample_closing` to `realign_closing`.

- Added support for [Blosc2](https://github.com/Blosc/python-blosc2) as the new default
Blosc implementation.

- Added option `HardStop` in [StopExitPrice](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopExitPrice)
to ignore the open price check when executing a stop order.

- Fixed Common Sense Ratio ([ReturnsAccessor.common_sense_ratio](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.common_sense_ratio)).

- Implemented a method to resample returns: [ReturnsAccessor.resample_returns](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.resample_returns).

- Fixed merging when a single object is passed.

- Enabled negative `cash_earnings` to be lower than the current (free) cash balance.

- Made parsing of timestamps and datetime indexes with `dateparser` optional.

- Added an option `settings.importing.clear_pycache` to clear the Python cache on startup.

- Optimized accessors to initialize a [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping)
instance only once.

- Fixed parsing of `technical` indicators.

- Fixed and redesigned selection based on datetime components. Slices such as `"14:30":"15:45"`
now work properly.

- Switched to date-based release segments.

- Migrated from `setup.py` to `pyproject.toml`.

## Version 1.14.0 (1 Sep, 2023)

- Optimized simulation methods for better performance. Both [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
are now faster when the number of orders or signals is relatively low. This improvement was achieved by
allowing these methods to skip much of the simulation logic whenever a NaN order or no signal is encountered.

- Implemented an ultrafast simulation method based on signals—[from_basic_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_basic_signals_nb)—
which is used automatically if you do not use stop and limit orders.

- Implemented a module [portfolio.nb.ctx_helpers](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/ctx_helpers/) with helper functions
for use in callbacks such as `signal_func_nb`. For example, to get the current SL target price, use
`vbt.pf_nb.get_sl_target_price_nb(c)`.

- Introduced a new callback `post_signal_func_nb` to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
Similar to `post_order_func_nb` in [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func),
it is called right after an order has been executed, regardless of its success.

- The adjustment function now also runs in [Portfolio.from_holding](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_holding).

- Renamed `max_orders` to `max_order_records` and `max_logs` to `max_log_records`
in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

- Introduced an argument `ignore_errors` to ignore almost any error when optimizing with
PyPortfolioOpt and Riskfolio-Lib.

- Implemented a new [Splitter.from_n_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_n_rolling)
configuration that, when using `length="optimize"`, optimizes the window length with SciPy so that splits
cover most of the index and test ranges do not overlap (or follow any other set specified via `optimize_anchor_set`).

- Renamed `skip_minus_one` to `skip_not_found` in point- and range-based indexing.

- Added the following four callbacks to [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute):

    - `pre_execute_func`: Called only once before executing all calls and chunks.

    - `pre_chunk_func`: Called before executing a chunk (chunking must be enabled).

    - `post_chunk_func`: Called after executing a chunk (chunking must be enabled).

    - `post_execute_func`: Called only once after executing all calls and chunks.

- [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized),
[Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply) (along
with the corresponding decorators), and [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run)
allow the function to return `vbt.NoResult` for any iteration that should be skipped.

- Redesigned custom indicators :checkered_flag:

    - Each indicator now has a 1-dimensional Numba-compiled function, as well as a corresponding
    2-dimensional parallelizable, chunkable function. The 2D function also accepts parameters as
    flexible arrays, allowing one parameter value per column.

    - Removed caching functionality to avoid excessive specialization.

    - Indicator classes have been moved to their respective modules under the new sub-package [indicators.custom](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/).

- Implemented an [ADX](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/adx/#vectorbtpro.indicators.custom.adx.ADX) indicator.

- To quickly run and plot a TA-Lib indicator on a single set of parameter values without using
the indicator factory, use [talib_func](https://vectorbt.pro/pvt_41531c19/api/indicators/talib_/#vectorbtpro.indicators.talib.talib_func)
and [talib_plot_func](https://vectorbt.pro/pvt_41531c19/api/indicators/talib_/#vectorbtpro.indicators.talib.talib_plot_func).
These functions take the name of an indicator and return a function that can run or plot it.
Unlike the official TA-Lib implementation, they can properly handle DataFrames, NaNs,
broadcasting, and resampling. The indicator factory's `from_talib` and `from_expr` are
both based on these two functions.

- All indicators implemented with [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_apply_func)
accept the argument `split_columns` to run only one column at a time while retaining dimensionality,
and the argument `skipna` to automatically skip NaN values (requires `split_columns`).

- When passing a column full of NaN to TA-Lib, it previously raised an "inputs are all NaN" error. Now,
it will return an output array full of NaN instead.

- Indicators can return raw outputs instead of an indicator instance by setting the option `return_raw="outputs"`.
This can be used to return NumPy arrays directly when the Pandas format is not needed, and also
to return arrays of arbitrary shapes now that wrapping is no longer required. It also has less overhead,
since the instance preparation step is skipped.

- Fixed handling of NaN in [IndicatorFactory.from_wqa101](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_wqa101).

- Fixed the issue that previously prevented running all World Quant Alpha indicators at once (now possible).

- Redesigned label generators :checkered_flag:

    - Each generator now has a 1-dimensional Numba-compiled function and a corresponding 2-dimensional
    parallelizable, chunkable function. The latter function also accepts parameters as flexible arrays,
    allowing one parameter value per column.

    - Adapted the pivot generation function to be consistent with [PIVOTINFO](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/pivotinfo/#vectorbtpro.indicators.custom.pivotinfo.PIVOTINFO).

    - Most functions now accept `high` and `low` instead of just `close`.
    If you do not have `high` and `low`, pass `close` twice.

    - Generator classes have been moved to their respective modules under the new sub-package [labels.generators](https://vectorbt.pro/pvt_41531c19/api/labels/generators/).

- Redesigned signal generators :checkered_flag:

    - Variable arguments such as `*args` were converted into regular arguments like `place_args`.

    - Generator classes have been moved to their respective modules under the new sub-package [signals.generators](https://vectorbt.pro/pvt_41531c19/api/signals/generators/).

- Implemented a method [SignalsAccessor.distance_from_last](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.distance_from_last)
to measure the distance from the last n-th signal to the current element.

- Redesigned the [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) class :checkered_flag:

    - Data and any associated information can now also be stored by features instead of symbols,
    by wrapping the dictionary with [feature_dict](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.feature_dict).
    In this case, the information is considered feature-oriented, where features are keys and symbols are columns.
    A data instance becomes feature-oriented if [Data.data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.data)
    is feature-oriented.

    - The behavior of features and symbols is interchangeable. For example, [Data.select](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.select)
    will select a symbol if the instance is symbol-oriented, and a feature if the instance is feature-oriented.
    Indexing a column will select a feature if the instance is symbol-oriented, and a symbol otherwise.

    - [Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull) can run [Data.fetch_feature](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_feature)
    to fetch features instead of symbols, as does [Data.update](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update).
    To switch to features, pass `keys` (the first argument) with `keys_are_features=True` or `features`.

    - Any instance can be switched to the opposite orientation using [Data.invert](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.invert).

    - Renamed the method `Data.fetch` to `Data.pull` since fetching is just downloading, while pulling involves merging.
    The method `Data.fetch` still exists for backward compatibility and calls the new `Data.pull` under the hood.

    - Renamed `column_config` to `feature_config` because columns are symbols in a feature-oriented instance.

    - Renamed `single_symbol` to `single_key` because keys are features in a feature-oriented instance.

    - Renamed `symbol_classes` to `classes` because keys are features in a feature-oriented instance.

    - Data classes have been moved to their respective modules under the new sub-package [data.custom](https://vectorbt.pro/pvt_41531c19/api/data/custom/).

    - Redesigned unit tests for more extensive testing.

- [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) uses
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) to run a sequence of functions.
This can be used to parallelize feature engineering.

- Removed hard-coded timeframes from [TVClient](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient)
to allow custom timeframes such as "30 seconds".

- The token argument in [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData) has been
renamed from `token` to `auth_token` for consistency with TradingView.

- [OHLCVDFAccessor](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor)
and [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) now share the same functionality for
managing OHLCV-based features, eliminating duplicated code. Also, `column_map` has been renamed to `feature_map`
and now works in the opposite way: custom column names must point to standardized column names.

- Split [OHLCVDFAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.plot)
into [OHLCVDFAccessor.plot_ohlc](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.plot_ohlc)
and [OHLCVDFAccessor.plot_volume](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.plot_volume).

- A custom function located under `vbt.settings.plotting.pre_show_func` can be executed on a figure each
time before it is shown. This is useful for figure preparations.

- New debugging tool: To check how two complex VBT objects differ, use `obj1.equals(obj2, debug=True)`.

- Fixed issues with time-based stop orders for 32-bit Python on Windows.

## Version 1.13.1 (10 Jul, 2023)

- Enabled changing `cash_sharing` in [Portfolio.column_stack](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.column_stack).

- Removed the size types `MNTargetValue`, `MNTargetPercent`, and `MNTargetPercent100`.
The corresponding regular size types can be used for market-neutral behavior. See
[this](https://discord.com/channels/918629562441695344/1127185180386394145) discussion on Discord.

- Created the alias `grouped_index` for [Grouper.get_index](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper.get_index).

- The simulation methods of [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),
such as `from_signals`, now accept a symbol as the first argument, like "BTC-USD". This will fetch
the symbol using [YFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/yf/#vectorbtpro.data.custom.yf.YFData)
(or you can also provide the class name using `class_name:symbol`) and use it as data.
This is designed for quick backtesting.

- Fixed a regular expression error that appeared on Python 3.11.

- Improved error handling when authenticating with [TVClient](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient).

## Version 1.13.0 (04 Jul, 2023)

- Added Python 3.11 support.

- Rewrote all functions implemented with `numba.generated_jit` (deprecated) to use overloading.

- Fixed backward compatibility for Pandas 1.*.

- Conversion of datetimes and timedeltas to integer timestamps has been standardized
across the entire codebase using [to_ns](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.to_ns).

- Fixed handling of duplicate allocation indices in [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer).

- The index column can be disabled in [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData).

- [Data.transform](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.transform) can now be run per symbol.

- Replaced the field `peak_idx` with `start_idx` and `peak_val` with `start_val` in [Drawdowns](https://vectorbt.pro/pvt_41531c19/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)
for better compatibility. Also, renamed `drawdown_ranges` to `decline_ranges`.

- More methods for working with indexes now support [ExceptLevel](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.base.indexes.ExceptLevel).

- [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades) can plot multiple
columns at once if `group_by=True` is provided to the method.

- Implemented [ExtPandasIndexer](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.ExtPandasIndexer)
as an extension class to [PandasIndexer](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.PandasIndexer),
which adds an additional indexing property `xloc` that uses
[get_idxs](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.get_idxs).

- The argument `fps` in [save_animation](https://vectorbt.pro/pvt_41531c19/api/utils/image_/#vectorbtpro.utils.image_.save_animation)
is now automatically converted to `duration`.

- The arguments `td_stop` and `dt_stop` can be accepted as parameters with multiple strings,
such as `vbt.Param(["daily", "weekly", "monthly"])`.

- Range breaks can be detected and applied automatically with `fig.auto_rangebreaks()`.

- Cumulative returns can be plotted in percentage scale by enabling `pct_scale`.

- Drawdowns can be plotted in absolute scale by disabling `pct_scale`.

- Changed the default start value for all Numba-compiled return functions from 0 to 1.

- Fixed `bm_close` not being resolved properly in [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).

- Changed parameter selection in [@cv_split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.cv_split)
from `argmin` and `argmax` to `nanargmin` and `nanargmax`, respectively.

- Symbols in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) can now be tuples.
Level names can be provided via `level_name`.

- Fixed dynamic resolution of stop entry price.

- Implemented product of indexes ([cross_indexes](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.base.indexes.cross_indexes))
and product of DataFrames ([BaseAccessor.cross](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.cross),
also as [BaseAccessor.x](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.x)).

- Implemented periodic datetime indexing through datetime components
([DTC](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_/#vectorbtpro.utils.datetime_.DTC)) and Numba functions. The new indexer
class [DTCIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.DTCIdxr) can receive a datetime component
or a sequence or slice of such and translate them into positions. It can also parse datetime-like strings
such as "12:00", "thursday", or "january 1st" using `dateutil.parser`.

- Multiple indexers of the same type can be combined using logical operations, such as
`vbt.autoidx(...) & vbt.autoidx(...)`. Both operands are used as masks.

- Indexers can be used to modify data. This feature is supported by
[BaseAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor), which can modify
the object in place, for example with `obj.vbt.iloc[...] = ...`.

- Added `rescale_to` argument to
[PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func) and
[PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func)
to rescale all weights generated by the callback to a given min-max range.

- Numba functions implementing crossovers can take the second array as a scalar or in
any other flexible format.

- Implemented a method ([Data.realign](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.realign))
to align data to any frequency or custom index. By default, open price is aligned with
[GenericAccessor.realign_opening](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_opening),
and any other feature is aligned with [GenericAccessor.realign_closing](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign_closing).
This is useful when symbols are scattered across different times or timezones.

- Extended the [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) decorator
with _mono-chunks_. Parameter combinations can be split into chunks, and each chunk can be merged
to allow passing multiple parameter combinations as a single array, making use of column stacking.
This approach can trade more RAM for better performance. The function itself must be adapted to take
arrays rather than single values.

- Added `warmup` argument to [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute)
to dry-run one function call.

- Fixed behavior of [Portfolio.get_market_value](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_market_value)
when a column consists entirely of NaN.

- [Data.to_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_csv) and
[Data.to_hdf](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.to_hdf) can take `path_or_buf` and `key` as templates.

- Added `run_arg_dict` to specify keyword arguments per function name in [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run)
(as an alternative to `run_func_dict`, which specifies function arguments per argument name).

- Wrote the first part of the [Cookbook](https://vectorbt.pro/pvt_41531c19/cookbook/) :salad:

## Version 1.12.0 (25 Apr, 2023)

- [Data.pull](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.pull) now also accepts as the first argument
a dictionary where symbols are keys and keyword arguments are values.

- [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData) now accepts symbols
in the format `EXCHANGE:SYMBOL`.

- Added support for broadcasting along one axis in [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast).

- Added support for wrapping multiple arrays with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
and broadcasting them in [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast).
Arrays of various shapes are stacked along columns and automatically expanded to the largest shape
by padding. Previously, only single-value parameters could be tested.

- Renamed the key "alpha_vantage" to "av" in [settings.data](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.data).

- Renamed `capture` to `capture_ratio` in [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor).

- Added `user_agent` argument to [TVClient](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient).

- Fixed [unstack_to_array](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.unstack_to_array)
producing incorrect mappings when sorting is disabled.

- Added merging functions "reset_column_stack", "from_start_column_stack", and "from_end_column_stack".

- Fixed the error in [Trades.plot_running_edge_ratio](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.plot_running_edge_ratio).

- Implemented stop laddering in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
By enabling `stop_ladder` (or using an option from [StopLadderMode](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.StopLadderMode)),
__each__ stop type now behaves like a ladder. Multiple ladder steps can be provided
as an array (either one-dimensional, or two-dimensional with ladder steps defined per column).
Additionally, ladders of different sizes can be wrapped with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
to test arbitrary combinations. Defining a ladder per row is not supported (this is possible only dynamically).
Ladders with a single value will function as before.

- Implemented [GenericSRAccessor.to_renko_ohlc](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericSRAccessor.to_renko_ohlc)
to convert a Series into an OHLC DataFrame in the Renko format.

- Parameters wrapped with [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) now have
a `hide` flag to hide the parameter from the product index, and a `map_template` argument
to generate its values dynamically.

- With automatic initial cash and no expenses, the determined initial cash will now be 1 instead of 0 for
correct calculation of returns.

- Implemented market-neutral size types `MNTargetValue`, `MNTargetPercent`, and `MNTargetPercent100`.

- The method [Splitter.take_range](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take_range)
takes an argument `point_wise` that, when True, selects an index at a time and returns a tuple of the selected values.

- Implemented classes for preparing functions and arguments. These classes take arguments, preprocess them,
broadcast them, postprocess them, substitute templates, prepare the target function, and
build the target set of arguments. All these steps run inside cached properties so that none of them
are executed twice. The result of a preparation can be easily changed and passed for execution.
All portfolio methods accept as the first argument either the preparer or its result.
The following classes were implemented:

    - [BasePreparer](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.preparing.BasePreparer): base class for vectorbtpro.

    - [BasePFPreparer](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.portfolio.preparing.BasePFPreparer): base class for [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

    - [FOPreparer](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.portfolio.preparing.FOPreparer): for preparing [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders).

    - [FSPreparer](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.portfolio.preparing.FSPreparer): for preparing [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

    - [FOFPreparer](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.portfolio.preparing.FOFPreparer): for preparing [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).

    - [FDOFPreparer](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.portfolio.preparing.FDOFPreparer): for preparing [Portfolio.from_def_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_def_order_func).

- When providing keyword arguments, they are generally merged with default keyword arguments to override them.
But how can you completely remove a key from the default keyword arguments? To do so, pass
[unsetkey](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.unsetkey) as the value of the key you want to remove.

- Complex VBT objects such as [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)
sometimes take other VBT objects like [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper)
as arguments. To change something deep inside the object, you previously had to manually and 
recursively rebuild the objects. For example: `wrapper.replace(grouper=wrapper.grouper.replace(group_by=True))`
to enable grouping. To simplify this, you can now pass `nested_=True` and provide the specification
as a nested dict. For example: `wrapper.replace(grouper=dict(group_by=True), nested_=True)`.

- Fixed `pivots` and `modes` in [PIVOTINFO](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/pivotinfo/#vectorbtpro.indicators.custom.pivotinfo.PIVOTINFO).

- Added a `delisted` argument to [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data). Symbols
with this argument enabled will not get updated.

- The object in [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
can be a template. For example, you can do `splitter.take(vbt.RepEval("x.iloc[range_]", context=dict(x=x))`.

- Switched to the newer paging feature in [TVClient.search_symbols](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient.search_symbols)
to download the complete set of symbols.

- Completely refactored indexing. Instead of converting complex indexing specifications to integer positions
in a single method `get_indices`, each kind of indexing (also called "indexer") is implemented in its own class.
For example, [LabelIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.LabelIdxr) is an indexer for labels, while
[DatetimeIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.DatetimeIdxr) is an indexer for datetime.
The indexer [AutoIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.AutoIdxr) determines the kind automatically.
Such indexers are generic and can be passed to [RowIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.RowIdxr)
to notify vectorbtpro that you want to select a row, and [ColIdxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.ColIdxr)
to select a column. The indexer [Idxr](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.Idxr)
can take any of the above, or a row and column indexer. For example, `vbt.Idxr(0)` will select the first row
and all columns, while `vbt.Idxr(0, "BTC-USD")` will select the first row of the column labeled "BTC-USD" only.

!!! note
    In your code, replace `RowIdx` with `rowidx`, `ColIdx` with `colidx`, `RowPoints` with `pointidx`,
    `RowRanges` with `rangeidx`, and `ElemIdx` with `idx`.

- Implemented the class [IdxSetter](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.IdxSetter), which
takes a list of tuples with indices (see above) and values, creates a new array, and modifies it
by setting each value at the corresponding index. Previously, this was done using
`ArrayWrapper.fill_using_index_dict`. Also, renamed `fill_using_index_dict` to
[ArrayWrapper.fill_and_set](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.fill_and_set).

- Implemented the class [IdxSetterFactory](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.IdxSetterFactory),
which can generate one or more [IdxSetter](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.IdxSetter) instances
from an index dictionary, Series, DataFrame, or a sequence of records. This allows you
to pass the `records` argument to any [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
method to unfold one or more arguments from it.

- [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals) now accepts
the new arguments `long_entries` and `long_exits`, which can replace (or enhance) `entries` and `exits`,
respectively.

- When any broadcastable argument in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
is provided as an index dictionary, the default value is now taken from the global settings rather than
being hard-coded, except for the `size` argument in [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders).
This argument is always NaN when a new array is created or an existing array is reindexed.

- Added the `layout` argument to the methods of the Plotly Express accessor to change the layout of any figure.

- Removed the `date_parser` argument from [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData)
since it is incompatible with Pandas 2.0.

- Added support for Pandas 2.0 :flying_saucer:

- Updated the dark theme of the website and graphs :waxing_crescent_moon:

## Version 1.11.1 (18 Mar, 2023)

- Fixed the staticization paths on Windows.

- Renamed class methods of [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
from `get_indicators` to `list_indicators`.

- Renamed class methods of [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data)
from `get_symbols` to `list_symbols`.

- Implemented various methods for better indicator search:

    - [IndicatorFactory.list_vbt_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_vbt_indicators)
    lists all VBT indicator names.

    - [IndicatorFactory.list_indicators](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.list_indicators)
    lists all supported indicator names.

    - [IndicatorFactory.get_indicator](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.get_indicator)
    and its shortcut `vbt.indicator` find and return any indicator by name.

- Enhanced indicators with the following:

    - [IndicatorBase.param_defaults](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.param_defaults)
    gets the parameter defaults of an indicator.

    - [IndicatorBase.unpack](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.unpack)
    returns the outputs of an indicator as a tuple.

    - [IndicatorBase.to_dict](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.to_dict)
    returns the outputs of an indicator as a dictionary.

    - [IndicatorBase.to_frame](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.to_frame)
    returns the outputs of an indicator as a DataFrame.

- Refactored [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) to use the new indicator search.

- Providing `tz` to [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) will localize or convert
each date to this timezone. This is especially useful when data contains dates with different timezones
(for example, due to daylight savings).

- All record classes subclassing [PriceRecords](https://vectorbt.pro/pvt_41531c19/api/generic/price_records/#vectorbtpro.generic.price_records.PriceRecords)
now have new fields describing the bar of the record. For example, you can now
get the low price of the bar of each order using `pf.orders.bar_low`.

- Implemented [MappedArray.to_readable](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.to_readable),
which is equivalent to [Records.records_readable](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.records_readable).

- Implemented [Orders.price_status](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.orders.Orders.price_status),
which returns whether the price of each order exceeds the bar's OHLC.

- Fixed `call_seq` being materialized even with `attach_cal_seq=False` in
[Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).

- Added a progress bar to [PathosEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.PathosEngine).
It is disabled by default. To enable it, pass `engine_config=dict(show_progress=True)`.

- Implemented [Ranges.crop](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.crop)
to remove irrelevant OHLC data from records, which is especially useful for plotting high-granular data.

## Version 1.11.0 (15 Mar, 2023)

- Made `run_pipeline` a class method of [IndicatorBase](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase)
so you can be override it.

- Implemented the accessor method [GenericAccessor.groupby_transform](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.groupby_transform)
to group and then transform a Series or DataFrame.

- Implemented new trade properties and methods:

    - [Trades.best_price_idx](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.best_price_idx)

    - [Trades.worst_price_idx](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.worst_price_idx)

    - [Trades.expanding_best_price](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.expanding_best_price)

    - [Trades.expanding_worst_price](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.expanding_worst_price)

    - [Trades.expanding_mfe](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.expanding_mfe)
    with `plot_` and `_returns` versions.

    - [Trades.expanding_mae](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.expanding_mae)
    with `plot_` and `_returns` versions.

- Data classes that work with the file system and can discover file paths now subclass
the special data class [FileData](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData).

- Moved [RemoteData.from_csv](https://vectorbt.pro/pvt_41531c19/api/data/custom/remote/#vectorbtpro.data.custom.remote.RemoteData.from_csv)
and [RemoteData.from_hdf](https://vectorbt.pro/pvt_41531c19/api/data/custom/remote/#vectorbtpro.data.custom.remote.RemoteData.from_hdf) to
[Data.from_csv](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_csv)
and [Data.from_hdf](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.from_hdf), respectively.

- [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
now supports new time-based stop orders provided via two arguments: `td_stop` for timedelta-like inputs
and `dt_stop` for datetime-like inputs. Both are similar to `limit_tif` and `limit_expiry`, respectively,
with the distinction that a time-based stop is executed one tick before the actual deadline.

- Greatly improved the resolution of `dt_stop` and `limit_expiry`. They can now distinguish
between timedelta and datetime strings, and also accept ISO time strings such as "16:00".

- Renamed simulator functions:

    - `simulate_from_orders_nb` → `from_orders_nb`

    - `simulate_from_signals_nb` → `from_signals_nb`

    - `simulate_from_signal_func_nb` → `from_signal_func_nb`

    - `simulate_nb` → `from_order_func_nb`

    - `simulate_row_wise_nb` → `from_order_func_rw_nb`

    - `flex_simulate_nb` → `from_flex_order_func_nb`

    - `flex_simulate_row_wise_nb` → `from_flex_order_func_rw_nb`

- The arguments `order_func_nb` and `order_args` are now optional in [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).
Also, the argument `flexible` has been removed in favor of `flex_order_func_nb` and `flex_order_args`.

- Integrated order mode (`order_mode=True`) directly into [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
enabling the use of target sizes instead of signals. Under the hood, it uses a dynamic
signal function that translates orders into signals.

- Class methods in [SignalsAccessor](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor)
that accept `shape` as the first argument can now also accept an array wrapper via this argument.

- Scheduler now supports zero_offset for seconds.

- Previously, the trailing target price was updated according to a percentage stop.
Now, it is updated according to an absolute stop.

- [SignalContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext) has a field
`fill_pos_info` of the type [trade_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.trade_dt),
which holds information about the current position such as average entry or exit price and open statistics.

- Fixed stop exit price provided as a floating array.

- Each stop can define its own exit size and size type via `exit_size` and `exit_size_type`, respectively.
This can only be used dynamically.

- Each stop can define its own ladder. By setting `ladder=True` in the information record,
the stop will not be cleared after execution. Upon execution, the current ladder step index
and row are automatically updated in `step` and `step_idx`, respectively. These values can be used
to update the current step with new information. This feature can only be used dynamically.

- Fixed the time resolution of stop orders among themselves. Previously, for example, if an SL
stop was marked for execution at the closing price and a TP stop was marked for execution at the opening
price, the SL stop was executed (pessimistically). Now, the TP stop will be executed since it precedes
the SL stop in time.

- Implemented [TVClient.scan_symbols](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient.scan_symbols)
and the corresponding `market` argument in [TVData.list_symbols](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData.list_symbols)
to scan a market and return the full list of symbols being traded in that market.

- __Important!__ To avoid confusion, passing an index (`pd.Index`) to the broadcaster will no longer behave
like a parameter ([Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)).
Now, an index will be converted into a Series with the same index and values.

!!! example
    If you previously used `pd.Index([0.1, 0.2])` to test multiple values, now use `vbt.Param([0.1, 0.2])`.

- Implemented the module ([utils.source](https://vectorbt.pro/pvt_41531c19/api/utils/source/)), which specializes in "cutting" annotated
sections from any code. It can cut out a block of code and also transform it
based on special commands annotated as comments in the code. This means you no longer need
to duplicate lengthy code, since you can copy, transform, and paste it to any Python file
in a fully automated fashion.

- Thanks to the cutting feature, [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
and [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func)
can now be instructed to transform their non-cacheable code to cacheable code. This process
is called "staticization" and can be enabled simply by passing `staticized=True`. For this to work,
all Numba simulation functions that take callbacks as arguments were annotated beforehand - by me :v:
After extraction and transformation, the new cacheable code is saved to a Python file in the same directory
(by default). Built-in callbacks are automatically included in the file as well.
User-defined callbacks are automatically imported at the beginning of the file, provided
they are defined in a Python file and not in a Jupyter notebook.

## Version 1.10.3 (26 Feb, 2023)

- Added the `adjustment` argument to [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData).

- Disabled searching for the earliest date in [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData).
You can still enable it by passing `find_earliest_date=True`. Also, both `start` and `end` are now None,
since some exchanges do not accept 0 as a start timestamp.

- Added various helper functions for working with the file system in [utils.path_](https://vectorbt.pro/pvt_41531c19/api/utils/path_/).

- Added `log_returns`, `daily_returns`, and `daily_log_returns` to both [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data)
and [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

- Scattergl is now disabled by default because it causes issues when one trace has it enabled and another does not.
You can still enable `use_gl` globally under `vbt.settings.plotting`.

- Fixed a circular import error when attempting to load a configuration file
with pickled data on startup.

- Fixed size granularity not being properly applied due to rounding errors.

- Fixed positions not being properly reversed when size granularity is enabled.

- [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) can now take
`sim_out` as the second argument and extract order records and other arrays from it.

- Added the `random_subset` argument to [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
to select a random subset from a single parameter.

- Added [Optuna](https://optuna.org/) as an optional dependency.

- Fixed split and set selection in [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
and [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply).

- Fixed the calculation of the grouped gross exposure for both directions.

- Fixed an issue with infinite leverage and order fees greater than the available free cash.

- Implemented NB and SP rolling z-score ([GenericAccessor.rolling_zscore](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_zscore)).

- Wrote [Pairs trading](https://vectorbt.pro/pvt_41531c19/tutorials/pairs-trading) :notebook_with_decorative_cover:

## Version 1.10.2 (15 Feb, 2023)

- Fixed cases where the size type was not being checked properly when resolving signals.

## Version 1.10.1 (15 Feb, 2023)

- Fixed cases where leverage was not being applied when reversing positions.

- Timezones are now parsed with Pandas instead of `zoneinfo` to avoid issues with Python 3.8.

## Version 1.10.0 (14 Feb, 2023)

- Added the `use_class_ids` argument to represent classes by their IDs instead of converting them
into a stream of bytes when saving to or loading from a config file.

- Fixed the "expected return exceeding the risk-free rate" error in the latest PyPortfolioOpt.

- Fixed the timezone in [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData).

- Renamed `AlphaVantageData` to `AVData`.

- Made grid lines darker in the dark theme.

- Outputs of [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) can now directly return indicator
outputs instead of indicator instances.

- Price types "nextopen" and "nextclose" can now be provided per column.

- Bar skipping in [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
now happens automatically.

- Fixed the color schema in the Seaborn theme.

- Added a signal function to translate target size types into signals. Also added the `pf_method`
option in [Portfolio.from_optimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_optimizer)
to rebalance using [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

- Added warnings when `init_position` is provided without `init_price`.

- Made accessors wrap their own objects much faster.

- Added a new execution engine, [PathosEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.PathosEngine),
that uses [pathos pools](https://pathos.readthedocs.io/en/latest/pathos.html#module-pathos.pools) for
better pickling.

- Renamed `SequenceEngine` and its identifier "sequence" to `SerialEngine` and "serial", respectively.

- Fixed the resolution of asset classes in [riskfolio_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.riskfolio_optimize)
after dropping NaN columns.

- Completely refactored the importing mechanics. For example, auto-import can now be disabled in the settings
to load VBT in under one second. Also, whether to make an object available via `vbt.[object]` is now
determined by the module where the object is defined using `__all__`. This also greatly improves type hints
in IDEs.

- There are two new shortcuts:

    - `vbt.PF` for `Portfolio`.

    - `vbt.PFO` for `PortfolioOptimizer`.

- Fixed best and worst price in [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades)
to include both entry and exit price. This also improves the (running) edge ratio, MAE, and MFE metrics.

- Renamed the argument `stop_price` to `init_price` in `check_stop_hit_nb`.

- Merged the `OLS` and `OLSS` indicators into a single `OLS`.

- Datetime-like strings are now mainly parsed using Pandas instead of the `dateparser` package.

- Made weekdays start from 0 in Numba methods by default, matching Pandas.

- Added three new methods for convenience (all support integer and timedelta lookback periods):

    - [GenericAccessor.ago](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.ago) - 
    get the value `n` periods ago.

    - [GenericAccessor.any_ago](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.any_ago) - 
    check whether a condition was True at any time in the last `n` periods.

    - [GenericAccessor.all_ago](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.all_ago) - 
    check whether a condition was True at all times in the last `n` periods.

- Simplified signal generation. You no longer need to manually set the signal in `out`; returning
its (relative) index is enough. Entry signal generators also now include new options `only_once` and `wait`
for full compatibility with exit signal generators.

- Completely refactored [RandomOHLCData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random/#vectorbtpro.data.custom.random.RandomData)
and [GBMOHLCData](https://vectorbt.pro/pvt_41531c19/api/data/custom/gbm_ohlc/#vectorbtpro.data.custom.gbm_ohlc.GBMOHLCData).
There is no longer a need to explicitly specify the frequency for resampling, as it now defaults to 50 ticks per bar.
The number of ticks per bar can also be specified as an array for greater flexibility.

- Limit and stop values in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
can now be provided as a target price when `delta_format="target"`.

- Greatly improved approximation of order values for automatic call sequences.

- [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO) and [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
are now completely independent of each other.

- Dropped Python 3.7 support.

- The default timezone can now be set using `tz` when fetching data. This has two effects:

    1. Any `start` and `end` date provided as a string without a timezone will be assumed to be in this default timezone.

    2. Output data will be converted to this timezone.

- Timeframe will now be converted into a frequency and saved in the wrapper when fetching data.
This way, if the fetched data has gaps, VBT objects such as portfolio will still know the true frequency.

- You no longer need to build a resampler to resample to a target index; passing the target index
directly is now supported.

- Renamed `fill_state` to `save_state`, `fill_returns` to `save_returns`, and added the `save_value` argument
to save the portfolio value in [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

- Refactored parameter generation. The function [combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params)
can now be used to generate parameters without building the index. Filtering conditional parameters
and selecting a random subset are also much faster, especially when used together.

- Renamed `entry_place_func` to `entry_place_func_nb` and `exit_place_func` to `exit_place_func_nb`.

- The [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function can now put multiple
calls into the same chunk using `distribute="chunks"`, execute them serially, and distribute the
chunks themselves. This makes certain engines, such as those for multiprocessing, much more effective.
Also, `tasks` can be provided as a template that is substituted once chunking metadata is
available (requires `size`).

- Reduced memory footprint when indicators execute many parameter combinations.

- Refactored the jitted loop in the indicator factory. Multiple parameter combinations can now be handled
in a single jitted loop, and multiple jitted loops can be distributed using
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute). The `jitted_loop` argument can now
be toggled when running an indicator, and there is a new `jitted_warmup` argument to run one parameter combination
for compilation before running everything else.

- Datetime is now mainly parsed using `pd.Timestamp` instead of `pd.to_datetime`.

- Improved parsing of datetime-like and timezone-like objects.

- Implemented the function [pdir](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.pdir)
to beautifully print the attributes of any Python object (including packages and modules!).
Use this function with `phelp` and `pprint` to discover objects.

- Added the `dropna` option when calculating crossovers to ignore NaNs (the default is to keep them).

- Enhanced [BaseAccessor.eval](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.eval)
to support multiline expressions.

- Added support for various pickle compression algorithms. Each algorithm has its own
set of file extensions that can be automatically recognized to decompress the saved data.
For example, pickling with the `blosc` compression will add the extension `.pickle.blosc`.

- Added the ability to select a date range with `start` and `end` in local data classes.
This can be done before loading data in [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData)
(efficient) and only after loading data in [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData)
(not as efficient).

- The default PyTables format is now "table".

- [YFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/yf/#vectorbtpro.data.custom.yf.YFData) now automatically recognizes the timezone
of each ticker and uses it. If tickers have different timezones, either fetch them separately,
or provide a unified timezone with `tz_convert`.

- Vectorbt will now not only check if optional packages are installed, but also
verify whether their versions match the required ones for full compatibility.

- Renamed `deep_substitute` to `substitute_templates`.

- Iterating over symbols when fetching or updating data is now managed by
[execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute), enabling distribution.
For example, using `execute_kwargs=dict(cooldown=1)` will sleep for one second after fetching each symbol
since it is now executed with [SerialEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.SerialEngine).

- Updated [Features](https://vectorbt.pro/pvt_41531c19/features) with dozens of new examples :notebook_with_decorative_cover:

## Version 1.9.0 (15 Jan, 2023)

- Implemented leverage. Two leverage modes are available:

    - `Lazy` :snail:: Uses available cash whenever possible and only uses leverage if it is not
    enough to fulfill the entire operation. This effectively increases available cash with debt.

    - `Eager` :rabbit2:: Uses leverage even if the whole operation could be fulfilled with available
    cash. This is similar to opening a micro-account with a portion of the available cash and then using
    lazy leverage on it.

- Leverage can also be infinite (`leverage=np.inf`) to fulfill any position size requirement, as long as
there is at least some free cash. You are responsible for choosing an appropriate position size.

- Refactored the lowest-level simulation logic:

    - Added [AccountState.locked_cash](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState.locked_cash),
    which, along with [AccountState.debt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState.debt),
    tracks the current leverage and shorting state. Both are used for the long and short directions.
    Remember to initialize it with zeros and to add it to
    [AccountState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState)
    and [ExecState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ExecState).

    - Buy and sell functionality is now distributed over four distinct functions
    (none of which support position reversal):

        - [long_buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.long_buy_nb)

        - [long_sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.long_sell_nb)

        - [short_sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.short_sell_nb)

        - [short_buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.short_buy_nb)

    - The functions [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb)
    and [sell_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.sell_nb) orchestrate these
    to enable position reversal.

    - Regular cash ([AccountState.cash](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState.cash))
    is no longer used in buy and sell operations, only in equity calculations. This is because it
    can fall below zero when leverage is used. Transactions now only use free cash
    ([AccountState.free_cash](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState.free_cash)).
    This also means free cash (and not regular cash) must be greater than zero to allow most operations.

    - Free cash is not guaranteed to stay at or above zero. Some operations, such as partially closing
    leveraged or short positions, may cause it to become negative, effectively offsetting costs to other columns.

    - The order in which account/execution state and order result are returned has been reversed
    (now `order_result, new_account_state` instead of `new_account_state, order_result`).

    - Renamed and reordered log fields.

    - When reversing positions, [SizeType.Percent](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType.Percent)
    now refers to the available resources after closing the current position. For example, in a long
    position that can be reversed, 50% means _close the current position and open a short one using
    50% of the available free cash left after closing_.

- The limitation in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
that prevented reversing positions when [SizeType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SizeType)
is `Percent` or `Percent100` has been removed.

- Some simulation functions now accept a `fill_state` argument, which can fill position, cash, debt,
and other fields from [AccountState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState) as arrays
to avoid reconstructing them later.

!!! note
    Reconstruction of free cash will only yield the same result as during the simulation if leverage is disabled.
    If leverage is enabled, use `fill_state` to pre-compute the array.

- Returns for negative positions will no longer be flipped, making them consistent with `pct_change`.
This change ensures equity and cumulative returns produce the same plot, even when equity falls below zero.

- Refactored flexible indexing due to an unresolved Numba bug:

    - The `flex_select_auto_nb` function is no longer available. Instead, there is now a full module
    [base.flex_indexing](https://vectorbt.pro/pvt_41531c19/api/base/flex_indexing/) with Numba functions tailored to different array formats.

    - Removed the `flex_2d` argument completely.

    - It is no longer necessary to wrap scalars with zero-dimensional NumPy arrays. Most Numba functions
    will convert any input into a one-dimensional or two-dimensional NumPy array. Here are the general
    rules depending on the annotation of an argument:

        - `FlexArray1dLike`: can be passed as a scalar or array that can broadcast to one dimension.

        - `FlexArray2dLike`: can be passed as a scalar or array that can broadcast to two dimensions.

        - `FlexArray1d`: must be passed as a flexible one-dimensional array.

        - `FlexArray2d`: must be passed as a flexible two-dimensional array.

- __Important!__ From now on, one-dimensional arrays will always broadcast against rows, as VBT
is primarily designed for time-series data. This makes VBT's broadcasting mechanism different
from NumPy's. A range of helper functions for array and shape broadcasting was created in the module
[base.reshaping](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/). Also, all VBT functions now use in-house
broadcasting functions, not NumPy's.

!!! example
    Previously, if a list `["longonly", "shortonly", "both"]` was applied per column
    and used to test multiple position directions, now the same list will be applied per row.
    Thus, use `[["longonly", "shortonly", "both"]]`.

- Made data typing errors in portfolio more descriptive.

- [Splitter.from_rolling](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_rolling)
now has an option to roll backwards.

- Fixed [Resampler.map_bounds_to_source_ranges](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.map_bounds_to_source_ranges)
returning incorrect values when `skip_not_found=True`.

- Renamed the argument `to_py_timezone` to `to_fixed_offset`.

- Series names are now automatically stringified in
[QSAdapter](https://vectorbt.pro/pvt_41531c19/api/returns/qs_adapter/#vectorbtpro.returns.qs_adapter.QSAdapter).

- The author of [tvdatafeed](https://github.com/StreamAlpha/tvdatafeed) has removed the package from
PyPi, so it cannot be installed with `pip` anymore. As a result, the package has been integrated
directly into VBT (see [data.custom.tv](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/)) and additionally refactored.

- The TradingView client [TVClient](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVClient) now uses the pro data
feed by default, making it possible to pull 20,000 data points at once, even without signing in.
If you want to sign in, you can use your username and password or manually parse the token from the
TradingView website and use it (for example, if proper authentication requires you to complete a captcha).

- Implemented functions [pprint](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.pprint)
and [phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp) that perform `print` and `help`
with improved formatting.

- Earliest date detection in [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData)
has been improved for exchanges that return the latest data point instead of the earliest
one when `limit=1` is used.

- Calculating and plotting allocations no longer requires grouping.

- Input arrays in indicators built with [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
are not cacheable by default, enabling pickling of indicator class definitions.

- The argument `order_records` now comes before the argument `close` in the constructor method of the
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio). Make sure to use
keyword arguments to avoid any issues.

- [MappedArray.to_pd](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.to_pd)
can take a reducing function or its name to automatically aggregate data points that fall into the same row.
This is especially useful for resampling P&L, where `reduce_func_nb="sum"` can be specified.

- The package [python-telegram-bot](https://github.com/python-telegram-bot/python-telegram-bot) for building
Telegram bots has released version 2.0.0, which introduces many breaking changes.
Although VBT has adapted to this version, the bot cannot run in the background due
to `asyncio` (if you know a fix, please let me know). If you need this functionality, install the latest
version of `python-telegram-bot` before 2.0.0, and VBT will switch to the previous functionality automatically.
The tutorial still requires the previous version until this limitation is fixed.

- Fixed errors when no symbol could be fetched in custom data classes.

- Refactored configs and pickling (persisting to disk):

    - Config options can no longer be passed directly; they need to be packed as `options_`.

    - Most VBT objects can now be pickled and unpickled using `pickle` or `dill`, without any
    extra preparation. The process is straightforward: each object defines its own reconstruction state
    of type [RecState](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.RecState), which is a tuple
    of positional arguments, keyword arguments, and attributes needed to create an identical instance.
    Only this state is pickled and unpickled, and together with the class, it is used to reconstruct
    the object using [reconstruct](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.reconstruct).
    This allows for maximum flexibility, as VBT objects can now be saved in lists, dicts, and even
    as values of DataFrames.

    - There is no longer a need to pickle class definitions: only the path to the class is pickled,
    making dumps lighter and more robust to API changes.

    - New functionality allows you to change how a pickled object is reconstructed if its class
    or arguments have changed.

!!! warning
    If you have any VBT objects pickled by previous versions of VBT, they will not be unpickled
    by the new version. It is recommended to either recalculate them using the new version, or first
    unpickle them with the previous version, save their components separately, and then import and connect
    them using the newer version. Contact me if you encounter any issues.

- Not only can VBT objects be seamlessly converted into a stream of bytes (i.e., pickled),
but now they can also be converted into regular configuration files with extensions `.config`,
`.cfg`, and `.ini`! For this, VBT extends the `configparser` functionality with its own parsing rules
(see [Pickleable.encode_config](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.encode_config)
and [Pickleable.decode_config](https://vectorbt.pro/pvt_41531c19/api/utils/pickling/#vectorbtpro.utils.pickling.Pickleable.decode_config)).
Since most VBT objects can be represented using dictionaries, configuration files are an ideal
text format where sections represent (sub-)dictionaries and key-value pairs
represent dictionary items. In addition, the following features were implemented:

    - Parsing of literals (strings `"True"` and `"100.0"` are recognized as bool and float respectively).

    - Nested dictionaries (section `[a.b]` becomes a sub-dictionary of the section `[a]`).

    - References (a key can reference another value in another section or even in the same section,
    so a reference `&a.b` will be substituted for the value of the key `b` in the dictionary `a`).

    - Single and multi-line expressions (`yf_data = !vbt.YFData.pull("BTC-USD")` will download the data
    and assign it to the key `yf_data`).

    - Round trip (a successful round-trip consists of converting an object to text
    and then back to an object without losing information).

- Upon import, VBT can automatically detect a file named "vbt" with any
supported extension in the current working directory, and update the settings.
This makes setting the default theme a breeze:

**File vbt.cfg**

```ini
[plotting]
default_theme = "dark"
```

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2022/
parent: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/
name: Release notes for 2022
type: page
---

# Release notes for 2022

All notable changes are listed in reverse chronological order.

## Version 1.8.2 (3 Dec, 2022)

- Added methods for plotting trades as signals: [EntryTrades.plot_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.EntryTrades.plot_signals),
[ExitTrades.plot_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.ExitTrades.plot_signals), and
[Portfolio.plot_trade_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.plot_trade_signals).

- Added a method for plotting ranges as shapes: [Ranges.plot_shapes](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.plot_shapes).

- Fixed the creation of three-dimensional `splits_arr` in [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).
Now, the array will always be two-dimensional and have the data type `object`.

- Added support for timestamps and timedeltas in [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter).

- Fixed [Trades.get_edge_ratio](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.get_edge_ratio)
and [Trades.get_running_edge_ratio](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.get_running_edge_ratio)
when `high` and/or `low` are not provided. In this case, rolling volatility will be used instead of ATR.

- The method [Splitter.plot_coverage](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.plot_coverage)
now returns a stacked area plot by default.

- Added the method [Splitter.get_overlap_matrix](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.get_overlap_matrix)
for calculating an overlapping coverage matrix.

- Added the method [GenericAccessor.proximity_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.proximity_apply)
for rolling and reducing a two-dimensional window of neighboring elements.

- Refactored [ArrayWrapper.indexing_func_meta](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.indexing_func_meta)
to make indexing operations on VBT objects more flexible and efficient.

- Enabled row stacking in [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take).

- Added methods [Splitter.shuffle_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.shuffle_splits)
and [Splitter.break_up_splits](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.break_up_splits)
for shuffling and breaking up splits, respectively.

- Fixed bound attachment in [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
and [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply).

- The method [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run) can now run multiple indicators by
passing a sequence, or all indicators by adding the suffix `_all` to a library name. Outputs can be stacked
along columns into a single DataFrame.

- The simulation method [Portfolio.from_holding](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_holding)
is now cached by default (set `dynamic_mode=True` to use a signal function).

- Made [ReturnsAccessor.deflated_sharpe_ratio](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.deflated_sharpe_ratio)
more robust to invalid Sharpe values (`np.nan` and `np.inf`).

- Added a method for calculating the Probabilistic Sharpe Ratio: [ReturnsAccessor.prob_sharpe_ratio](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.prob_sharpe_ratio).

- Published the [Cross validation](https://vectorbt.pro/pvt_41531c19/tutorials/cross-validation) tutorial :notebook_with_decorative_cover:

## Version 1.8.1 (15 Nov, 2022)

- Fixed an error in pattern fitting when `max_error_interp_mode` is not set.

- The data dictionary can be constructed from data keyed by feature instead of symbol.
This can be enabled using the argument `columns_are_symbols`.

- Replaced the argument name `stretch` with `extend` in the projection context.

- Added a decorator for splitting: [@split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.split).

- Added a decorator for cross-validated splitting: [@cv_split](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/decorators/#vectorbtpro.generic.splitting.decorators.cv_split).

- Fixed formatting of auto-generated record attributes.

- Parameters of type [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) can now conditionally depend on
other parameters. For example, `param1 > param2` will select only parameter combinations where the value
of the first parameter is greater than the value of the second parameter.

- Iterative portfolio Numba-compiled functions can override `i`, `col`, and `flex_2d` provided via the context.

- Fixed exit signal generation with `skip_until_exit`.

- Added [Edge Ratio](https://tradingtact.com/edge-ratio/) in [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades).

- Introduced a one-to-one mapping between orders and trades: each trade record
now includes the ID of the order that opened and closed the trade (but not the other way around).

- Added [Portfolio.get_trade_history](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_trade_history),
which merges orders and trades and returns the information as a human-readable DataFrame.

- [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData) now has an update method.

- Added squeezing of splits and sets in [Splitter.take](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.take)
and [Splitter.apply](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.apply).

- Added a splitter class method ([Splitter.from_grouper](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter.from_grouper))
that can build a splitter based on grouping or resampling using VBT or Pandas
(for example, "M" for monthly periodicity).

- Each [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper) and
[Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) instance now has a convenient `split` method.

- Updated the notebooks that used the outdated splitting API.

## Version 1.8.0 (24 Oct, 2022)

- Added an accessor for Pandas Indexes, [BaseIDXAccessor](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseIDXAccessor).
Migrated some index-related functionality from [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper),
such as [BaseIDXAccessor.get_grouper](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseIDXAccessor.get_grouper).
Also created methods for many functions from the [base.indexes](https://vectorbt.pro/pvt_41531c19/api/base/indexes/) module.

- Added `clear_cache` and `collect_garbage` arguments to [SerialEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.SerialEngine),
allowing it to clear the cache and collect garbage (globally) after each iteration.

- Added a method for preparing returns ([prepare_returns](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.prepare_returns)).
The function [riskfolio_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.riskfolio_optimize)
now prepares the returns automatically (for example, it can remove columns that are all NaN or zero).

- Added a base module for merging arrays: [base.merging](https://vectorbt.pro/pvt_41531c19/api/base/merging/). Also, unified merging
functionality across the [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized)
and [@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked) decorators.

- Introduced two new execution engines: [ThreadPoolEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.ThreadPoolEngine)
and [ProcessPoolEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.ProcessPoolEngine),
which are lightweight alternatives to Dask and Ray, respectively.

- Fixed an issue where using the Plotly Express accessor caused an error.

- Added a method for fitting a pattern: [GenericSRAccessor.fit_pattern](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericSRAccessor.fit_pattern).

- Updated [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData) to `tvdatafeed>=2.1.0`.

- Made VBT load much faster by moving import statements into the functions that actually need them.
Auto-import of many optional functionalities can now be controlled from [settings.importing](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.importing).
Disabling all options reduces the loading time to around one and a half seconds.

- Fixed an issue where selecting a date range from a signal portfolio caused an error.

- Fixed [PolygonData](https://vectorbt.pro/pvt_41531c19/api/data/custom/polygon/#vectorbtpro.data.custom.polygon.PolygonData).

- Added a new signal detection indicator, [SIGDET](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/sigdet/#vectorbtpro.indicators.custom.sigdet.SIGDET)
(see https://stackoverflow.com/a/22640362).

- Fixed many potential division by zero conditions in records.

- Fixed the calculation of gross exposure for short positions.

- Redesigned accessors to take an array wrapper as the first argument and accept arbitrary array-like objects.

- Added [Splitter](https://vectorbt.pro/pvt_41531c19/api/generic/splitting/base/#vectorbtpro.generic.splitting.base.Splitter) :boom:

## Version 1.7.1 (23 Sep, 2022)

- Data stored before `1.7.0` can be unpicked again after this release.
Made pickled VBT objects more resistant to API changes.

- Fixed incorrect `min_size` and `max_size` defaults.

- Fixed division by zero in the VIDYA indicator.

- Merged the `ZIGZAG` indicator into [PIVOTINFO](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/pivotinfo/#vectorbtpro.indicators.custom.pivotinfo.PIVOTINFO).

- Plotting accessor methods now also accept DataFrames, with the ability to select a column to plot.

- Added the streaming [SUPERTREND](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/supertrend/#vectorbtpro.indicators.custom.supertrend.SUPERTREND)
indicator from the tutorial.

- Added the streaming [ReturnsAccessor.rolling_sharpe_ratio](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.rolling_sharpe_ratio)
method from the tutorial.

- Removed `apply` and `custom` prefixes from the Numba-compiled indicator functions.
Also added chunking specifications.

## Version 1.7.0 (20 Sep, 2022)

- Added a portfolio method (and the corresponding property) for calculating real allocations:
[Portfolio.get_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_allocations).

- Added a portfolio method (and the corresponding property) for calculating asset PnL,
which is similar to asset returns but calculated in notional terms: [Portfolio.get_asset_pnl](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_asset_pnl).

- Fixed an issue where asset returns were calculated incorrectly when a position was being reversed.

- Improved plotting of allocations in [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
to eliminate diagonal lines. Allocations now appear as bars and take considerably less time to draw.

- Implemented (NB and SP) [VIDYA](https://www.metatrader5.com/en/terminal/help/indicators/trend_indicators/vida),
and made it available as a window type in custom indicators.

- [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData) now has a class method
to find the earliest date of any symbol on any exchange, which is called automatically.

- Lifted the limitation of merging only different symbols. If two data instances have the same symbols,
their dataframes will now be merged across rows and columns.

- Extended `ConflictMode.Opposite` to issue an entry signal when not in position.

- Added convenience methods to custom data classes. For example, instead of calling
`DataClass.set_settings(path_id="custom", ...)` to change settings globally for a class,
you can now simply call `DataClass.set_custom_settings(...)`. Also, new methods
[RemoteData.from_csv](https://vectorbt.pro/pvt_41531c19/api/data/custom/remote/#vectorbtpro.data.custom.remote.RemoteData.from_csv)
and [RemoteData.from_hdf](https://vectorbt.pro/pvt_41531c19/api/data/custom/remote/#vectorbtpro.data.custom.remote.RemoteData.from_hdf)
let you pull data from a CSV or HDF file, respectively, and set up the class.
This provides a better alternative to using local data classes when you want data updates.

- Enabled random search in indicators with the `random_subset` argument, which randomly selects
a subset of parameter combinations.

- Integrated freqtrade's [technical](https://github.com/freqtrade/technical) library
as both technical indicators (`vbt.technical(...)`) and consensus indicators (`vbt.techcon(...)`).

- Kaleido is now one of the optional dependencies.

- Added a [ZIGZAG](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/zigzag/#vectorbtpro.indicators.custom.zigzag.ZIGZAG) indicator,
inspired by this implementation: https://github.com/jbn/ZigZag. Best used for plotting.
Note that the first pivot is determined by looking into the future.

- Added a [PIVOTINFO](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/pivotinfo/#vectorbtpro.indicators.custom.pivotinfo.PIVOTINFO)
indicator, with a similar implementation but with look-ahead bias resistance. It contains lots of useful information,
such as the last confirmed pivot type, index, and value, as well as the most recent (not yet confirmed)
pivot type, index, and value.

- Added global options (available under [settings.wrapping](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.wrapping)
with the prefix `prec_`) to set minimum and maximum integer and floating precision for
wrapped arrays. Depending on the platform, Numba mostly returns 64-bit numbers.
When these arrays are wrapped with Pandas using [ArrayWrapper.wrap](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap),
VBT can cast them to 32 bits or lower to save memory. Note that Numba does not support 16-bit
numbers yet, but you can still use them with Pandas.

- Implemented an accessor method [GenericSRAccessor.to_renko](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericSRAccessor.to_renko)
that converts any time series into [the Renko format](https://www.investopedia.com/terms/r/renkochart.asp).
Note that Renko data may contain irregular or duplicate datetime indices and should not
be used for anything requiring a frequency (such as Sharpe).

- Order arguments `min_size` and `max_size` now depend on `size_type`. For example, specifying
`min_size` of 10% when `size_type` is `SizeType.TargetPercent` will not execute the order if
the requested size is less than 10% of the current portfolio value.

- Profit factor, expectancy, and SQN can now be computed from returns rather than PnL
(see the respective [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades) property,
with the prefix `rel_`).

- Added a decorator [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized),
which can wrap any Python function and parameterize its inputs with
[Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) or with special parameter configs.
This allows the original function, which can process only one parameter combination at a time,
to be wrapped so it appears unchanged, but can handle unlimited combinations!

- Refactored the class methods [PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func)
and [PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func).
They are now parameter-based, with workings similar to the [@parameterized](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.parameterized) decorator.

- Renamed the argument `product_idx` to `level` in [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param)
and [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO).

- Added support for symbol classes in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data).
They can be specified when fetching using `classes`. When preparing the data, one or more column
levels are created. This is useful for attaching information on industries, sectors, and other asset classes.

- Integrated [Riskfolio-Lib](https://github.com/dcajasn/Riskfolio-Lib) in a manner similar
to PyPortfolioOpt. There is now a function [riskfolio_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.riskfolio_optimize)
and supporting infrastructure to run arbitrary optimization models from Riskfolio-Lib with
a single function call. Rebalancing is done using [PortfolioOptimizer.from_riskfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_riskfolio).

- Fixed a compatibility issue between NumPy and TA-Lib in the Dockerfile.
Also included the installation of universal-portfolios.

- Lifted the limit of 10k fetched data points in [AlpacaData](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData).

- Added a `list_symbols` method for a variety of local and remote data classes to fetch the
list of all available symbols (and automatically filter them using regular expressions and other conditions).

- Updated [Portfolio optimization](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization) :notebook_with_decorative_cover:

- Wrote [Stop signals](https://vectorbt.pro/pvt_41531c19/tutorials/stop-signals) :notebook_with_decorative_cover:

## Version 1.6.1 (20 Aug, 2022)

- Scheduler now accepts two new arguments: `zero_offset` and `force_missed_run`.

- Migrated the website to another private repository to maintain separation of concerns.

- The website is now divided into a public part and a private part (for members only).
Most documentation and API documentation are exclusive. Added a new getting started guide for members.
Each documentation and tutorial page can be auto-converted into Python code.

## Version 1.6.0 (15 Aug, 2022)

- Fixed the conversion of datetime indexes to nanosecond format on Windows by replacing `int_` with `np.int64`.
Additionally, implemented the methods [Wrapping.ns_index](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.ns_index),
[Wrapping.get_period_ns_index](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.get_period_ns_index),
and [Wrapping.ns_freq](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.ns_freq), which perform
this conversion automatically.

- Added missing layout keyword arguments to [GenericSRAccessor.plot_pattern](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericSRAccessor.plot_pattern).

- Unified the timeframe format throughout the codebase, including custom data classes. This now allows
passing human-readable strings like "15 minutes".

- Switched the third-party package for [AlpacaData](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData)
from the outdated `alpaca-trade-api-python` to the latest `alpaca-py`.

- Added configuration examples under the API documentation for each custom data class.

- Enabled configured classes to communicate with their globally defined settings. This includes
the methods [Configured.get_settings](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.get_settings),
[Configured.set_settings](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.set_settings), and
[Configured.reset_settings](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Configured.reset_settings).
The key that points to the global settings is stored in the class variable `_settings_key`.
This makes it easy to change the settings of custom data classes.

- Fixed forced import of dill.

- Enabled usage of mappings in [MappedArray.to_pd](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.to_pd).

- Renamed the default key `_default` to `_def` throughout the codebase (in [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO)
instances, index dictionaries, etc.).

- Fixed an issue with calculating the duration of range-like records when the index is not datetime-like.

- [Orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.orders.Orders) now has two new fields: `signed_size` and `value`.

- Refactored iterative helper functions for portfolios. For example, `get_elem_nb` has been renamed to `select_nb`.
Added iterative "above" and "crossover" functions. All such functions have been moved to the
[portfolio.nb.iter_](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/iter_/) module.

- Implemented iterative "above" and "crossover" functions in [generic.nb.iter_](https://vectorbt.pro/pvt_41531c19/api/generic/nb/iter_/).

- Added methods [Data.switch_class](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.switch_class) and
[Data.update_fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update_fetch_kwargs) for easier
switching between data classes.

- Updated the Basic RSI tutorial, documentation, and label generators to use the new `wtype` argument.

- Changed the default start value for cumulative returns from 0 to 1.

- Refactored [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals) again:

    - Renamed `StopExitMode` and `upon_stop_exit` to `StopExitType` and `stop_exit_type`, respectively.

    - Changed the suffix of index fields in limit and stop order records from `i` to `idx`.

    - Added exit price, exit type, and order type to stop order records. This information is now
    pulled from the entry point rather than the exit point.

    - Disabled bounds checking globally, as VBT's functionality has been thoroughly tested.
    You can still add `boundscheck=True` to the `@njit` decorator to enable bounds checking in your functions.

    - In-outputs in flexible mode are now handled the same as in [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).
    The `fill_returns` argument has been disabled for this mode.

    - Delayed the resolution of the `limit_tif` and `limit_expiry` arguments to after broadcasting.

    - Simulation now iterates in group-major order in flexible mode, regardless of cash sharing.

    - The `stop_exit_price` argument can now accept a custom price.

    - In groups with cash sharing, if the automatic call sequence is disabled, orders are further
    sorted by bar zone.

    - Added several convenient functions for use in callbacks, such as
    [set_limit_info_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.set_limit_info_nb).

    - Groups are now valued strictly using the valuation price (usually the opening price)
    to avoid look-ahead bias when working with limit and stop orders.

    - Fixed an issue where the valuation price was set to the order price in limit and stop orders.

- Added the convenient function `vbt.clear_pycache()` to clear the cache of the entire package.

- Added support for Python 3.10 :partying_face:

- Wrote documentation covering [From signals](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-signals/) :notebook_with_decorative_cover:

## Version 1.5.0 (29 Jul, 2022)

- Implemented [from_signals_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/from_signals/#vectorbtpro.portfolio.nb.from_signals.from_signals_nb),
a cached version that is used automatically by [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
if the `adjust_func_nb`, `signal_func_nb`, or `post_segment_func_nb` arguments are not provided.
This avoids repeated compilation in each new runtime.

- Added a post-segment function to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
This can be used, for example, to pre-compute custom metrics during the simulation.

- Created the method [Records.prepare_customdata](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.prepare_customdata),
which prepares custom data to be displayed when hovering over a Plotly trace, using only the options specified
in the field config. This method is used in all record classes, such as [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades),
so you can subclass them, define custom fields, and have them automatically displayed on hover.

- Implemented several functions and classes for pattern detection:

    - NB function [interp_resize_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.interp_resize_1d_nb)
    for interpolating values in an array.

    - NB function [pattern_similarity_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/patterns/#vectorbtpro.generic.nb.patterns.pattern_similarity_nb)
    for calculating similarity scores between two arrays.

    - NB function [rolling_pattern_similarity_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_pattern_similarity_nb)
    and accessor method [GenericAccessor.rolling_pattern_similarity](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_pattern_similarity)
    for rolling similarity scores between two arrays.

    - NB function [find_pattern_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.find_pattern_nb)
    for searching and storing patterns as range records.

    - Class [PatternRanges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges) for holding,
    analyzing, and plotting results of one or multiple pattern searches.

    - Method [PatternRanges.from_pattern_search](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.from_pattern_search)
    as a parameterizable interface to [find_pattern_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.find_pattern_nb),
    which wraps the filled range records with [PatternRanges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges).
    Works similarly to an indicator.

    - Indicator [PATSIM](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/patsim/#vectorbtpro.indicators.custom.patsim.PATSIM)
    based on [rolling_pattern_similarity_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_pattern_similarity_nb).

- Implemented several functions for projections:

    - NB function [map_ranges_to_projections_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.map_ranges_to_projections_nb)
    and method [Ranges.get_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.get_projections)
    for mapping (the price during) ranges of any type to projections.

    - Accessor method [GenericDFAccessor.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericDFAccessor.plot_projections)
    for plotting projections from any DataFrame.

    - Method [Ranges.plot_projections](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.plot_projections)
    for generating and plotting projections using the two methods above.

- Implemented the NB function [get_ranges_from_delta_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/records/#vectorbtpro.generic.nb.records.get_ranges_from_delta_nb),
the class method [Ranges.from_delta](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.from_delta),
and the instance method [Ranges.with_delta](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges.with_delta)
to map records, mapped arrays, or index arrays to [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges)
that start at a specific index and last a set duration. This is useful for converting pattern ranges
to projections.

- Added the `klines_type` argument to [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData)
to support fetching futures data.

- Implemented the indicator [VWAP](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/vwap/#vectorbtpro.indicators.custom.vwap.VWAP)
for calculating VWAP with any frequency-like anchor (daily by default).

- Refactored custom indicators and their plotting methods:

    - Removed the `ewm` argument in favor of the enumerated argument `wtype` (window type) of type
    [WType](https://vectorbt.pro/pvt_41531c19/api/generic/enums/#vectorbtpro.generic.enums.WType), which lets the moving average and
    standard deviation be computed using the new NB functions
    [ma_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.ma_nb) and
    [msd_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.msd_nb).

    - Renamed `MSTD` to `MSD` (Moving Standard Deviation).

    - Enumerated types are now displayed in the column hierarchy using their field names.

    - Switched [RSI](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/rsi/#vectorbtpro.indicators.custom.rsi.RSI)
    to use Wilder's moving average by default, and reimplemented its NB function to be SP.

    - Reimplemented [STOCH](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/stoch/#vectorbtpro.indicators.custom.stoch.STOCH)
    to return fast %K, slow %K, and slow %D.

    - Switched [MACD](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/macd/#vectorbtpro.indicators.custom.macd.MACD) to
    exponential moving average by default.

- The functionality to combine parameters has been moved out of [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast)
to [combine_params](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.combine_params) and generalized.
It can now be used to parameterize any function that accepts instances of
[Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param), as seen in
[PatternRanges.from_pattern_search](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.PatternRanges.from_pattern_search).
Additionally, [Param](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.Param) instances can now be passed to
[broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast).

- Implemented the method [Data.run](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.run),
which takes a function or the name of an indicator (including all supported third-party indicators),
automatically detects which features the function accepts (such as `close` and `volume`), and
runs the indicator on those features. This allows for quick indicator runs, as there is no need
to manually search for input names.

- Drawdowns can now be built from OHLC data, not just close.

- [ArrayWrapper.wrap](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap) now supports
complex broadcasting.

- Disabled Plotly resampler globally (it can be enabled in settings), since it does not suit all graphs.

- Enabled bound checking globally (can be disabled in settings), as it makes finding indexing errors easier
with only a slight impact on performance.

- Various record classes can now be converted into [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges)
for range analysis. For example, the class [Drawdowns](https://vectorbt.pro/pvt_41531c19/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns)
has the method [Drawdowns.get_recovery_ranges](https://vectorbt.pro/pvt_41531c19/api/generic/drawdowns/#vectorbtpro.generic.drawdowns.Drawdowns.get_recovery_ranges)
and the auto-generated property `recovery_ranges` to build ranges between the valley point and the recovery point.

- Added methods for retrieving the first ([Records.first_n](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.first_n)),
last ([Records.last_n](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.last_n)), and
random ([Records.random_n](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records.random_n)) records.

- Improved the formatting of dicts and configs in the API documentation.

- Added flags such as `plot_close` to allow disabling plotting of various traces in ranges,
drawdowns, orders, and trades.

- Wrote [Patterns and projections](https://vectorbt.pro/pvt_41531c19/tutorials/patterns-and-projections) :notebook_with_decorative_cover:

## Version 1.4.2 (28 Jun, 2022)

- Integrated [plotly-resampler](https://predict-idlab.github.io/plotly-resampler/). If installed,
resampled figures and widgets will be used automatically. If not installed, the system will fall back
to regular Plotly figures and widgets. You can force-enable or disable it in the global settings.
Only scatter traces are supported.

- The hit detection of limit order price in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
can now be reversed using the `limit_reverse` argument.

- Previously, any unexpected arguments passed to class methods were accepted by the class. Now,
passing such arguments will raise an error. This helps prevent mistyped argument names.

- Created a Dockerfile that runs JupyterLab and includes VBT along with most of its optional dependencies.

## Version 1.4.1 (25 Jun, 2022)

- Fixed the error when providing any stop in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
as an index or a [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO) instance.

- Added a new argument `from_ago` to [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
which controls how many bars ago the order information should be taken from. This allows you to avoid
manually shifting arrays. For example, setting `from_ago=1` will shift order information by one bar,
and thus execute using the next close by default.

- [PriceType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceType) has been extended, and
`price` can now be specified as `nextopen` and `nextclose`, which automatically sets `from_ago` to 1.
These types cannot be used as part of arrays.

- Limit signals, stop signals, and user signals in [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
can now be more precisely distinguished according to where exactly in a bar they execute. Limit signals
can be executed at open and in the middle of the bar. Stop signals can be executed at open, in the middle
of the bar, or postponed to close. User signals can execute at open, in the middle of the bar, and at close.
This makes it possible to build an accurate execution chain that respects the timing of each signal.

## Version 1.4.0 (21 Jun, 2022)

- Completely refactored [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals):

    - The signal function `signal_func_nb` now takes the context [SignalContext](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SignalContext),
    which provides much more useful information than before. User-defined arguments are not included
    in the context, but you can always forward them to the function using templates.

    - Preset signal functions now take an adjustment function `adjust_func_nb` as an argument, which allows you to
    change any current information before returning the signals. This is especially useful for adjusting limits
    and stops.

    - When there is no signal in any column of a group, the group will not be executed. This
    leads to a significant speedup for sparsely distributed signals.

    - Stop loss is now split into two separate stop orders: SL (`sl_stop`) and TSL (`tsl_stop`).

    - TTP has been integrated into TSL by adding a threshold option (`tsl_th`).

    - Added the option to specify the format of any delta using `delta_format` of type [DeltaFormat](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.DeltaFormat),
    which makes it possible to provide stop values in absolute terms.

    - Information on each stop order is now stored in one record array: one record per column, not separate arrays.
    For example, stop loss information is stored in a record array with the data type
    [sl_info_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.sl_info_dt).

    - Added limit order support. When a signal of the limit order type is placed, it is first
    checked against the current bar information (OHLC if the signal was placed at open, only C otherwise).
    It is then stored in a record array of type [limit_info_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.limit_info_dt),
    which is propagated through time and checked to see whether the limit has been hit or expired at each bar.
    Only one active limit order is allowed at any time.

    - Like stop orders, a limit delta can be specified to represent a distance between the initial
    price (`price`) and the target price. For example, a delta of 0.1 will place a limit order with
    a target price of 10% above or below the initial price when selling or buying.

    - Limit orders can expire after a time delta (`limit_tif`) or at a specific time (`limit_expiry`).
    The format of both is controlled by `time_delta_format` of type
    [TimeDeltaFormat](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.TimeDeltaFormat).
    If the format is based on index, time deltas and expiry times will be converted to integer nanosecond
    format. If the input is a timedelta or datetime-like Series/DataFrame, preprocessing will remove
    the timezone and convert it to integer format automatically.

    - Previously, stop orders were pure market orders. Now, they can create a limit order once their stop
    has been hit. They also have their own limit delta.

    - Added options for resolving conflicts between executable and pending signals. For example,
    providing `upon_opp_limit_conflict="cancelexecute"` will cancel any pending limit order
    and execute the user-defined signal if its sign is opposite to the limit order's sign.
    The same rules apply for pending stop orders, which can be canceled by executable limit or user signals.

    - Order records generated by [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
    have been extended to include the timestamp of the signal (`signal_idx`), timestamp of (stop/limit)
    order creation (`creation_idx`), order type (`type`), and stop type (`stop_type`). The new data type is
    [fs_order_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.fs_order_dt). The new order type can be
    analyzed using [FSOrders](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.orders.FSOrders), a
    subclass of [Orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.orders.Orders). Other simulation methods,
    such as [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders),
    still generate records of type [order_dt](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.order_dt).

- [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) now allows you to provide custom record classes.
For example, you can create a custom subclass of [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades)
and pass it as `trades_cls` for the portfolio to use by default.

- Added `ValuePercent`, a new size type for ordering a percentage of the current group value.
Additionally, `Percent`, `ValuePercent`, and `TargetPercent` now have alternatives for providing percentages
in a human-readable format (1.0 = 1%).

- Instead of passing `-np.inf` or `np.inf` as order or valuation price to represent open/latest and close/price,
you can now use enumerated values [PriceType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.PriceType)/
[ValPriceType](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ValPriceType).
For example, to specify that the opening price should be used as the order price, pass `price="open"`.

- [QSAdapter](https://vectorbt.pro/pvt_41531c19/api/returns/qs_adapter/#vectorbtpro.returns.qs_adapter.QSAdapter) will now remove the timezone
from benchmark returns and require you to select a column (via the `column` argument),
as quantstats can only use one benchmark column at a time.

- The OHLC chart ([OHLCVDFAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.plot))
has been updated to match TradingView's style.

- Specifying the filename when saving or loading any VBT object is no longer necessary;
VBT will use the class name by default.

- Fixed the issue where chunking lots of parameter combinations resulted in a progressive slowdown.

- Fixed the handling of input arrays with the data type `float32` in generic NB functions.

- Implemented [TVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/tv/#vectorbtpro.data.custom.tv.TVData), which wraps [tvdatafeed](https://github.com/StreamAlpha/tvdatafeed).

- Removed the "Close time" column from any DataFrame returned by Binance.

## Version 1.3.0 (4 Jun, 2022)

- Added default resamplers for trade count and VWAP in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data),
as well as resamplers for stock splits, dividends, and other columns in custom data classes.

- Fixed resampling of the open price when NaN values are present.

- Improved data type handling when wrapping. Previously, casting a floating array with NaN values to an integer
data type could throw an error. Now, data types are cast softly.

- Fixed the implementation of VWAP. The indicator now resets each day or at any custom frequency.

- Implemented a method [ArrayWrapper.get_index_grouper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_grouper),
which, like [ArrayWrapper.get_resampler](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_resampler),
can be passed to any accelerated method in VBT that uses row grouping.

- Added the ability to generate log returns and daily returns (or both) along with simple returns,
such as by using `pf.get_returns(log_returns=True)`. Each metric in [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor)
can now natively work on log returns. Since the accessor cannot determine whether the passed return
series is simple or log, you must pass the flag `log_returns=True`. This is done automatically
by [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio). For example, to access
the accessor with log returns, use `pf.get_returns_acc(log_returns=True)`. To use log or daily returns
in portfolio stats, set the corresponding flag in the `settings` dictionary.

- Implemented NB and SP rolling linear regression using OLS ([GenericAccessor.rolling_linreg](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_linreg)),
which returns two arrays: slope and intercept. This algorithm can be used in
cointegration tests and is 1100x faster than [RollingOLS](https://www.statsmodels.org/dev/generated/statsmodels.regression.rolling.RollingOLS.html) :fire:

- Implemented two new indicators:

    1. [OLS](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/ols/#vectorbtpro.indicators.custom.ols.OLS) for computing and
    plotting the rolling linear regression between two time series.

    2. [OLSS](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/olss/#vectorbtpro.indicators.custom.ols.OLSS) for computing and
    plotting the spread and the z-score of the spread of the linear regression between two time series
    (for cointegration).

- Added the `minp` argument to all custom indicators.

- Updated [PolygonData](https://vectorbt.pro/pvt_41531c19/api/data/custom/polygon/#vectorbtpro.data.custom.polygon.PolygonData) for `polygon==1.0.0`.

- Implemented row ([Wrapping.row_stack](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.row_stack))
and column ([Wrapping.column_stack](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.column_stack))
stacking for VBT objects of arbitrary complexity. Since every VBT object wraps multiple arrays,
objects can be effectively merged by stacking their internally stored arrays and unifying other
input information. Under the hood, stacking starts between wrappers of all objects
using [ArrayWrapper.row_stack](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.row_stack)
and [ArrayWrapper.column_stack](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.column_stack).
While some objects, such as [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor),
can be stacked easily, stacking records and portfolios is a complex process and requires some assumptions
(see the corresponding methods).

- Refactored the indexing mechanism in [ArrayWrapper](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper)
to also return slices when possible, since data can be accessed faster using slices than index arrays.

- Lifted the limitation that combined objects must have the same index.
Previously, an error was raised if any object had a different index.
Now, similar to how Pandas works, indexes of all objects are aligned using a "union" set operation,
and missing values are set to NaN or a user-defined value specified in `reindex_kwargs`. Additionally,
each broadcastable argument in all simulation methods of
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio), such as
`size` in [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders),
now has sensible fill values.

- Fixed axis labels in the [Volume](https://vectorbt.pro/pvt_41531c19/api/generic/plotting/#vectorbtpro.generic.plotting.Volume) widget.

- Changed the representation of index ranges from one two-dimensional array to a tuple of
one-dimensional arrays, which are easier to manage.

- Deprecated pytz and now use [zoneinfo](https://docs.python.org/3/library/zoneinfo.html).

- Implemented an entirely new way of creating arrays using index dictionaries.
Instead of building an array and setting its data manually, index dictionaries allow you to provide
row and column information as keys and data as values. The function [get_indices](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.get_indices)
converts each key of such a dictionary into integer indices, and the method [ArrayWrapper.fill_and_set](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.fill_and_set)
sets the corresponding values at those indices, taking into account flexible indexing requirements.
This makes it much easier to provide data such as order size at specific timestamps or even time intervals.
For example, to deposit $100 monthly, provide the following to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals):
`cash_deposits=vbt.index_dict({vbt.pointidx(every="M"): 100})` (see the [base.indexing](https://vectorbt.pro/pvt_41531c19/api/base/indexing/) module).
Also, all arguments passed to the broadcaster can be passed as templates to be created using the final shape,
instead of participating in actual broadcasting. This means you can now pass the following to [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
or any other method: `size=vbt.RepEval("size = wrapper.fill(); size.iloc[0] = np.inf; size")`,
which waits until all other arrays are successfully broadcast and the final wrapper is built, then
creates an array and fills the first row with `np.inf`.

- Enabled row selection in all VBT objects except [ColumnMapper](https://vectorbt.pro/pvt_41531c19/api/records/col_mapper/#vectorbtpro.records.col_mapper.ColumnMapper).
For objects with complex data, such as records and portfolios, only slices (non-interrupting ranges)
are allowed. For example, selecting a date range from a portfolio means taking the last cash, position,
and close prior to the first point in the new slice, and using them as `init_cash`, `init_position`, and
`init_price`, respectively.

- Every simulation method in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
now accepts an instance of [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) as `close`.
Open, high, low, and close price series will be extracted automatically.

- [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) now stores and uses OHLC data.
Moreover, all records, from [Ranges](https://vectorbt.pro/pvt_41531c19/api/generic/ranges/#vectorbtpro.generic.ranges.Ranges)
to [Logs](https://vectorbt.pro/pvt_41531c19/api/portfolio/logs/#vectorbtpro.portfolio.logs.Logs), also use OHLC data by subclassing
the new class [PriceRecords](https://vectorbt.pro/pvt_41531c19/api/generic/price_records/#vectorbtpro.generic.price_records.PriceRecords).

- Implemented MFE and MAE metrics and plots in [Trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades).

- Fixed [Portfolio.from_holding](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_holding),
which previously set the entry signal at the first timestamp. Since not all assets may start
from the first timestamp, the entry signal could be ignored. Now, the entry signal is set dynamically
using a signal function whenever a new valid value in `close` is encountered.

- Optimized the indexer in [ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points)
and [ArrayWrapper.get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges)
by adding the `indexer_tolerance` argument to the first method and removing the `indexer_method` argument from
the second method.

## Version 1.2.3 (7 May, 2022)

- Fixed marker color in [OHLCVDFAccessor.plot](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor.plot)
when one of the price points is NaN.

- Ordering in the long-only direction from a short position and in the short-only direction
from a long position is now allowed.

- Fixed output parsing in [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr).

- Renamed `plot_as_entry_markers` to [SignalsSRAccessor.plot_as_entries](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_entries)
and `plot_as_exit_markers` to [SignalsSRAccessor.plot_as_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_exits).
Created two additional methods for marking entries ([SignalsSRAccessor.plot_as_entry_marks](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_entry_marks))
and exits ([SignalsSRAccessor.plot_as_exit_marks](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsSRAccessor.plot_as_exit_marks)).

- Rolling apply can now run on variable, frequency-based windows such as `7d`.

- Implemented [BaseAccessor.ago](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.ago)
to get the values from a specified number of periods ago.

- Implemented NB and SP rolling "any" ([GenericAccessor.rolling_any](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_any))
and "all" ([GenericAccessor.rolling_all](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_all)).

- Updated the MTF tutorial with a section on forward filling.

- Indicator parameters can now be manually mapped to an index level using `post_index_func`
specified in `param_settings`.

- Complex objects are now counted per type rather than by their position in
[param_to_index](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.utils.params.param_to_index).

- Implemented a complete module [utils.datetime_nb](https://vectorbt.pro/pvt_41531c19/api/utils/datetime_nb/) with Numba-compiled
functions for operations on (mostly integer-formatted) date and time.

- Added contexts [GenEnContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.GenEnContext),
[GenExContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.GenEnContext), and
[GenEnExContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.GenEnContext) to signal generation functions.

- Signal placement functions are now required to return the local position of the last placed signal,
improving generation speed significantly.

- Completely refactored [SignalsAccessor.generate_stop_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_stop_exits)
to accept the inputs `entry_ts` and `follow_ts`, and write the in-place output `stop_ts_out`. This enables
many more options for generating uni-directional stops.

- Completely refactored [SignalsAccessor.generate_ohlc_exits](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_ohlc_exits)
to separate the entry price (`entry_price`) from the open price (`open`), and to better represent
and be able to simultaneously process four different stop types: SL, TSL, TP, and TTP (new).
Also updated the enum [StopType](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.StopType) for the new stop types.

- Refactored ranking. Added the `after_reset` argument to [SignalsAccessor.rank_nb](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.rank_nb)
to allow removing any signal that comes before the first resetting signal. Added the `reset_wait` argument
to postpone resetting the position by a number of ticks. Removed `prepare_func`
in favor of templates. Made the function contextualized by adding the context
[RankContext](https://vectorbt.pro/pvt_41531c19/api/signals/enums/#vectorbtpro.signals.enums.RankContext) with useful information.

- Implemented after-reset versions for various signal selection functions, such as
[SignalsAccessor.first_after](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first_after)
for [SignalsAccessor.first](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.first).
Also implemented [SignalsAccessor.to_nth](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.to_nth)
to select the first `n` signals.

- Accessor methods in [GenericAccessor](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor)
that accept a UDF now also accept the name of any UDF from [generic.nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/).

- Fixed casting the column "Trade count" to an integer data type in custom data classes.

- By enabling the `parse_index` flag in [settings.datetime](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.datetime),
any index with human-readable datetime strings passed to an array wrapper will be automatically
parsed and converted to a Pandas index.

- Added time components to [ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points)
and [ArrayWrapper.get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges).
This allows signals to be placed and portfolios to be optimized during specific times of the day.

- Reimplemented [SignalsAccessor.clean](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.clean)
to accept the arguments `force_first`, `keep_conflicts`, and `reverse_order` for better control
over the cleaning process.

- Implemented [Data.transform](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.transform)
to transform the underlying data with a UDF and replace the instance.

- Wrote [Signal development](https://vectorbt.pro/pvt_41531c19/tutorials/signal-development) :notebook_with_decorative_cover:

## Version 1.2.2 (21 Apr, 2022)

- Made the call sequence array completely optional. Previously, you had to create an array
with the same size as the target shape, which was unnecessary and consumed a lot of memory. Now,
setting `call_seq` to None or `auto` means you no longer need a user-defined array—only the call
sequence for the current row is kept in memory.

- The close price in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
no longer broadcasts to the full shape. Since all methods use flexible indexing,
there is no need to expand and materialize the close price. Instead, it keeps its original shape while 
the broadcasting operation returns a wrapper holding the target shape and other Pandas metadata.
This lowers memory usage.

- Disabled `flex_2d` everywhere for more consistency across the codebase. Passing one-dimensional
arrays now treats them per-row by default.

- Fixed the shape in the wrapper returned by [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast).

- When specifying enum fields such as "targetamount," the mapper will ignore all non-alphanumeric
characters by default. For example, "Target Amount" can now be passed as well.

- Added the `incl_doc` option to show or hide the docstring of the function when using
[phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp).

- [ArrayWrapper.wrap](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.wrap)
will broadcast the array being wrapped using NumPy rules if its shape does not match the shape of the wrapper.
This allows smaller arrays to expand to the target shape only when needed, which benefits memory usage.

- Added the `indexer_method` option to specify the indexer method in
[BaseAccessor.set](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set)
and [BaseAccessor.set_between](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set_between)
to control which timestamp (previous or next) should be used if there is no exact match.

- Cash deposits and cash earnings now respect the cash balance in
[Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).
Additionally, cash deposits now behave like cash earnings: a full array is created during
simulation if any element of the cash deposits is not zero. This array is overridden
in place at each row and group and then returned as part of the simulation output.

- Added the `skipna` option in [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
to skip rows where the size in all columns of a group is NaN.

- Wrote documentation for [From orders](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/from-orders/) :notebook_with_decorative_cover:

## Version 1.2.1 (10 Apr, 2022)

- Fixed the check for zero size in [buy_nb](https://vectorbt.pro/pvt_41531c19/api/portfolio/nb/core/#vectorbtpro.portfolio.nb.core.buy_nb).

- Fixed [is_numba_func](https://vectorbt.pro/pvt_41531c19/api/utils/checks/#vectorbtpro.utils.checks.is_numba_func) when Numba is disabled globally.

- Simulation methods with order functions now use record counts (`order_counts` and `log_counts`)
instead of last indices (`last_oidx` and `last_lidx`).

- Added the `inplace` argument to [BaseAccessor.set](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set)
and [BaseAccessor.set_between](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set_between).

- Fixed `ndim` in [Data.get_symbol_wrapper](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get_symbol_wrapper).

- Renamed `ExecuteOrderState` to [AccountState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.AccountState)
and `ProcessOrderState` to [ExecState](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.ExecState).

- Rotational indexing is now disabled by default and can be enabled globally through settings.

- The order approximation function now takes an instance of `ExecState` instead of state variables.

- Created the classes [RandomOHLCData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random/#vectorbtpro.data.custom.random.RandomData)
and [GBMOHLCData](https://vectorbt.pro/pvt_41531c19/api/data/custom/gbm_ohlc/#vectorbtpro.data.custom.gbm_ohlc.GBMOHLCData)
for generating random OHLC data.

- Made numeric tests more precise by reducing tolerance values.

- Fixed documentation where the argument `interval` was used instead of `timeframe`.

- Wrote documentation on [Portfolio simulation](https://vectorbt.pro/pvt_41531c19/documentation/portfolio/) :notebook_with_decorative_cover:

## Version 1.2.0 (3 Apr, 2022)

- Integrated [PyPortfolioOpt](https://pyportfolioopt.readthedocs.io/en/latest/): implemented
the function [pypfopt_optimize](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.pypfopt_optimize)
and supporting infrastructure to run any optimization model from PyPortfolioOpt with a single function call.

- Split generation and reduction of resampling metadata. The generation now resides in
[base.resampling.nb](https://vectorbt.pro/pvt_41531c19/api/base/resampling/nb/), while the reduction is handled in
[generic.nb.apply_reduce](https://vectorbt.pro/pvt_41531c19/api/generic/nb/apply_reduce/).

- Implemented wrapper methods for generating index points ([ArrayWrapper.get_index_points](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_points))
and index ranges ([ArrayWrapper.get_index_ranges](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.get_index_ranges))
from human-readable queries. This is especially helpful for rebalancing.

- Implemented the [PortfolioOptimizer](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer)
class, which can perform portfolio optimization at regular or irregular intervals,
store the generated allocation data in a compressed format, and analyze and plot it.
Supports the following input modes:

    1. Custom allocation function ([PortfolioOptimizer.from_allocate_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocate_func)).

    2. Custom optimization function ([PortfolioOptimizer.from_optimize_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_optimize_func)).

    3. Custom allocations ([PortfolioOptimizer.from_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_allocations)).

    4. Custom filled allocations ([PortfolioOptimizer.from_filled_allocations](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_filled_allocations)).

    5. Random allocation ([PortfolioOptimizer.from_random](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_random)).

    6. Uniform allocation ([PortfolioOptimizer.from_uniform](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_uniform)).

    7. PyPortfolioOpt ([PortfolioOptimizer.from_pypfopt](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_pypfopt)).

    8. Universal Portfolios ([PortfolioOptimizer.from_universal_algo](https://vectorbt.pro/pvt_41531c19/api/portfolio/pfopt/base/#vectorbtpro.portfolio.pfopt.base.PortfolioOptimizer.from_universal_algo)).

- Added time frames as a parameter to TA-Lib indicators. This downsamples input data
(such as the close price), runs the indicator, and upsamples it back to the original time frame.
Multiple time frame combinations are supported out of the box.

- Implemented convenience methods [BaseAccessor.set](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set)
and [BaseAccessor.set_between](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.set_between)
for setting data based on index points and ranges, respectively.

- Added the attribute class [ExceptLevel](https://vectorbt.pro/pvt_41531c19/api/base/indexes/#vectorbtpro.base.indexes.ExceptLevel)
that can be used to specify which level **not** to group by in `group_by`. This is useful
when there are many levels and you need to group by all levels except, for example, assets.

- Wrote [Portfolio optimization](https://vectorbt.pro/pvt_41531c19/tutorials/portfolio-optimization) :notebook_with_decorative_cover:

## Version 1.1.2 (12 Mar, 2022)

- Added the `skipna` option to run a TA-Lib indicator only on non-NA values (TA-Lib does not work with NaN).

- Implemented a range of (NB and SP) resampling functions for:

    1. Getting the latest information at each timestamp, supporting bar data ([GenericAccessor.realign](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.realign)).

    2. Resampling to a custom index, available as both a regular and meta method ([GenericAccessor.resample_to_index](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_to_index)).

    3. Resampling between custom bounds, available as both a regular and meta method ([GenericAccessor.resample_between_bounds](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.resample_between_bounds)).

- Implemented the [Resampler](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler) class,
which acts as a mapper between the source and target index for maximum flexibility. It can parse a resampler
from Pandas. Also implemented a set of helper functions for:

    1. Generating a datetime index from a frequency (NB).

    2. Getting the right bound of a datetime index.

    3. Mapping one datetime index to another (NB, [Resampler.map_to_target_index](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.map_to_target_index)).

    4. Getting the difference between datetime indexes (NB, [Resampler.index_difference](https://vectorbt.pro/pvt_41531c19/api/base/resampling/base/#vectorbtpro.base.resampling.base.Resampler.index_difference)).

- Implemented an interface for resampling complex VBT objects using the abstract method
[Wrapping.resample](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping.resample). Also defined
resampling logic for all classes whose Pandas objects can be resampled:

    1. [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data).

    2. [OHLCVDFAccessor](https://vectorbt.pro/pvt_41531c19/api/ohlcv/accessors/#vectorbtpro.ohlcv.accessors.OHLCVDFAccessor).

    3. [ReturnsAccessor](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor).

    4. [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

    5. [Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records) and all its subclasses.

- Introduced the feature config [Data.feature_config](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.feature_config),
which can be used to define a resampling function for each custom column in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data).
OHLCV data is resampled automatically.

- Completely refactored in-place output handling in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).
Introduced the in-place output config [Portfolio.in_output_config](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.in_output_config),
which lets you define the layout, array type, and resampling function for each custom in-place output.
Appending suffixes (such as `_pcg`) to in-place output names is now optional. The suffix `_pcgs` has been renamed to `_cs`.
The resolution mechanism for in-place outputs is now distributed over multiple class methods and is more transparent.

- The in-place output name for returns has been changed from `returns_pcgs` to `returns`.

- Added the argument `init_price` to specify the original entry price of `init_position`.
This makes the calculation of P&L and other metrics more precise and flexible.

- Fixed the issue where only the first row in `cash_deposits` was applied in
[Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals).

- Distributed generic Numba-compiled functions across multiple files.

- Enabled passing additional keyword arguments to `get_klines` in [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData).

- Wrote [MTF analysis](https://vectorbt.pro/pvt_41531c19/tutorials/mtf-analysis) :notebook_with_decorative_cover:

## Version 1.1.1 (25 Feb, 2022)

- [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) now removes duplicates in the index, keeping
only the last entry.

- Added the `concat` option to [Data.update](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.update) to allow
disabling concatenation of new data with existing data and returning only new data.

- Added support for chunking in [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData) and
[HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) using `chunk_func`.

- Implemented the [DataSaver](https://vectorbt.pro/pvt_41531c19/api/data/saver/#vectorbtpro.data.saver.DataSaver) class, which can periodically
collect and save data to disk, as well as two subclasses: [CSVDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/saver/#vectorbtpro.data.saver.CSVDataSaver)
for writing to CSV files and [HDFDataSaver](https://vectorbt.pro/pvt_41531c19/api/data/saver/#vectorbtpro.data.saver.HDFDataSaver)
for writing to HDF files. This allows for gradual collection and persistence of any data from any data provider.

- Fixed unpickling of [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config).

- Fixed scheduling in [DataUpdater](https://vectorbt.pro/pvt_41531c19/api/data/updater/#vectorbtpro.data.updater.DataUpdater) so that
repeatedly stopping and starting the same updater will not trigger the same job more than once.

- [QSAdapter](https://vectorbt.pro/pvt_41531c19/api/returns/qs_adapter/#vectorbtpro.returns.qs_adapter.QSAdapter) will now remove
timezone info automatically to prevent issues with QuantStats.

- Implemented three new data classes:

    1. [PolygonData](https://vectorbt.pro/pvt_41531c19/api/data/custom/polygon/#vectorbtpro.data.custom.polygon.PolygonData) for [Polygon.io](https://polygon.io/).
Can load data of any size in batches and respects the API rate limits.

    2. [AVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/av/#vectorbtpro.data.custom.av.AVData) for [Alpha Vantage](https://www.alphavantage.co/).
Does not use the `alpha_vantage` library, which is not actively developed. Instead, it implements a parser
for the Alpha Vantage documentation website and handles communication with the API using the parsed metadata.
This allows instant adaptation to any changes in the Alpha Vantage API. Users can still disable the parsing
and specify every detail manually.

    3. [NDLData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ndl/#vectorbtpro.data.custom.ndl.NDLData) for [Nasdaq Data Link](https://data.nasdaq.com/).
Supports (time-series) datasets.

- Fixed the condition for backward fill in [fbfill_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/base/#vectorbtpro.generic.nb.base.fbfill_nb).

- Fixed passing `execute_kwargs` in [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_apply_func).
Whenever `apply_func` is Numba compiled, the parameter selection function is now also Numba compiled (`jit_select_params=True`)
and releases the GIL (`nogil=True`), so the indicator can be used in multithreading immediately.

## Version 1.1.0 (20 Feb, 2022)

- Removed support for Python 3.6. Planned support for Python 3.10, but encountered issues
that can be tracked in numba/numba#7812 and numba/numba#7839.

- Removed [Bottleneck](https://github.com/pydata/bottleneck) from the optional requirements as it
causes installation issues. However, it can still be installed manually.

- Formatted the entire codebase using [Black](https://black.readthedocs.io/en/stable/) :black_large_square:

- You can now select one or more symbols from a [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instance.

- Refactored [Data.fetch_kwargs](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.fetch_kwargs) into a symbol dictionary.
Retrieving the keyword arguments used for fetching a specific symbol is now as simple as `fetch_kwargs[symbol]`.

- Implemented the [Data.merge](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.merge) method for merging
multiple [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instances.

- You can now rename one or more symbols in a [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) instance.

- Improved aggregation of metrics in [StatsBuilderMixin](https://vectorbt.pro/pvt_41531c19/api/generic/stats_builder/#vectorbtpro.generic.stats_builder.StatsBuilderMixin).

- Removed the `ohlc` accessor to avoid confusion. The only accessor is now `ohlcv`.

- [Data.plot](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.plot) now automatically displays the data as a candlestick
chart if the correct price columns are found. Otherwise, it plots each column as a separate line,
as before. You can now select a symbol to plot by using the `symbol` argument.

- Any metric or subplot can be expanded into multiple metrics or subplots using templates. This allows
displaying a variable number of metrics or subplots.

- [Data.plots](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.plots) now plots one subplot per symbol.

- The delimiter is now recognized automatically when working with CSV and TSV files.

- Implemented a function for pretty-printing directory trees: [tree](https://vectorbt.pro/pvt_41531c19/api/utils/path_/#vectorbtpro.utils.path_.tree).

- Refactored the path matching mechanism for CSV and HDF files. In particular:

    - Path unfolding has been renamed to path matching.

    - Wildcards (`*`) are now supported for groups and keys in HDF files.

    - Paths can be further filtered using a regex pattern `match_regex`.

    - Functions for path matching have become class methods for seamless inheritance.

    - The argument `parse_paths` in [FileData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull)
    has been renamed to `match_paths`.

    - The argument `path` in [FileData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData.pull)
    has been renamed to `paths`.

- Symbols that return `None` or an empty array are skipped. When `raise_on_error` is True,
any symbol that raises an error is also skipped.

- Similar to Python's `help` command, VBT now includes the
[phelp](https://vectorbt.pro/pvt_41531c19/api/utils/formatting/#vectorbtpro.utils.formatting.phelp) function,
which pretty-prints the arguments and docstring of any function. Its main advantage is the ability to skip
annotations, which can sometimes reduce readability when exploring more complex VBT functions using `help`.

- Settings have been refactored again: you can now clearly see which keys can be accessed
via dot notation (_hint_: it must be of type [ChildDict](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.ChildDict)).
Also, the argument `convert_dicts_` in [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config)
has been renamed to `convert_children_`.

- Moved default values of various [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data) classes from the
[data.custom](https://vectorbt.pro/pvt_41531c19/api/data/custom/) module to [settings.data](https://vectorbt.pro/pvt_41531c19/api/_settings/#vectorbtpro._settings.data)
to allow global setting.

- Minor fixes and enhancements across the project.

- Wrote documentation on [Data](https://vectorbt.pro/pvt_41531c19/documentation/data/) :notebook_with_decorative_cover:

## Version 1.0.10 (11 Feb, 2022)

- Moved the `per_column` logic from [IndicatorBase.run_pipeline](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorBase.run_pipeline)
to the indicator function (`custom_func`). Previously, the indicator function received only one column
and parameter combination at a time, which caused issues for caching. Now, the pipeline passes all
columns and parameter combinations, so it is now the responsibility of the indicator function to distribute
the combinations properly (do not worry, `apply_func` will handle this automatically).

- Apply functions (`apply_func`) can now run on one-dimensional input data as well as on two-dimensional
input data by passing `takes_1d=True` to [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_apply_func).
This mode splits each input array (both Pandas and NumPy) into columns and builds a product of columns
and parameter combinations. Benchmarks show that this has no real effect on performance, plus
functions that process one column at a time are much easier to write.

- The `@talib` and `@talib_1d` annotations were merged into a single `@talib` annotation that
can handle both one- and two-dimensional input data.

- Removed automatic module search when parsing indicator expressions, as it degraded performance. It is now
recommended to use multi-line expressions and `import` statements.

- Renamed `kwargs_to_args` to `kwargs_as_args` in [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_apply_func).

- Refactored the [execute](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.execute) function to enable
chunking in indicators.

- Fixed the calculation of crossovers. Previously, a crossover was ignored if there was another crossover
one tick earlier.

- Greatly optimized stacking of output arrays with one column after running an indicator.

- Refactored the `as_attrs_` behavior in [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config).
Keys will no longer be attached to the config instance but managed dynamically (which means fewer side effects
when pickling and unpickling).

- Implemented a wide range of input states, output states, and accumulators for use in
streaming functions, such as [rolling_mean_1d_nb](https://vectorbt.pro/pvt_41531c19/api/generic/nb/rolling/#vectorbtpro.generic.nb.rolling.rolling_mean_1d_nb).

- Made flexible indexing with [flex_select_auto_nb](https://vectorbt.pro/pvt_41531c19/api/base/indexing/#vectorbtpro.base.indexing.flex_select_auto_nb)
rotational. For example, if you have a smaller array with 3 columns and a bigger one with 6 columns,
you no longer need to tile the smaller array 2 times to match the larger one—you can simply rotate
over the smaller array.

- Added support for short names in indicator expressions.

- Returns can now be precomputed in both [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
by passing the `fill_returns=True` flag.

- Fixed [save_animation](https://vectorbt.pro/pvt_41531c19/api/utils/image_/#vectorbtpro.utils.image_.save_animation), which previously
produced one fewer iteration.

- Wrote [SuperFast SuperTrend](https://vectorbt.pro/pvt_41531c19/tutorials/superfast-supertrend) :notebook_with_decorative_cover:

## Version 1.0.9 (2 Feb, 2022)

- Upgraded the parser for indicator expressions:

    - Can evaluate expressions using [pandas.eval](https://pandas.pydata.org/docs/reference/api/pandas.eval.html).

    - Special variables and commands are annotated with the prefix `@`.

    - Supports both single and multi-line expressions.

    - Context variables with the same name are no longer needlessly recomputed.

    - Can parse the class name at the beginning of the expression.

    - Supports both one- and two-dimensional TA-Lib indicators out of the box.

    - Supports magnet names for inputs, in-place outputs, and parameters.

    - Can parse settings from within the expression and pass them to [IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr).

    - Can automatically resolve annotated indicators.

- Enabled programmatic plotting of any TA-Lib indicator by parsing its output flags.

- Implemented Numba-compiled functions for Wilder's EMA and STD (SP).

- Renamed `from_custom_func` to `with_custom_func` and `from_apply_func` to `with_apply_func`
since these are instance methods.

- Set `minp` (minimum periods) to `None` in generic rolling functions to make incomplete windows contain NaN values.

- Made parameter selection optional in [IndicatorFactory.with_apply_func](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_apply_func).
Setting `select_params` to False will prepend the iteration index to the arguments of the apply function.

- Refactored the `keep_pd=True` flag to avoid back-and-forth conversion between broadcasted Pandas objects
and NumPy arrays. Pandas objects are now directly forwarded to the custom function without any preprocessing.

- Renamed `custom_output_props` to `lazy_outputs`.

- Renamed `select_one` and `select_one_from_obj` to `select_col` and `select_col_from_obj` respectively
in [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping).

- Input arrays passed to TA-Lib indicators are now converted to the data type `np.double`.

- Renamed `mapping` to `context` in the context of templates.

- Updated plotting methods in [indicators.custom](https://vectorbt.pro/pvt_41531c19/api/indicators/custom/).

- [Data.get](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.get) now also accepts symbol(s).

- Greatly optimized [Data.concat](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data.concat).

- Wrote documentation on [Indicators](https://vectorbt.pro/pvt_41531c19/documentation/indicators/) :notebook_with_decorative_cover:

## Version 1.0.8 (25 Jan, 2022)

- Implemented the following Numba-compiled functions:

    - Ranking and rolling ranking (SP).

    - Covariance and rolling covariance (SP).

    - Rolling sum and product (SP).

    - Rolling weighted average (SP).

    - Rolling argmin and argmax.

    - Demeaning within a group.

- Implemented volume-weighted average price (VWAP).

- Built a powerful engine for parsing indicator expressions. Using
[IndicatorFactory.from_expr](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_expr),
you can automatically construct an indicator from an expression string such as
`"rolling_mean((low + high) / 2, 10)"`! [IndicatorFactory](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory)
can recognize common inputs such as `low` and the number of outputs. For parameters and more cryptic inputs,
you can provide a prefix: `"rolling_mean(abs(in_ts), p_window)"`. In addition to preset functions
such as `rolling_mean`, whenever an unknown function is recognized, it searches for its implementation
in various parts of VBT and NumPy. Supports NumExpr to accelerate simpler expressions.

- Translated each of [WorldQuant's 101 Formulaic Alphas](https://arxiv.org/pdf/1601.00991.pdf)
into an indicator expression and implemented a convenience method
[IndicatorFactory.from_wqa101](https://vectorbt.pro/pvt_41531c19/api/indicators/factory/#vectorbtpro.indicators.factory.IndicatorFactory.from_wqa101)
(also available as the shortcut `vbt.wqa101`) for executing them.

- Unified column grouping for matrices and records across the entire codebase.
Most logic now resides in the sub-package [base.grouping](https://vectorbt.pro/pvt_41531c19/api/base/grouping/). Also, most functions
(apart from the portfolio-related ones) no longer require strict group ordering.

- Added chunking specification for labeling functions.

- Fixed [Config.prettify](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config.prettify) for non-string keys.

- Wrote [Basic RSI strategy](https://vectorbt.pro/pvt_41531c19/tutorials/basic-rsi) :notebook_with_decorative_cover:

## Version 1.0.7 (16 Jan, 2022)

- Changed `int_` to `np.integer` when passing to `np.issubdtype`.

- Refactored auto-aligned initial cash to be based on free cash flows and cash deposits.

- Upgraded the string parser of [deep_getattr](https://vectorbt.pro/pvt_41531c19/api/utils/attr_/#vectorbtpro.utils.attr_.deep_getattr)
to accept strings of chained method calls (more intuitive).

- Implemented superfast Pearson correlation coefficient and its rolling version.

- Created the class [Analyzable](https://vectorbt.pro/pvt_41531c19/api/generic/analyzable/#vectorbtpro.generic.analyzable.Analyzable),
which combines [Wrapping](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.Wrapping) and builder mixins.

- Metrics and subplots that require a single column no longer raise an error if the object
is two-dimensional and has only one column.

- [Grouper](https://vectorbt.pro/pvt_41531c19/api/base/grouping/#vectorbtpro.base.grouping.base.Grouper) can return a group map,
which is not tied to strict group ordering and is easier to use outside of Numba.

- Wrote documentation on [Building blocks](https://vectorbt.pro/pvt_41531c19/documentation/building-blocks/) :notebook_with_decorative_cover:

## Version 1.0.6 (9 Jan, 2022)

- Benchmark can be disabled by passing `bm_close=False` to
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

- Wrote documentation on [Fundamentals](https://vectorbt.pro/pvt_41531c19/documentation/fundamentals/) :notebook_with_decorative_cover:

## Version 1.0.5 (8 Jan, 2022)

- Fixed [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals)
for `direction='both'` and `size_type='value'`. Previously, the position could not be properly reversed.

- Avoid sorting paths in [FileData](https://vectorbt.pro/pvt_41531c19/api/data/custom/file/#vectorbtpro.data.custom.file.FileData)
if they are passed as a sequence.

## Version 1.0.4 (6 Jan, 2022)

- Set benchmark easily and globally (#32) by passing `bm_close` to
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).
Works similarly to [Portfolio.close](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.close).

- Shortened registry names (such as `ca_registry` to `ca_reg`).

## Version 1.0.3 (5 Jan, 2022)

- Automatic discovery of symbols for local data (#27): You no longer need to specify a path to each
CSV/HDF file or HDF key. Passing a path to a directory will traverse every file in that directory.
Passing a [glob-style pattern](https://en.wikipedia.org/wiki/Glob_(programming)) will use `glob.glob`
to search all files matching the pattern. Passing an HDF file will extract all keys inside that file.
All these options can be combined in any way.

- Minor fixes in the formatting module.

- Removed strict ordering of group and shape suffixes in
[Portfolio.get_in_output](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_in_output)
and [Portfolio.in_outputs_indexing_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.in_outputs_indexing_func).
For example, there is now no difference between in-place outputs named `myinout_2d_pc` and `myinout_pc_2d`.

- Improved readability of cacheable setups: When displaying the status overview, objects are shown
with shorter, human-readable strings. Their position among all objects registered globally and sorted by time
is also displayed. For example, `portfolio:2` means there are 2 more
[Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio) objects created earlier than
this one. This makes it easier to manage memory and debug garbage collection.

- Wrapping in-place outputs automatically: Implements the method
[Portfolio.get_in_output](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_in_output),
which you can use to access and automatically wrap any in-place output object from
[Portfolio.in_outputs](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.in_outputs).

- All registries can now be accessed directly via `vbt`.

- Updates and minor fixes to the ca_registry and ch_registry modules.

- Disabled parallelization of [generate_both](https://vectorbt.pro/pvt_41531c19/api/signals/accessors/#vectorbtpro.signals.accessors.SignalsAccessor.generate_both),
which was behaving unexpectedly.

- Fixed `in_outputs` in [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func)
to work with Numba 0.53.1. You must now define `in_outputs` as a named tuple before
passing it to the method.

- Added mapping of `window` to `rolling_period` in the QuantStats adapter.

---
link: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/2021/
parent: https://vectorbt.pro/pvt_41531c19/getting-started/release-notes/
name: Release notes for 2021
type: page
---

# Release notes for 2021

All notable changes are listed in reverse chronological order.

## Version 1.0.2 (31 Dec, 2021)

- Added Alpaca data source (#31). Unlike the open-source version, this version also allows you to pass a
pre-configured REST object to the
[AlpacaData.pull](https://vectorbt.pro/pvt_41531c19/api/data/custom/alpaca/#vectorbtpro.data.custom.alpaca.AlpacaData.pull) method.

- Changed the default index field of
[EntryTrades](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.EntryTrades) from `exit_idx` to `entry_idx`.

- Removed JSON and implemented a custom formatting engine that represents objects in Python format.
This change aligns seamlessly with VBT's move to dataclasses. Below is a comparison of a wrapper printed
by the open-source version with JSON, and VBT with the new engine:

```plaintext
ArrayWrapper(**Config({
    "index": "<RangeIndex at 0x1045815e8> of shape (3,)",
    "columns": "<Int64Index at 0x1045815e8> of shape (1,)",
    "ndim": 1,
    "freq": null,
    "column_only_select": null,
    "group_select": null,
    "grouped_ndim": null,
    "group_by": null,
    "allow_enable": true,
    "allow_disable": true,
    "allow_modify": true
}))
```

```python
ArrayWrapper(
    index=<RangeIndex at 0x1045815e8 of shape (3,)>,
    columns=<Int64Index at 0x1045815e8 of shape (1,)>,
    ndim=1,
    freq=None,
    column_only_select=None,
    group_select=None,
    grouped_ndim=None,
    grouper=Grouper(
        index=<Int64Index at 0x1045815e8 of shape (1,)>,
        group_by=None,
        allow_enable=True,
        allow_disable=True,
        allow_modify=True
    )
)
```

## Version 1.0.1 (21 Dec, 2021)

- Adapted the codebase to the new documentation format.

- Upgraded the documentation website generator from pdoc3 to MkDocs (Material Insiders).
The API is now automatically converted to Markdown files by a modified version of pdoc3 that
resides in a private repository of @polakowo.

## Version 1.0.0 (13 Dec, 2021)

!!! info
    This section briefly describes major changes made to the open-source version. For more details, see the commits.

### Execution

- Parallelized most functions that process 2-dimensional arrays using
[Explicit Parallel Loops](https://numba.pydata.org/numba-doc/0.37.0/user/parallel.html#explicit-parallel-loops).

- Built an infrastructure for chunking. Any Python function can be wrapped with the
[@chunked](https://vectorbt.pro/pvt_41531c19/api/utils/chunking/#vectorbtpro.utils.chunking.chunked) decorator, which returns a new
function with the same signature, but is able to 1) split the passed positional and keyword arguments into
multiple chunks, 2) execute each chunk of arguments using the wrapped function, and 3) merge the results
back together. The rules for splitting arguments and merging results must be clearly defined using
`arg_take_spec` and `merge_func`, respectively. Chunk taking and merging specifications are provided to
most Numba-compiled functions that handle 2-dimensional arrays. To allow chunking functions only when
requested, the [register_chunkable](https://vectorbt.pro/pvt_41531c19/api/registries/ch_registry/#vectorbtpro.registries.ch_registry.register_chunkable)
decorator was created. It leaves the Python function unwrapped while registering a "setup" with all
specifications in the global registry [ChunkableRegistry](https://vectorbt.pro/pvt_41531c19/api/registries/ch_registry/#vectorbtpro.registries.ch_registry.ChunkableRegistry).
There are several available engines for executing chunks:
[SerialEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.SerialEngine) (a simple queue),
[DaskEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.DaskEngine) (mainly for multithreading),
and [RayEngine](https://vectorbt.pro/pvt_41531c19/api/utils/execution/#vectorbtpro.utils.execution.RayEngine) (mainly for multiprocessing).

- Built an infrastructure for wrapping and running JIT-able functions. The core is the
[register_jitted](https://vectorbt.pro/pvt_41531c19/api/registries/jit_registry/#vectorbtpro.registries.jit_registry.register_jitted) decorator,
which registers a Python function and the instructions for JIT compilation with the global registry
[JITRegistry](https://vectorbt.pro/pvt_41531c19/api/registries/jit_registry/#vectorbtpro.registries.jit_registry.JITRegistry).
When instructed, the registry locates the function's setup and passes the function to a jitting class
(also known as a "jitter") for wrapping. Preset jitters include [NumPyJitter](https://vectorbt.pro/pvt_41531c19/api/utils/jitting/#vectorbtpro.utils.jitting.NumPyJitter)
for NumPy implementations, and [NumbaJitter](https://vectorbt.pro/pvt_41531c19/api/utils/jitting/#vectorbtpro.utils.jitting.NumbaJitter)
for Numba-compiled functions. The registry can also register tasks by task id and capture multiple jitter
candidates for the same task. You can switch between different implementations by specifying `jitter`.

### Generic

- Refactored many methods that take UDFs (such as [GenericAccessor.rolling_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_apply))
by converting each to both a class (meta) and an instance (regular) method using
[class_or_instancemethod](https://vectorbt.pro/pvt_41531c19/api/utils/decorators/#vectorbtpro.utils.decorators.class_or_instancemethod).
When the method is called on an instance, the UDFs only need to take a part of the array, such as
`apply_func_nb(window, *args)`. If the method is called on the class, it iterates over an abstract shape,
and its UDFs must take metadata for each iteration, which can be used to select part of any custom array
passed as a variable argument, such as `apply_func_nb(from_i, to_i, col, *args)`. Previously, UDFs had
to accept both the metadata and the array, even if the metadata was not used.

- Most functions that take custom UDFs and variable arguments, such as
[GenericAccessor.rolling_apply](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.rolling_apply),
now support [utils.template](https://vectorbt.pro/pvt_41531c19/api/utils/template/). The same applies to broadcasting named arguments,
a practice originally introduced in [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio).

- Made crossovers more resilient to NaN values and moved them to
[GenericAccessor.crossed_above](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_above)
and [GenericAccessor.crossed_below](https://vectorbt.pro/pvt_41531c19/api/generic/accessors/#vectorbtpro.generic.accessors.GenericAccessor.crossed_below).

- Added [BaseAccessor.eval](https://vectorbt.pro/pvt_41531c19/api/base/accessors/#vectorbtpro.base.accessors.BaseAccessor.eval), which is
similar to `pd.eval` but broadcasts inputs before evaluation and can switch between NumPy and NumExpr.

- Improved conflict control in [MappedArray](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray).
Multiple elements pointing to the same timestamp can be reduced using
[Mapped.reduce_segments](https://vectorbt.pro/pvt_41531c19/api/records/mapped_array/#vectorbtpro.records.mapped_array.MappedArray.reduce_segments).
They can also be safely converted to Pandas by repeating the index.

- Made tolerance checks and values for Numba math functions such as
[is_less_nb](https://vectorbt.pro/pvt_41531c19/api/utils/math_/#vectorbtpro.utils.math_.is_less_nb) globally adjustable.
Disabling tolerance checks improves performance but may cause round-off errors.

- Implemented context managers for profiling time ([Timer](https://vectorbt.pro/pvt_41531c19/api/utils/profiling/#vectorbtpro.utils.profiling.Timer))
and memory ([MemTracer](https://vectorbt.pro/pvt_41531c19/api/utils/profiling/#vectorbtpro.utils.profiling.MemTracer)).

- Added support for [Scattergl](https://plotly.github.io/plotly.py-docs/generated/plotly.graph_objects.Scattergl.html)
for plotting large datasets with increased speed. It is now used by default for more than 10,000 points.

### Broadcasting

- Refactored the broadcasting mechanism within [broadcast](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.broadcast).
Added the [BCO](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.BCO) dataclass, whose instances can be
passed to change broadcasting behavior for individual objects. Introduced the ability to build a Cartesian
product of scalar-like parameters and other broadcastable objects (using both `BCO` and `pd.Index` as
shortcuts) through operation trees and [generate_param_combs](https://vectorbt.pro/pvt_41531c19/api/utils/params/#vectorbtpro.utils.params.generate_param_combs).
A random subset of parameter combinations can also be automatically selected to emulate random search.
[Default](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Default),
[Ref](https://vectorbt.pro/pvt_41531c19/api/base/reshaping/#vectorbtpro.base.reshaping.Ref), and their surrounding logic have been moved
to this module.

### Data

- Implemented data classes for working with local files: [CSVData](https://vectorbt.pro/pvt_41531c19/api/data/custom/csv/#vectorbtpro.data.custom.csv.CSVData)
for CSV files and [HDFData](https://vectorbt.pro/pvt_41531c19/api/data/custom/hdf/#vectorbtpro.data.custom.hdf.HDFData) for HDF files and keys.
Both support efficient updates without reading the entire file. To support this, symbol fetching methods
can return a state, which is preserved for use in data updates.

- Refactored [RandomData](https://vectorbt.pro/pvt_41531c19/api/data/custom/random/#vectorbtpro.data.custom.random.RandomData) and
[GBMData](https://vectorbt.pro/pvt_41531c19/api/data/custom/gbm/#vectorbtpro.data.custom.gbm.GBMData).

- Improved handling of missing data. Made [BinanceData](https://vectorbt.pro/pvt_41531c19/api/data/custom/binance/#vectorbtpro.data.custom.binance.BinanceData)
and [CCXTData](https://vectorbt.pro/pvt_41531c19/api/data/custom/ccxt/#vectorbtpro.data.custom.ccxt.CCXTData) more robust: in case of
connectivity issues, data will not be lost and will be returned so it can be updated later.

- Moved progress bar logic into a separate module to standardize progress bar handling across VBT.
Added a progress bar for symbols in [Data](https://vectorbt.pro/pvt_41531c19/api/data/base/#vectorbtpro.data.base.Data).

- Renamed `download` to `fetch` throughout, since not all data sources are online.

### Portfolio

- Added a new simulation method,
[Portfolio.from_def_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_def_order_func),
that combines [Portfolio.from_orders](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_orders)
and [Portfolio.from_order_func](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_order_func).
It uses a custom order function to transform array-like objects into orders and allows
attaching and overriding user-defined callbacks to modify and monitor the simulation.

- Added support for in-place output simulation objects. Instead of creating various arrays during
the simulation, you can create them manually (or have them automatically created and broadcasted
by templates) outside the simulation, pass them as regular arguments, and modify them in-place.
These arrays are then conveniently stored in [Portfolio.in_outputs](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.in_outputs)
for further analysis. Additionally, [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
can detect when an in-place output array overrides a regular portfolio attribute and will use this array
instead of reconstructing the attribute. This provides an efficient way to precompute
various artifacts, such as returns.

- Implemented shortcut properties for [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio)
and [Records](https://vectorbt.pro/pvt_41531c19/api/records/base/#vectorbtpro.records.base.Records). These are cacheable properties
that call their respective getter methods with default arguments. This enables dot notation such as
`pf.trades.winning.pnl.count()`, where `trades` and `winning` are cached properties that call the
[Portfolio.get_trades](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_trades)
and [Trades.get_winning](https://vectorbt.pro/pvt_41531c19/api/portfolio/trades/#vectorbtpro.portfolio.trades.Trades.get_winning)
methods, respectively. In [Portfolio](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio),
shortcut properties can also use in-place outputs.

- Made various portfolio attributes (such as [Portfolio.get_returns](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.get_returns))
flexible by converting each into both a class and an instance method using
[class_or_instancemethod](https://vectorbt.pro/pvt_41531c19/api/utils/decorators/#vectorbtpro.utils.decorators.class_or_instancemethod).
If the method is called on the class, the operation runs using only the passed arguments.
If the method is called on an instance, the operation runs on the instance's data,
which can be overridden by setting any of the arguments.

- Introduced extra validation for arguments passed to simulations. For example, passing arrays
that appear boolean but have an object data type raises an informative error.

- Now, not only [Portfolio.from_signals](https://vectorbt.pro/pvt_41531c19/api/portfolio/base/#vectorbtpro.portfolio.base.Portfolio.from_signals),
but all simulation functions, accept `open`, `high`, and `low` (all optional).
This enables interesting automation: an order price of `-np.inf` is automatically
replaced by the opening price, and `np.inf` (default everywhere) is replaced by the closing price.
The highest and lowest prices are used for bar boundary checks.

- Added the following arguments:

    - `cash_deposits`: cash deposits or withdrawals at the beginning of each time step.

    - `cash_earnings`: cash earnings (independent of position) at the end of each time step.

    - `cash_dividends`: dividends (relative to position) at the end of each time step.

    - `init_position`: the initial position.

    - `stop_signal_priority`: which signal to prioritize: stop or user.

- Allowed price of `0`. This enables profitable insertion and removal of cash and assets.
For example, to periodically charge a fee, you can create a range of orders with zero price and
non-zero fees. These are visible as regular trades and appear in records.

- Allowed `max_order_records=0` and `max_log_records=0` to disable filling records.
For example, this may be desirable if performance is monitored during the simulation
and there is no need to retain this data for post-simulation analysis.
Also, for parallelization, both numbers refer to the maximum number of records **per column** rather
than for the entire input.

- Allowed negative fees (-0.05 means you earn 0.05% per trade instead of paying a fee).

- Converted simulation outputs to named tuples of type [SimulationOutput](https://vectorbt.pro/pvt_41531c19/api/portfolio/enums/#vectorbtpro.portfolio.enums.SimulationOutput).

### Returns

- Updated metrics based on returns to account for datetime-like properties. For example,
having two data points with timestamps "2020-01-01" and "2021-01-01" is
considered as a full year, rather than 2 days as before.
See [ArrayWrapper.dt_period](https://vectorbt.pro/pvt_41531c19/api/base/wrapping/#vectorbtpro.base.wrapping.ArrayWrapper.dt_period).

- Rolling metrics such as [ReturnsAccessor.rolling_sortino_ratio](https://vectorbt.pro/pvt_41531c19/api/returns/accessors/#vectorbtpro.returns.accessors.ReturnsAccessor.rolling_sortino_ratio)
are now much faster thanks to refactoring their Numba-compiled functions.

### Caching

- Completely refactored caching. Previously, caching was managed by specialized property and
method decorators. When you invoked such a property or method, it checked global settings to see
whether it was blacklisted and stored the cache on the instance it was bound to.
Cached attributes were not easily discoverable, resulting in less transparency.
The new approach uses a global registry, [CacheableRegistry](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CacheableRegistry),
which registers "setups" for all cacheable objects, including functions, properties, methods,
instances, and even classes. These all form a well-connected hierarchy that can propagate actions.
For example, disabling caching in a class setup of type [CAClassSetup](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CAClassSetup)
will disable caching for all of its child setups, down to [CARunSetup](https://vectorbt.pro/pvt_41531c19/api/registries/ca_registry/#vectorbtpro.registries.ca_registry.CARunSetup),
which handles the actual caching. Cacheable decorators such as [cacheable](https://vectorbt.pro/pvt_41531c19/api/utils/decorators/#vectorbtpro.utils.decorators.cacheable)
communicate with the registry and perform all actions on their particular setup.
Users can easily find the setup for any cacheable object to, for instance, display various caching
statistics.

- Removed caching of attributes that return DataFrames (with a few exceptions) to avoid wasting memory.

### Design

- Restructured the project and reformatted the codebase. Notably, Numba-compiled simulation
functions are now distributed across multiple modules.

- Some previously required packages, such as Plotly and Dill, were made optional to make
the VBT core even more lightweight. Optional packages are tracked in
[utils.opt_packages](https://vectorbt.pro/pvt_41531c19/api/utils/opt_packages/), and when code requires a missing package,
an error is raised with instructions for installation.

- Converted minimalistic classes to dataclasses using [attrs](https://www.attrs.org/en/stable/).

- Refactored [Config](https://vectorbt.pro/pvt_41531c19/api/utils/config/#vectorbtpro.utils.config.Config), reducing
the initialization time of various VBT objects by 25%. Config respects the Liskov substitution principle
and, similar to a dict, can be initialized with both positional and keyword arguments.
Also, created read-only and hybrid preset classes to unify configs created across VBT.

- Removed expected key checks, making subclassing VBT classes easier but removing
dynamic checks of keyword arguments passed to the initializer (which was unnecessary anyway).

- Accessors are now cached by default (which can be changed in the settings) to avoid repeated
initialization, and all options for changing data in-place have been removed.

- Made [_settings](https://vectorbt.pro/pvt_41531c19/api/_settings/) more modular and easier to embed in documentation.
Additionally, upon import, VBT looks for an environment variable containing the path
to a settings file and updates or replaces the current settings in-place.

- Created and set up a private repository :sparkler:

*[SP]: Single-pass algorithm for best performance
*[NB]: Numba-compiled

---
link: https://vectorbt.pro/pvt_41531c19/terms/terms-of-use/
parent: https://vectorbt.pro/pvt_41531c19/terms/
name: Terms of Use
type: page
---

# Terms of Use

**Last updated** **September 21, 2023**

**AGREEMENT TO OUR LEGAL TERMS**

These Legal Terms constitute a legally binding agreement made between
you, whether personally or on behalf of an entity ("**you**"),
and Oleg Polakow ("**Company**," "**we**," "**us**," "**our**"),
concerning your access to and use of the private GitHub repository
vectorbt.pro (the "**Software**"), the private Discord server
vectorbt.pro (the "**Discord Server**"), the
website vectorbt.pro (the "**Site**"), as well as any other related
products and services that refer or link to these legal terms
(the "**Legal Terms**") (collectively, the "**Services**"). You
agree that by accessing the Services, you have read, understood, and
agreed to be bound by all of these Legal Terms. IF YOU DO NOT AGREE WITH
ALL OF THESE LEGAL TERMS, THEN YOU ARE EXPRESSLY PROHIBITED FROM USING
THE SERVICES AND YOU MUST DISCONTINUE USE IMMEDIATELY.

Supplemental terms and conditions or documents that may be posted on the
Services from time to time are hereby expressly incorporated herein by
reference. We reserve the right, in our sole discretion, to make changes
or modifications to these Legal Terms from time to time. We will alert
you about any changes by updating the "Last updated" date of these
Legal Terms, and you waive any right to receive specific notice of each
such change. It is your responsibility to periodically review these
Legal Terms to stay informed of updates. You will be subject to, and
will be deemed to have been made aware of and to have accepted, the
changes in any revised Legal Terms by your continued use of the Services
after the date such revised Legal Terms are posted.

The Services are intended for users who are at least 13 years of age.
All users who are minors in the jurisdiction in which they reside
(generally under the age of 18) must have the permission of, and be
directly supervised by, their parent or guardian to use the Services. If
you are a minor, you must have your parent or guardian read and agree to
these Legal Terms prior to you using the Services.

We recommend that you print a copy of these Legal Terms for your
records.

**1. OUR SERVICES**

The information provided when using the Services is not intended for
distribution to or use by any person or entity in any jurisdiction or
country where such distribution or use would be contrary to law or
regulation or which would subject us to any registration requirement
within such jurisdiction or country. Accordingly, those persons who
choose to access the Services from other locations do so on their own
initiative and are solely responsible for compliance with local laws, if
and to the extent local laws are applicable.

The Services are not tailored to comply with industry-specific
regulations (Health Insurance Portability and Accountability Act
(HIPAA), Federal Information Security Management Act (FISMA), etc.), so
if your interactions would be subjected to such laws, you may not use
the Services. You may not use the Services in a way that would violate
the Gramm-Leach-Bliley Act (GLBA).

**2. INTELLECTUAL PROPERTY RIGHTS**

**Our intellectual property**

We are the owner or the licensee of all intellectual property rights in
our Services, including all source code, databases, functionality,
software, website designs, audio, video, text, photographs, and graphics
in the Services (collectively, the "Content"), as well as the
trademarks, service marks, and logos contained therein (the "Marks").

Our Content and Marks are protected by copyright and trademark laws (and
various other intellectual property rights and unfair competition laws)
and treaties in the United States and around the world.

The Content and Marks are provided in or through the Services "AS
IS" for your personal, non-commercial use only.

**Your use of our Services**

Subject to your compliance with these Legal Terms, including
the "PROHIBITED ACTIVITIES" section below, we grant you
a non-exclusive, non-transferable, revocable license to:

-   access the Services; and

-   download or print a copy of any portion of the Content to which you
    have properly gained access.

solely for your personal, non-commercial use.

Except as set out in this section or elsewhere in our Legal Terms, no
part of the Services and no Content or Marks may be copied, reproduced,
aggregated, republished, uploaded, posted, publicly displayed, encoded,
translated, transmitted, distributed, sold, licensed, or otherwise
exploited for any commercial purpose whatsoever, without our express
prior written permission.

If you wish to make any use of the Services, Content, or Marks other
than as set out in this section or elsewhere in our Legal Terms, please
address your request to: olegpolakow@vectorbt.pro. If we ever grant you
the permission to post, reproduce, or publicly display any part of our
Services or Content, you must identify us as the owners or licensors of
the Services, Content, or Marks and ensure that any copyright or
proprietary notice appears or is visible on posting, reproducing, or
displaying our Content.

We reserve all rights not expressly granted to you in and to the
Services, Content, and Marks.

Any breach of these Intellectual Property Rights will constitute a
material breach of our Legal Terms and your right to use our Services
will terminate immediately.

**Your submissions**

Please review this section and the "PROHIBITED 
ACTIVITIES" section carefully prior to using our Services
to understand the (a) rights you give us and (b) obligations you have
when you post or upload any content through the Services.

**Submissions:** By directly sending us any question, comment,
suggestion, idea, feedback, or other information about the Services
("Submissions"), you agree to assign to us all intellectual property
rights in such Submission. You agree that we shall own this Submission
and be entitled to its unrestricted use and dissemination for any lawful
purpose, commercial or otherwise, without acknowledgment or compensation
to you.

**You are responsible for what you post or upload:** By sending us
Submissions through any part of the Services you:

-   confirm that you have read and agree with our "PROHIBITED 
    ACTIVITIES" and will not post, send, publish, upload,
    or transmit through the Services any Submission that is illegal,
    harassing, hateful, harmful, defamatory, obscene, bullying, abusive,
    discriminatory, threatening to any person or group, sexually
    explicit, false, inaccurate, deceitful, or misleading;

-   to the extent permissible by applicable law, waive any and all moral
    rights to any such Submission;

-   warrant that any such Submission are original to you or that you
    have the necessary rights and licenses to submit such
    Submissions and that you have full authority to grant us the
    above-mentioned rights in relation to your Submissions; and

-   warrant and represent that your Submissions do not constitute
    confidential information.

You are solely responsible for your Submissions and you expressly agree
to reimburse us for any and all losses that we may suffer because of
your breach of (a) this section, (b) any third party's intellectual
property rights, or (c) applicable law.

**3. USER REPRESENTATIONS**

By using the Services, you represent and warrant that: (1) you have the
legal capacity and you agree to comply with these Legal Terms; (2) you
are not under the age of 13; (3) you are not a minor in the jurisdiction
in which you reside, or if a minor, you have received parental
permission to use the Services; (4) you will not access the Services
through automated or non-human means, whether through a bot, script or
otherwise; (5) you will not use the Services for any illegal
or unauthorized purpose; and (6) your use of the Services will not
violate any applicable law or regulation.

If you provide any information that is untrue, inaccurate, not current,
or incomplete, we have the right to suspend or terminate your account
and refuse any and all current or future use of the Services (or any
portion thereof).

**4. PURCHASES AND PAYMENT**

We accept the following forms of payment:

\-  GitHub Sponsors

\-  Cryptocurrency

\-  Ko-Fi

You agree to provide current, complete, and accurate purchase and
account information for all purchases made via the Services. You further
agree to promptly update account and payment information, including
email address, payment method, and payment card expiration date, so that
we can complete your transactions and contact you as needed. Sales tax
will be added to the price of purchases as deemed required by us. We may
change prices at any time. All payments shall be in US dollars.

You agree to pay all charges at the prices then in effect for your
purchases and any applicable shipping fees, and you authorize us to
charge your chosen payment provider for any such amounts upon placing
your order. If your order is subject to recurring charges, then you
consent to our charging your payment method on a recurring basis without
requiring your prior approval for each recurring charge, until such time
as you cancel the applicable order. We reserve the right to correct any
errors or mistakes in pricing, even if we have already requested or
received payment.

We reserve the right to refuse any order placed through the Services. We
may, in our sole discretion, limit or cancel quantities purchased per
person, per household, or per order. These restrictions may include
orders placed by or under the same customer account, the same payment
method, and/or orders that use the same billing or shipping address. We
reserve the right to limit or prohibit orders that, in our
sole judgment, appear to be placed by dealers, resellers, or
distributors.

**5. CANCELLATION**

All purchases are non-refundable. You can cancel your subscription at
any time by logging into your account. Your cancellation will take
effect at the end of the current paid term.

If you are unsatisfied with our Services, please email us
at olegpolakow@vectorbt.pro.

**6. SOFTWARE**

We may include software for use in connection with our Services. If such
software is accompanied by an end user license agreement ("EULA"), the
terms of the EULA will govern your use of the software. If such software
is not accompanied by a EULA, then we grant to you a non-exclusive,
revocable, personal, and non-transferable license to use such software
solely in connection with our services and in accordance with these
Legal Terms. Any software and any related documentation is provided "AS
IS" without warranty of any kind, either express or implied, including,
without limitation, the implied warranties of merchantability, fitness
for a particular purpose, or non-infringement. You accept any and all
risk arising out of use or performance of any software. You may not
reproduce or redistribute any software except in accordance with the
EULA or these Legal Terms.

**7. PROHIBITED ACTIVITIES**

You may not access or use the Services for any purpose other than that
for which we make the Services available. The Services may not be used
in connection with any commercial endeavors except those that are
specifically endorsed or approved by us.

As a user of the Services, you agree not to:

-   Systematically retrieve data or other content from the Services to
    create or compile, directly or indirectly, a collection,
    compilation, database, or directory without written permission from
    us.

-   Trick, defraud, or mislead us and other users, especially in any
    attempt to learn sensitive account information such as user
    passwords.

-   Circumvent, disable, or otherwise interfere with security-related
    features of the Services, including features that prevent or
    restrict the use or copying of any Content or enforce limitations on
    the use of the Services and/or the Content contained therein.

-   Disparage, tarnish, or otherwise harm, in our opinion, us and/or the
    Services.

-   Use any information obtained from the Services in order to harass,
    abuse, or harm another person.

-   Make improper use of our support services or submit false reports of
    abuse or misconduct.

-   Use the Services in a manner inconsistent with any applicable laws
    or regulations.

-   Engage in unauthorized framing of or linking to the Services.

-   Upload or transmit (or attempt to upload or to transmit) viruses,
    Trojan horses, or other material, including excessive use of capital
    letters and spamming (continuous posting of repetitive text), that
    interferes with any party's uninterrupted use and enjoyment of the
    Services or modifies, impairs, disrupts, alters, or interferes with
    the use, features, functions, operation, or maintenance of the
    Services.

-   Engage in any automated use of the system, such as using scripts to
    send comments or messages, or using any data mining, robots, or
    similar data gathering and extraction tools.

-   Delete the copyright or other proprietary rights notice from any
    Content.

-   Attempt to impersonate another user or person or use the username of
    another user.

-   Upload or transmit (or attempt to upload or to transmit) any
    material that acts as a passive or active information collection or
    transmission mechanism, including without limitation, clear graphics
    interchange formats ("gifs"), 1×1 pixels, web bugs, cookies, or
    other similar devices (sometimes referred to as "spyware" or
    "passive collection mechanisms" or "pcms").

-   Interfere with, disrupt, or create an undue burden on the Services
    or the networks or services connected to the Services.

-   Harass, annoy, intimidate, or threaten any of our employees or
    agents engaged in providing any portion of the Services to you.

-   Attempt to bypass any measures of the Services designed to prevent
    or restrict access to the Services, or any portion of the Services.

-   Copy or adapt the Services\' software, including but not limited to
    Flash, PHP, HTML, JavaScript, or other code.

-   Except as permitted by applicable law, decipher, decompile,
    disassemble, or reverse engineer any of the software comprising or
    in any way making up a part of the Services.

-   Except as may be the result of standard search engine or Internet
    browser usage, use, launch, develop, or distribute any automated
    system, including without limitation, any spider, robot, cheat
    utility, scraper, or offline reader that accesses the Services, or
    use or launch any unauthorized script or other software.

-   Use a buying agent or purchasing agent to make purchases on the
    Services.

-   Make any unauthorized use of the Services, including collecting
    usernames and/or email addresses of users by electronic or other
    means for the purpose of sending unsolicited email, or creating user
    accounts by automated means or under false pretenses.

-   Use the Services as part of any effort to compete with us or
    otherwise use the Services and/or the Content for any
    revenue-generating endeavor or commercial enterprise.

-   Use the Services to advertise or offer to sell goods and services.

-   Share software and associated documentation with third parties or
    use for commercial purposes

**8. USER GENERATED CONTRIBUTIONS**

The Services does not offer users to submit or post content. We may
provide you with the opportunity to create, submit, post, display,
transmit, perform, publish, distribute, or broadcast content and
materials to us or on the Services, including but not limited to text,
writings, video, audio, photographs, graphics, comments, suggestions, or
personal information or other material
(collectively, "Contributions"). Contributions may be viewable by
other users of the Services and through third-party websites. When you
create or make available any Contributions, you thereby represent and
warrant that:

-   The creation, distribution, transmission, public display, or
    performance, and the accessing, downloading, or copying of your
    Contributions do not and will not infringe the proprietary rights,
    including but not limited to the copyright, patent, trademark, trade
    secret, or moral rights of any third party.

-   You are the creator and owner of or have the necessary licenses,
    rights, consents, releases, and permissions to use and
    to authorize us, the Services, and other users of the Services to
    use your Contributions in any manner contemplated by the Services
    and these Legal Terms.

-   You have the written consent, release, and/or permission of each and
    every identifiable individual person in your Contributions to use
    the name or likeness of each and every such identifiable individual
    person to enable inclusion and use of your Contributions in any
    manner contemplated by the Services and these Legal Terms.

-   Your Contributions are not false, inaccurate, or misleading.

-   Your Contributions are not unsolicited or unauthorized advertising,
    promotional materials, pyramid schemes, chain letters, spam, mass
    mailings, or other forms of solicitation.

-   Your Contributions are not obscene, lewd, lascivious, filthy,
    violent, harassing, libelous, slanderous, or otherwise objectionable
    (as determined by us).

-   Your Contributions do not ridicule, mock, disparage, intimidate, or
    abuse anyone.

-   Your Contributions are not used to harass or threaten (in the legal
    sense of those terms) any other person and to promote violence
    against a specific person or class of people.

-   Your Contributions do not violate any applicable law, regulation, or
    rule.

-   Your Contributions do not violate the privacy or publicity rights of
    any third party.

-   Your Contributions do not violate any applicable law concerning
    child pornography, or otherwise intended to protect the health or
    well-being of minors.

-   Your Contributions do not include any offensive comments that are
    connected to race, national origin, gender, sexual preference, or
    physical handicap.

-   Your Contributions do not otherwise violate, or link to material
    that violates, any provision of these Legal Terms, or any applicable
    law or regulation.

Any use of the Services in violation of the foregoing violates these
Legal Terms and may result in, among other things, termination or
suspension of your rights to use the Services.

**9. CONTRIBUTION LICENSE**

You and Services agree that we may access, store, process, and use any
information and personal data that you provide and your choices
(including settings).

By submitting suggestions or other feedback regarding the Services, you
agree that we can use and share such feedback for any purpose without
compensation to you.

We do not assert any ownership over your Contributions. You retain full
ownership of all of your Contributions and any intellectual property
rights or other proprietary rights associated with your Contributions.
We are not liable for any statements or representations in your
Contributions provided by you in any area on the Services. You are
solely responsible for your Contributions to the Services and you
expressly agree to exonerate us from any and all responsibility and to
refrain from any legal action against us regarding your Contributions.

**10. THIRD-PARTY WEBSITES AND CONTENT**

The Services may contain (or you may be sent via the Site) links to
other websites ("Third-Party Websites") as well as articles,
photographs, text, graphics, pictures, designs, music, sound, video,
information, applications, software, and other content or items
belonging to or originating from third parties ("Third-Party
Content"). Such Third-Party Websites and Third-Party Content are not
investigated, monitored, or checked for accuracy, appropriateness, or
completeness by us, and we are not responsible for any Third-Party
Websites accessed through the Services or any Third-Party Content posted
on, available through, or installed from the Services, including the
content, accuracy, offensiveness, opinions, reliability, privacy
practices, or other policies of or contained in the Third-Party Websites
or the Third-Party Content. Inclusion of, linking to, or permitting the
use or installation of any Third-Party Websites or
any Third-Party Content does not imply approval or endorsement thereof
by us. If you decide to leave the Services and access
the Third-Party Websites or to use or install any Third-Party Content,
you do so at your own risk, and you should be aware these Legal Terms no
longer govern. You should review the applicable terms and policies,
including privacy and data gathering practices, of any website to which
you navigate from the Services or relating to any applications you use
or install from the Services. Any purchases you make
through Third-Party Websites will be through other websites and from
other companies, and we take no responsibility whatsoever in relation to
such purchases which are exclusively between you and the applicable
third party. You agree and acknowledge that we do not endorse the
products or services offered on Third-Party Websites and you shall hold
us blameless from any harm caused by your purchase of such products or
services. Additionally, you shall hold us blameless from any losses
sustained by you or harm caused to you relating to or resulting in any
way from any Third-Party Content or any contact
with Third-Party Websites.

**11. ADVERTISERS**

We allow advertisers to display their advertisements and other
information in certain areas of the Services, such as sidebar
advertisements or banner advertisements. We simply provide the space to
place such advertisements, and we have no other relationship with
advertisers.

**12. SERVICES MANAGEMENT**

We reserve the right, but not the obligation, to: (1) monitor the
Services for violations of these Legal Terms; (2) take appropriate legal
action against anyone who, in our sole discretion, violates the law or
these Legal Terms, including without limitation, reporting such user to
law enforcement authorities; (3) in our sole discretion and without
limitation, refuse, restrict access to, limit the availability of, or
disable (to the extent technologically feasible) any of your
Contributions or any portion thereof; (4) in our sole discretion and
without limitation, notice, or liability, to remove from the Services or
otherwise disable all files and content that are excessive in size or
are in any way burdensome to our systems; and (5) otherwise manage the
Services in a manner designed to protect our rights and property and to
facilitate the proper functioning of the Services.

**13. PRIVACY POLICY**

We care about data privacy and security. By using the Services, you
agree to be bound by our Privacy Policy posted on the Services, which is
incorporated into these Legal Terms. Please be advised the Services are
hosted in the United States. If you access the Services from any other
region of the world with laws or other requirements governing personal
data collection, use, or disclosure that differ from applicable laws
in the United States, then through your continued use of the Services,
you are transferring your data to the United States, and you expressly
consent to have your data transferred to and processed in the United
States. Further, we do not knowingly accept, request, or solicit
information from children or knowingly market to children. Therefore, in
accordance with the U.S. Children's Online Privacy Protection Act, if we
receive actual knowledge that anyone under the age of 13 has provided
personal information to us without the requisite and verifiable parental
consent, we will delete that information from the Services as quickly as
is reasonably practical.

**14. TERM AND TERMINATION**

These Legal Terms shall remain in full force and effect while you use
the Services. WITHOUT LIMITING ANY OTHER PROVISION OF THESE LEGAL TERMS,
WE RESERVE THE RIGHT TO, IN OUR SOLE DISCRETION AND WITHOUT NOTICE OR
LIABILITY, DENY ACCESS TO AND USE OF THE SERVICES (INCLUDING BLOCKING
CERTAIN IP ADDRESSES), TO ANY PERSON FOR ANY REASON OR FOR NO REASON,
INCLUDING WITHOUT LIMITATION FOR BREACH OF ANY REPRESENTATION, WARRANTY,
OR COVENANT CONTAINED IN THESE LEGAL TERMS OR OF ANY APPLICABLE LAW OR
REGULATION. WE MAY TERMINATE YOUR USE OR PARTICIPATION IN THE SERVICES
OR DELETE ANY CONTENT OR INFORMATION THAT YOU POSTED AT ANY TIME,
WITHOUT WARNING, IN OUR SOLE DISCRETION.

If we terminate or suspend your account for any reason, you are
prohibited from registering and creating a new account under your name,
a fake or borrowed name, or the name of any third party, even if you may
be acting on behalf of the third party. In addition to terminating or
suspending your account, we reserve the right to take appropriate legal
action, including without limitation pursuing civil, criminal, and
injunctive redress.

**15. MODIFICATIONS AND INTERRUPTIONS**

We reserve the right to change, modify, or remove the contents of the
Services at any time or for any reason at our sole discretion without
notice. However, we have no obligation to update any information on our
Services. We will not be liable to you or any third party for any
modification, price change, suspension, or discontinuance of the
Services.

We cannot guarantee the Services will be available at all times. We may
experience hardware, software, or other problems or need to perform
maintenance related to the Services, resulting in interruptions, delays,
or errors. We reserve the right to change, revise, update, suspend,
discontinue, or otherwise modify the Services at any time or for any
reason without notice to you. You agree that we have no liability
whatsoever for any loss, damage, or inconvenience caused by your
inability to access or use the Services during any downtime or
discontinuance of the Services. Nothing in these Legal Terms will be
construed to obligate us to maintain and support the Services or to
supply any corrections, updates, or releases in connection therewith.

**16. GOVERNING LAW**

These Legal Terms are governed by and interpreted following the laws
of Germany, and the use of the United Nations Convention of Contracts
for the International Sales of Goods is expressly excluded. If your
habitual residence is in the EU, and you are a consumer, you
additionally possess the protection provided to you by obligatory
provisions of the law in your country to residence. Oleg Polakow and
yourself both agree to submit to the non-exclusive jurisdiction of the
courts of Munich, which means that you may make a claim to defend your
consumer protection rights in regards to these Legal Terms in Germany,
or in the EU country in which you reside.

**17. DISPUTE RESOLUTION**

**Informal Negotiations**

To expedite resolution and control the cost of any dispute, controversy,
or claim related to these Legal Terms (each a "Dispute" and
collectively, the "Disputes") brought by either you or us
(individually, a "Party" and collectively, the "Parties"), the
Parties agree to first attempt to negotiate any Dispute (except those
Disputes expressly provided below) informally for at least thirty
(30) days before initiating arbitration. Such informal negotiations
commence upon written notice from one Party to the other Party.

**Binding Arbitration**

Any dispute arising from the relationships between the Parties to these
Legal Terms shall be determined by one arbitrator who will be chosen in
accordance with the Arbitration and Internal Rules of the European Court
of Arbitration being part of the European Centre of Arbitration having
its seat in Strasbourg, and which are in force at the time the
application for arbitration is filed, and of which adoption of this
clause constitutes acceptance. The seat of arbitration shall
be Munich, Germany. The language of the proceedings shall be English.
Applicable rules of substantive law shall be the law of Germany.

**Restrictions**

The Parties agree that any arbitration shall be limited to the Dispute
between the Parties individually. To the full extent permitted by law,
(a) no arbitration shall be joined with any other proceeding; (b) there
is no right or authority for any Dispute to be arbitrated on a
class-action basis or to utilize class action procedures; and (c) there
is no right or authority for any Dispute to be brought in a purported
representative capacity on behalf of the general public or any other
persons.

**Exceptions to Informal Negotiations and Arbitration**

The Parties agree that the following Disputes are not subject to the
above provisions concerning informal negotiations binding arbitration:
(a) any Disputes seeking to enforce or protect, or concerning the
validity of, any of the intellectual property rights of a Party; (b) any
Dispute related to, or arising from, allegations of theft, piracy,
invasion of privacy, or unauthorized use; and (c) any claim for
injunctive relief. If this provision is found to be illegal or
unenforceable, then neither Party will elect to arbitrate any Dispute
falling within that portion of this provision found to be illegal or
unenforceable and such Dispute shall be decided by a court of competent
jurisdiction within the courts listed for jurisdiction above, and the
Parties agree to submit to the personal jurisdiction of that court.

**18. CORRECTIONS**

There may be information on the Services that contains typographical
errors, inaccuracies, or omissions, including descriptions, pricing,
availability, and various other information. We reserve the right to
correct any errors, inaccuracies, or omissions and to change or update
the information on the Services at any time, without prior notice.

**19. DISCLAIMER**

THE SERVICES ARE PROVIDED ON AN AS-IS AND AS-AVAILABLE BASIS. YOU AGREE
THAT YOUR USE OF THE SERVICES WILL BE AT YOUR SOLE RISK. TO THE FULLEST
EXTENT PERMITTED BY LAW, WE DISCLAIM ALL WARRANTIES, EXPRESS OR IMPLIED,
IN CONNECTION WITH THE SERVICES AND YOUR USE THEREOF, INCLUDING, WITHOUT
LIMITATION, THE IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
PARTICULAR PURPOSE, AND NON-INFRINGEMENT. WE MAKE NO WARRANTIES OR
REPRESENTATIONS ABOUT THE ACCURACY OR COMPLETENESS OF THE SERVICES\'
CONTENT OR THE CONTENT OF ANY WEBSITES OR MOBILE APPLICATIONS LINKED TO
THE SERVICES AND WE WILL ASSUME NO LIABILITY OR RESPONSIBILITY FOR ANY
(1) ERRORS, MISTAKES, OR INACCURACIES OF CONTENT AND MATERIALS, (2)
PERSONAL INJURY OR PROPERTY DAMAGE, OF ANY NATURE WHATSOEVER, RESULTING
FROM YOUR ACCESS TO AND USE OF THE SERVICES, (3) ANY UNAUTHORIZED ACCESS
TO OR USE OF OUR SECURE SERVERS AND/OR ANY AND ALL PERSONAL INFORMATION
AND/OR FINANCIAL INFORMATION STORED THEREIN, (4) ANY INTERRUPTION OR
CESSATION OF TRANSMISSION TO OR FROM THE SERVICES, (5) ANY BUGS,
VIRUSES, TROJAN HORSES, OR THE LIKE WHICH MAY BE TRANSMITTED TO OR
THROUGH THE SERVICES BY ANY THIRD PARTY, AND/OR (6) ANY ERRORS OR
OMISSIONS IN ANY CONTENT AND MATERIALS OR FOR ANY LOSS OR DAMAGE OF ANY
KIND INCURRED AS A RESULT OF THE USE OF ANY CONTENT POSTED, TRANSMITTED,
OR OTHERWISE MADE AVAILABLE VIA THE SERVICES. WE DO NOT WARRANT,
ENDORSE, GUARANTEE, OR ASSUME RESPONSIBILITY FOR ANY PRODUCT OR SERVICE
ADVERTISED OR OFFERED BY A THIRD PARTY THROUGH THE SERVICES, ANY
HYPERLINKED WEBSITE, OR ANY WEBSITE OR MOBILE APPLICATION FEATURED IN
ANY BANNER OR OTHER ADVERTISING, AND WE WILL NOT BE A PARTY TO OR IN ANY
WAY BE RESPONSIBLE FOR MONITORING ANY TRANSACTION BETWEEN YOU AND ANY
THIRD-PARTY PROVIDERS OF PRODUCTS OR SERVICES. AS WITH THE PURCHASE OF A
PRODUCT OR SERVICE THROUGH ANY MEDIUM OR IN ANY ENVIRONMENT, YOU SHOULD
USE YOUR BEST JUDGMENT AND EXERCISE CAUTION WHERE APPROPRIATE.

**20. LIMITATIONS OF LIABILITY**

IN NO EVENT WILL WE OR OUR DIRECTORS, EMPLOYEES, OR AGENTS BE LIABLE TO
YOU OR ANY THIRD PARTY FOR ANY DIRECT, INDIRECT, CONSEQUENTIAL,
EXEMPLARY, INCIDENTAL, SPECIAL, OR PUNITIVE DAMAGES, INCLUDING LOST
PROFIT, LOST REVENUE, LOSS OF DATA, OR OTHER DAMAGES ARISING FROM YOUR
USE OF THE SERVICES, EVEN IF WE HAVE BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES. NOTWITHSTANDING ANYTHING TO THE CONTRARY CONTAINED HEREIN,
OUR LIABILITY TO YOU FOR ANY CAUSE WHATSOEVER AND REGARDLESS OF THE FORM
OF THE ACTION, WILL AT ALL TIMES BE LIMITED TO THE AMOUNT PAID, IF ANY,
BY YOU TO US DURING THE SIX (6) MONTH PERIOD PRIOR TO ANY CAUSE OF
ACTION ARISING. CERTAIN US STATE LAWS AND INTERNATIONAL LAWS DO NOT
ALLOW LIMITATIONS ON IMPLIED WARRANTIES OR THE EXCLUSION OR LIMITATION
OF CERTAIN DAMAGES. IF THESE LAWS APPLY TO YOU, SOME OR ALL OF THE ABOVE
DISCLAIMERS OR LIMITATIONS MAY NOT APPLY TO YOU, AND YOU MAY HAVE
ADDITIONAL RIGHTS.

**21. INDEMNIFICATION**

You agree to defend, indemnify, and hold us harmless, including our
subsidiaries, affiliates, and all of our respective officers, agents,
partners, and employees, from and against any loss, damage, liability,
claim, or demand, including reasonable attorneys' fees and expenses,
made by any third party due to or arising out of: (1) use of the
Services; (2) breach of these Legal Terms; (3) any breach of your
representations and warranties set forth in these Legal Terms; (4) your
violation of the rights of a third party, including but not limited to
intellectual property rights; or (5) any overt harmful act toward any
other user of the Services with whom you connected via the Services.
Notwithstanding the foregoing, we reserve the right, at your expense, to
assume the exclusive defense and control of any matter for which you are
required to indemnify us, and you agree to cooperate, at your expense,
with our defense of such claims. We will use reasonable efforts to
notify you of any such claim, action, or proceeding which is subject to
this indemnification upon becoming aware of it.

**22. USER DATA**

We will maintain certain data that you transmit to the Services for the
purpose of managing the performance of the Services, as well as data
relating to your use of the Services. Although we perform regular
routine backups of data, you are solely responsible for all data that
you transmit or that relates to any activity you have undertaken using
the Services. You agree that we shall have no liability to you for any
loss or corruption of any such data, and you hereby waive any right of
action against us arising from any such loss or corruption of such data.

**23. ELECTRONIC COMMUNICATIONS, TRANSACTIONS, AND SIGNATURES**

Visiting the Services, sending us emails, and completing online forms
constitute electronic communications. You consent to receive electronic
communications, and you agree that all agreements, notices, disclosures,
and other communications we provide to you electronically, via email and
on the Services, satisfy any legal requirement that such communication
be in writing. YOU HEREBY AGREE TO THE USE OF ELECTRONIC SIGNATURES,
CONTRACTS, ORDERS, AND OTHER RECORDS, AND TO ELECTRONIC DELIVERY OF
NOTICES, POLICIES, AND RECORDS OF TRANSACTIONS INITIATED OR COMPLETED BY
US OR VIA THE SERVICES. You hereby waive any rights or requirements
under any statutes, regulations, rules, ordinances, or other laws in any
jurisdiction which require an original signature or delivery or
retention of non-electronic records, or to payments or the granting of
credits by any means other than electronic means.

**24. CALIFORNIA USERS AND RESIDENTS**

If any complaint with us is not satisfactorily resolved, you can contact
the Complaint Assistance Unit of the Division of Consumer Services of
the California Department of Consumer Affairs in writing at 1625 North
Market Blvd., Suite N 112, Sacramento, California 95834 or by telephone
at (800) 952-5210 or (916) 445-1254.

**25. MISCELLANEOUS**

These Legal Terms and any policies or operating rules posted by us on
the Services or in respect to the Services constitute the entire
agreement and understanding between you and us. Our failure to exercise
or enforce any right or provision of these Legal Terms shall not operate
as a waiver of such right or provision. These Legal Terms operate to the
fullest extent permissible by law. We may assign any or all of our
rights and obligations to others at any time. We shall not be
responsible or liable for any loss, damage, delay, or failure to act
caused by any cause beyond our reasonable control. If any provision or
part of a provision of these Legal Terms is determined to be unlawful,
void, or unenforceable, that provision or part of the provision is
deemed severable from these Legal Terms and does not affect the validity
and enforceability of any remaining provisions. There is no joint
venture, partnership, employment or agency relationship created between
you and us as a result of these Legal Terms or use of the Services. You
agree that these Legal Terms will not be construed against us by virtue
of having drafted them. You hereby waive any and all defenses you may
have based on the electronic form of these Legal Terms and the lack of
signing by the parties hereto to execute these Legal Terms.

**26. CONTACT US**

In order to resolve a complaint regarding the Services or to receive
further information regarding use of the Services, please contact us at:

**Oleg Polakow**

**\_\_\_\_\_\_\_\_\_\_**

**olegpolakow@vectorbt.pro**

---
link: https://vectorbt.pro/pvt_41531c19/terms/software-license/
parent: https://vectorbt.pro/pvt_41531c19/terms/
name: Software License
type: page
---

# Software License

~~~text
VECTORBT® PRO LICENSE

Last Updated: January 5, 2025

This Software License ("License") is a legal agreement between the Copyright
Holder and the Authorized User, governing the use of VECTORBT® PRO. By
downloading, installing, or using the Software, you agree to be bound by this
License.

1. DEFINITIONS

1.1 "Software"
Refers to VECTORBT® PRO, including all related documentation, updates, and
files provided under this License.

1.2 "Copyright Holder"
Refers to Oleg Polakow, along with any successors, assigns, or authorized legal
representatives.

1.3 "Authorized User"
Means:
  (a) Individual: A single person who obtains the Software for personal,
      private, and non-commercial use ("Private Use").
  (b) Organization: A legal entity that acquires the Software solely for
      internal, non-commercial purposes. Use is restricted to employees or
      direct contractors who are explicitly granted access to the Software as
      part of their duties for internal purposes.

1.4 "Private Use" / "Non-Commercial Use"
Refers to any use of the Software where no direct or indirect monetary
compensation, commercial advantage, or for-profit purpose is obtained. This
includes:
  i.  Individual Users: Personal, private use with no commercial intent.
  ii. Organizations (including for-profit entities): Internal, in-house usage
      for purposes such as evaluation, data analysis, prototyping, or research
      and development (R&D), provided it does not result in a commercial
      product, service, or outcome. Any use that contributes to or is
      integrated into a commercial offering requires a separate commercial
      license.

1.5 "Sell"
Means to use, reproduce, distribute, host, monetize, or otherwise commercialize
the Software—directly or indirectly—in exchange for payment or other
compensation. This includes offering services that derive significant value
from the Software’s core functionality.

2. LICENSE GRANT & SCOPE

2.1 License Grant
The Copyright Holder grants the Authorized User a limited, revocable,
non-exclusive, and non-transferable license to download, install, use, copy,
and modify the Software exclusively for Private Use / Non-Commercial Use, as
defined in Section 1.4.

2.2 Scope of Authorized Use
  (a) Individual Users: May use the Software on their personal devices solely
      for private, non-commercial purposes.
  (b) Organizations: May use the Software internally for non-commercial
      purposes. The Organization must ensure that only authorized employees or
      contractors access or interact with the Software. The Organization is
      responsible for preventing:
      i.  Unauthorized internal sharing with unapproved employees or
          contractors.
      ii. Any external access by third parties unless explicitly permitted in
          writing by the Copyright Holder.

2.3 Modifications & Internal Sharing
  i.   Authorized Users may create modifications or derivative works for
        Private Use / Non-Commercial Use only.
  ii.  Modifications or derivative works may be shared among Authorized Users,
        provided they remain inaccessible to any party not covered by this
        License.
  iii. Modifications or improvements may be shared directly with the Copyright
        Holder without violating this License.

2.4 Updates & Versions
  i.  All updates, patches, or new versions of the Software provided by the
      Copyright Holder are subject to this License unless otherwise specified.
  ii. The Copyright Holder reserves the right to modify the terms of this
      License for future versions. Continued use of any updated version
      indicates acceptance of the revised terms.

Note: Any use outside the scope defined above—including but not limited to
commercial use—requires a separate written agreement with the Copyright Holder.

3. RESTRICTIONS & LIMITATIONS

3.1 No Unauthorized Distribution or Sharing
The Software must not be published, shared, uploaded to publicly accessible
platforms, distributed, transmitted, or made available to any third party
without prior written consent from the Copyright Holder.

Exception: Internal hosting within an Organization’s private, secure network is
permitted, provided it remains inaccessible to external parties or unauthorized
personnel.

3.2 No Sublicensing or Commercial Use
The Authorized User may not sublicense, Sell, or otherwise use the Software for
commercial purposes.

Clarification: "Sell" includes any form of monetization derived from the
Software’s core functionality, such as hosting, consulting, or support services
offered for a fee.

3.3 Retention of Copyright Notices
All copies, modifications, or derivative works of the Software must retain the
original copyright notice and this License. The removal or alteration of any
proprietary notices is prohibited.

3.4 No Reverse Engineering
The Authorized User may not decompile, decrypt, reverse engineer, disassemble,
or otherwise attempt to derive the source code of the Software, except where
permitted by applicable law.

3.5 Third-Party Components
If the Software includes third-party components covered by separate licenses,
those components are subject to their respective terms. This License does not
override any third-party agreements.

4. ADDITIONAL RIGHTS REQUIRE WRITTEN CONSENT

Any rights to distribute, sublicense, Sell, or otherwise commercially exploit
the Software—whether modified or unmodified—must be explicitly granted through
a separate written agreement with the Copyright Holder.

No additional rights are implied or granted beyond those expressly stated in
this License.

5. TERMINATION & RESERVATION OF RIGHTS

5.1 Termination
This License automatically terminates if the Authorized User breaches any of
its terms. Upon termination, the Authorized User must immediately cease all use
of the Software and permanently delete all copies, including backups.

5.2 Reservation of Rights
All rights not expressly granted under this License are reserved by the
Copyright Holder.

6. DISCLAIMER & INDEMNIFICATION

6.1 Disclaimer of Warranty
THE SOFTWARE IS PROVIDED "AS IS," WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE, OR NON-INFRINGEMENT. THE COPYRIGHT HOLDER SHALL NOT
BE LIABLE FOR ANY CLAIMS, DAMAGES, OR LIABILITIES ARISING FROM THE USE OR
INABILITY TO USE THE SOFTWARE, EXCEPT AS REQUIRED BY APPLICABLE LAW.

6.2 Indemnification
The Authorized User agrees to indemnify, defend, and hold harmless the
Copyright Holder from any claims, damages, liabilities, costs, or expenses
(including reasonable attorneys’ fees) arising from the Authorized User’s use
or misuse of the Software.

7. SEVERABILITY

If any provision of this License is found by a court of competent jurisdiction
to be invalid, illegal, or unenforceable, that provision shall be deemed
modified to the minimum extent necessary to render it valid and enforceable.
If modification is not possible, the provision shall be severed, and the
remaining provisions shall continue in full force and effect.

8. TRADEMARK USAGE

8.1 Ownership
"VECTORBT®" is a registered trademark owned by the Copyright Holder.

8.2 Limited Reference Rights
This License permits limited use of the VECTORBT® trademark to refer to the
Software in documentation or promotional materials, provided it does not
suggest endorsement by the Copyright Holder.

8.3 Prohibited Uses
You may not use, adopt, or register any trademark or domain name that is
identical or confusingly similar to VECTORBT®.

8.4 No Endorsement
Use of the VECTORBT® trademark may not imply endorsement or affiliation without
explicit written authorization.

[End of License]
~~~

---
link: https://vectorbt.pro/pvt_41531c19/terms/remarks/
parent: https://vectorbt.pro/pvt_41531c19/terms/
name: Remarks
type: page
---

# Remarks

VBT is hosted in a private repository on GitHub. Access to this repository is licensed and provided
by invitation only on a paid basis. Users pay for ongoing access to the repository, which includes
updates, support, and maintenance. If your license expires, your software or system will continue to
function properly with the most recent version you have at that time. However, if you discontinue your
paid license, you will no longer receive any updates or critical bug fixes released after that point.

## Important note

__It is illegal to vend, publish, distribute, or commercialize the vectorbtpro source code without
separate permission. Violating the licensing terms will result in a ban and potential legal action.__

Any licensed user, developer, team, or company that has obtained paid access to the vectorbtpro
repository from us may use vectorbtpro as a dependency, subject to the terms and limitations of the
paid subscription plans. Licensees may use, copy, and modify vectorbtpro as long as they do not vend,
publish, distribute, or commercialize the source code of vectorbtpro. You may specify vectorbtpro as
a dependency for your software, provided you do not include a copy of the vectorbtpro source code in
your software. If you are a software developer, you should specify vectorbtpro as a requirement. The
end user of your software is responsible for obtaining their own individual license. The best practice
is to make this clear in your documentation or on your website.